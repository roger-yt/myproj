{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    # List to store filenames\n    filenames = []\n\n    try:\n        # Connect to FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        print(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n        return filenames\n\n    try:\n        # Login to FTP server\n        ftp.login(ftp_user, ftp_password)\n    except Exception as e:\n        print(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n        return filenames\n\n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        print(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        return filenames\n\n    # Get a list of files in the directory\n    try:\n        ftp.retrlines('LIST', filenames.append)\n    except Exception as e:\n        print(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        return filenames\n\n    # Download files\n    for filename in filenames:\n        try:\n            with open(filename, 'wb') as file:\n                ftp.retrbinary('RETR ' + filename, file.write)\n        except Exception as e:\n            print(f\"Failed to download file {filename} from server {ftp_server}: {str(e)}\")\n\n    # Close FTP connection\n    ftp.quit()\n\n    return filenames", "raw_solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    # List to store filenames\n    filenames = []\n\n    try:\n        # Connect to FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        print(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n        return filenames\n\n    try:\n        # Login to FTP server\n        ftp.login(ftp_user, ftp_password)\n    except Exception as e:\n        print(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n        return filenames\n\n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        print(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        return filenames\n\n    # Get a list of files in the directory\n    try:\n        ftp.retrlines('LIST', filenames.append)\n    except Exception as e:\n        print(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        return filenames\n\n    # Download files\n    for filename in filenames:\n        try:\n            with open(filename, 'wb') as file:\n                ftp.retrbinary('RETR ' + filename, file.write)\n        except Exception as e:\n            print(f\"Failed to download file {filename} from server {ftp_server}: {str(e)}\")\n\n    # Close FTP connection\n    ftp.quit()\n\n    return filenames"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_file_paths = []\n\n    with open(commands_file_path, 'r') as commands_file:\n        reader = csv.reader(commands_file)\n        for i, row in enumerate(reader):\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{i}_output.txt')\n            output_file_paths.append(output_file_path)\n\n            try:\n                result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=60)\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n            except subprocess.CalledProcessError as e:\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(f\"Command execution failed with exit code {e.returncode}.\\n\")\n                    output_file.write(f\"Error message: {str(e)}\")\n\n    return output_file_paths", "raw_solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_file_paths = []\n\n    with open(commands_file_path, 'r') as commands_file:\n        reader = csv.reader(commands_file)\n        for i, row in enumerate(reader):\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{i}_output.txt')\n            output_file_paths.append(output_file_path)\n\n            try:\n                result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=60)\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n            except subprocess.CalledProcessError as e:\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(f\"Command execution failed with exit code {e.returncode}.\\n\")\n                    output_file.write(f\"Error message: {str(e)}\")\n\n    return output_file_paths"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # If process is running, terminate it\n            subprocess.Popen(['taskkill', '/F', '/IM', process_name])\n            return f\"Process found. Restarting {process_name}.\"\n    # If process is not running, start it\n    subprocess.Popen(process_name)\n    return f\"Process not found. Starting {process_name}.\"", "raw_solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # If process is running, terminate it\n            subprocess.Popen(['taskkill', '/F', '/IM', process_name])\n            return f\"Process found. Restarting {process_name}.\"\n    # If process is not running, start it\n    subprocess.Popen(process_name)\n    return f\"Process not found. Starting {process_name}.\"\n\n# Test the function"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(\"The specified directory does not exist.\")\n\n    files = glob.glob(os.path.join(directory, '*'))\n    files = [f for f in files if os.path.isfile(f)]  # filter out directories\n\n    if not files:\n        return None\n\n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n\n    return zip_path", "raw_solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(\"The specified directory does not exist.\")\n\n    files = glob.glob(os.path.join(directory, '*'))\n    files = [f for f in files if os.path.isfile(f)]  # filter out directories\n\n    if not files:\n        return None\n\n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n\n    return zip_path"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text\n    url_pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = url_pattern.sub('', text)\n\n    # Split the text into words\n    words = text.split()\n\n    # If there are no words available, raise an exception\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n\n    # Join the words back into a string\n    text = ' '.join(words)\n\n    # Generate a word cloud\n    wordcloud = WordCloud().generate(text)\n\n    # Plot the word cloud\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n\n    return wordcloud", "raw_solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs from the text\n    url_pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = url_pattern.sub('', text)\n\n    # Split the text into words\n    words = text.split()\n\n    # If there are no words available, raise an exception\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n\n    # Join the words back into a string\n    text = ' '.join(words)\n\n    # Generate a word cloud\n    wordcloud = WordCloud().generate(text)\n\n    # Plot the word cloud\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n\n    return wordcloud"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_column):\n    # Split the data into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train the random forest classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X, y)\n\n    # Get the feature importances\n    importances = clf.feature_importances_\n\n    # Create a dataframe for the feature importances\n    df_importances = pd.DataFrame({\n        'Features': X.columns,\n        'Importance': importances\n    })\n\n    # Sort the dataframe by importance\n    df_importances = df_importances.sort_values('Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Features', data=df_importances)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.tight_layout()\n\n    return clf, plt.gca()", "raw_solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, target_column):\n    # Split the data into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train the random forest classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X, y)\n\n    # Get the feature importances\n    importances = clf.feature_importances_\n\n    # Create a dataframe for the feature importances\n    df_importances = pd.DataFrame({\n        'Features': X.columns,\n        'Importance': importances\n    })\n\n    # Sort the dataframe by importance\n    df_importances = df_importances.sort_values('Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Features', data=df_importances)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.tight_layout()\n\n    return clf, plt.gca()"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash", "raw_solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass User(UserMixin):\n    def __init__(self, username, password):\n        self.id = username\n        self.password_hash = generate_password_hash(password)\n\n    def check_password(self, password):\n        return check_password_hash(self.password_hash, password)\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return User('admin', 'admin')\n\napp = Flask(__name__, template_folder='templates')\napp.config['SECRET_KEY'] = 'secret'\nlogin_manager.init_app(app)\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    form = LoginForm()\n    if form.validate_on_submit():\n        user = User(form.username.data, form.password.data)\n        if user.id == 'admin' and user.check_password(form.password.data):\n            login_user(user)\n            return redirect(url_for('protected'))\n    return render_template('login.html', form=form)\n\n@app.route('/logout')\n@login_required\ndef logout():\n    logout_user()\n    return redirect(url_for('login'))\n\n@app.route('/protected')\n@login_required\ndef protected():\n    return 'Logged in as: ' + current_user.id\n"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    # Standardize the column\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[column].values.reshape(-1,1))\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data[column]))\n    \n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n    \n    # Remove outliers\n    data_no_outliers = data.drop(data.index[outliers])\n    \n    # Visualize data before and after outlier removal\n    plt.figure(figsize=(12,6))\n    \n    plt.subplot(1,2,1)\n    plt.scatter(range(len(data[column])), data[column])\n    plt.title('Data with Outliers')\n    \n    plt.subplot(1,2,2)\n    plt.scatter(range(len(data_no_outliers[column])), data_no_outliers[column])\n    plt.title('Data without Outliers')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return (data, data_no_outliers, outliers)", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    # Standardize the column\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[column].values.reshape(-1,1))\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data[column]))\n    \n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n    \n    # Remove outliers\n    data_no_outliers = data.drop(data.index[outliers])\n    \n    # Visualize data before and after outlier removal\n    plt.figure(figsize=(12,6))\n    \n    plt.subplot(1,2,1)\n    plt.scatter(range(len(data[column])), data[column])\n    plt.title('Data with Outliers')\n    \n    plt.subplot(1,2,2)\n    plt.scatter(range(len(data_no_outliers[column])), data_no_outliers[column])\n    plt.title('Data without Outliers')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return (data, data_no_outliers, outliers)"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\ndef task_func(data, n_clusters=3):\n    # Check if data is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n\n    # Check if n_clusters is an integer greater than 1\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' should be an integer greater than 1\")\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n\n    # Generate scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels)\n\n    # Plot centroids\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=169, linewidths=3, color='black', zorder=10)\n\n    return labels, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef task_func(data, n_clusters=3):\n    # Check if data is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n\n    # Check if n_clusters is an integer greater than 1\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' should be an integer greater than 1\")\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n\n    # Generate scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels)\n\n    # Plot centroids\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=169, linewidths=3, color='black', zorder=10)\n\n    return labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    transformed_df = pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('2D PCA Scatter Plot')\n\n    return transformed_df, ax.figure", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    transformed_df = pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('2D PCA Scatter Plot')\n\n    return transformed_df, ax.figure"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target\n\n    # Set Arial font\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create pair plot\n    g = sns.pairplot(iris_df, hue='species', diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'k'})\n    g.fig.suptitle(\"Iris Dataset Pair Plot\", y=1.08)\n\n    return g.fig", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Load iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target\n\n    # Set Arial font\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create pair plot\n    g = sns.pairplot(iris_df, hue='species', diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'k'})\n    g.fig.suptitle(\"Iris Dataset Pair Plot\", y=1.08)\n\n    return g.fig\n\n# Test the function\nfig = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate a DataFrame with dates and random values\n        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]\n        values = [random.random() for _ in range(30)]\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        df['Date'] = pd.to_datetime(df['Date'])\n\n        # Create the plot\n        plt.figure(figsize=(10, 6))\n        plt.plot(df['Date'], df['Value'])\n        plt.xlabel('Date')\n        plt.ylabel('Value')\n        plt.title('Random Time Series Data')\n        plt.xticks(rotation=45)\n        plt.grid(True)\n        plt.rcParams[\"font.family\"] = \"Arial\"\n\n        return plt.gca()\n\n    except Exception as e:\n        raise ValueError('Issue generating the data or plot: {}'.format(e))", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate a DataFrame with dates and random values\n        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]\n        values = [random.random() for _ in range(30)]\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        df['Date'] = pd.to_datetime(df['Date'])\n\n        # Create the plot\n        plt.figure(figsize=(10, 6))\n        plt.plot(df['Date'], df['Value'])\n        plt.xlabel('Date')\n        plt.ylabel('Value')\n        plt.title('Random Time Series Data')\n        plt.xticks(rotation=45)\n        plt.grid(True)\n        plt.rcParams[\"font.family\"] = \"Arial\"\n\n        return plt.gca()\n\n    except Exception as e:\n        raise ValueError('Issue generating the data or plot: {}'.format(e))\n\n# Test the function\nax = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_to=None):\n    try:\n        np.random.seed(seed)\n        data = pd.read_csv(data_url, delimiter=\"\\s+\", skiprows=22, header=None)\n        data.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n        correlation = data.corr()\n        plt.figure(figsize=(12, 10))\n        sns.heatmap(correlation, annot=True, cmap='coolwarm')\n        if save_to:\n            plt.savefig(save_to)\n        return plt.gca()\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_to=None):\n    try:\n        np.random.seed(seed)\n        data = pd.read_csv(data_url, delimiter=\"\\s+\", skiprows=22, header=None)\n        data.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n        correlation = data.corr()\n        plt.figure(figsize=(12, 10))\n        sns.heatmap(correlation, annot=True, cmap='coolwarm')\n        if save_to:\n            plt.savefig(save_to)\n        return plt.gca()\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n\n# Test the function\ntask_func(save_to='boston_housing_correlation.png')"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n\n    # Check if df has 'value' column\n    if 'value' not in df.columns:\n        raise ValueError(\"'df' should have a 'value' column\")\n\n    # Check if 'value' column has numeric data\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column should have numeric data\")\n\n    # Check if freq is a valid frequency string\n    if freq not in ['D', 'M', 'A', 'Q', 'Y']:\n        raise ValueError(\"'freq' should be a valid frequency string\")\n\n    # Check if decomposition_model is valid\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' should be either 'additive' or 'multiplicative'\")\n\n    # Decompose the time series\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, freq=freq)\n\n    # Plot the decomposition\n    fig, ax = plt.subplots(4, 1, figsize=(10,10))\n    decomposition.trend.plot(ax=ax[0])\n    decomposition.seasonal.plot(ax=ax[1])\n    decomposition.resid.plot(ax=ax[2])\n    ax[3].set_visible(False)\n    plt.tight_layout()\n\n    return (decomposition, ax)", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n\n    # Check if df has 'value' column\n    if 'value' not in df.columns:\n        raise ValueError(\"'df' should have a 'value' column\")\n\n    # Check if 'value' column has numeric data\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column should have numeric data\")\n\n    # Check if freq is a valid frequency string\n    if freq not in ['D', 'M', 'A', 'Q', 'Y']:\n        raise ValueError(\"'freq' should be a valid frequency string\")\n\n    # Check if decomposition_model is valid\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' should be either 'additive' or 'multiplicative'\")\n\n    # Decompose the time series\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, freq=freq)\n\n    # Plot the decomposition\n    fig, ax = plt.subplots(4, 1, figsize=(10,10))\n    decomposition.trend.plot(ax=ax[0])\n    decomposition.seasonal.plot(ax=ax[1])\n    decomposition.resid.plot(ax=ax[2])\n    ax[3].set_visible(False)\n    plt.tight_layout()\n\n    return (decomposition, ax)"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Check if start_date and end_date are datetime instances\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"'start_date' and 'end_date' should be datetime instances\")\n\n    # Check if start_date is not later than end_date\n    if start_date > end_date:\n        raise ValueError(\"'start_date' should not be later than 'end_date'\")\n\n    # Set the seed for the random number generator\n    random_seed(seed)\n\n    # Calculate the number of days between start_date and end_date\n    delta = end_date - start_date\n    days = [start_date + timedelta(days=i) for i in range(delta.days + 1)]\n\n    # Generate a pandas Series of random dates\n    random_dates = pd.Series(days).sample(frac=1, random_state=seed)\n\n    return random_dates", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Check if start_date and end_date are datetime instances\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"'start_date' and 'end_date' should be datetime instances\")\n\n    # Check if start_date is not later than end_date\n    if start_date > end_date:\n        raise ValueError(\"'start_date' should not be later than 'end_date'\")\n\n    # Set the seed for the random number generator\n    random_seed(seed)\n\n    # Calculate the number of days between start_date and end_date\n    delta = end_date - start_date\n    days = [start_date + timedelta(days=i) for i in range(delta.days + 1)]\n\n    # Generate a pandas Series of random dates\n    random_dates = pd.Series(days).sample(frac=1, random_state=seed)\n\n    return random_dates\n\n# Test the function"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n\n    # Add '12' to the list\n    my_list.append('12')\n\n    # Check if files exist in the directory\n    files = glob.glob(os.path.join(file_dir, '*'+file_ext))\n    if not files:\n        raise FileNotFoundError(\"No files found in the specified directory\")\n\n    # Concatenate CSV files into a single DataFrame\n    df = pd.DataFrame()\n    for file in files[:sum(my_list)]:\n        df = pd.concat([df, pd.read_csv(file)])\n\n    return df", "raw_solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n\n    # Add '12' to the list\n    my_list.append('12')\n\n    # Check if files exist in the directory\n    files = glob.glob(os.path.join(file_dir, '*'+file_ext))\n    if not files:\n        raise FileNotFoundError(\"No files found in the specified directory\")\n\n    # Concatenate CSV files into a single DataFrame\n    df = pd.DataFrame()\n    for file in files[:sum(my_list)]:\n        df = pd.concat([df, pd.read_csv(file)])\n\n    return df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"All elements in 'my_list' should be numeric (int or float)\")\n\n    my_list.append(12)\n    total_sum = sum(my_list)\n\n    if total_sum > size:\n        raise ValueError(\"Sum of elements in 'my_list' should not exceed 'size'\")\n\n    random_seed(seed)\n    random_list = [randint(1, 100) for _ in range(total_sum)]\n\n    start_time = time.time()\n\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 101), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    return (time_taken, ax)", "raw_solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"All elements in 'my_list' should be numeric (int or float)\")\n\n    my_list.append(12)\n    total_sum = sum(my_list)\n\n    if total_sum > size:\n        raise ValueError(\"Sum of elements in 'my_list' should not exceed 'size'\")\n\n    random_seed(seed)\n    random_list = [randint(1, 100) for _ in range(total_sum)]\n\n    start_time = time.time()\n\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 101), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    return (time_taken, ax)"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        print (\"HTTP Error:\",errh)\n        return None\n    except requests.exceptions.ConnectionError as errc:\n        print (\"Error Connecting:\",errc)\n        return None\n    except requests.exceptions.Timeout as errt:\n        print (\"Timeout Error:\",errt)\n        return None\n    except requests.exceptions.RequestException as err:\n        print (\"Something went wrong\",err)\n        return None\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    table = soup.find_all('table')\n    if not table:\n        raise ValueError('No table data found on the page')\n\n    table = table[0]\n    df = pd.DataFrame(data=[[td.get_text(strip=True) for td in tr.find_all('td')] for tr in table.find_all('tr')])\n\n    header = [th.get_text(strip=True) for th in table.find_all('th')]\n    if header:\n        df.columns = header\n\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        print (\"HTTP Error:\",errh)\n        return None\n    except requests.exceptions.ConnectionError as errc:\n        print (\"Error Connecting:\",errc)\n        return None\n    except requests.exceptions.Timeout as errt:\n        print (\"Timeout Error:\",errt)\n        return None\n    except requests.exceptions.RequestException as err:\n        print (\"Something went wrong\",err)\n        return None\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    table = soup.find_all('table')\n    if not table:\n        raise ValueError('No table data found on the page')\n\n    table = table[0]\n    df = pd.DataFrame(data=[[td.get_text(strip=True) for td in tr.find_all('td')] for tr in table.find_all('tr')])\n\n    header = [th.get_text(strip=True) for th in table.find_all('th')]\n    if header:\n        df.columns = header\n\n    return df"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input should be a non-empty DataFrame\")\n\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if not numeric_cols.any():\n        raise ValueError(\"There are no numeric columns in the DataFrame\")\n\n    axes_list = []\n    for col in numeric_cols:\n        ax = df[col].plot(kind='hist', rwidth=0.8, color='green')\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n\n    plt.tight_layout()\n    return axes_list", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input should be a non-empty DataFrame\")\n\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if not numeric_cols.any():\n        raise ValueError(\"There are no numeric columns in the DataFrame\")\n\n    axes_list = []\n    for col in numeric_cols:\n        ax = df[col].plot(kind='hist', rwidth=0.8, color='green')\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n\n    plt.tight_layout()\n    return axes_list"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef check_port(ip, port):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.settimeout(5)\n    result = sock.connect_ex((str(ip), port))\n    sock.close()\n    return result == 0\ndef task_func(ip_range, port):\n    ip_dict = {}\n    for ip in IPv4Network(ip_range):\n        Thread(target=lambda ip=ip: ip_dict.update({ip: check_port(ip, port)})).start()\n    return ip_dict\nip_range = \"192.168.1.0/29\"\nport = 80", "raw_solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef check_port(ip, port):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.settimeout(5)\n    result = sock.connect_ex((str(ip), port))\n    sock.close()\n    return result == 0\n\ndef task_func(ip_range, port):\n    ip_dict = {}\n    for ip in IPv4Network(ip_range):\n        Thread(target=lambda ip=ip: ip_dict.update({ip: check_port(ip, port)})).start()\n    return ip_dict\n\n# Test the function\nip_range = \"192.168.1.0/29\"\nport = 80"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    # Define the regular expression pattern\n    pattern = r\"(?P<type>\\w+): \\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (?P<message>.*)\"\n\n    # Initialize lists to store the data\n    types = []\n    timestamps = []\n    messages = []\n\n    # Open the log file and read it line by line\n    with open(log_file, 'r') as f:\n        lines = f.readlines()\n\n    # Iterate over the lines\n    for line in lines:\n        # Use the regular expression to match the line\n        match = re.match(pattern, line)\n\n        # If a match is found, append the data to the corresponding lists\n        if match:\n            types.append(match.group('type'))\n            timestamps.append(match.group('timestamp'))\n            messages.append(match.group('message'))\n        else:\n            raise ValueError(\"Invalid log entry: \" + line)\n\n    # If no valid log entries are found, raise an exception\n    if not types:\n        raise ValueError(\"No valid log entries found\")\n\n    # Create a pandas DataFrame from the data\n    df = pd.DataFrame({\n        'Type': types,\n        'Timestamp': timestamps,\n        'Message': messages\n    })\n\n    # Convert the Timestamp column to datetime format\n    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n\n    # Save the DataFrame to a CSV file\n    csv_file = log_file.split('.')[0] + '.csv'\n    df.to_csv(csv_file, index=False)\n\n    return csv_file", "raw_solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    # Define the regular expression pattern\n    pattern = r\"(?P<type>\\w+): \\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (?P<message>.*)\"\n\n    # Initialize lists to store the data\n    types = []\n    timestamps = []\n    messages = []\n\n    # Open the log file and read it line by line\n    with open(log_file, 'r') as f:\n        lines = f.readlines()\n\n    # Iterate over the lines\n    for line in lines:\n        # Use the regular expression to match the line\n        match = re.match(pattern, line)\n\n        # If a match is found, append the data to the corresponding lists\n        if match:\n            types.append(match.group('type'))\n            timestamps.append(match.group('timestamp'))\n            messages.append(match.group('message'))\n        else:\n            raise ValueError(\"Invalid log entry: \" + line)\n\n    # If no valid log entries are found, raise an exception\n    if not types:\n        raise ValueError(\"No valid log entries found\")\n\n    # Create a pandas DataFrame from the data\n    df = pd.DataFrame({\n        'Type': types,\n        'Timestamp': timestamps,\n        'Message': messages\n    })\n\n    # Convert the Timestamp column to datetime format\n    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n\n    # Save the DataFrame to a CSV file\n    csv_file = log_file.split('.')[0] + '.csv'\n    df.to_csv(csv_file, index=False)\n\n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    # Filter out non-alphabetic characters and split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # If there are no words or all words are filtered out, return an empty histogram\n    if not words:\n        return plt.hist([], rwidth=rwidth)\n\n    # Calculate the length of each word and create a histogram\n    word_lengths = [len(word) for word in words]\n    return plt.hist(word_lengths, bins='auto', rwidth=rwidth)\ntext = \"The quick brown fox jumps over the lazy dog\"", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    # Filter out non-alphabetic characters and split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # If there are no words or all words are filtered out, return an empty histogram\n    if not words:\n        return plt.hist([], rwidth=rwidth)\n\n    # Calculate the length of each word and create a histogram\n    word_lengths = [len(word) for word in words]\n    return plt.hist(word_lengths, bins='auto', rwidth=rwidth)\n\n# Test the function\ntext = \"The quick brown fox jumps over the lazy dog\"\nax = task_func(text)\nplt.show()"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\ndef task_func(df):\n    # Check if DataFrame is empty or does not contain necessary columns\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    # Extract articles whose titles contain specific case-insensitive keywords\n    df = df[df['Title'].str.contains(r'(?i)like|(?i)what')]\n\n    # If no articles found, return an empty dictionary\n    if df.empty:\n        return {}\n\n    # Analyze the frequency of each word in the content of these articles\n    words = nltk.word_tokenize(' '.join(df['Content']))\n    words = [word.lower() for word in words if word.isalpha()]  # convert to lower case and exclude punctuation\n\n    # Count the frequency of each word\n    word_freq = dict(Counter(words))\n\n    return word_freq", "raw_solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\ndef task_func(df):\n    # Check if DataFrame is empty or does not contain necessary columns\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    # Extract articles whose titles contain specific case-insensitive keywords\n    df = df[df['Title'].str.contains(r'(?i)like|(?i)what')]\n\n    # If no articles found, return an empty dictionary\n    if df.empty:\n        return {}\n\n    # Analyze the frequency of each word in the content of these articles\n    words = nltk.word_tokenize(' '.join(df['Content']))\n    words = [word.lower() for word in words if word.isalpha()]  # convert to lower case and exclude punctuation\n\n    # Count the frequency of each word\n    word_freq = dict(Counter(words))\n\n    return word_freq"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    # Remove stopwords, numbers, and punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join(\n        [word for word in x.split() if word not in STOPWORDS and not word.isdigit() and word.isalpha()]))\n\n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the data\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Create a DataFrame from the matrix\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    # Remove stopwords, numbers, and punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join(\n        [word for word in x.split() if word not in STOPWORDS and not word.isdigit() and word.isalpha()]))\n\n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the data\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Create a DataFrame from the matrix\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not isinstance(dic, dict):\n        raise ValueError(\"Input should be a dictionary\")\n    if not isinstance(cities, list):\n        raise ValueError(\"Cities should be a list\")\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"'Lon' and 'Lat' keys are required\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"'Lon' and 'Lat' values should be tuples\")\n\n    # Generate random coordinates\n    np.random.seed(0)\n    lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1], len(cities))\n    lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1], len(cities))\n\n    # Create Points\n    points = [Point(xy) for xy in zip(lon, lat)]\n\n    # Create GeoDataFrame\n    df = gpd.GeoDataFrame(cities, geometry=points, crs=4326)\n    df.columns = ['City', 'Coordinates']\n\n    return df", "raw_solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not isinstance(dic, dict):\n        raise ValueError(\"Input should be a dictionary\")\n    if not isinstance(cities, list):\n        raise ValueError(\"Cities should be a list\")\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"'Lon' and 'Lat' keys are required\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"'Lon' and 'Lat' values should be tuples\")\n\n    # Generate random coordinates\n    np.random.seed(0)\n    lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1], len(cities))\n    lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1], len(cities))\n\n    # Create Points\n    points = [Point(xy) for xy in zip(lon, lat)]\n\n    # Create GeoDataFrame\n    df = gpd.GeoDataFrame(cities, geometry=points, crs=4326)\n    df.columns = ['City', 'Coordinates']\n\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Check if utc_datetime is a datetime object\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime should be a datetime object\")\n\n    # Check if all parameters are in the expected format\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"All cities should be strings\")\n    if not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"All weather conditions should be strings\")\n    if not all(isinstance(timezone, str) for timezone in timezones.values()):\n        raise ValueError(\"All timezones should be strings\")\n\n    # Set the seed for the random number generator\n    set_seed(seed)\n\n    # Generate the weather report for each city\n    data = []\n    for city in cities:\n        # Convert the utc_datetime to the local time of the city\n        local_time = utc_datetime.astimezone(pytz.timezone(timezones[city]))\n\n        # Generate a random weather condition\n        condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n\n        # Add the data to the list\n        data.append({\n            'City': city,\n            'Local Time': local_time.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': condition\n        })\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    return df", "raw_solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Check if utc_datetime is a datetime object\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime should be a datetime object\")\n\n    # Check if all parameters are in the expected format\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"All cities should be strings\")\n    if not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"All weather conditions should be strings\")\n    if not all(isinstance(timezone, str) for timezone in timezones.values()):\n        raise ValueError(\"All timezones should be strings\")\n\n    # Set the seed for the random number generator\n    set_seed(seed)\n\n    # Generate the weather report for each city\n    data = []\n    for city in cities:\n        # Convert the utc_datetime to the local time of the city\n        local_time = utc_datetime.astimezone(pytz.timezone(timezones[city]))\n\n        # Generate a random weather condition\n        condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n\n        # Add the data to the list\n        data.append({\n            'City': city,\n            'Local Time': local_time.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': condition\n        })\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    return df"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    np.random.seed(seed)\n    random_walk = np.random.choice([-1, 1], size=elements)\n    random_walk_cumulative = np.cumsum(random_walk)\n\n    stats = {\n        'count': elements,\n        'mean': np.mean(random_walk),\n        'std': np.std(random_walk),\n        'min': np.min(random_walk),\n        '5th percentile': np.percentile(random_walk, 5),\n        '25th percentile': np.percentile(random_walk, 25),\n        'median': np.median(random_walk),\n        '75th percentile': np.percentile(random_walk, 75),\n        '95th percentile': np.percentile(random_walk, 95),\n        'max': np.max(random_walk),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(random_walk_cumulative)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Cumulative Sum')\n\n    return stats, ax.figure", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    np.random.seed(seed)\n    random_walk = np.random.choice([-1, 1], size=elements)\n    random_walk_cumulative = np.cumsum(random_walk)\n\n    stats = {\n        'count': elements,\n        'mean': np.mean(random_walk),\n        'std': np.std(random_walk),\n        'min': np.min(random_walk),\n        '5th percentile': np.percentile(random_walk, 5),\n        '25th percentile': np.percentile(random_walk, 25),\n        'median': np.median(random_walk),\n        '75th percentile': np.percentile(random_walk, 75),\n        '95th percentile': np.percentile(random_walk, 95),\n        'max': np.max(random_walk),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(random_walk_cumulative)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Cumulative Sum')\n\n    return stats, ax.figure\n\n# Test the function\nstats, fig = task_func(1000)"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\nfrom io import BytesIO\ndef task_func(url, destination_directory, headers=None):\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n\n    # Raise an exception if the GET request is unsuccessful\n    response.raise_for_status()\n\n    # Create a BytesIO object from the content of the response\n    zip_file = BytesIO(response.content)\n\n    # Create a ZipFile object from the BytesIO object\n    with zipfile.ZipFile(zip_file) as zf:\n        # Extract all files to the destination directory\n        zf.extractall(destination_directory)\n\n    # Get a list of all files in the destination directory\n    extracted_files = os.listdir(destination_directory)\n\n    return extracted_files", "raw_solution": "import requests\nimport os\nimport zipfile\nfrom io import BytesIO\n\ndef task_func(url, destination_directory, headers=None):\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n\n    # Raise an exception if the GET request is unsuccessful\n    response.raise_for_status()\n\n    # Create a BytesIO object from the content of the response\n    zip_file = BytesIO(response.content)\n\n    # Create a ZipFile object from the BytesIO object\n    with zipfile.ZipFile(zip_file) as zf:\n        # Extract all files to the destination directory\n        zf.extractall(destination_directory)\n\n    # Get a list of all files in the destination directory\n    extracted_files = os.listdir(destination_directory)\n\n    return extracted_files"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low should be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size)\n\n    fig, ax = plt.subplots(1, 1)\n    ax.imshow(image)\n    plt.show()\n\n    return ax, image", "raw_solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low should be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size)\n\n    fig, ax = plt.subplots(1, 1)\n    ax.imshow(image)\n    plt.show()\n\n    return ax, image"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(\"The specified audio file does not exist.\")\n\n    # Read the audio file\n    data, sample_rate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    SPL = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10 ** (SPL / 20))\n\n    # Generate the spectrogram\n    spectrogram = np.abs(librosa.stft(normalized_matrix.flatten()))\n\n    # Create the figure for the spectrogram\n    fig, ax = plt.subplots()\n\n    # Display the spectrogram with a logarithmic scale for frequency and a linear scale for time\n    img = ax.imshow(librosa.amplitude_to_db(spectrogram, ref=np.max), origin='lower', aspect='auto', extent=[0, len(normalized_matrix)/sample_rate, 0, sample_rate/2])\n\n    return normalized_matrix, fig", "raw_solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(\"The specified audio file does not exist.\")\n\n    # Read the audio file\n    data, sample_rate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    SPL = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10 ** (SPL / 20))\n\n    # Generate the spectrogram\n    spectrogram = np.abs(librosa.stft(normalized_matrix.flatten()))\n\n    # Create the figure for the spectrogram\n    fig, ax = plt.subplots()\n\n    # Display the spectrogram with a logarithmic scale for frequency and a linear scale for time\n    img = ax.imshow(librosa.amplitude_to_db(spectrogram, ref=np.max), origin='lower', aspect='auto', extent=[0, len(normalized_matrix)/sample_rate, 0, sample_rate/2])\n\n    return normalized_matrix, fig"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values\n    numeric_values = [val for sublist in original for val in sublist if isinstance(val, (int, float))]\n    numeric_values = np.array(numeric_values)\n\n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'standard deviation': np.std(numeric_values),\n        'minimum': np.min(numeric_values),\n        'maximum': np.max(numeric_values)\n    }\n\n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n\n    # Generate PDF\n    x_values = np.linspace(np.min(numeric_values), np.max(numeric_values), 1000)\n    pdf_values = stats.norm.pdf(x_values, np.mean(numeric_values), np.std(numeric_values))\n\n    ax.plot(x_values, pdf_values, label='PDF')\n    ax.legend()\n\n    return numeric_values, stats_dict, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extract numeric values\n    numeric_values = [val for sublist in original for val in sublist if isinstance(val, (int, float))]\n    numeric_values = np.array(numeric_values)\n\n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'standard deviation': np.std(numeric_values),\n        'minimum': np.min(numeric_values),\n        'maximum': np.max(numeric_values)\n    }\n\n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n\n    # Generate PDF\n    x_values = np.linspace(np.min(numeric_values), np.max(numeric_values), 1000)\n    pdf_values = stats.norm.pdf(x_values, np.mean(numeric_values), np.std(numeric_values))\n\n    ax.plot(x_values, pdf_values, label='PDF')\n    ax.legend()\n\n    return numeric_values, stats_dict, ax"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array], norm='l2')[0]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the original and normalized arrays\n    ax.plot(original_array, label='Original Data')\n    ax.plot(normalized_array, label='Normalized Data')\n\n    # Set the title and labels\n    ax.set_title('Original vs. Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    # Add a legend\n    ax.legend()\n\n    # Return the arrays and the axes\n    return original_array, normalized_array, ax\noriginal = [1, 2, 3, 4, 5]", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array], norm='l2')[0]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the original and normalized arrays\n    ax.plot(original_array, label='Original Data')\n    ax.plot(normalized_array, label='Normalized Data')\n\n    # Set the title and labels\n    ax.set_title('Original vs. Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    # Add a legend\n    ax.legend()\n\n    # Return the arrays and the axes\n    return original_array, normalized_array, ax\n\n# Test the function\noriginal = [1, 2, 3, 4, 5]\noriginal_array, normalized_array, ax = task_func(original)"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # Generate a signal based on the values in \"data\"\n    signal = list(data.values())\n\n    # Run a Fast Fourier Transform (FFT) on the signal\n    fft_signal = fftpack.fft(signal)\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n\n    # Plot and return the FFT of the signal with a title of 'FFT of the signal'\n    plt.figure(figsize=(10, 5))\n    plt.plot(freqs, np.abs(fft_signal))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n\n    return (fft_signal, plt.gca())", "raw_solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # Generate a signal based on the values in \"data\"\n    signal = list(data.values())\n\n    # Run a Fast Fourier Transform (FFT) on the signal\n    fft_signal = fftpack.fft(signal)\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n\n    # Plot and return the FFT of the signal with a title of 'FFT of the signal'\n    plt.figure(figsize=(10, 5))\n    plt.plot(freqs, np.abs(fft_signal))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n\n    return (fft_signal, plt.gca())"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json", "raw_solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\nclass PostHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_length = int(self.headers.get('content-length', 0))\n        content_type = self.headers.get('content-type')\n\n        if content_type != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({'error': 'Content-Type header is not application/json'}).encode())\n            return\n\n        post_data = self.rfile.read(content_length)\n\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({'error': 'Invalid JSON'}).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({'error': 'No data key in request'}).encode())\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\n"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n\n            try:\n                email_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_response(400)\n                self.end_headers()\n                self.wfile.write(json.dumps({\"error\": \"Invalid JSON\"}).encode())\n                return\n\n            required_keys = ['subject', 'message', 'to']\n            for key in required_keys:\n                if key not in email_data:\n                    self.send_response(400)\n                    self.end_headers()\n                    self.wfile.write(json.dumps({\"error\": f\"Missing key: {key}\"}).encode())\n                    return\n\n            msg = MIMEText(email_data['message'])\n            msg['Subject'] = email_data['subject']\n            msg['From'] = smtp_username\n            msg['To'] = email_data['to']\n\n            try:\n                smtp = smtplib.SMTP(smtp_server, smtp_port)\n                smtp.login(smtp_username, smtp_password)\n                smtp.send_message(msg)\n                smtp.quit()\n            except smtplib.SMTPAuthenticationError:\n                self.send_response(535)\n                self.end_headers()\n                self.wfile.write(json.dumps({\"error\": \"Authentication Failed\"}).encode())\n                return\n\n            response = {\"status\": \"Email sent successfully\"}\n            self.wfile.write(json.dumps(response).encode())\n\n    return EmailHandler", "raw_solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n\n            try:\n                email_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_response(400)\n                self.end_headers()\n                self.wfile.write(json.dumps({\"error\": \"Invalid JSON\"}).encode())\n                return\n\n            required_keys = ['subject', 'message', 'to']\n            for key in required_keys:\n                if key not in email_data:\n                    self.send_response(400)\n                    self.end_headers()\n                    self.wfile.write(json.dumps({\"error\": f\"Missing key: {key}\"}).encode())\n                    return\n\n            msg = MIMEText(email_data['message'])\n            msg['Subject'] = email_data['subject']\n            msg['From'] = smtp_username\n            msg['To'] = email_data['to']\n\n            try:\n                smtp = smtplib.SMTP(smtp_server, smtp_port)\n                smtp.login(smtp_username, smtp_password)\n                smtp.send_message(msg)\n                smtp.quit()\n            except smtplib.SMTPAuthenticationError:\n                self.send_response(535)\n                self.end_headers()\n                self.wfile.write(json.dumps({\"error\": \"Authentication Failed\"}).encode())\n                return\n\n            response = {\"status\": \"Email sent successfully\"}\n            self.wfile.write(json.dumps(response).encode())\n\n    return EmailHandler"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(directory):\n    word_counts = Counter()\n    total_words = 0\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                words = file.read().split()\n                word_counts[filename] = len(words)\n                total_words += len(words)\n\n    with open('word_counts.json', 'w') as json_file:\n        json.dump(word_counts, json_file)\n\n    return total_words", "raw_solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(directory):\n    word_counts = Counter()\n    total_words = 0\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                words = file.read().split()\n                word_counts[filename] = len(words)\n                total_words += len(words)\n\n    with open('word_counts.json', 'w') as json_file:\n        json.dump(word_counts, json_file)\n\n    return total_words"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Check if DataFrame has the correct columns\n    if not all(col in df.columns for col in COLUMNS):\n        raise ValueError(\"DataFrame does not have the correct columns\")\n\n    # Check if 'Value' column contains valid lists\n    for value in df['Value']:\n        if not isinstance(value, list):\n            raise ValueError(\"'Value' column contains invalid data\")\n\n    # Split lists in the 'Value' column into separate columns\n    df = df.assign(**{f'Value_{i}': x.values for i, x in enumerate(df['Value'].apply(pd.Series), 1)})\n\n    # Calculate the Pearson correlation coefficient\n    corr = df.corr(method='pearson')\n\n    if plot:\n        # Visualize the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr, annot=True, cmap='coolwarm')\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n\n    return corr", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Check if DataFrame has the correct columns\n    if not all(col in df.columns for col in COLUMNS):\n        raise ValueError(\"DataFrame does not have the correct columns\")\n\n    # Check if 'Value' column contains valid lists\n    for value in df['Value']:\n        if not isinstance(value, list):\n            raise ValueError(\"'Value' column contains invalid data\")\n\n    # Split lists in the 'Value' column into separate columns\n    df = df.assign(**{f'Value_{i}': x.values for i, x in enumerate(df['Value'].apply(pd.Series), 1)})\n\n    # Calculate the Pearson correlation coefficient\n    corr = df.corr(method='pearson')\n\n    if plot:\n        # Visualize the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr, annot=True, cmap='coolwarm')\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n\n    return corr"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n    # Generate random grades for each student in each subject\n    data = {field: [random.randint(0, 100) for _ in range(100)] for field in FIELDS}\n\n    # Add additional fields if any\n    for field in additional_fields:\n        data[field] = [random.randint(0, 100) for _ in range(100)]\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=STUDENTS)\n\n    # Calculate average grade for each student\n    df['Average'] = df.mean(axis=1)\n\n    # Calculate average grade for each subject\n    df['Average Grade'] = df.mean(axis=0)\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    # Generate random grades for each student in each subject\n    data = {field: [random.randint(0, 100) for _ in range(100)] for field in FIELDS}\n\n    # Add additional fields if any\n    for field in additional_fields:\n        data[field] = [random.randint(0, 100) for _ in range(100)]\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=STUDENTS)\n\n    # Calculate average grade for each student\n    df['Average'] = df.mean(axis=1)\n\n    # Calculate average grade for each subject\n    df['Average Grade'] = df.mean(axis=0)\n\n    return df\n\n# Test the function\ndf = task_func(['Art', 'Geography'])"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nimport string\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef generate_random_data():\n    return [\n        ''.join(random.choices(string.ascii_uppercase, k=5)),  # Random name\n        random.randint(18, 80),  # Random age\n        random.randint(140, 200),  # Random height\n        random.randint(40, 150),  # Random weight\n    ]\ndef task_func(filename):\n    # Create CSV file\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n\n        # Write column names\n        writer.writerow(COLUMNS)\n\n        # Write simulated data for 100 people\n        people_data = [generate_random_data() for _ in range(PEOPLE_COUNT)]\n        writer.writerows(people_data)\n\n    # Calculate and append the average age, height, and weight\n    with open(filename, 'a', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n\n        # Calculate averages\n        avg_age = mean(row[1] for row in people_data)\n        avg_height = mean(row[2] for row in people_data)\n        avg_weight = mean(row[3] for row in people_data)\n\n        # Append averages\n        writer.writerow(['Average', avg_age, avg_height, avg_weight])\n\n    return filename", "raw_solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nimport string\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef generate_random_data():\n    return [\n        ''.join(random.choices(string.ascii_uppercase, k=5)),  # Random name\n        random.randint(18, 80),  # Random age\n        random.randint(140, 200),  # Random height\n        random.randint(40, 150),  # Random weight\n    ]\n\ndef task_func(filename):\n    # Create CSV file\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n\n        # Write column names\n        writer.writerow(COLUMNS)\n\n        # Write simulated data for 100 people\n        people_data = [generate_random_data() for _ in range(PEOPLE_COUNT)]\n        writer.writerows(people_data)\n\n    # Calculate and append the average age, height, and weight\n    with open(filename, 'a', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n\n        # Calculate averages\n        avg_age = mean(row[1] for row in people_data)\n        avg_height = mean(row[2] for row in people_data)\n        avg_weight = mean(row[3] for row in people_data)\n\n        # Append averages\n        writer.writerow(['Average', avg_age, avg_height, avg_weight])\n\n    return filename"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    if not os.path.isdir(directory):\n        return \"Invalid directory path\"\n\n    result_dict = {}\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'r') as file:\n                content = file.read()\n                match = re.search(r'\\[([^\\]]*)\\]', content)\n                if match:\n                    subdir_name = match.group(1)\n                    if not os.path.exists(os.path.join(directory, subdir_name)):\n                        os.mkdir(os.path.join(directory, subdir_name))\n                    shutil.move(file_path, os.path.join(directory, subdir_name))\n                    if subdir_name in result_dict:\n                        result_dict[subdir_name].append(filename)\n                    else:\n                        result_dict[subdir_name] = [filename]\n\n    return (directory, result_dict)", "raw_solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    if not os.path.isdir(directory):\n        return \"Invalid directory path\"\n\n    result_dict = {}\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'r') as file:\n                content = file.read()\n                match = re.search(r'\\[([^\\]]*)\\]', content)\n                if match:\n                    subdir_name = match.group(1)\n                    if not os.path.exists(os.path.join(directory, subdir_name)):\n                        os.mkdir(os.path.join(directory, subdir_name))\n                    shutil.move(file_path, os.path.join(directory, subdir_name))\n                    if subdir_name in result_dict:\n                        result_dict[subdir_name].append(filename)\n                    else:\n                        result_dict[subdir_name] = [filename]\n\n    return (directory, result_dict)"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    # Create a list to store the results\n    results = []\n\n    # Define a function to run a file as a subprocess\n    def run_file(file):\n        result = subprocess.run([file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        results.append(result.returncode)\n\n    # Create a list to store the threads\n    threads = []\n\n    # Start each file as a subprocess in a new thread\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        thread.start()\n        threads.append(thread)\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    # Return the results\n    return results\nfile_list = ['ls', 'pwd', 'echo', 'nonexistent_file']", "raw_solution": "import subprocess\nimport time\nimport threading\n\ndef task_func(file_list):\n    # Create a list to store the results\n    results = []\n\n    # Define a function to run a file as a subprocess\n    def run_file(file):\n        result = subprocess.run([file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        results.append(result.returncode)\n\n    # Create a list to store the threads\n    threads = []\n\n    # Start each file as a subprocess in a new thread\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        thread.start()\n        threads.append(thread)\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    # Return the results\n    return results\n\n# Test the function\nfile_list = ['ls', 'pwd', 'echo', 'nonexistent_file']"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    # Check if the directory exists\n    if not os.path.isdir(directory_path):\n        print(f\"Directory {directory_path} does not exist.\")\n        return None\n\n    # Get all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n\n    # List to store the file names and exit codes\n    results = []\n\n    # Run each .bat file\n    for file in bat_files:\n        try:\n            # Run the file and get the exit code\n            exit_code = subprocess.call([\"cmd.exe\", \"/c\", file])\n            results.append((file, exit_code))\n        except Exception as e:\n            print(f\"Error running file {file}: {str(e)}\")\n            results.append((file, None))\n\n    return results", "raw_solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    # Check if the directory exists\n    if not os.path.isdir(directory_path):\n        print(f\"Directory {directory_path} does not exist.\")\n        return None\n\n    # Get all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n\n    # List to store the file names and exit codes\n    results = []\n\n    # Run each .bat file\n    for file in bat_files:\n        try:\n            # Run the file and get the exit code\n            exit_code = subprocess.call([\"cmd.exe\", \"/c\", file])\n            results.append((file, exit_code))\n        except Exception as e:\n            print(f\"Error running file {file}: {str(e)}\")\n            results.append((file, None))\n\n    return results\n\n# Test the function"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"The input df must be DataFrame, not be empty\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the column {col}\")\n\n    fig, axes = plt.subplots(2, 1, figsize=(10,10))\n\n    if df[col].dtype in ['int64', 'float64']:\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.countplot(y=col, data=df, ax=axes[0])\n    axes[0].set_title(f'Histogram of {col}')\n\n    sns.boxplot(y=col, data=df, ax=axes[1])\n    axes[1].set_title(f'Box plot of {col}')\n\n    plt.tight_layout()\n\n    return fig", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"The input df must be DataFrame, not be empty\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the column {col}\")\n\n    fig, axes = plt.subplots(2, 1, figsize=(10,10))\n\n    if df[col].dtype in ['int64', 'float64']:\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.countplot(y=col, data=df, ax=axes[0])\n    axes[0].set_title(f'Histogram of {col}')\n\n    sns.boxplot(y=col, data=df, ax=axes[1])\n    axes[1].set_title(f'Box plot of {col}')\n\n    plt.tight_layout()\n\n    return fig"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    if not os.path.exists(script_path):\n        raise ValueError(\"The script does not exist.\")\n\n    if wait:\n        try:\n            process = subprocess.run(['python', script_path] + list(args), capture_output=True, text=True, check=True)\n            return process.returncode\n        except subprocess.CalledProcessError as e:\n            raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output)\n    else:\n        subprocess.Popen(['python', script_path] + list(args))\n        return None", "raw_solution": "import subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    if not os.path.exists(script_path):\n        raise ValueError(\"The script does not exist.\")\n\n    if wait:\n        try:\n            process = subprocess.run(['python', script_path] + list(args), capture_output=True, text=True, check=True)\n            return process.returncode\n        except subprocess.CalledProcessError as e:\n            raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output)\n    else:\n        subprocess.Popen(['python', script_path] + list(args))\n        return None\n\n# Test the function"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    # Check if file exists\n    if not os.path.isfile(file_location):\n        raise FileNotFoundError(\"The specified file does not exist.\")\n\n    # Load data from Excel\n    try:\n        data = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(\"The specified sheet does not exist in the workbook.\")\n\n    # Calculate mean and standard deviation\n    stats_dict = data.agg(['mean', 'std']).to_dict()\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(len(stats_dict['mean'])), stats_dict['mean'].values(), tick_label=stats_dict['mean'].keys())\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n\n    return stats_dict, fig", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    # Check if file exists\n    if not os.path.isfile(file_location):\n        raise FileNotFoundError(\"The specified file does not exist.\")\n\n    # Load data from Excel\n    try:\n        data = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(\"The specified sheet does not exist in the workbook.\")\n\n    # Calculate mean and standard deviation\n    stats_dict = data.agg(['mean', 'std']).to_dict()\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(len(stats_dict['mean'])), stats_dict['mean'].values(), tick_label=stats_dict['mean'].keys())\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n\n    return stats_dict, fig"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    if not all(isinstance(i, datetime) for i in activities):\n        raise TypeError(\"All elements in the list must be datetime objects\")\n\n    day_counts = defaultdict(int)\n    for activity in activities:\n        day_counts[activity.strftime('%A')] += 1\n\n    days = list(day_counts.keys())\n    counts = list(day_counts.values())\n\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    return ax", "raw_solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    if not all(isinstance(i, datetime) for i in activities):\n        raise TypeError(\"All elements in the list must be datetime objects\")\n\n    day_counts = defaultdict(int)\n    for activity in activities:\n        day_counts[activity.strftime('%A')] += 1\n\n    days = list(day_counts.keys())\n    counts = list(day_counts.values())\n\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    return ax"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    # Seed the random number generator\n    random.seed(seed)\n\n    # Check if source directory exists\n    if not os.path.isdir(src_dir):\n        raise ValueError(f\"Source directory {src_dir} does not exist.\")\n\n    # Check if destination directory exists\n    if not os.path.isdir(dest_dir):\n        raise ValueError(f\"Destination directory {dest_dir} does not exist.\")\n\n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n\n    # If there are no files in the source directory, raise an error\n    if not files:\n        raise ValueError(f\"Source directory {src_dir} is empty.\")\n\n    # Select a random file\n    file = random.choice(files)\n\n    # Construct the full path to the file\n    src_file_path = os.path.join(src_dir, file)\n\n    # Construct the full path to the destination directory\n    dest_file_path = os.path.join(dest_dir, file)\n\n    # Move the file\n    shutil.move(src_file_path, dest_file_path)\n\n    # Return the name of the file\n    return file", "raw_solution": "import os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    # Seed the random number generator\n    random.seed(seed)\n\n    # Check if source directory exists\n    if not os.path.isdir(src_dir):\n        raise ValueError(f\"Source directory {src_dir} does not exist.\")\n\n    # Check if destination directory exists\n    if not os.path.isdir(dest_dir):\n        raise ValueError(f\"Destination directory {dest_dir} does not exist.\")\n\n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n\n    # If there are no files in the source directory, raise an error\n    if not files:\n        raise ValueError(f\"Source directory {src_dir} is empty.\")\n\n    # Select a random file\n    file = random.choice(files)\n\n    # Construct the full path to the file\n    src_file_path = os.path.join(src_dir, file)\n\n    # Construct the full path to the destination directory\n    dest_file_path = os.path.join(dest_dir, file)\n\n    # Move the file\n    shutil.move(src_file_path, dest_file_path)\n\n    # Return the name of the file\n    return file"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    # Get all .xlsx files in the directory\n    files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Initialize counter\n    count = 0\n\n    # Process each file\n    for file in files:\n        # Load the workbook\n        wb = load_workbook(filename=file, read_only=False, keep_vba=True)\n\n        # Iterate over each worksheet\n        for ws in wb.worksheets:\n            # Iterate over each cell\n            for row in ws.iter_rows():\n                for cell in row:\n                    # If the cell contains a double quote, replace it with a double backslash\n                    if cell.value and '\"' in str(cell.value):\n                        cell.value = re.sub(r'\"', '\\\\\"', str(cell.value))\n\n        # Save the workbook\n        wb.save(file)\n\n        # Increment the counter\n        count += 1\n\n    return count", "raw_solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Get all .xlsx files in the directory\n    files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Initialize counter\n    count = 0\n\n    # Process each file\n    for file in files:\n        # Load the workbook\n        wb = load_workbook(filename=file, read_only=False, keep_vba=True)\n\n        # Iterate over each worksheet\n        for ws in wb.worksheets:\n            # Iterate over each cell\n            for row in ws.iter_rows():\n                for cell in row:\n                    # If the cell contains a double quote, replace it with a double backslash\n                    if cell.value and '\"' in str(cell.value):\n                        cell.value = re.sub(r'\"', '\\\\\"', str(cell.value))\n\n        # Save the workbook\n        wb.save(file)\n\n        # Increment the counter\n        count += 1\n\n    return count"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n\n    time = np.linspace(0, 1, sample_size, False)\n    sine_wave = np.sin(2 * np.pi * frequency * time)\n    cosine_wave = np.cos(2 * np.pi * frequency * time)\n\n    fig, ax = plt.subplots()\n    ax.plot(time, sine_wave, label='Sine Wave')\n    ax.plot(time, cosine_wave, label='Cosine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n\n    return fig, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n\n    time = np.linspace(0, 1, sample_size, False)\n    sine_wave = np.sin(2 * np.pi * frequency * time)\n    cosine_wave = np.cos(2 * np.pi * frequency * time)\n\n    fig, ax = plt.subplots()\n    ax.plot(time, sine_wave, label='Sine Wave')\n    ax.plot(time, cosine_wave, label='Cosine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n\n    return fig, ax"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    app = Flask(app_name)\n\n    # Set default configuration values\n    app.config['MAIL_SERVER'] = 'localhost'\n    app.config['MAIL_PORT'] = 25\n    app.config['MAIL_USE_TLS'] = False\n    app.config['MAIL_USERNAME'] = None\n    app.config['MAIL_PASSWORD'] = None\n\n    # Override with environment variables if they exist\n    app.config['MAIL_SERVER'] = os.getenv('MAIL_SERVER', app.config['MAIL_SERVER'])\n    app.config['MAIL_PORT'] = int(os.getenv('MAIL_PORT', app.config['MAIL_PORT']))\n    app.config['MAIL_USE_TLS'] = os.getenv('MAIL_USE_TLS', app.config['MAIL_USE_TLS']) == 'True'\n    app.config['MAIL_USERNAME'] = os.getenv('MAIL_USERNAME', app.config['MAIL_USERNAME'])\n    app.config['MAIL_PASSWORD'] = os.getenv('MAIL_PASSWORD', app.config['MAIL_PASSWORD'])\n\n    mail = Mail(app)\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']", "raw_solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    app = Flask(app_name)\n\n    # Set default configuration values\n    app.config['MAIL_SERVER'] = 'localhost'\n    app.config['MAIL_PORT'] = 25\n    app.config['MAIL_USE_TLS'] = False\n    app.config['MAIL_USERNAME'] = None\n    app.config['MAIL_PASSWORD'] = None\n\n    # Override with environment variables if they exist\n    app.config['MAIL_SERVER'] = os.getenv('MAIL_SERVER', app.config['MAIL_SERVER'])\n    app.config['MAIL_PORT'] = int(os.getenv('MAIL_PORT', app.config['MAIL_PORT']))\n    app.config['MAIL_USE_TLS'] = os.getenv('MAIL_USE_TLS', app.config['MAIL_USE_TLS']) == 'True'\n    app.config['MAIL_USERNAME'] = os.getenv('MAIL_USERNAME', app.config['MAIL_USERNAME'])\n    app.config['MAIL_PASSWORD'] = os.getenv('MAIL_PASSWORD', app.config['MAIL_PASSWORD'])\n\n    mail = Mail(app)\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']\n\n# Test the function\nmail, server, port, use_tls, username, password = task_func('test_app')"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    # Check if file exists\n    if not os.path.isfile(os.path.join(excel_file_path, file_name)):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path.\")\n\n    # Load the excel file\n    df = pd.read_excel(os.path.join(excel_file_path, file_name))\n\n    # Check if column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the file.\")\n\n    # Calculate mean, median, and standard deviation\n    mean = df[column_name].mean()\n    median = df[column_name].median()\n    std_dev = df[column_name].std()\n\n    # Return as a dictionary\n    return {'mean': mean, 'median': median, 'std_dev': std_dev}", "raw_solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    # Check if file exists\n    if not os.path.isfile(os.path.join(excel_file_path, file_name)):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path.\")\n\n    # Load the excel file\n    df = pd.read_excel(os.path.join(excel_file_path, file_name))\n\n    # Check if column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the file.\")\n\n    # Calculate mean, median, and standard deviation\n    mean = df[column_name].mean()\n    median = df[column_name].median()\n    std_dev = df[column_name].std()\n\n    # Return as a dictionary\n    return {'mean': mean, 'median': median, 'std_dev': std_dev}"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y, learning_rate=0.01):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct the Sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model\n    sgd = SGD(lr=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # Fit the model\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plot the training and validation loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    ax = plt.gca()\n\n    return model, ax", "raw_solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y, learning_rate=0.01):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct the Sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model\n    sgd = SGD(lr=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # Fit the model\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plot the training and validation loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    ax = plt.gca()\n\n    return model, ax"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model\n    model = keras.models.Sequential()\n    model.add(keras.layers.Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n\n    # Fit the model\n    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n\n    # Predict probabilities\n    y_pred_prob = model.predict_proba(X_test)\n\n    # Compute ROC curve and ROC area for each class\n    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model, plt.gca()", "raw_solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model\n    model = keras.models.Sequential()\n    model.add(keras.layers.Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n\n    # Fit the model\n    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n\n    # Predict probabilities\n    y_pred_prob = model.predict_proba(X_test)\n\n    # Compute ROC curve and ROC area for each class\n    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(\"The image file does not exist at the specified path.\")\n\n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' should be a positive integer.\")\n\n    # Read the image\n    img = cv2.imread(image_path)\n\n    # Reshape the image to be a list of RGB pixels\n    img_reshape = img.reshape((-1, 3))\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(img_reshape)\n\n    # Get the cluster labels for each pixel\n    labels = kmeans.labels_\n\n    # Replace each pixel's color with the centroid of the cluster it belongs to\n    segmented_img = kmeans.cluster_centers_[labels].reshape(img.shape)\n\n    # Convert the segmented image to uint8\n    segmented_img = np.uint8(segmented_img)\n\n    return img, segmented_img", "raw_solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(\"The image file does not exist at the specified path.\")\n\n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' should be a positive integer.\")\n\n    # Read the image\n    img = cv2.imread(image_path)\n\n    # Reshape the image to be a list of RGB pixels\n    img_reshape = img.reshape((-1, 3))\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(img_reshape)\n\n    # Get the cluster labels for each pixel\n    labels = kmeans.labels_\n\n    # Replace each pixel's color with the centroid of the cluster it belongs to\n    segmented_img = kmeans.cluster_centers_[labels].reshape(img.shape)\n\n    # Convert the segmented image to uint8\n    segmented_img = np.uint8(segmented_img)\n\n    return img, segmented_img"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \n    # Calculate the product of a matrix 'P' and a 3D tensor 'T'\n    product = np.matmul(P, T)\n    \n    # Flatten the result\n    flattened = product.flatten()\n    \n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n    \n    # Visualize it\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result)\n    ax.set_title('KMeans Clustering Visualization')\n    \n    return cluster_result, ax", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \n    # Calculate the product of a matrix 'P' and a 3D tensor 'T'\n    product = np.matmul(P, T)\n    \n    # Flatten the result\n    flattened = product.flatten()\n    \n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n    \n    # Visualize it\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result)\n    ax.set_title('KMeans Clustering Visualization')\n    \n    return cluster_result, ax"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    # Check if points is a numpy array and of shape (n, 2)\n    if not isinstance(points, np.ndarray) or points.shape[1] != 2:\n        raise ValueError(\"Points should be a numpy array of shape (n, 2)\")\n\n    # Check if seed is an integer\n    if not isinstance(seed, int):\n        raise ValueError(\"Seed should be an integer\")\n\n    # Set the seed for the random number generator\n    np.random.seed(seed)\n\n    # Apply jitter to the points\n    points = points + np.random.uniform(-1, 1, size=points.shape)\n\n    # Compute the Voronoi diagram\n    vor = Voronoi(points)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the Voronoi diagram\n    voronoi_plot_2d(vor, ax=ax)\n\n    # Return the Voronoi object and the axes\n    return vor, ax", "raw_solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    # Check if points is a numpy array and of shape (n, 2)\n    if not isinstance(points, np.ndarray) or points.shape[1] != 2:\n        raise ValueError(\"Points should be a numpy array of shape (n, 2)\")\n\n    # Check if seed is an integer\n    if not isinstance(seed, int):\n        raise ValueError(\"Seed should be an integer\")\n\n    # Set the seed for the random number generator\n    np.random.seed(seed)\n\n    # Apply jitter to the points\n    points = points + np.random.uniform(-1, 1, size=points.shape)\n\n    # Compute the Voronoi diagram\n    vor = Voronoi(points)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the Voronoi diagram\n    voronoi_plot_2d(vor, ax=ax)\n\n    # Return the Voronoi object and the axes\n    return vor, ax"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    # Check if source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(\"Source directory does not exist\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(\"Destination directory does not exist\")\n\n    # Get a list of files with the specified extension in the source directory\n    files = glob.glob(os.path.join(src_dir, f'*.{ext}'))\n\n    # Initialize an empty list to store the paths of successfully moved files\n    moved_files = []\n\n    for file in files:\n        # Get the file name and its full path\n        file_name = os.path.basename(file)\n        dest_file_path = os.path.join(dest_dir, file_name)\n\n        # Check if a file with the same name already exists in the destination directory\n        if not os.path.exists(dest_file_path):\n            # Move the file to the destination directory\n            shutil.move(file, dest_dir)\n            # Add the full path of the moved file to the list\n            moved_files.append(dest_file_path)\n\n    return moved_files", "raw_solution": "import os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    # Check if source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(\"Source directory does not exist\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(\"Destination directory does not exist\")\n\n    # Get a list of files with the specified extension in the source directory\n    files = glob.glob(os.path.join(src_dir, f'*.{ext}'))\n\n    # Initialize an empty list to store the paths of successfully moved files\n    moved_files = []\n\n    for file in files:\n        # Get the file name and its full path\n        file_name = os.path.basename(file)\n        dest_file_path = os.path.join(dest_dir, file_name)\n\n        # Check if a file with the same name already exists in the destination directory\n        if not os.path.exists(dest_file_path):\n            # Move the file to the destination directory\n            shutil.move(file, dest_dir)\n            # Add the full path of the moved file to the list\n            moved_files.append(dest_file_path)\n\n    return moved_files"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    # Check if the JSON string is empty\n    if not json_str:\n        return pd.DataFrame()\n\n    # Load JSON string into a dictionary\n    data_dict = json.loads(json_str)\n\n    # Normalize the dictionary by doubling the numerical values\n    def normalize_value(value):\n        if isinstance(value, (int, float)):\n            return value * 2\n        elif isinstance(value, list):\n            return [v * 2 if isinstance(v, (int, float)) else v for v in value]\n        elif isinstance(value, str):\n            # Extract numbers from the string and double them\n            numbers = re.findall(r'-?\\d+\\.?\\d*', value)\n            return [float(num) * 2 for num in numbers] if numbers else value\n        else:\n            return value\n\n    data_dict = {key: normalize_value(value) for key, value in data_dict.items()}\n\n    # Create a Pandas DataFrame from the dictionary\n    df = pd.DataFrame(data_dict)\n\n    return df", "raw_solution": "import json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    # Check if the JSON string is empty\n    if not json_str:\n        return pd.DataFrame()\n\n    # Load JSON string into a dictionary\n    data_dict = json.loads(json_str)\n\n    # Normalize the dictionary by doubling the numerical values\n    def normalize_value(value):\n        if isinstance(value, (int, float)):\n            return value * 2\n        elif isinstance(value, list):\n            return [v * 2 if isinstance(v, (int, float)) else v for v in value]\n        elif isinstance(value, str):\n            # Extract numbers from the string and double them\n            numbers = re.findall(r'-?\\d+\\.?\\d*', value)\n            return [float(num) * 2 for num in numbers] if numbers else value\n        else:\n            return value\n\n    data_dict = {key: normalize_value(value) for key, value in data_dict.items()}\n\n    # Create a Pandas DataFrame from the dictionary\n    df = pd.DataFrame(data_dict)\n\n    return df"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n\n    start_time = time.time()\n    process = subprocess.Popen(['python', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    cpu_percent = 0.0\n    memory_usage = 0.0\n\n    while True:\n        if time.time() - start_time > timeout:\n            process.terminate()\n            break\n\n        if process.poll() is not None:\n            break\n\n        for proc in psutil.process_iter(['pid', 'cpu_percent', 'memory_info']):\n            if proc.info['pid'] == process.pid:\n                cpu_percent += proc.info['cpu_percent'] / psutil.cpu_count()\n                memory_usage += proc.info['memory_info'].rss\n                break\n\n        time.sleep(0.1)\n\n    return {'CPU Usage': cpu_percent, 'Memory Usage': memory_usage}", "raw_solution": "import subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n\n    start_time = time.time()\n    process = subprocess.Popen(['python', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    cpu_percent = 0.0\n    memory_usage = 0.0\n\n    while True:\n        if time.time() - start_time > timeout:\n            process.terminate()\n            break\n\n        if process.poll() is not None:\n            break\n\n        for proc in psutil.process_iter(['pid', 'cpu_percent', 'memory_info']):\n            if proc.info['pid'] == process.pid:\n                cpu_percent += proc.info['cpu_percent'] / psutil.cpu_count()\n                memory_usage += proc.info['memory_info'].rss\n                break\n\n        time.sleep(0.1)\n\n    return {'CPU Usage': cpu_percent, 'Memory Usage': memory_usage}"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n\n    # Generate DataFrame\n    df = pd.DataFrame({\n        \"x\": np.random.rand(N),\n        \"y\": np.random.rand(N),\n    })\n\n    # Generate categories\n    if N >= len(CATEGORIES):\n        df[\"category\"] = np.random.choice(CATEGORIES, N)\n    else:\n        df[\"category\"] = np.random.choice(CATEGORIES, N, replace=False)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(df[\"x\"], df[\"y\"], c=df[\"category\"])\n\n    return (df, ax)", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n\n    # Generate DataFrame\n    df = pd.DataFrame({\n        \"x\": np.random.rand(N),\n        \"y\": np.random.rand(N),\n    })\n\n    # Generate categories\n    if N >= len(CATEGORIES):\n        df[\"category\"] = np.random.choice(CATEGORIES, N)\n    else:\n        df[\"category\"] = np.random.choice(CATEGORIES, N, replace=False)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(df[\"x\"], df[\"y\"], c=df[\"category\"])\n\n    return (df, ax)\n\n# Test the function\ndf, ax = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    np.random.seed(seed)\n\n    # Generate a time series from start_time to end_time with specified step\n    time_range = pd.date_range(start=start_time, end=end_time, freq=step)\n\n    # Generate values from a normal distribution\n    values = np.random.normal(0, 1, len(time_range))\n\n    # Add trend to the values\n    values += np.arange(len(time_range)) * trend\n\n    # Create a DataFrame\n    df = pd.DataFrame({'Time': time_range, 'Value': values})\n\n    # Plot the time series\n    ax = df.plot(x='Time', y='Value')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n\n    return ax\nstart_time = datetime.now()\nend_time = datetime.now() + timedelta(days=10)\nstep = '1H'\ntrend = 2", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    np.random.seed(seed)\n\n    # Generate a time series from start_time to end_time with specified step\n    time_range = pd.date_range(start=start_time, end=end_time, freq=step)\n\n    # Generate values from a normal distribution\n    values = np.random.normal(0, 1, len(time_range))\n\n    # Add trend to the values\n    values += np.arange(len(time_range)) * trend\n\n    # Create a DataFrame\n    df = pd.DataFrame({'Time': time_range, 'Value': values})\n\n    # Plot the time series\n    ax = df.plot(x='Time', y='Value')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n\n    return ax\n\n# Test the function\nstart_time = datetime.now()\nend_time = datetime.now() + timedelta(days=10)\nstep = '1H'\ntrend = 2\nax = task_func(start_time, end_time, step, trend)\nplt.show()"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or not isinstance(random_seed, int) or not isinstance(products, list) or len(products) != 5:\n        raise ValueError(\"Invalid input\")\n\n    # Set random seed\n    random.seed(random_seed)\n\n    # Convert epoch time to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_date = datetime.now()\n\n    # Generate sales data\n    data = []\n    for day_diff in range((current_date - start_date).days + 1):\n        date = start_date + timedelta(days=day_diff)\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append([product, date, sales])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Product\", \"Date\", \"Sales\"])\n\n    return df\nrandom_seed = 0\nepoch_milliseconds = int(time.mktime(datetime(2022, 1, 1).timetuple()) * 1000)", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or not isinstance(random_seed, int) or not isinstance(products, list) or len(products) != 5:\n        raise ValueError(\"Invalid input\")\n\n    # Set random seed\n    random.seed(random_seed)\n\n    # Convert epoch time to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    current_date = datetime.now()\n\n    # Generate sales data\n    data = []\n    for day_diff in range((current_date - start_date).days + 1):\n        date = start_date + timedelta(days=day_diff)\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append([product, date, sales])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Product\", \"Date\", \"Sales\"])\n\n    return df\n\n# Test the function\nrandom_seed = 0\nepoch_milliseconds = int(time.mktime(datetime(2022, 1, 1).timetuple()) * 1000)\ndf = task_func(epoch_milliseconds, random_seed)"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    try:\n        data = json.loads(json_str)\n    except ValueError:\n        raise ValueError(\"json_str is not valid JSON\")\n    try:\n        if isinstance(data, list) and not data:\n            # If the JSON string represents an empty array, create an Excel file with no data rows\n            workbook = xlwt.Workbook()\n            worksheet = workbook.add_sheet(sheet_name)\n            workbook.save(filename)\n        else:\n            # Convert JSON to DataFrame and save it to Excel\n            df = pd.DataFrame(data)\n            df.to_excel(filename, sheet_name=sheet_name, index=False)\n    except Exception as e:\n        raise Exception(\"Error while writing to file: {}\".format(e))\n    return os.path.abspath(filename)", "raw_solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    try:\n        data = json.loads(json_str)\n    except ValueError:\n        raise ValueError(\"json_str is not valid JSON\")\n    try:\n        if isinstance(data, list) and not data:\n            # If the JSON string represents an empty array, create an Excel file with no data rows\n            workbook = xlwt.Workbook()\n            worksheet = workbook.add_sheet(sheet_name)\n            workbook.save(filename)\n        else:\n            # Convert JSON to DataFrame and save it to Excel\n            df = pd.DataFrame(data)\n            df.to_excel(filename, sheet_name=sheet_name, index=False)\n    except Exception as e:\n        raise Exception(\"Error while writing to file: {}\".format(e))\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n\n    # Define activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n\n    # Generate data\n    data = []\n    for i in range(days_in_past):\n        date = datetime.now() - timedelta(days=i)\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append([date, activity, duration])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n\n    # Plot\n    plt.figure(figsize=(10,6))\n    ax = sns.lineplot(x='Date', y='Duration', hue='Activity', data=df)\n\n    return (ax, df)", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n\n    # Define activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n\n    # Generate data\n    data = []\n    for i in range(days_in_past):\n        date = datetime.now() - timedelta(days=i)\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append([date, activity, duration])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n\n    # Plot\n    plt.figure(figsize=(10,6))\n    ax = sns.lineplot(x='Date', y='Duration', hue='Activity', data=df)\n\n    return (ax, df)\n\n# Test the function\nax, df = task_func(7)"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    np.random.seed(random_seed)\n    dates = pd.date_range(start=datetime.now() - timedelta(days=days_in_past), periods=days_in_past).tolist()\n    prices = np.random.rand(days_in_past, len(stock_names)).tolist()\n    df = pd.DataFrame(prices, index=dates, columns=stock_names)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    np.random.seed(random_seed)\n    dates = pd.date_range(start=datetime.now() - timedelta(days=days_in_past), periods=days_in_past).tolist()\n    prices = np.random.rand(days_in_past, len(stock_names)).tolist()\n    df = pd.DataFrame(prices, index=dates, columns=stock_names)\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r') as f1, open(file_path2, 'r') as f2:\n            lines1 = f1.readlines()\n            lines2 = f2.readlines()\n\n        if not lines1 or not lines2:\n            raise ValueError(\"One or both of the files are empty.\")\n\n        diff_lines = list(ndiff(lines1, lines2))\n\n        diff_data = []\n        for i, line in enumerate(diff_lines, start=1):\n            status = ' ' if line.startswith('  ') else ('-' if line.startswith('- ') else '+')\n            content = line.strip().split(' ', 1)[-1]\n            diff_data.append([i, status, content])\n\n        df = pd.DataFrame(diff_data, columns=['Line Number', 'Status', 'Content'])\n\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files cannot be found.\")\n    except Exception as e:\n        raise Exception(\"An error occurred: {}\".format(e))", "raw_solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r') as f1, open(file_path2, 'r') as f2:\n            lines1 = f1.readlines()\n            lines2 = f2.readlines()\n\n        if not lines1 or not lines2:\n            raise ValueError(\"One or both of the files are empty.\")\n\n        diff_lines = list(ndiff(lines1, lines2))\n\n        diff_data = []\n        for i, line in enumerate(diff_lines, start=1):\n            status = ' ' if line.startswith('  ') else ('-' if line.startswith('- ') else '+')\n            content = line.strip().split(' ', 1)[-1]\n            diff_data.append([i, status, content])\n\n        df = pd.DataFrame(diff_data, columns=['Line Number', 'Status', 'Content'])\n\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files cannot be found.\")\n    except Exception as e:\n        raise Exception(\"An error occurred: {}\".format(e))\n\n# Test the function\ndf = task_func('file1.csv', 'file2.csv')"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if data.empty:\n        stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n        fig, ax = plt.subplots()\n        ax.pie([], labels=[])\n        return stats, ax\n    else:\n        stats = {\n            'sum': data[column].sum(),\n            'mean': data[column].mean(),\n            'min': data[column].min(),\n            'max': data[column].max()\n        }\n        fig, ax = plt.subplots()\n        ax.pie(data[column].value_counts(), labels=data[column].value_counts().index)\n        return stats, ax\ndata = pd.DataFrame({'Age': [25, 35, 45, 55, 25, 35, 45, 55, 25, 35]})", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if data.empty:\n        stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n        fig, ax = plt.subplots()\n        ax.pie([], labels=[])\n        return stats, ax\n    else:\n        stats = {\n            'sum': data[column].sum(),\n            'mean': data[column].mean(),\n            'min': data[column].min(),\n            'max': data[column].max()\n        }\n        fig, ax = plt.subplots()\n        ax.pie(data[column].value_counts(), labels=data[column].value_counts().index)\n        return stats, ax\n\n# Test the function\ndata = pd.DataFrame({'Age': [25, 35, 45, 55, 25, 35, 45, 55, 25, 35]})\nstats, ax = task_func('Age', data)"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Check if data is not empty\n    if not data:\n        raise ValueError(\"Data list is empty\")\n\n    # Convert data to pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if column exists in data\n    if column not in df.columns:\n        raise KeyError(f\"Specified column {column} is not valid\")\n\n    # Check if all numeric values in the column are non-negative\n    if np.any(df[column] < 0):\n        raise ValueError(f\"Negative values are not allowed for {column}\")\n\n    # Calculate sum, mean, min, max\n    summary_stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Plot line chart\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column])\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    ax.set_title(f'Line Chart of {column}')\n\n    return summary_stats, ax.figure", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Check if data is not empty\n    if not data:\n        raise ValueError(\"Data list is empty\")\n\n    # Convert data to pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if column exists in data\n    if column not in df.columns:\n        raise KeyError(f\"Specified column {column} is not valid\")\n\n    # Check if all numeric values in the column are non-negative\n    if np.any(df[column] < 0):\n        raise ValueError(f\"Negative values are not allowed for {column}\")\n\n    # Calculate sum, mean, min, max\n    summary_stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Plot line chart\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column])\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    ax.set_title(f'Line Chart of {column}')\n\n    return summary_stats, ax.figure"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    # Load data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize dictionaries to store mean and median values\n    mean_dict = defaultdict(list)\n    median_dict = defaultdict(list)\n\n    # Iterate over each dictionary in the list\n    for d in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in d.items():\n            # Ignore non-numeric or missing values\n            if isinstance(value, (int, float)):\n                mean_dict[key].append(value)\n                median_dict[key].append(value)\n\n    # Calculate mean and median for each key\n    for key in mean_dict.keys():\n        mean_dict[key] = np.mean(mean_dict[key])\n        median_dict[key] = np.median(median_dict[key])\n\n    # Convert the results into a Pandas DataFrame\n    df = pd.DataFrame(index=mean_dict.keys(), data={'mean': mean_dict.values(), 'median': median_dict.values()})\n\n    # Sort the DataFrame by the variable names\n    df.sort_index(inplace=True)\n\n    return df", "raw_solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    # Load data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize dictionaries to store mean and median values\n    mean_dict = defaultdict(list)\n    median_dict = defaultdict(list)\n\n    # Iterate over each dictionary in the list\n    for d in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in d.items():\n            # Ignore non-numeric or missing values\n            if isinstance(value, (int, float)):\n                mean_dict[key].append(value)\n                median_dict[key].append(value)\n\n    # Calculate mean and median for each key\n    for key in mean_dict.keys():\n        mean_dict[key] = np.mean(mean_dict[key])\n        median_dict[key] = np.median(median_dict[key])\n\n    # Convert the results into a Pandas DataFrame\n    df = pd.DataFrame(index=mean_dict.keys(), data={'mean': mean_dict.values(), 'median': median_dict.values()})\n\n    # Sort the DataFrame by the variable names\n    df.sort_index(inplace=True)\n\n    return df"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File should be in .csv format\")\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    df = pd.DataFrame(data)\n    duplicates = df[df.duplicated()]\n    counts = Counter(duplicates.apply(tuple,1))\n\n    fig, ax = plt.subplots()\n    ax.bar(range(len(counts)), list(counts.values()), align='center')\n    ax.set_xticks(range(len(counts)))\n    ax.set_xticklabels(list(counts.keys()), rotation=90)\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Counts')\n    ax.set_title('Count of Duplicate Rows')\n\n    return counts, ax", "raw_solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File should be in .csv format\")\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    df = pd.DataFrame(data)\n    duplicates = df[df.duplicated()]\n    counts = Counter(duplicates.apply(tuple,1))\n\n    fig, ax = plt.subplots()\n    ax.bar(range(len(counts)), list(counts.values()), align='center')\n    ax.set_xticks(range(len(counts)))\n    ax.set_xticklabels(list(counts.keys()), rotation=90)\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Counts')\n    ax.set_title('Count of Duplicate Rows')\n\n    return counts, ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    df['age'] = df['age'].apply(np.floor)\n    df = df[df['age'] >= 0]\n    if df.empty:\n        raise ValueError(\"All ages are negative\")\n    duplicates = df[df.duplicated(subset='name', keep=False)]\n    if duplicates.empty:\n        return (Counter(), None)\n    ages = duplicates['age'].values\n    age_counter = Counter(ages)\n    min_age = min(ages)\n    max_age = max(ages)\n    bins = list(range(int(min_age), int(max_age) + 2))\n    plt.hist(ages, bins=bins, edgecolor='black')\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Age distribution for duplicate names')\n    return (age_counter, plt.gca())", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    df['age'] = df['age'].apply(np.floor)\n    df = df[df['age'] >= 0]\n    if df.empty:\n        raise ValueError(\"All ages are negative\")\n    duplicates = df[df.duplicated(subset='name', keep=False)]\n    if duplicates.empty:\n        return (Counter(), None)\n    ages = duplicates['age'].values\n    age_counter = Counter(ages)\n    min_age = min(ages)\n    max_age = max(ages)\n    bins = list(range(int(min_age), int(max_age) + 2))\n    plt.hist(ages, bins=bins, edgecolor='black')\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Age distribution for duplicate names')\n    return (age_counter, plt.gca())"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, bins=4):\n    # Check if 'value' column exists in the DataFrame\n    if 'value' not in df.columns:\n        raise ValueError(\"'value' column not found in DataFrame\")\n\n    # Identify and count duplicate values\n    counter = Counter(df['value'])\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, color='green', alpha=0.6, edgecolor='black')\n\n    # Fit a normal distribution to the data\n    mu, sigma = norm.fit(df['value'])\n\n    # Create a range of values for the normal distribution\n    x_axis = np.linspace(min(df['value']), max(df['value']), 100)\n    pdf = norm.pdf(x_axis, mu, sigma)\n\n    # Plot the normal distribution\n    ax.plot(x_axis, pdf, color='black', linewidth=2, label='Normal Distribution')\n\n    # Set plot title and labels\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return counter, ax.figure\ndf = pd.DataFrame({'value': [1, 2, 2, 3, 4, 4, 4, 4, 5, 5, 5, 5]})", "raw_solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, bins=4):\n    # Check if 'value' column exists in the DataFrame\n    if 'value' not in df.columns:\n        raise ValueError(\"'value' column not found in DataFrame\")\n\n    # Identify and count duplicate values\n    counter = Counter(df['value'])\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, color='green', alpha=0.6, edgecolor='black')\n\n    # Fit a normal distribution to the data\n    mu, sigma = norm.fit(df['value'])\n\n    # Create a range of values for the normal distribution\n    x_axis = np.linspace(min(df['value']), max(df['value']), 100)\n    pdf = norm.pdf(x_axis, mu, sigma)\n\n    # Plot the normal distribution\n    ax.plot(x_axis, pdf, color='black', linewidth=2, label='Normal Distribution')\n\n    # Set plot title and labels\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return counter, ax.figure\n\n# Test the function\ndf = pd.DataFrame({'value': [1, 2, 2, 3, 4, 4, 4, 4, 5, 5, 5, 5]})\ncounter, fig = task_func(df, bins=5)\nplt.show()"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    # Generate a pandas DataFrame with random values\n    df = pd.DataFrame(np.random.rand(len(a), len(b)), index=a, columns=COLUMNS[:len(b)])\n\n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar')\n\n    return ax\na = ['Row1', 'Row2', 'Row3']\nb = [1, 2, 3, 4, 5]", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    # Generate a pandas DataFrame with random values\n    df = pd.DataFrame(np.random.rand(len(a), len(b)), index=a, columns=COLUMNS[:len(b)])\n\n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar')\n\n    return ax\n\n# Test the function\na = ['Row1', 'Row2', 'Row3']\nb = [1, 2, 3, 4, 5]\nax = task_func(a, b)\nplt.show()"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    # Convert the 'date' column to datetime format\n    data['date'] = pd.to_datetime(data['date'])\n\n    # Set the 'date' column as the index\n    data.set_index('date', inplace=True)\n\n    # Resample the data to monthly frequency and take the mean\n    monthly_data = data.resample('M').mean()\n\n    # Create a bar chart\n    ax = monthly_data.plot(kind='bar', figsize=(12, 6))\n\n    # Set the title and labels\n    ax.set_title(f'Monthly Data for {data.index[0].year}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n\n    return ax\ndata = pd.DataFrame({\n    'date': ['2022-01-01', '2022-02-01', '2022-03-01', '2022-04-01', '2022-05-01', '2022-06-01'],\n    'value': [1, 2, 3, 4, 5, 6]\n})", "raw_solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    # Convert the 'date' column to datetime format\n    data['date'] = pd.to_datetime(data['date'])\n\n    # Set the 'date' column as the index\n    data.set_index('date', inplace=True)\n\n    # Resample the data to monthly frequency and take the mean\n    monthly_data = data.resample('M').mean()\n\n    # Create a bar chart\n    ax = monthly_data.plot(kind='bar', figsize=(12, 6))\n\n    # Set the title and labels\n    ax.set_title(f'Monthly Data for {data.index[0].year}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n\n    return ax\n\n# Test the function\ndata = pd.DataFrame({\n    'date': ['2022-01-01', '2022-02-01', '2022-03-01', '2022-04-01', '2022-05-01', '2022-06-01'],\n    'value': [1, 2, 3, 4, 5, 6]\n})\n\nax = task_func(data)\nplt.show()"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert string data to numeric\n    data = pd.to_numeric(data, errors='coerce')\n\n    # Remove NaN values\n    data = data.dropna()\n\n    # Calculate bins\n    bins = np.arange(data.min(), data.max()+2) - 0.5\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, edgecolor='black')\n\n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n\n    return ax\ndata = pd.Series(['1', '2', '3', '2', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'])", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert string data to numeric\n    data = pd.to_numeric(data, errors='coerce')\n\n    # Remove NaN values\n    data = data.dropna()\n\n    # Calculate bins\n    bins = np.arange(data.min(), data.max()+2) - 0.5\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, edgecolor='black')\n\n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n\n    return ax\n\n# Test the function\ndata = pd.Series(['1', '2', '3', '2', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'])\nax = task_func(data)\nplt.show()"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    def sine_func(x, a, b, c):\n        return a * np.sin(b * x) + c\n\n    x = np.linspace(0, 2 * np.pi, array_length)\n    y = np.sin(x) + np.random.normal(0, noise_level, array_length)\n\n    popt, _ = curve_fit(sine_func, x, y)\n\n    plt.figure()\n    plt.plot(x, y, 'ko', label='Noisy Sine Wave')\n    plt.plot(x, sine_func(x, *popt), 'r-', label='Adjusted Curve')\n    plt.legend()\n    plt.show()\n\n    return plt.gca()", "raw_solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    def sine_func(x, a, b, c):\n        return a * np.sin(b * x) + c\n\n    x = np.linspace(0, 2 * np.pi, array_length)\n    y = np.sin(x) + np.random.normal(0, noise_level, array_length)\n\n    popt, _ = curve_fit(sine_func, x, y)\n\n    plt.figure()\n    plt.plot(x, y, 'ko', label='Noisy Sine Wave')\n    plt.plot(x, sine_func(x, *popt), 'r-', label='Adjusted Curve')\n    plt.legend()\n    plt.show()\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            data = [row[0] for row in reader]\n    except FileNotFoundError:\n        print(f\"File {csv_file} not found.\")\n        return\n    except IOError:\n        print(f\"Error reading file {csv_file}.\")\n        return\n\n    # Normalize text to ASCII\n    data = [unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') for text in data]\n\n    # Tokenize text\n    words = ' '.join(data).split()\n\n    # Count words\n    word_counts = Counter(words)\n\n    # Get 10 most common words\n    common_words = word_counts.most_common(10)\n\n    # Create bar plot\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('10 Most Common Words')\n\n    return ax, common_words", "raw_solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            data = [row[0] for row in reader]\n    except FileNotFoundError:\n        print(f\"File {csv_file} not found.\")\n        return\n    except IOError:\n        print(f\"Error reading file {csv_file}.\")\n        return\n\n    # Normalize text to ASCII\n    data = [unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') for text in data]\n\n    # Tokenize text\n    words = ' '.join(data).split()\n\n    # Count words\n    word_counts = Counter(words)\n\n    # Get 10 most common words\n    common_words = word_counts.most_common(10)\n\n    # Create bar plot\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('10 Most Common Words')\n\n    return ax, common_words"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(size=size)\n\n    # Create a figure\n    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n\n    # Plot histogram\n    ax[0].hist(data, bins=30, density=True)\n    ax[0].set_title('Histogram')\n\n    # Plot PDF\n    x = np.linspace(-4, 4, 1000)\n    ax[1].plot(x, stats.norm.pdf(x))\n    ax[1].hist(data, bins=30, density=True, alpha=0.2)\n    ax[1].set_title('PDF')\n\n    # Adjust layout and show plot\n    plt.tight_layout()\n    plt.show()\n\n    return fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(size=size)\n\n    # Create a figure\n    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n\n    # Plot histogram\n    ax[0].hist(data, bins=30, density=True)\n    ax[0].set_title('Histogram')\n\n    # Plot PDF\n    x = np.linspace(-4, 4, 1000)\n    ax[1].plot(x, stats.norm.pdf(x))\n    ax[1].hist(data, bins=30, density=True, alpha=0.2)\n    ax[1].set_title('PDF')\n\n    # Adjust layout and show plot\n    plt.tight_layout()\n    plt.show()\n\n    return fig\n\n# Test the function\nfig = task_func()"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Generate random 8 bytes for filename\n    random_bytes = get_random_bytes(8)\n\n    # Convert bytes to hex\n    hex_string = random_bytes.hex()\n\n    # Create filename\n    filename = f\"private_key_{hex_string}.txt\"\n\n    # Generate AES key and nonce\n    aes_key = get_random_bytes(16)\n    aes_nonce = get_random_bytes(16)\n\n    # Encrypt RSA private key with AES\n    cipher_aes = AES.new(aes_key, AES.MODE_EAX)\n    ciphertext, tag = cipher_aes.encrypt_and_digest(rsa.key.exportKey('PEM'))\n\n    # Save encrypted private key to file\n    with open(filename, 'wb') as file:\n        [file.write(x) for x in (cipher_aes.nonce, ciphertext, tag)]\n\n    return pubkey, filename, aes_key, aes_nonce\ncipher_aes = AES.new(aes_key, AES.MODE_EAX, nonce=nonce)", "raw_solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Generate random 8 bytes for filename\n    random_bytes = get_random_bytes(8)\n\n    # Convert bytes to hex\n    hex_string = random_bytes.hex()\n\n    # Create filename\n    filename = f\"private_key_{hex_string}.txt\"\n\n    # Generate AES key and nonce\n    aes_key = get_random_bytes(16)\n    aes_nonce = get_random_bytes(16)\n\n    # Encrypt RSA private key with AES\n    cipher_aes = AES.new(aes_key, AES.MODE_EAX)\n    ciphertext, tag = cipher_aes.encrypt_and_digest(rsa.key.exportKey('PEM'))\n\n    # Save encrypted private key to file\n    with open(filename, 'wb') as file:\n        [file.write(x) for x in (cipher_aes.nonce, ciphertext, tag)]\n\n    return pubkey, filename, aes_key, aes_nonce\n\n# Test the function\npubkey, filename, aes_key, aes_nonce = task_func()\n\n# Test decryption\nwith open(filename, 'rb') as file:\n    nonce = file.read(16)\n    ciphertext = file.read(16)\n    tag = file.read(16)\n\ncipher_aes = AES.new(aes_key, AES.MODE_EAX, nonce=nonce)\nplaintext = cipher_aes.decrypt_and_verify(ciphertext, tag)\n"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Read file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # AES encryption\n    aes_key = os.urandom(32)  # 256-bit key\n    cipher = Cipher(algorithms.AES(aes_key), modes.ECB())\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    padded_content = padder.update(file_content) + padder.finalize()\n    encrypted_file_content = encryptor.update(padded_content) + encryptor.finalize()\n\n    # AES key encryption with RSA\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save encrypted file and encrypted AES key\n    encrypted_file_path = 'encrypted_file.bin'\n    encrypted_aes_key_path = 'encrypted_aes_key.bin'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_file_content)\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pubkey, encrypted_file_path, encrypted_aes_key_path", "raw_solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Read file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # AES encryption\n    aes_key = os.urandom(32)  # 256-bit key\n    cipher = Cipher(algorithms.AES(aes_key), modes.ECB())\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    padded_content = padder.update(file_content) + padder.finalize()\n    encrypted_file_content = encryptor.update(padded_content) + encryptor.finalize()\n\n    # AES key encryption with RSA\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save encrypted file and encrypted AES key\n    encrypted_file_path = 'encrypted_file.bin'\n    encrypted_aes_key_path = 'encrypted_aes_key.bin'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_file_content)\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pubkey, encrypted_file_path, encrypted_aes_key_path"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL is invalid or empty\")\n\n    try:\n        response = urllib.request.urlopen(url)\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"Network connectivity or server issue\") from e\n\n    html = response.read().decode('utf-8')\n    d = pq(html)\n\n    data = []\n    for tag in d('a'):\n        text = tag.text\n        href = tag.attrib['href']\n        data.append((text, href))\n\n    df = pd.DataFrame(data, columns=['text', 'href'])\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    return df", "raw_solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL is invalid or empty\")\n\n    try:\n        response = urllib.request.urlopen(url)\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"Network connectivity or server issue\") from e\n\n    html = response.read().decode('utf-8')\n    d = pq(html)\n\n    data = []\n    for tag in d('a'):\n        text = tag.text\n        href = tag.attrib['href']\n        data.append((text, href))\n\n    df = pd.DataFrame(data, columns=['text', 'href'])\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    return df"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filename = os.path.join(output_dir, 'sensor_data.csv')\n\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n\n        start_time = datetime.now()\n        for _ in range(hours):\n            for _ in range(60):  # 60 minutes in an hour\n                time = (start_time + timedelta(minutes=1)).strftime('%Y-%m-%d %H:%M:%S')\n                data = [time] + [randint(0, 100) for _ in SENSORS]\n                writer.writerow(data)\n                start_time += timedelta(minutes=1)", "raw_solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filename = os.path.join(output_dir, 'sensor_data.csv')\n\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n\n        start_time = datetime.now()\n        for _ in range(hours):\n            for _ in range(60):  # 60 minutes in an hour\n                time = (start_time + timedelta(minutes=1)).strftime('%Y-%m-%d %H:%M:%S')\n                data = [time] + [randint(0, 100) for _ in SENSORS]\n                writer.writerow(data)\n                start_time += timedelta(minutes=1)\n\n# Test the function\ntask_func(2)"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate traffic data\n    traffic_data = []\n    current_time = datetime.now()\n    for i in range(hours):\n        traffic_counts = {vehicle_type: randint(0, 100) for vehicle_type in VEHICLE_TYPES}\n        traffic_data.append({'Time': current_time, **traffic_counts})\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV file\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        fieldnames = ['Time'] + VEHICLE_TYPES\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for data in traffic_data:\n            writer.writerow(data)\n\n    # Load data from CSV file into pandas DataFrame\n    df = pd.read_csv(csv_file_path)\n    df['Time'] = pd.to_datetime(df['Time'])\n\n    # Plot data\n    fig, ax = plt.subplots()\n    for vehicle_type in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle_type], label=vehicle_type)\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n\n    return csv_file_path, ax", "raw_solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate traffic data\n    traffic_data = []\n    current_time = datetime.now()\n    for i in range(hours):\n        traffic_counts = {vehicle_type: randint(0, 100) for vehicle_type in VEHICLE_TYPES}\n        traffic_data.append({'Time': current_time, **traffic_counts})\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV file\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        fieldnames = ['Time'] + VEHICLE_TYPES\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for data in traffic_data:\n            writer.writerow(data)\n\n    # Load data from CSV file into pandas DataFrame\n    df = pd.read_csv(csv_file_path)\n    df['Time'] = pd.to_datetime(df['Time'])\n\n    # Plot data\n    fig, ax = plt.subplots()\n    for vehicle_type in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle_type], label=vehicle_type)\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n\n    return csv_file_path, ax\n\n# Test the function\ncsv_file_path, ax = task_func(10)"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\ndef task_func(hours, output_dir=OUTPUT_DIR, backup_dir=BACKUP_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create backup directory if it doesn't exist\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    # Generate weather data\n    weather_data = []\n    start_time = datetime.now()\n    for i in range(hours):\n        time = (start_time + timedelta(hours=i)).strftime(\"%Y-%m-%d %H:%M:%S\")\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        weather_data.append([time, condition])\n\n    # Save weather data to CSV file\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file to the backup directory\n    backup_file_path = os.path.join(backup_dir, 'weather_data.csv')\n    shutil.copyfile(csv_file_path, backup_file_path)\n\n    return csv_file_path", "raw_solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\n\ndef task_func(hours, output_dir=OUTPUT_DIR, backup_dir=BACKUP_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create backup directory if it doesn't exist\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    # Generate weather data\n    weather_data = []\n    start_time = datetime.now()\n    for i in range(hours):\n        time = (start_time + timedelta(hours=i)).strftime(\"%Y-%m-%d %H:%M:%S\")\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        weather_data.append([time, condition])\n\n    # Save weather data to CSV file\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file to the backup directory\n    backup_file_path = os.path.join(backup_dir, 'weather_data.csv')\n    shutil.copyfile(csv_file_path, backup_file_path)\n\n    return csv_file_path"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\ndef task_func(goals, penalties):\n    # Generate random goals and penalties\n    team_goals = {team: randint(0, goals) for team in TEAMS}\n    team_penalties = {team: randint(0, penalties) for team in TEAMS}\n\n    # Calculate penalty costs\n    team_penalty_costs = {team: cost * team_penalties[team] for team, cost in zip(TEAMS, team_penalties.values())}\n\n    # Create DataFrame\n    df = pd.DataFrame(list(zip(TEAMS, team_goals.values(), team_penalties.values(), team_penalty_costs.values())),\n                      columns=['Team', 'Goals', 'Penalties', 'Penalty Cost'])\n\n    # Create plots\n    fig, axes = plt.subplots(2, figsize=(10, 10))\n    sns.barplot(x='Team', y='Goals', data=df, ax=axes[0])\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=axes[1])\n\n    return df, axes", "raw_solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    # Generate random goals and penalties\n    team_goals = {team: randint(0, goals) for team in TEAMS}\n    team_penalties = {team: randint(0, penalties) for team in TEAMS}\n\n    # Calculate penalty costs\n    team_penalty_costs = {team: cost * team_penalties[team] for team, cost in zip(TEAMS, team_penalties.values())}\n\n    # Create DataFrame\n    df = pd.DataFrame(list(zip(TEAMS, team_goals.values(), team_penalties.values(), team_penalty_costs.values())),\n                      columns=['Team', 'Goals', 'Penalties', 'Penalty Cost'])\n\n    # Create plots\n    fig, axes = plt.subplots(2, figsize=(10, 10))\n    sns.barplot(x='Team', y='Goals', data=df, ax=axes[0])\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=axes[1])\n\n    return df, axes\n\n# Test the function\ndf, axes = task_func(10, 5)"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    # Create a DataFrame with random integer values between 0 and 9\n    df = pd.DataFrame(np.random.randint(0,10,size=(rows, len(COLUMNS))), columns=COLUMNS)\n\n    # Count the non-zero values in each column\n    non_zero_counts = df.ne(0).sum()\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n\n    ax.set_title('Non-zero values in each column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Counts')\n\n    return (df, ax)", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    # Create a DataFrame with random integer values between 0 and 9\n    df = pd.DataFrame(np.random.randint(0,10,size=(rows, len(COLUMNS))), columns=COLUMNS)\n\n    # Count the non-zero values in each column\n    non_zero_counts = df.ne(0).sum()\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n\n    ax.set_title('Non-zero values in each column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Counts')\n\n    return (df, ax)\n\n# Test the function\ndf, ax = task_func(100)\nplt.show()"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students):\n    np.random.seed(0)\n    courses = ['Math', 'English', 'Science', 'History', 'Art']\n    grades = np.random.randint(50, 100, (num_students, len(courses)))\n    students = [f'Student_{i}' for i in range(1, num_students+1)]\n\n    df = pd.DataFrame(grades, index=students, columns=courses)\n\n    avg_grades = df.mean()\n    passing_grades = (df >= 60).sum()\n\n    df_output = pd.DataFrame({'Average Grade': avg_grades, 'Passing Grades': passing_grades})\n\n    fig, ax = plt.subplots()\n    df_output[['Average Grade', 'Passing Grades']].plot(kind='bar', ax=ax)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_ylabel('Counts')\n    ax.legend(loc='upper left')\n\n    return df_output, ax", "raw_solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students):\n    np.random.seed(0)\n    courses = ['Math', 'English', 'Science', 'History', 'Art']\n    grades = np.random.randint(50, 100, (num_students, len(courses)))\n    students = [f'Student_{i}' for i in range(1, num_students+1)]\n\n    df = pd.DataFrame(grades, index=students, columns=courses)\n\n    avg_grades = df.mean()\n    passing_grades = (df >= 60).sum()\n\n    df_output = pd.DataFrame({'Average Grade': avg_grades, 'Passing Grades': passing_grades})\n\n    fig, ax = plt.subplots()\n    df_output[['Average Grade', 'Passing Grades']].plot(kind='bar', ax=ax)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_ylabel('Counts')\n    ax.legend(loc='upper left')\n\n    return df_output, ax\n\n# Test the function\ndf, ax = task_func(20)\nplt.show()"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    # Define the exponential decay function\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Find the indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)\n\n    # Extract the corresponding y values\n    y = array[indices, 1]\n\n    # Perform the curve fitting\n    popt, _ = optimize.curve_fit(func, indices, y)\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot the original data\n    ax.scatter(indices, y, color='red', label='Data')\n\n    # Plot the fitted curve\n    ax.plot(indices, func(indices, *popt), color='blue', label='Fit')\n\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    plt.show()\n\n    return popt, ax\narray = np.array([[1, 2], [2, 3], [3, 4], [1, 5], [2, 6], [3, 7]])", "raw_solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    # Define the exponential decay function\n    def func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Find the indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)\n\n    # Extract the corresponding y values\n    y = array[indices, 1]\n\n    # Perform the curve fitting\n    popt, _ = optimize.curve_fit(func, indices, y)\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot the original data\n    ax.scatter(indices, y, color='red', label='Data')\n\n    # Plot the fitted curve\n    ax.plot(indices, func(indices, *popt), color='blue', label='Fit')\n\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    plt.show()\n\n    return popt, ax\n\n# Test the function\narray = np.array([[1, 2], [2, 3], [3, 4], [1, 5], [2, 6], [3, 7]])\npopt, ax = task_func(array, 1)"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef preprocess_text(text):\n    # Remove non-alphanumeric characters\n    text = ALPHANUMERIC.sub(' ', text)\n    # Convert to lowercase\n    text = text.lower()\n    # Remove stopwords\n    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n    return text\ndef task_func(texts, num_topics):\n    # Preprocess texts\n    texts = [preprocess_text(text) for text in texts]\n    # Vectorize texts\n    vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(texts)\n    # Apply NMF\n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(tfidf_matrix)\n    # Extract topics\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_features_ind = topic.argsort()[:-num_topics - 1:-1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        topics.append(top_features)\n    return topics", "raw_solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef preprocess_text(text):\n    # Remove non-alphanumeric characters\n    text = ALPHANUMERIC.sub(' ', text)\n    # Convert to lowercase\n    text = text.lower()\n    # Remove stopwords\n    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n    return text\n\ndef task_func(texts, num_topics):\n    # Preprocess texts\n    texts = [preprocess_text(text) for text in texts]\n    # Vectorize texts\n    vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(texts)\n    # Apply NMF\n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(tfidf_matrix)\n    # Extract topics\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_features_ind = topic.argsort()[:-num_topics - 1:-1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        topics.append(top_features)\n    return topics"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    # If no stopwords provided, use nltk's english stopwords\n    if stopwords is None:\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n\n    # Clean texts\n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space\n        text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase\n        text = text.lower()\n        # Split into words\n        words = text.split()\n        # Remove stopwords\n        words = [word for word in words if word not in stopwords]\n        cleaned_texts.append(words)\n\n    # Train Word2Vec model\n    model = Word2Vec(cleaned_texts, min_count=1)\n\n    return model", "raw_solution": "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(texts, stopwords=None):\n    # If no stopwords provided, use nltk's english stopwords\n    if stopwords is None:\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n\n    # Clean texts\n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space\n        text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase\n        text = text.lower()\n        # Split into words\n        words = text.split()\n        # Remove stopwords\n        words = [word for word in words if word not in stopwords]\n        cleaned_texts.append(words)\n\n    # Train Word2Vec model\n    model = Word2Vec(cleaned_texts, min_count=1)\n\n    return model"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    # Check if the path exists, if not create it\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    # Check if the processed directory exists, if not create it\n    processed_dir = os.path.join(path, 'processed')\n    if not os.path.exists(processed_dir):\n        os.makedirs(processed_dir)\n\n    # Get a list of all json files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n\n    # Sort the files alphabetically\n    json_files.sort()\n\n    # Initialize an empty DataFrame\n    df = pd.DataFrame()\n\n    # Loop over all json files\n    for file in json_files:\n        # Read the json file into a DataFrame\n        temp_df = pd.read_json(os.path.join(path, file), orient='records')\n\n        # Add a 'Source' column with the filename\n        temp_df['Source'] = file\n\n        # Append the DataFrame to the main DataFrame\n        df = df.append(temp_df, ignore_index=True)\n\n        # Move the processed file to the processed directory\n        shutil.move(os.path.join(path, file), os.path.join(processed_dir, file))\n\n    return df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    # Check if the path exists, if not create it\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    # Check if the processed directory exists, if not create it\n    processed_dir = os.path.join(path, 'processed')\n    if not os.path.exists(processed_dir):\n        os.makedirs(processed_dir)\n\n    # Get a list of all json files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n\n    # Sort the files alphabetically\n    json_files.sort()\n\n    # Initialize an empty DataFrame\n    df = pd.DataFrame()\n\n    # Loop over all json files\n    for file in json_files:\n        # Read the json file into a DataFrame\n        temp_df = pd.read_json(os.path.join(path, file), orient='records')\n\n        # Add a 'Source' column with the filename\n        temp_df['Source'] = file\n\n        # Append the DataFrame to the main DataFrame\n        df = df.append(temp_df, ignore_index=True)\n\n        # Move the processed file to the processed directory\n        shutil.move(os.path.join(path, file), os.path.join(processed_dir, file))\n\n    return df"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Define the path to the CSV file\n    file_path = \"task_func_data/Output.txt\"\n\n    # Create the CSV file if it doesn't exist\n    if not os.path.exists(\"task_func_data\"):\n        os.makedirs(\"task_func_data\")\n\n    # Generate random temperature and humidity data\n    temperature = random.uniform(0, 100)\n    humidity = random.uniform(0, 100)\n\n    # Write the data to the CSV file in append mode\n    with open(file_path, 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([datetime.now(), temperature, humidity])\n\n    # Return the path to the CSV file\n    print(f\"CSV file path: {file_path}\")\n\n    # Delete the CSV file\n    os.remove(file_path)\n\n    return file_path", "raw_solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Define the path to the CSV file\n    file_path = \"task_func_data/Output.txt\"\n\n    # Create the CSV file if it doesn't exist\n    if not os.path.exists(\"task_func_data\"):\n        os.makedirs(\"task_func_data\")\n\n    # Generate random temperature and humidity data\n    temperature = random.uniform(0, 100)\n    humidity = random.uniform(0, 100)\n\n    # Write the data to the CSV file in append mode\n    with open(file_path, 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([datetime.now(), temperature, humidity])\n\n    # Return the path to the CSV file\n    print(f\"CSV file path: {file_path}\")\n\n    # Delete the CSV file\n    os.remove(file_path)\n\n    return file_path\n\n# Test the function\nfile_path = task_func()"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    # Open the URL\n    response = urllib.request.urlopen(url)\n    # Parse the HTML and save to BeautifulSoup object\n    soup = BeautifulSoup(response, 'html.parser')\n\n    # Find all table rows\n    rows = soup.find_all('tr')\n\n    # Open the CSV file for writing\n    with open(CSV_FILE_PATH, 'w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write each row to the CSV file\n        for row in rows:\n            row_data = [td.get_text(strip=True) for td in row.find_all('td')]\n            writer.writerow(row_data)\n\n    return CSV_FILE_PATH", "raw_solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    # Open the URL\n    response = urllib.request.urlopen(url)\n    # Parse the HTML and save to BeautifulSoup object\n    soup = BeautifulSoup(response, 'html.parser')\n\n    # Find all table rows\n    rows = soup.find_all('tr')\n\n    # Open the CSV file for writing\n    with open(CSV_FILE_PATH, 'w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write each row to the CSV file\n        for row in rows:\n            row_data = [td.get_text(strip=True) for td in row.find_all('td')]\n            writer.writerow(row_data)\n\n    return CSV_FILE_PATH"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data should be a pandas DataFrame\")\n    # Check if data is not empty\n    if data.empty:\n        raise ValueError(\"Data should not be empty\")\n    # Check if target_column is a column of data\n    if target_column not in data.columns:\n        raise ValueError(f\"{target_column} is not a column of data\")\n    # Check if data contains only numeric values\n    if not all(data.dtypes == 'float64'):\n        raise ValueError(\"Data should contain only numeric values\")\n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state should be an integer\")\n    # Check if test_size is between 0 and 1\n    if not 0 < test_size < 1:\n        raise ValueError(\"Test size should be between 0 and 1\")\n\n    # Split data into training and test sets\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Train a linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Return the model score of the test set\n    return model.score(X_test, y_test)", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data should be a pandas DataFrame\")\n    # Check if data is not empty\n    if data.empty:\n        raise ValueError(\"Data should not be empty\")\n    # Check if target_column is a column of data\n    if target_column not in data.columns:\n        raise ValueError(f\"{target_column} is not a column of data\")\n    # Check if data contains only numeric values\n    if not all(data.dtypes == 'float64'):\n        raise ValueError(\"Data should contain only numeric values\")\n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state should be an integer\")\n    # Check if test_size is between 0 and 1\n    if not 0 < test_size < 1:\n        raise ValueError(\"Test size should be between 0 and 1\")\n\n    # Split data into training and test sets\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Train a linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Return the model score of the test set\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport random\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \n    # Set the random seed for reproducibility\n    if rng_seed is not None:\n        random.seed(rng_seed)\n\n    # Generate random data\n    ids = list(range(1, 101))\n    names = latin_names + other_names\n    random.shuffle(names)\n    dobs = [datetime.datetime(random.randint(start_year, end_year), random.randint(1, 12), random.randint(1, 28)) for _ in range(100)]\n    emails = [f\"{name.lower()}{dob.year}@{email_domain}\" for name, dob in zip(names, dobs)]\n\n    # Create DataFrame\n    df = pd.DataFrame(list(zip(ids, names, dobs, emails)), \n                      columns =['ID', 'Name', 'Date of Birth', 'Email'])\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport random\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \n    # Set the random seed for reproducibility\n    if rng_seed is not None:\n        random.seed(rng_seed)\n\n    # Generate random data\n    ids = list(range(1, 101))\n    names = latin_names + other_names\n    random.shuffle(names)\n    dobs = [datetime.datetime(random.randint(start_year, end_year), random.randint(1, 12), random.randint(1, 28)) for _ in range(100)]\n    emails = [f\"{name.lower()}{dob.year}@{email_domain}\" for name, dob in zip(names, dobs)]\n\n    # Create DataFrame\n    df = pd.DataFrame(list(zip(ids, names, dobs, emails)), \n                      columns =['ID', 'Name', 'Date of Birth', 'Email'])\n\n    return df\n\n# Test the function\ndf = task_func(start_year=1980, end_year=2000, email_domain='test.at', rng_seed=3)"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\nFIELDNAMES = ['field', 'mean', 'median']\ndef task_func(input_file, output_file):\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize defaultdict to store mean and median\n    mean_median_dict = defaultdict(lambda: {'mean': 0, 'median': 0})\n\n    # Calculate mean and median for each key\n    for key in data[0].keys():\n        values = [d[key] for d in data if isinstance(d.get(key), (int, float))]\n        mean_median_dict[key]['mean'] = np.mean(values)\n        mean_median_dict[key]['median'] = np.median(values)\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.DictWriter(f, fieldnames=FIELDNAMES)\n        writer.writeheader()\n        for field, stats in mean_median_dict.items():\n            stats['field'] = field\n            writer.writerow(stats)\n\n    return dict(mean_median_dict)", "raw_solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\nFIELDNAMES = ['field', 'mean', 'median']\n\ndef task_func(input_file, output_file):\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize defaultdict to store mean and median\n    mean_median_dict = defaultdict(lambda: {'mean': 0, 'median': 0})\n\n    # Calculate mean and median for each key\n    for key in data[0].keys():\n        values = [d[key] for d in data if isinstance(d.get(key), (int, float))]\n        mean_median_dict[key]['mean'] = np.mean(values)\n        mean_median_dict[key]['median'] = np.median(values)\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.DictWriter(f, fieldnames=FIELDNAMES)\n        writer.writeheader()\n        for field, stats in mean_median_dict.items():\n            stats['field'] = field\n            writer.writerow(stats)\n\n    return dict(mean_median_dict)"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(file_dict, target_dir=\"non_none_files\"):\n    copied_files = []\n\n    # Create target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Iterate over the dictionary\n    for file_name, file_path in file_dict.items():\n        # Check if the file exists\n        if os.path.isfile(file_path):\n            # Check if the file is not empty\n            if os.path.getsize(file_path) > 0:\n                # Copy the file to the target directory\n                shutil.copy2(file_path, target_dir)\n                # Append the full file path to the list\n                copied_files.append(os.path.join(target_dir, file_name))\n\n    return copied_files", "raw_solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(file_dict, target_dir=\"non_none_files\"):\n    copied_files = []\n\n    # Create target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Iterate over the dictionary\n    for file_name, file_path in file_dict.items():\n        # Check if the file exists\n        if os.path.isfile(file_path):\n            # Check if the file is not empty\n            if os.path.getsize(file_path) > 0:\n                # Copy the file to the target directory\n                shutil.copy2(file_path, target_dir)\n                # Append the full file path to the list\n                copied_files.append(os.path.join(target_dir, file_name))\n\n    return copied_files"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    # Initialize an empty list to store the new filenames\n    new_files = []\n\n    # Get a list of all CSV files in the directory\n    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n\n    # Process each CSV file\n    for csv_file in csv_files:\n        # Check if the filename matches the pattern\n        match = re.match(pattern, csv_file)\n        if match:\n            # If the filename matches the pattern, create a new filename\n            new_filename = match.group(1) + '.csv'\n            new_filepath = os.path.join(directory, new_filename)\n\n            # If the new file already exists, append a number to the filename\n            i = 1\n            while os.path.exists(new_filepath):\n                new_filename = match.group(1) + '-' + str(i) + '.csv'\n                new_filepath = os.path.join(directory, new_filename)\n                i += 1\n\n            # Rename the file\n            os.rename(os.path.join(directory, csv_file), new_filepath)\n\n            # Add the new filename to the list\n            new_files.append(new_filename)\n\n    # Return the list of new filenames\n    return new_files", "raw_solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    # Initialize an empty list to store the new filenames\n    new_files = []\n\n    # Get a list of all CSV files in the directory\n    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n\n    # Process each CSV file\n    for csv_file in csv_files:\n        # Check if the filename matches the pattern\n        match = re.match(pattern, csv_file)\n        if match:\n            # If the filename matches the pattern, create a new filename\n            new_filename = match.group(1) + '.csv'\n            new_filepath = os.path.join(directory, new_filename)\n\n            # If the new file already exists, append a number to the filename\n            i = 1\n            while os.path.exists(new_filepath):\n                new_filename = match.group(1) + '-' + str(i) + '.csv'\n                new_filepath = os.path.join(directory, new_filename)\n                i += 1\n\n            # Rename the file\n            os.rename(os.path.join(directory, csv_file), new_filepath)\n\n            # Add the new filename to the list\n            new_files.append(new_filename)\n\n    # Return the list of new filenames\n    return new_files"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    for filename in os.listdir(directory):\n        match = re.match(pattern, filename)\n        if match:\n            prefix = match.group(1)\n            if not os.path.exists(os.path.join(directory, prefix)):\n                os.mkdir(os.path.join(directory, prefix))\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(os.path.join(directory, prefix))\n            extracted_dirs.append(prefix)\n            os.remove(os.path.join(directory, filename))\n    return extracted_dirs", "raw_solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    for filename in os.listdir(directory):\n        match = re.match(pattern, filename)\n        if match:\n            prefix = match.group(1)\n            if not os.path.exists(os.path.join(directory, prefix)):\n                os.mkdir(os.path.join(directory, prefix))\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(os.path.join(directory, prefix))\n            extracted_dirs.append(prefix)\n            os.remove(os.path.join(directory, filename))\n    return extracted_dirs"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nimport shutil\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find files that match the pattern\n    files = glob.glob(pattern)\n\n    # Archive and delete each file\n    for file in files:\n        # Get the base name of the file\n        base_name = os.path.basename(file)\n\n        # Create the archive file path\n        archive_file = os.path.join(ARCHIVE_DIR, base_name)\n\n        # Archive the file\n        shutil.move(file, archive_file)\n\n    # Return the archive file path\n    return archive_file", "raw_solution": "import subprocess\nimport os\nimport glob\nimport shutil\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find files that match the pattern\n    files = glob.glob(pattern)\n\n    # Archive and delete each file\n    for file in files:\n        # Get the base name of the file\n        base_name = os.path.basename(file)\n\n        # Create the archive file path\n        archive_file = os.path.join(ARCHIVE_DIR, base_name)\n\n        # Archive the file\n        shutil.move(file, archive_file)\n\n    # Return the archive file path\n    return archive_file\n\n# Test the function\narchive_file = task_func('/tmp/*.txt')"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize counters\n    total_goals = 0\n    total_penalties = 0\n\n    # Open CSV file\n    with open(csv_file_path, 'r') as csv_file:\n        reader = csv.reader(csv_file)\n        next(reader)  # Skip header\n\n        # Iterate over rows\n        for row in reader:\n            # Add goals and penalties to total\n            total_goals += int(row[1])\n            total_penalties += int(row[2])\n\n    # Add given goals and penalties to total\n    total_goals += goals\n    total_penalties += penalties\n\n    # Return Counter object with total counts of goals and penalties\n    return Counter({'goals': total_goals, 'penalties': total_penalties})", "raw_solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize counters\n    total_goals = 0\n    total_penalties = 0\n\n    # Open CSV file\n    with open(csv_file_path, 'r') as csv_file:\n        reader = csv.reader(csv_file)\n        next(reader)  # Skip header\n\n        # Iterate over rows\n        for row in reader:\n            # Add goals and penalties to total\n            total_goals += int(row[1])\n            total_penalties += int(row[2])\n\n    # Add given goals and penalties to total\n    total_goals += goals\n    total_penalties += penalties\n\n    # Return Counter object with total counts of goals and penalties\n    return Counter({'goals': total_goals, 'penalties': total_penalties})\n\n# Setup CSV file for testing\nsetup_csv()\n\n# Test task_func"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(source_dir) or not os.path.exists(target_dir):\n        raise ValueError(\"Source or target directory does not exist.\")\n\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count\nsource_dir = \"/path/to/source\"\ntarget_dir = \"/path/to/target\"", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(source_dir) or not os.path.exists(target_dir):\n        raise ValueError(\"Source or target directory does not exist.\")\n\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count\n\n# Test the function\nsource_dir = \"/path/to/source\"\ntarget_dir = \"/path/to/target\""}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    # Clean the texts\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Compute term frequency\n    counter1 = Counter(text1.split())\n    counter2 = Counter(text2.split())\n\n    # Compute cosine similarity\n    cosine_similarity = sum(counter1.get(k, 0) * counter2.get(k, 0) for k in set(counter1.keys()).union(counter2.keys())) / (np.sqrt(sum(counter1[k]**2 for k in counter1.keys()) * sum(counter2[k]**2 for k in counter2.keys())) + 1e-10)\n\n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return (cosine_similarity, levenshtein_ratio)\ntext1 = \"Hello, world!\"\ntext2 = \"hello, world!\"", "raw_solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    # Clean the texts\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Compute term frequency\n    counter1 = Counter(text1.split())\n    counter2 = Counter(text2.split())\n\n    # Compute cosine similarity\n    cosine_similarity = sum(counter1.get(k, 0) * counter2.get(k, 0) for k in set(counter1.keys()).union(counter2.keys())) / (np.sqrt(sum(counter1[k]**2 for k in counter1.keys()) * sum(counter2[k]**2 for k in counter2.keys())) + 1e-10)\n\n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return (cosine_similarity, levenshtein_ratio)\n\n# Test the function\ntext1 = \"Hello, world!\"\ntext2 = \"hello, world!\""}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input should be a list of integers.\")\n    if not all(isinstance(i, int) for i in numbers):\n        raise TypeError(\"Input should be a list of integers.\")\n    if any(i < 0 for i in numbers):\n        raise ValueError(\"Input should not contain negative numbers.\")\n    \n    fac = []\n    perm = []\n    \n    if numbers:\n        perm = [i for i in permutations(numbers, len(numbers))]\n        fac = [sum(math.factorial(n) for n in p) for p in perm]\n    \n    return fac, perm", "raw_solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input should be a list of integers.\")\n    if not all(isinstance(i, int) for i in numbers):\n        raise TypeError(\"Input should be a list of integers.\")\n    if any(i < 0 for i in numbers):\n        raise ValueError(\"Input should not contain negative numbers.\")\n    \n    fac = []\n    perm = []\n    \n    if numbers:\n        perm = [i for i in permutations(numbers, len(numbers))]\n        fac = [sum(math.factorial(n) for n in p) for p in perm]\n    \n    return fac, perm"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    if not os.path.exists(SOURCE_DIR):\n        warnings.warn(f\"Source directory {SOURCE_DIR} does not exist.\")\n        return []\n\n    if not os.path.exists(DEST_DIR):\n        warnings.warn(f\"Destination directory {DEST_DIR} does not exist.\")\n        return []\n\n    transferred_files = []\n\n    for extension in EXTENSIONS:\n        files = glob.glob(os.path.join(SOURCE_DIR, f'*.{extension}'))\n\n        for file in files:\n            try:\n                shutil.move(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Could not transfer file {file} due to {str(e)}\")\n\n    return transferred_files", "raw_solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    if not os.path.exists(SOURCE_DIR):\n        warnings.warn(f\"Source directory {SOURCE_DIR} does not exist.\")\n        return []\n\n    if not os.path.exists(DEST_DIR):\n        warnings.warn(f\"Destination directory {DEST_DIR} does not exist.\")\n        return []\n\n    transferred_files = []\n\n    for extension in EXTENSIONS:\n        files = glob.glob(os.path.join(SOURCE_DIR, f'*.{extension}'))\n\n        for file in files:\n            try:\n                shutil.move(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Could not transfer file {file} due to {str(e)}\")\n\n    return transferred_files"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Convert the input data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n\n    # Normalize the counts using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n\n    # Normalize the weights using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    # Return the DataFrame with the normalized counts and weights\n    return df[['Item', 'Normalized Count', 'Normalized Weight']]\ndata = [('Item1', 10, 20), ('Item2', 20, 30), ('Item3', 30, 40)]", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Convert the input data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n\n    # Normalize the counts using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n\n    # Normalize the weights using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    # Return the DataFrame with the normalized counts and weights\n    return df[['Item', 'Normalized Count', 'Normalized Weight']]\n\n# Test the function\ndata = [('Item1', 10, 20), ('Item2', 20, 30), ('Item3', 30, 40)]"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize an empty list to store the mean values\n    mean_values = []\n    \n    # Iterate over the tuples in the list\n    for i in range(len(data_list[0])):\n        # Initialize a list to store the numerical values at the current position\n        num_values = []\n        \n        # Iterate over the tuples in the list\n        for j in range(len(data_list)):\n            # Check if the current position in the tuple is a number\n            if isinstance(data_list[j][i], (int, float)):\n                # If it is, add it to the list of numerical values\n                num_values.append(data_list[j][i])\n        \n        # Calculate the mean of the numerical values at the current position\n        mean = np.mean(num_values) if num_values else np.nan\n        \n        # Add the mean to the list of mean values\n        mean_values.append(mean)\n    \n    # Create a pandas DataFrame with the mean values\n    df = pd.DataFrame(mean_values, columns=['Mean Value'], index=['Position ' + str(i) for i in range(len(mean_values))])\n    \n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize an empty list to store the mean values\n    mean_values = []\n    \n    # Iterate over the tuples in the list\n    for i in range(len(data_list[0])):\n        # Initialize a list to store the numerical values at the current position\n        num_values = []\n        \n        # Iterate over the tuples in the list\n        for j in range(len(data_list)):\n            # Check if the current position in the tuple is a number\n            if isinstance(data_list[j][i], (int, float)):\n                # If it is, add it to the list of numerical values\n                num_values.append(data_list[j][i])\n        \n        # Calculate the mean of the numerical values at the current position\n        mean = np.mean(num_values) if num_values else np.nan\n        \n        # Add the mean to the list of mean values\n        mean_values.append(mean)\n    \n    # Create a pandas DataFrame with the mean values\n    df = pd.DataFrame(mean_values, columns=['Mean Value'], index=['Position ' + str(i) for i in range(len(mean_values))])\n    \n    return df\n\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"Data is empty\")\n\n    # Check if col1 and col2 are in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"One or both of the columns are not in the data\")\n\n    # Check if one or both of the columns have multiple categories\n    if data[col1].nunique() == 1 or data[col2].nunique() == 1:\n        raise ValueError(\"One or both of the columns do not have multiple categories\")\n\n    # Check if all categories have at least 5 observations\n    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\n        raise ValueError(\"Some categories have less than 5 observations\")\n\n    # Check if one or both of the columns contain non-categorical data\n    if not pd.api.types.is_categorical_dtype(data[col1]) or not pd.api.types.is_categorical_dtype(data[col2]):\n        raise TypeError(\"One or both of the columns contain non-categorical data\")\n\n    # Construct contingency table and perform chi-square test\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"Data is empty\")\n\n    # Check if col1 and col2 are in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"One or both of the columns are not in the data\")\n\n    # Check if one or both of the columns have multiple categories\n    if data[col1].nunique() == 1 or data[col2].nunique() == 1:\n        raise ValueError(\"One or both of the columns do not have multiple categories\")\n\n    # Check if all categories have at least 5 observations\n    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\n        raise ValueError(\"Some categories have less than 5 observations\")\n\n    # Check if one or both of the columns contain non-categorical data\n    if not pd.api.types.is_categorical_dtype(data[col1]) or not pd.api.types.is_categorical_dtype(data[col2]):\n        raise TypeError(\"One or both of the columns contain non-categorical data\")\n\n    # Construct contingency table and perform chi-square test\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    frequencies = np.bincount(results)\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequencies)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    return (frequencies, ax.figure)", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    frequencies = np.bincount(results)\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequencies)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    return (frequencies, ax.figure)\n\n# Test the function\nfrequencies, figure = task_func(1000, seed=42)"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Archive all files with '_processed' in their name\n    with zipfile.ZipFile(os.path.join(target_dir, archive_name), 'w') as zipf:\n        for root, dirs, files in os.walk(source_dir):\n            for file in files:\n                if '_processed' in file:\n                    file_path = os.path.join(root, file)\n                    zipf.write(file_path, arcname=file)\n\n    # Move processed files to target directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            if '_processed' in file:\n                file_path = os.path.join(root, file)\n                shutil.move(file_path, os.path.join(target_dir, file))\n\n    return os.path.join(target_dir, archive_name)\nsource_dir = '/path/to/source/directory'\ntarget_dir = '/path/to/target/directory'", "raw_solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Archive all files with '_processed' in their name\n    with zipfile.ZipFile(os.path.join(target_dir, archive_name), 'w') as zipf:\n        for root, dirs, files in os.walk(source_dir):\n            for file in files:\n                if '_processed' in file:\n                    file_path = os.path.join(root, file)\n                    zipf.write(file_path, arcname=file)\n\n    # Move processed files to target directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            if '_processed' in file:\n                file_path = os.path.join(root, file)\n                shutil.move(file_path, os.path.join(target_dir, file))\n\n    return os.path.join(target_dir, archive_name)\n\n# Test the function\nsource_dir = '/path/to/source/directory'\ntarget_dir = '/path/to/target/directory'\narchive_path = task_func(source_dir, target_dir)"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Resample the data to daily frequency and fill missing values\n    df = df.resample('D').mean().fillna(method='ffill')\n\n    # Create a new column 'Time' which represents the time in seconds since the start of the data\n    df['Time'] = (df.index - df.index[0]).total_seconds()\n\n    # Reshape the data for the linear regression model\n    X = df['Time'].values.reshape(-1, 1)\n    y = df['Close'].values.reshape(-1, 1)\n\n    # Create and fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Generate future timestamps\n    future_timestamps = pd.date_range(start=df.index[-1], periods=7, freq='D')\n    future_timestamps = (future_timestamps - df.index[0]).total_seconds()\n\n    # Predict the closing prices for the next 7 days\n    future_X = future_timestamps.reshape(-1, 1)\n    future_y = model.predict(future_X)\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df['Close'], label='Actual')\n    plt.plot(future_timestamps, future_y, label='Predicted')\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Prices Prediction')\n    plt.legend()\n    plt.grid(True)\n\n    return (future_y.flatten().tolist(), plt.gca())", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Resample the data to daily frequency and fill missing values\n    df = df.resample('D').mean().fillna(method='ffill')\n\n    # Create a new column 'Time' which represents the time in seconds since the start of the data\n    df['Time'] = (df.index - df.index[0]).total_seconds()\n\n    # Reshape the data for the linear regression model\n    X = df['Time'].values.reshape(-1, 1)\n    y = df['Close'].values.reshape(-1, 1)\n\n    # Create and fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Generate future timestamps\n    future_timestamps = pd.date_range(start=df.index[-1], periods=7, freq='D')\n    future_timestamps = (future_timestamps - df.index[0]).total_seconds()\n\n    # Predict the closing prices for the next 7 days\n    future_X = future_timestamps.reshape(-1, 1)\n    future_y = model.predict(future_X)\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df['Close'], label='Actual')\n    plt.plot(future_timestamps, future_y, label='Predicted')\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Prices Prediction')\n    plt.legend()\n    plt.grid(True)\n\n    return (future_y.flatten().tolist(), plt.gca())"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n    df['z_score'] = zscore(df['closing_price'])\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.scatter(df.index, df['closing_price'])\n    ax.scatter(outliers.index, outliers['closing_price'], color='r')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    return outliers, ax.figure", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    df['z_score'] = zscore(df['closing_price'])\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.scatter(df.index, df['closing_price'])\n    ax.scatter(outliers.index, outliers['closing_price'], color='r')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    return outliers, ax.figure\n\n# Test the function\ndf2 = pd.DataFrame({\n    'closing_price': [10, 20, 30, 40, 50, 100]\n})\n\noutliers2, plot2 = task_func(df2, z_threshold=1.5)"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    # Ensure the DataFrame has a 'Close' column\n    assert 'Close' in df.columns, \"DataFrame must have a 'Close' column\"\n\n    # Create a new figure\n    fig, axes = plt.subplots(2, figsize=(10,10))\n\n    # Create a boxplot\n    sns.boxplot(df['Close'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n\n    # Create a histogram\n    sns.histplot(df['Close'], ax=axes[1], kde=True)\n    axes[1].set_title('Histogram of Closing Prices')\n\n    # Return the axes\n    return (axes[0], axes[1])", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    # Ensure the DataFrame has a 'Close' column\n    assert 'Close' in df.columns, \"DataFrame must have a 'Close' column\"\n\n    # Create a new figure\n    fig, axes = plt.subplots(2, figsize=(10,10))\n\n    # Create a boxplot\n    sns.boxplot(df['Close'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n\n    # Create a histogram\n    sns.histplot(df['Close'], ax=axes[1], kde=True)\n    axes[1].set_title('Histogram of Closing Prices')\n\n    # Return the axes\n    return (axes[0], axes[1])"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Fit the model\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit(disp=0)\n\n    # Forecast\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot\n    fig, ax = plt.subplots(1, 1)\n    ax.plot(df['Close'])\n    ax.plot(pd.date_range(start=df.index[-1], periods=8)[1:], forecast)\n\n    return forecast.tolist(), ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Fit the model\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit(disp=0)\n\n    # Forecast\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot\n    fig, ax = plt.subplots(1, 1)\n    ax.plot(df['Close'])\n    ax.plot(pd.date_range(start=df.index[-1], periods=8)[1:], forecast)\n\n    return forecast.tolist(), ax"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=2)]\n\n    # Create a dictionary with all combinations and their counts\n    word_counter = Counter(word)\n    result = {key: sum(word_counter[c] for c in combinations if c.startswith(key)) for key in combinations}\n\n    return result\nword = \"hello\"", "raw_solution": "from collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=2)]\n\n    # Create a dictionary with all combinations and their counts\n    word_counter = Counter(word)\n    result = {key: sum(word_counter[c] for c in combinations if c.startswith(key)) for key in combinations}\n\n    return result\n\n# Test the function\nword = \"hello\""}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create a DataFrame\n    df = pd.DataFrame(dates, columns=['Date'])\n    df['Category'] = np.random.choice(categories, size=periods)\n    df['Sales'] = np.random.randint(100, 1000, size=periods)\n\n    # Plot the data\n    ax = df.groupby('Category')['Sales'].sum().plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Sales Report for Different Categories')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Sales')\n\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create a DataFrame\n    df = pd.DataFrame(dates, columns=['Date'])\n    df['Category'] = np.random.choice(categories, size=periods)\n    df['Sales'] = np.random.randint(100, 1000, size=periods)\n\n    # Plot the data\n    ax = df.groupby('Category')['Sales'].sum().plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Sales Report for Different Categories')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Sales')\n\n    return df, ax.figure\n\n# Test the function\ndf, fig = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    if sales_data is None:\n        sales_data = pd.DataFrame(np.random.randint(1, 100, size=(periods, 1)), columns=['Sales'], \n                                 index=pd.date_range(start=start_date, periods=periods, freq=freq))\n    else:\n        sales_data = pd.DataFrame(sales_data, columns=['Sales'], \n                                 index=pd.date_range(start=start_date, periods=periods, freq=freq))\n\n    # Create a new column 'Time' to represent time step\n    sales_data['Time'] = range(1, len(sales_data) + 1)\n    sales_data = sales_data[['Time', 'Sales']]\n\n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(sales_data[['Time']], sales_data['Sales'])\n\n    # Generate future time steps\n    future_time = pd.DataFrame(range(len(sales_data) + 1, len(sales_data) + len(sales_data)), columns=['Time'])\n\n    # Predict future sales\n    future_sales = model.predict(future_time)\n\n    return future_sales", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    if sales_data is None:\n        sales_data = pd.DataFrame(np.random.randint(1, 100, size=(periods, 1)), columns=['Sales'], \n                                 index=pd.date_range(start=start_date, periods=periods, freq=freq))\n    else:\n        sales_data = pd.DataFrame(sales_data, columns=['Sales'], \n                                 index=pd.date_range(start=start_date, periods=periods, freq=freq))\n\n    # Create a new column 'Time' to represent time step\n    sales_data['Time'] = range(1, len(sales_data) + 1)\n    sales_data = sales_data[['Time', 'Sales']]\n\n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(sales_data[['Time']], sales_data['Sales'])\n\n    # Generate future time steps\n    future_time = pd.DataFrame(range(len(sales_data) + 1, len(sales_data) + len(sales_data)), columns=['Time'])\n\n    # Predict future sales\n    future_sales = model.predict(future_time)\n\n    return future_sales\n\n# Test the function"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n    if seed is not None:\n        random.seed(seed)\n    today = datetime.now().date()\n    data = []\n    for task in random.sample(task_list, n_tasks):\n        task_name = task.replace(\" \", \"_\")\n        assigned_to = random.choice(employees)\n        data.append([task_name, assigned_to, today])\n    df = pd.DataFrame(data, columns=[\"Task Name\", \"Assigned To\", \"Due Date\"])\n    return df\ntask_list = [\"Task 1\", \"Task 2\", \"Task 3\", \"Task 4\", \"Task 5\", \"Task 6\", \"Task 7\", \"Task 8\", \"Task 9\", \"Task 10\"]", "raw_solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n    if seed is not None:\n        random.seed(seed)\n    today = datetime.now().date()\n    data = []\n    for task in random.sample(task_list, n_tasks):\n        task_name = task.replace(\" \", \"_\")\n        assigned_to = random.choice(employees)\n        data.append([task_name, assigned_to, today])\n    df = pd.DataFrame(data, columns=[\"Task Name\", \"Assigned To\", \"Due Date\"])\n    return df\n\n# Test the function\ntask_list = [\"Task 1\", \"Task 2\", \"Task 3\", \"Task 4\", \"Task 5\", \"Task 6\", \"Task 7\", \"Task 8\", \"Task 9\", \"Task 10\"]"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty\")\n\n    # Replace spaces with underscores\n    text = text.lower()\n    text = re.sub(r'\\s', '_', text)\n\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n\n    # Create a list of words and their counts\n    words = list(word_counts.keys())\n    counts = list(word_counts.values())\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n\n    # Set the x-axis label and y-axis label\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n\n    # Rotate the x-axis labels for better readability\n    plt.xticks(rotation=90)\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty\")\n\n    # Replace spaces with underscores\n    text = text.lower()\n    text = re.sub(r'\\s', '_', text)\n\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n\n    # Create a list of words and their counts\n    words = list(word_counts.keys())\n    counts = list(word_counts.values())\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n\n    # Set the x-axis label and y-axis label\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n\n    # Rotate the x-axis labels for better readability\n    plt.xticks(rotation=90)\n\n    return ax"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n\n    # Create target directory if it does not exist\n    if not os.path.isdir(target_directory):\n        os.makedirs(target_directory)\n\n    # Get all files with valid extensions from source directory\n    files_to_zip = []\n    for extension in valid_extensions:\n        files_to_zip.extend(glob.glob(f\"{source_directory}/*{extension}\"))\n\n    # Create zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files_to_zip:\n            zipf.write(file, arcname=Path(file).name)\n\n    return zip_path", "raw_solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n\n    # Create target directory if it does not exist\n    if not os.path.isdir(target_directory):\n        os.makedirs(target_directory)\n\n    # Get all files with valid extensions from source directory\n    files_to_zip = []\n    for extension in valid_extensions:\n        files_to_zip.extend(glob.glob(f\"{source_directory}/*{extension}\"))\n\n    # Create zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files_to_zip:\n            zipf.write(file, arcname=Path(file).name)\n\n    return zip_path"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\nimport zipfile\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise FileNotFoundError(\"Source directory does not exist.\")\n\n    # Get list of files in source directory\n    files = os.listdir(source_directory)\n\n    # Filter files by extension\n    txt_files = [f for f in files if f.endswith('.txt')]\n    docx_files = [f for f in files if f.endswith('.docx')]\n    xlsx_files = [f for f in files if f.endswith('.xlsx')]\n    csv_files = [f for f in files if f.endswith('.csv')]\n\n    # Initialize counter\n    count = 0\n\n    # Convert .txt files\n    for txt_file in txt_files:\n        # Read text file\n        with open(os.path.join(source_directory, txt_file), 'r') as file:\n            data = file.read()\n\n        # Save as CSV\n        df = pd.DataFrame({'Text': [data]})\n        df.to_csv(os.path.join(target_directory, txt_file.replace('.txt', '.csv')), index=False)\n\n        count += 1\n\n    # Convert .docx files\n    for docx_file in docx_files:\n        # Read docx file\n        doc = docx.Document(os.path.join(source_directory, docx_file))\n        data = ' '.join([p.text for p in doc.paragraphs])\n\n        # Save as CSV\n        df = pd.DataFrame({'Text': [data]})\n        df.to_csv(os.path.join(target_directory, docx_file.replace('.docx', '.csv')), index=False)\n\n        count += 1\n\n    # Convert .xlsx files\n    for xlsx_file in xlsx_files:\n        # Read xlsx file\n        xls = pd.ExcelFile(os.path.join(source_directory, xlsx_file))\n        sheets = xls.sheet_names()\n        data = []\n        for sheet in sheets:\n            df = pd.read_excel(os.path.join(source_directory, xlsx_file), sheet_name=sheet)\n            data.append(df.to_csv(index=False))\n\n        # Save as CSV\n        df = pd.DataFrame({'Text': data})\n        df.to_csv(os.path.join(target_directory, xlsx_file.replace('.xlsx', '.csv')), index=False)\n\n        count += 1\n\n    # Return number of files converted\n    return count", "raw_solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\nimport zipfile\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise FileNotFoundError(\"Source directory does not exist.\")\n\n    # Get list of files in source directory\n    files = os.listdir(source_directory)\n\n    # Filter files by extension\n    txt_files = [f for f in files if f.endswith('.txt')]\n    docx_files = [f for f in files if f.endswith('.docx')]\n    xlsx_files = [f for f in files if f.endswith('.xlsx')]\n    csv_files = [f for f in files if f.endswith('.csv')]\n\n    # Initialize counter\n    count = 0\n\n    # Convert .txt files\n    for txt_file in txt_files:\n        # Read text file\n        with open(os.path.join(source_directory, txt_file), 'r') as file:\n            data = file.read()\n\n        # Save as CSV\n        df = pd.DataFrame({'Text': [data]})\n        df.to_csv(os.path.join(target_directory, txt_file.replace('.txt', '.csv')), index=False)\n\n        count += 1\n\n    # Convert .docx files\n    for docx_file in docx_files:\n        # Read docx file\n        doc = docx.Document(os.path.join(source_directory, docx_file))\n        data = ' '.join([p.text for p in doc.paragraphs])\n\n        # Save as CSV\n        df = pd.DataFrame({'Text': [data]})\n        df.to_csv(os.path.join(target_directory, docx_file.replace('.docx', '.csv')), index=False)\n\n        count += 1\n\n    # Convert .xlsx files\n    for xlsx_file in xlsx_files:\n        # Read xlsx file\n        xls = pd.ExcelFile(os.path.join(source_directory, xlsx_file))\n        sheets = xls.sheet_names()\n        data = []\n        for sheet in sheets:\n            df = pd.read_excel(os.path.join(source_directory, xlsx_file), sheet_name=sheet)\n            data.append(df.to_csv(index=False))\n\n        # Save as CSV\n        df = pd.DataFrame({'Text': data})\n        df.to_csv(os.path.join(target_directory, xlsx_file.replace('.xlsx', '.csv')), index=False)\n\n        count += 1\n\n    # Return number of files converted\n    return count"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input should be a pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame contains NaN values\")\n    if not df.select_dtypes(include=[np.number]).empty:\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_normalized.cumsum()", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input should be a pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame contains NaN values\")\n    if not df.select_dtypes(include=[np.number]).empty:\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_normalized.cumsum()"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    if not os.path.isdir(directory_path):\n        raise ValueError(\"Provided directory does not exist.\")\n\n    result = []\n    for file_name in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, file_name)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path)\n            creation_time = os.path.getctime(file_path)\n            modification_time = os.path.getmtime(file_path)\n            creation_time = datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat()\n            modification_time = datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat()\n            result.append((file_name, file_size, creation_time, modification_time))\n\n    return result", "raw_solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    if not os.path.isdir(directory_path):\n        raise ValueError(\"Provided directory does not exist.\")\n\n    result = []\n    for file_name in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, file_name)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path)\n            creation_time = os.path.getctime(file_path)\n            modification_time = os.path.getmtime(file_path)\n            creation_time = datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat()\n            modification_time = datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat()\n            result.append((file_name, file_size, creation_time, modification_time))\n\n    return result"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        # Check if JSON data is malformed\n        json_data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n\n    # Check if JSON data is empty\n    if not json_data:\n        raise ValueError(\"The JSON data is empty.\")\n\n    # Create a DataFrame from the JSON data\n    df = pd.DataFrame(json_data)\n\n    # Check if country names are non-string\n    if not all(df['Country'].apply(lambda x: isinstance(x, str))):\n        raise ValueError(\"Non-string country names are not allowed.\")\n\n    # Check if populations are non-numeric or negative\n    if not all(df['Population'].apply(lambda x: isinstance(x, (int, float)) and x >= 0)):\n        raise ValueError(\"Non-numeric or negative populations are not allowed.\")\n\n    # Round down the population to the nearest integer\n    df['Population'] = df['Population'].apply(lambda x: math.floor(x))\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Save the DataFrame to a CSV file\n    file_path = os.path.join(output_dir, file_name)\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"The file cannot be written to the specified directory.\")\n\n    return file_path, df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        # Check if JSON data is malformed\n        json_data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n\n    # Check if JSON data is empty\n    if not json_data:\n        raise ValueError(\"The JSON data is empty.\")\n\n    # Create a DataFrame from the JSON data\n    df = pd.DataFrame(json_data)\n\n    # Check if country names are non-string\n    if not all(df['Country'].apply(lambda x: isinstance(x, str))):\n        raise ValueError(\"Non-string country names are not allowed.\")\n\n    # Check if populations are non-numeric or negative\n    if not all(df['Population'].apply(lambda x: isinstance(x, (int, float)) and x >= 0)):\n        raise ValueError(\"Non-numeric or negative populations are not allowed.\")\n\n    # Round down the population to the nearest integer\n    df['Population'] = df['Population'].apply(lambda x: math.floor(x))\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Save the DataFrame to a CSV file\n    file_path = os.path.join(output_dir, file_name)\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"The file cannot be written to the specified directory.\")\n\n    return file_path, df"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(\"Specified directory does not exist or is not a directory.\")\n    \n    result = {}\n    for item in os.listdir(dir_path):\n        item_path = os.path.join(dir_path, item)\n        for predicate in predicates:\n            if re.search(predicate, item):\n                result[item] = True\n                break\n        else:\n            result[item] = False\n    return result", "raw_solution": "import os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(\"Specified directory does not exist or is not a directory.\")\n    \n    result = {}\n    for item in os.listdir(dir_path):\n        item_path = os.path.join(dir_path, item)\n        for predicate in predicates:\n            if re.search(predicate, item):\n                result[item] = True\n                break\n        else:\n            result[item] = False\n    return result"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    # Convert hexadecimal to bytes\n    bytes_obj = binascii.unhexlify(hex_string)\n\n    # Convert bytes to UTF-8 string\n    utf8_str = bytes_obj.decode('utf-8')\n\n    # Initialize dictionary\n    encodings = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_obj).decode('utf-8'),\n        'utf-8': utf8_str,\n        'utf-16': utf8_str,\n        'utf-32': utf8_str,\n        'ASCII': 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_str),\n        'ROT13': codecs.encode(utf8_str, 'rot_13')\n    }\n\n    # Check if string can be represented in ASCII\n    for char in utf8_str:\n        if ord(char) > 127:\n            encodings['ASCII'] = 'Not representable in ASCII'\n            break\n        else:\n            encodings['ASCII'] = utf8_str\n\n    return encodings", "raw_solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    # Convert hexadecimal to bytes\n    bytes_obj = binascii.unhexlify(hex_string)\n\n    # Convert bytes to UTF-8 string\n    utf8_str = bytes_obj.decode('utf-8')\n\n    # Initialize dictionary\n    encodings = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_obj).decode('utf-8'),\n        'utf-8': utf8_str,\n        'utf-16': utf8_str,\n        'utf-32': utf8_str,\n        'ASCII': 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_str),\n        'ROT13': codecs.encode(utf8_str, 'rot_13')\n    }\n\n    # Check if string can be represented in ASCII\n    for char in utf8_str:\n        if ord(char) > 127:\n            encodings['ASCII'] = 'Not representable in ASCII'\n            break\n        else:\n            encodings['ASCII'] = utf8_str\n\n    return encodings\n\n# Test the function"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        return False\n\n    # Calculate the MD5 checksum of the downloaded file\n    with open(TARGET_TAR_FILE, 'rb') as f:\n        md5_returned = hashlib.md5(f.read()).hexdigest()\n\n    # Check if the MD5 checksum matches the expected value\n    if md5_returned != EXPECTED_MD5_CHECKSUM:\n        print(\"MD5 checksum does not match. Deleting the file.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # If the MD5 checksum matches, extract the contents of the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, 'r:gz') as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Extraction failed: {e}\")\n        return False\n\n    # Remove the downloaded file\n    os.remove(TARGET_TAR_FILE)\n\n    return True", "raw_solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        return False\n\n    # Calculate the MD5 checksum of the downloaded file\n    with open(TARGET_TAR_FILE, 'rb') as f:\n        md5_returned = hashlib.md5(f.read()).hexdigest()\n\n    # Check if the MD5 checksum matches the expected value\n    if md5_returned != EXPECTED_MD5_CHECKSUM:\n        print(\"MD5 checksum does not match. Deleting the file.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # If the MD5 checksum matches, extract the contents of the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, 'r:gz') as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Extraction failed: {e}\")\n        return False\n\n    # Remove the downloaded file\n    os.remove(TARGET_TAR_FILE)\n\n    return True"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except urllib.error.URLError:\n        print(\"Failed to download the file from the provided URL.\")\n        return\n\n    if not os.path.isfile(csv_file_path):\n        print(\"Failed to download the file from the provided URL.\")\n        return\n\n    try:\n        with open(csv_file_path, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            column_values = [row[column_name] for row in csv_reader if column_name in row]\n\n            if not column_values:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n\n            count_dict = dict(collections.Counter(column_values))\n\n            # Delete the downloaded file\n            os.remove(csv_file_path)\n\n            return count_dict\n    except csv.Error:\n        print(\"Failed to read the CSV file.\")\n        return", "raw_solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except urllib.error.URLError:\n        print(\"Failed to download the file from the provided URL.\")\n        return\n\n    if not os.path.isfile(csv_file_path):\n        print(\"Failed to download the file from the provided URL.\")\n        return\n\n    try:\n        with open(csv_file_path, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            column_values = [row[column_name] for row in csv_reader if column_name in row]\n\n            if not column_values:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n\n            count_dict = dict(collections.Counter(column_values))\n\n            # Delete the downloaded file\n            os.remove(csv_file_path)\n\n            return count_dict\n    except csv.Error:\n        print(\"Failed to read the CSV file.\")\n        return"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        response = urllib.request.urlopen(url)\n        xml_content = response.read()\n\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n\n        # Find all 'item' elements\n        items = root.findall('.//item')\n\n        # Create a DataFrame from the 'item' elements\n        data = []\n        for item in items:\n            item_data = {}\n            for child in item:\n                item_data[child.tag] = child.text\n            data.append(item_data)\n\n        df = pd.DataFrame(data)\n\n        return df\n\n    except etree.XMLSyntaxError:\n        raise ValueError(\"The XML file has invalid syntax.\")\n    except urllib.error.URLError:\n        raise ValueError(\"The URL is invalid or the XML file cannot be fetched from the URL.\")\n    except Exception as e:\n        raise ValueError(\"XML structure does not match expected format.\") from e", "raw_solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        response = urllib.request.urlopen(url)\n        xml_content = response.read()\n\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n\n        # Find all 'item' elements\n        items = root.findall('.//item')\n\n        # Create a DataFrame from the 'item' elements\n        data = []\n        for item in items:\n            item_data = {}\n            for child in item:\n                item_data[child.tag] = child.text\n            data.append(item_data)\n\n        df = pd.DataFrame(data)\n\n        return df\n\n    except etree.XMLSyntaxError:\n        raise ValueError(\"The XML file has invalid syntax.\")\n    except urllib.error.URLError:\n        raise ValueError(\"The URL is invalid or the XML file cannot be fetched from the URL.\")\n    except Exception as e:\n        raise ValueError(\"XML structure does not match expected format.\") from e\n\n# Test the function\ntry:\n    df = task_func('http://example.com/invalid_structure.xml')\n    print(df)\nexcept ValueError as e:\n    print(e)"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Download the text file\n    response = urllib.request.urlopen(url)\n    data = response.read()\n    text = data.decode('utf-8')\n\n    # Process the text to count the frequency of each word\n    words = re.findall(r'\\b\\w+\\b', text)\n    counter = Counter(words)\n\n    # Plot a bar chart showing the ten most frequently occurring words\n    most_common_words = counter.most_common(10)\n    words, frequencies = zip(*most_common_words)\n    ax = plt.bar(words, frequencies)\n\n    return (counter, ax)", "raw_solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Download the text file\n    response = urllib.request.urlopen(url)\n    data = response.read()\n    text = data.decode('utf-8')\n\n    # Process the text to count the frequency of each word\n    words = re.findall(r'\\b\\w+\\b', text)\n    counter = Counter(words)\n\n    # Plot a bar chart showing the ten most frequently occurring words\n    most_common_words = counter.most_common(10)\n    words, frequencies = zip(*most_common_words)\n    ax = plt.bar(words, frequencies)\n\n    return (counter, ax)"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        response = requests.get(url, stream=True)\n        if response.status_code != 200:\n            return \"Error: Unable to download the file from the provided URL.\"\n        content_type = response.headers.get('content-type')\n        if 'application/zip' not in content_type:\n            return \"Error: The URL does not point to a ZIP file.\"\n        zip_file_path = os.path.join(download_path, 'downloaded.zip')\n        with open(zip_file_path, 'wb') as f:\n            f.write(response.content)\n        try:\n            with ZipFile(zip_file_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n        return download_path\n    except requests.exceptions.RequestException as e:\n        return f\"Error: {str(e)}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"", "raw_solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        response = requests.get(url, stream=True)\n        if response.status_code != 200:\n            return \"Error: Unable to download the file from the provided URL.\"\n        content_type = response.headers.get('content-type')\n        if 'application/zip' not in content_type:\n            return \"Error: The URL does not point to a ZIP file.\"\n        zip_file_path = os.path.join(download_path, 'downloaded.zip')\n        with open(zip_file_path, 'wb') as f:\n            f.write(response.content)\n        try:\n            with ZipFile(zip_file_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n        return download_path\n    except requests.exceptions.RequestException as e:\n        return f\"Error: {str(e)}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        print (\"HTTP Error:\",errh)\n        return\n    except requests.exceptions.ConnectionError as errc:\n        print (\"Error Connecting:\",errc)\n        return\n    except requests.exceptions.Timeout as errt:\n        print (\"Timeout Error:\",errt)\n        return\n    except requests.exceptions.RequestException as err:\n        print (\"Something went wrong\",err)\n        return\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if table is None:\n        raise ValueError('Table with the specified ID not found.')\n\n    csv = pd.read_html(str(table))[0].to_csv(index=False)\n    df = pd.read_csv(StringIO(csv))\n\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        print (\"HTTP Error:\",errh)\n        return\n    except requests.exceptions.ConnectionError as errc:\n        print (\"Error Connecting:\",errc)\n        return\n    except requests.exceptions.Timeout as errt:\n        print (\"Timeout Error:\",errt)\n        return\n    except requests.exceptions.RequestException as err:\n        print (\"Something went wrong\",err)\n        return\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if table is None:\n        raise ValueError('Table with the specified ID not found.')\n\n    csv = pd.read_html(str(table))[0].to_csv(index=False)\n    df = pd.read_csv(StringIO(csv))\n\n    return df"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    try:\n        # Create download directory if it doesn't exist\n        DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n\n        # Download file\n        response = requests.get(url)\n        response.raise_for_status()\n\n        # Save file\n        download_path = DOWNLOAD_DIR / filename\n        download_path.write_bytes(response.content)\n\n        # Create unzip directory if it doesn't exist\n        ZIP_DIR.mkdir(parents=True, exist_ok=True)\n\n        # Extract files\n        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # Get list of files\n        file_list = [str(f.name) for f in ZIP_DIR.iterdir() if f.is_file()]\n\n        return \"Success\", file_list\n\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Network-related exception: {str(e)}\", []\n\n    except Exception as e:\n        return f\"Error: File-related exception: {str(e)}\", []\nurl = \"https://github.com/twbs/bootstrap/releases/download/v4.1.1/bootstrap-4.1.1-dist.zip\"\nfilename = \"bootstrap.zip\"", "raw_solution": "import requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    try:\n        # Create download directory if it doesn't exist\n        DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n\n        # Download file\n        response = requests.get(url)\n        response.raise_for_status()\n\n        # Save file\n        download_path = DOWNLOAD_DIR / filename\n        download_path.write_bytes(response.content)\n\n        # Create unzip directory if it doesn't exist\n        ZIP_DIR.mkdir(parents=True, exist_ok=True)\n\n        # Extract files\n        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # Get list of files\n        file_list = [str(f.name) for f in ZIP_DIR.iterdir() if f.is_file()]\n\n        return \"Success\", file_list\n\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Network-related exception: {str(e)}\", []\n\n    except Exception as e:\n        return f\"Error: File-related exception: {str(e)}\", []\n\n# Test the function\nurl = \"https://github.com/twbs/bootstrap/releases/download/v4.1.1/bootstrap-4.1.1-dist.zip\"\nfilename = \"bootstrap.zip\""}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Send a GET request\n    response = requests.get(url)\n\n    # Parse the content of the request with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all the links\n    links = soup.find_all('a')\n\n    # Set to store unique absolute URLs\n    unique_links = set()\n\n    # Open the CSV file for writing\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        # Iterate over the links\n        for link in links:\n            # Get the href attribute\n            href = link.get('href')\n            # If the href attribute exists and is not None\n            if href:\n                # Join the base URL and the href to get the absolute URL\n                absolute_url = urljoin(base_url, href)\n                # Add the absolute URL to the set\n                unique_links.add(absolute_url)\n                # Write the absolute URL to the CSV file\n                writer.writerow([absolute_url])\n\n    # Return the number of unique absolute links\n    return len(unique_links)", "raw_solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Send a GET request\n    response = requests.get(url)\n\n    # Parse the content of the request with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all the links\n    links = soup.find_all('a')\n\n    # Set to store unique absolute URLs\n    unique_links = set()\n\n    # Open the CSV file for writing\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        # Iterate over the links\n        for link in links:\n            # Get the href attribute\n            href = link.get('href')\n            # If the href attribute exists and is not None\n            if href:\n                # Join the base URL and the href to get the absolute URL\n                absolute_url = urljoin(base_url, href)\n                # Add the absolute URL to the set\n                unique_links.add(absolute_url)\n                # Write the absolute URL to the CSV file\n                writer.writerow([absolute_url])\n\n    # Return the number of unique absolute links\n    return len(unique_links)\n\n# Test the function"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the HTML content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error: {e}\")\n        return 0\n\n    # Parse the HTML content\n    try:\n        parsed_html = html.fromstring(response.content)\n        table_html = parsed_html.get_element_by_id('my_table')\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return 0\n\n    # Convert the HTML table to a pandas DataFrame\n    try:\n        df = pd.read_html(html.tostring(table_html))[0]\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return 0\n\n    # Store the DataFrame into SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n        conn.close()\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return 0\n\n    return df.shape[0]", "raw_solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the HTML content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error: {e}\")\n        return 0\n\n    # Parse the HTML content\n    try:\n        parsed_html = html.fromstring(response.content)\n        table_html = parsed_html.get_element_by_id('my_table')\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return 0\n\n    # Convert the HTML table to a pandas DataFrame\n    try:\n        df = pd.read_html(html.tostring(table_html))[0]\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return 0\n\n    # Store the DataFrame into SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n        conn.close()\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return 0\n\n    return df.shape[0]"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        image = Image.open(filename)\n\n        # Extract text from the image using OCR\n        ocr_text = pytesseract.image_to_string(image, lang='eng')\n\n        # Convert the OCR text encoding\n        ocr_text = ocr_text.encode(from_encoding).decode(to_encoding)\n\n        return ocr_text\n\n    except (ValueError, UnicodeDecodeError, LookupError) as e:\n        try:\n            # If OCR extraction fails, try to extract text from the image comment\n            exif_data = image._getexif()\n            if exif_data is not None:\n                comment_text = exif_data.get(0x013B)\n                if comment_text is not None:\n                    comment_text = comment_text.encode(from_encoding).decode(to_encoding)\n                    return comment_text\n\n        except (ValueError, UnicodeDecodeError, LookupError) as e:\n            # If both OCR extraction and comment processing fail, return an empty string\n            return \"\"", "raw_solution": "from PIL import Image\nimport codecs\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        image = Image.open(filename)\n\n        # Extract text from the image using OCR\n        ocr_text = pytesseract.image_to_string(image, lang='eng')\n\n        # Convert the OCR text encoding\n        ocr_text = ocr_text.encode(from_encoding).decode(to_encoding)\n\n        return ocr_text\n\n    except (ValueError, UnicodeDecodeError, LookupError) as e:\n        try:\n            # If OCR extraction fails, try to extract text from the image comment\n            exif_data = image._getexif()\n            if exif_data is not None:\n                comment_text = exif_data.get(0x013B)\n                if comment_text is not None:\n                    comment_text = comment_text.encode(from_encoding).decode(to_encoding)\n                    return comment_text\n\n        except (ValueError, UnicodeDecodeError, LookupError) as e:\n            # If both OCR extraction and comment processing fail, return an empty string\n            return \"\"\n\n# Test the function"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        content = response.content\n\n        if not content:\n            return {}\n\n        if not from_encoding:\n            from_encoding = chardet.detect(content)['encoding']\n            if not from_encoding:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        decoded_content = content.decode(from_encoding)\n        reencoded_content = decoded_content.encode(to_encoding)\n\n        return json.loads(reencoded_content)\n\n    except Exception as e:\n        raise e", "raw_solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        content = response.content\n\n        if not content:\n            return {}\n\n        if not from_encoding:\n            from_encoding = chardet.detect(content)['encoding']\n            if not from_encoding:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        decoded_content = content.decode(from_encoding)\n        reencoded_content = decoded_content.encode(to_encoding)\n\n        return json.loads(reencoded_content)\n\n    except Exception as e:\n        raise e\n\n# Test the function"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    # Read CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n\n    # Check if column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the CSV file.\")\n\n    # Convert date values to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n\n    # Filter rows based on the current date\n    df = df[df[column_name] <= datetime.now()]\n\n    # Sort the resulting data\n    df = df.sort_values(by=[column_name])\n\n    return df", "raw_solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    # Read CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n\n    # Check if column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the CSV file.\")\n\n    # Convert date values to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n\n    # Filter rows based on the current date\n    df = df[df[column_name] <= datetime.now()]\n\n    # Sort the resulting data\n    df = df.sort_values(by=[column_name])\n\n    return df"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Secure the client socket using SSL/TLS\n        context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n        context.load_cert_chain(cert_file, key_file)\n        secure_sock = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive the file path from the client\n        file_path = secure_sock.recv(buffer_size).decode()\n\n        # Check if the file exists\n        if not os.path.isfile(file_path):\n            return 'File not found'\n\n        # Calculate the SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, \"rb\") as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n            return sha256_hash.hexdigest()\n\n    except Exception as e:\n        return str(e)", "raw_solution": "import ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Secure the client socket using SSL/TLS\n        context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n        context.load_cert_chain(cert_file, key_file)\n        secure_sock = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive the file path from the client\n        file_path = secure_sock.recv(buffer_size).decode()\n\n        # Check if the file exists\n        if not os.path.isfile(file_path):\n            return 'File not found'\n\n        # Calculate the SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, \"rb\") as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n            return sha256_hash.hexdigest()\n\n    except Exception as e:\n        return str(e)"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # List for socket descriptors\n    sockets_list = [server_socket]\n\n    # List for connected clients - socket as a key, user header and name as data\n    clients = {}\n\n    start_time = datetime.now()\n\n    while (datetime.now() - start_time).seconds < run_duration:\n        read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list)\n\n        # Handle received messages\n        for notified_socket in read_sockets:\n            if notified_socket == server_socket:\n                client_socket, client_address = server_socket.accept()\n                sockets_list.append(client_socket)\n                clients[client_socket] = queue.Queue()\n            else:\n                data = notified_socket.recv(buffer_size)\n                if data:\n                    # Append server's current time to received data\n                    data = data + str(datetime.now()).encode()\n                    clients[notified_socket].put(data)\n                else:\n                    # If client disconnected\n                    sockets_list.remove(notified_socket)\n                    del clients[notified_socket]\n\n        # Handle exceptions\n        for notified_socket in exception_sockets:\n            sockets_list.remove(notified_socket)\n            del clients[notified_socket]\n\n        # Send data to clients\n        for client_socket in clients:\n            while not clients[client_socket].empty():\n                data = clients[client_socket].get()\n                client_socket.send(data)\n\n    server_socket.close()\n\n    return f\"Server operation completed successfully. Run duration: {run_duration} seconds.\"", "raw_solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # List for socket descriptors\n    sockets_list = [server_socket]\n\n    # List for connected clients - socket as a key, user header and name as data\n    clients = {}\n\n    start_time = datetime.now()\n\n    while (datetime.now() - start_time).seconds < run_duration:\n        read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list)\n\n        # Handle received messages\n        for notified_socket in read_sockets:\n            if notified_socket == server_socket:\n                client_socket, client_address = server_socket.accept()\n                sockets_list.append(client_socket)\n                clients[client_socket] = queue.Queue()\n            else:\n                data = notified_socket.recv(buffer_size)\n                if data:\n                    # Append server's current time to received data\n                    data = data + str(datetime.now()).encode()\n                    clients[notified_socket].put(data)\n                else:\n                    # If client disconnected\n                    sockets_list.remove(notified_socket)\n                    del clients[notified_socket]\n\n        # Handle exceptions\n        for notified_socket in exception_sockets:\n            sockets_list.remove(notified_socket)\n            del clients[notified_socket]\n\n        # Send data to clients\n        for client_socket in clients:\n            while not clients[client_socket].empty():\n                data = clients[client_socket].get()\n                client_socket.send(data)\n\n    server_socket.close()\n\n    return f\"Server operation completed successfully. Run duration: {run_duration} seconds.\"\n"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    # Receive message from client socket\n    data = client_socket.recv(1024).decode()\n\n    # Prompt for sender's email, recipient's email, and password\n    sender_email = input(\"Enter your email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    password = getpass.getpass(\"Enter your password: \")\n\n    # Create an email message\n    msg = EmailMessage()\n    msg.set_content(data)\n    msg['Subject'] = 'Message from client'\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n\n    # Send the email\n    server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n    server.starttls()\n    server.login(sender_email, password)\n    server.send_message(msg)\n    server.quit()\n\n    return None", "raw_solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    # Receive message from client socket\n    data = client_socket.recv(1024).decode()\n\n    # Prompt for sender's email, recipient's email, and password\n    sender_email = input(\"Enter your email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    password = getpass.getpass(\"Enter your password: \")\n\n    # Create an email message\n    msg = EmailMessage()\n    msg.set_content(data)\n    msg['Subject'] = 'Message from client'\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n\n    # Send the email\n    server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n    server.starttls()\n    server.login(sender_email, password)\n    server.send_message(msg)\n    server.quit()\n\n    return None"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    # If the CSV file does not have a header, use the first column as the text data\n    if 'Text' not in df.columns:\n        df.columns = ['Text']\n\n    # Split the text into words\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    X = vectorizer.fit_transform(df['Text'])\n    word_counts = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    word_counts = word_counts.sum().sort_values(ascending=False)\n\n    # Generate a histogram of the frequency of the top ten most common words\n    top_10_words = word_counts[:10]\n    ax = top_10_words.plot(kind='bar')\n\n    # Save the plot to a file if a save_path is provided\n    if save_path is not None:\n        plt.savefig(save_path)\n        return None\n\n    return ax", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    # If the CSV file does not have a header, use the first column as the text data\n    if 'Text' not in df.columns:\n        df.columns = ['Text']\n\n    # Split the text into words\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    X = vectorizer.fit_transform(df['Text'])\n    word_counts = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    word_counts = word_counts.sum().sort_values(ascending=False)\n\n    # Generate a histogram of the frequency of the top ten most common words\n    top_10_words = word_counts[:10]\n    ax = top_10_words.plot(kind='bar')\n\n    # Save the plot to a file if a save_path is provided\n    if save_path is not None:\n        plt.savefig(save_path)\n        return None\n\n    return ax"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    # Handling of Special Cases\n    if animals is None and foods is None:\n        return pd.DataFrame()\n    elif animals is None:\n        animals = ['Dog', 'Cat', 'Bird']  # predefined list\n    elif foods is None:\n        foods = ['Burger', 'Pizza', 'Sushi']  # predefined list\n\n    # Generate all possible combinations of the provided 'animals' and 'foods'\n    combos = list(itertools.product(animals, foods))\n\n    # Shuffle the combinations to ensure variety in the DataFrame layout\n    np.random.shuffle(combos)\n\n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combos, columns=['Animal:Food'])\n\n    return df", "raw_solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    # Handling of Special Cases\n    if animals is None and foods is None:\n        return pd.DataFrame()\n    elif animals is None:\n        animals = ['Dog', 'Cat', 'Bird']  # predefined list\n    elif foods is None:\n        foods = ['Burger', 'Pizza', 'Sushi']  # predefined list\n\n    # Generate all possible combinations of the provided 'animals' and 'foods'\n    combos = list(itertools.product(animals, foods))\n\n    # Shuffle the combinations to ensure variety in the DataFrame layout\n    np.random.shuffle(combos)\n\n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combos, columns=['Animal:Food'])\n\n    return df"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert timezone\n    converted_times = [pytz.timezone(timezone).localize(datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")) for time in time_strings]\n\n    # Calculate time differences\n    time_diffs = [(converted_times[i] - converted_times[i-1]).total_seconds() for i in range(1, len(converted_times))]\n\n    # Return average time difference\n    return float(np.mean(time_diffs))", "raw_solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert timezone\n    converted_times = [pytz.timezone(timezone).localize(datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")) for time in time_strings]\n\n    # Calculate time differences\n    time_diffs = [(converted_times[i] - converted_times[i-1]).total_seconds() for i in range(1, len(converted_times))]\n\n    # Return average time difference\n    return float(np.mean(time_diffs))\n\n# Test the function"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Split into words\n    words = text.split()\n    # Count the frequency of each word\n    word_counts = Counter(words)\n    # Get the 10 most common words\n    top_10_words = word_counts.most_common(10)\n    # Create a bar chart\n    words, counts = zip(*top_10_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 most common words')\n    return top_10_words, ax", "raw_solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Split into words\n    words = text.split()\n    # Count the frequency of each word\n    word_counts = Counter(words)\n    # Get the 10 most common words\n    top_10_words = word_counts.most_common(10)\n    # Create a bar chart\n    words, counts = zip(*top_10_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 most common words')\n    return top_10_words, ax"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Extract URL from the string\n    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n\n    if not url:\n        return \"No valid URL found in the provided string.\"\n\n    url = url[0]\n\n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n    except requests.exceptions.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # If the fetch is successful, parse the content to find the title\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.find('title')\n\n    if title is None:\n        return \"No title tag found in the webpage.\"\n\n    return title.text", "raw_solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Extract URL from the string\n    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n\n    if not url:\n        return \"No valid URL found in the provided string.\"\n\n    url = url[0]\n\n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n    except requests.exceptions.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # If the fetch is successful, parse the content to find the title\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.find('title')\n\n    if title is None:\n        return \"No title tag found in the webpage.\"\n\n    return title.text"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    data = json.loads(json_data)\n    url = data[unknown_key]\n\n    if save_dir is None:\n        save_dir = os.getcwd()\n\n    response = requests.get(url)\n    filename = f\"{unknown_key}_{datetime.now().strftime('%Y%m%d%H%M%S%f')}.txt\"\n    filepath = os.path.join(save_dir, filename)\n\n    with open(filepath, 'wb') as f:\n        f.write(response.content)\n\n    return os.path.abspath(filepath)", "raw_solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    data = json.loads(json_data)\n    url = data[unknown_key]\n\n    if save_dir is None:\n        save_dir = os.getcwd()\n\n    response = requests.get(url)\n    filename = f\"{unknown_key}_{datetime.now().strftime('%Y%m%d%H%M%S%f')}.txt\"\n    filepath = os.path.join(save_dir, filename)\n\n    with open(filepath, 'wb') as f:\n        f.write(response.content)\n\n    return os.path.abspath(filepath)"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    # Regular expression pattern for phone numbers\n    phone_pattern = re.compile(r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')\n\n    # If url starts with 'http', fetch the content, otherwise treat it as a local file\n    if url.startswith('http'):\n        response = requests.get(url)\n        content = bs4.BeautifulSoup(response.text, 'html.parser')\n    else:\n        with open(url, 'r') as file:\n            content = bs4.BeautifulSoup(file.read(), 'html.parser')\n\n    # Find all phone numbers in the content\n    phone_numbers = re.findall(phone_pattern, str(content))\n\n    # Save the phone numbers to the output file in JSON format\n    with open(output_path, 'w') as file:\n        json.dump(phone_numbers, file)\n\n    return phone_numbers", "raw_solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    # Regular expression pattern for phone numbers\n    phone_pattern = re.compile(r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')\n\n    # If url starts with 'http', fetch the content, otherwise treat it as a local file\n    if url.startswith('http'):\n        response = requests.get(url)\n        content = bs4.BeautifulSoup(response.text, 'html.parser')\n    else:\n        with open(url, 'r') as file:\n            content = bs4.BeautifulSoup(file.read(), 'html.parser')\n\n    # Find all phone numbers in the content\n    phone_numbers = re.findall(phone_pattern, str(content))\n\n    # Save the phone numbers to the output file in JSON format\n    with open(output_path, 'w') as file:\n        json.dump(phone_numbers, file)\n\n    return phone_numbers"}
