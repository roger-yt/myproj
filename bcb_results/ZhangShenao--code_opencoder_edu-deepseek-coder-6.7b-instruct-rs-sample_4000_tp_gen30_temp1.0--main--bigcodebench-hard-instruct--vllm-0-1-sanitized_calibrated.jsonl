{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\nfrom typing import List\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test') -> List[str]:\n    \"\"\"\n    Downloads all files from a specific directory on an FTP server using wget.\n    \"\"\"\n    # List to store filenames\n    filenames = []\n\n    try:\n        # Connect to FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        print(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n        return filenames\n\n    try:\n        # Login to FTP server\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        print(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n        return filenames\n\n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        print(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        return filenames\n\n    # Get a list of files in the directory\n    try:\n        ftp.retrlines('LIST', filenames.append)\n    except Exception as e:\n        print(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        return filenames\n\n    # Download each file using wget\n    for filename in filenames:\n        try:\n            subprocess.run(['wget', f'ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}'])\n        except Exception as e:\n            print(f\"Failed to download file {filename} from server {ftp_server}: {str(e)}\")\n\n    # Close the FTP connection\n    ftp.quit()\n\n    return filenames", "raw_solution": "import subprocess\nimport ftplib\nimport os\nfrom typing import List\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test') -> List[str]:\n    \"\"\"\n    Downloads all files from a specific directory on an FTP server using wget.\n    \"\"\"\n    # List to store filenames\n    filenames = []\n\n    try:\n        # Connect to FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        print(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n        return filenames\n\n    try:\n        # Login to FTP server\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        print(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n        return filenames\n\n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        print(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        return filenames\n\n    # Get a list of files in the directory\n    try:\n        ftp.retrlines('LIST', filenames.append)\n    except Exception as e:\n        print(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        return filenames\n\n    # Download each file using wget\n    for filename in filenames:\n        try:\n            subprocess.run(['wget', f'ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}'])\n        except Exception as e:\n            print(f\"Failed to download file {filename} from server {ftp_server}: {str(e)}\")\n\n    # Close the FTP connection\n    ftp.quit()\n\n    return filenames"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    Executes a list of shell commands read from a CSV file and saves the outputs in separate files.\n    Each command's output is written to a unique file in the specified output directory.\n    If a command fails, the error message along with the exit code is appended to the respective output file.\n    \"\"\"\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_file_paths = []\n\n    with open(commands_file_path, 'r') as commands_file:\n        reader = csv.reader(commands_file)\n        for i, row in enumerate(reader):\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{i}_output.txt')\n            output_file_paths.append(output_file_path)\n\n            try:\n                result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n            except subprocess.CalledProcessError as e:\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(f\"Command '{command}' failed with exit code {e.returncode}.\\n\")\n                    output_file.write(f\"Error message: {str(e)}\")\n\n    return output_file_paths", "raw_solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    Executes a list of shell commands read from a CSV file and saves the outputs in separate files.\n    Each command's output is written to a unique file in the specified output directory.\n    If a command fails, the error message along with the exit code is appended to the respective output file.\n    \"\"\"\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_file_paths = []\n\n    with open(commands_file_path, 'r') as commands_file:\n        reader = csv.reader(commands_file)\n        for i, row in enumerate(reader):\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{i}_output.txt')\n            output_file_paths.append(output_file_path)\n\n            try:\n                result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n            except subprocess.CalledProcessError as e:\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(f\"Command '{command}' failed with exit code {e.returncode}.\\n\")\n                    output_file.write(f\"Error message: {str(e)}\")\n\n    return output_file_paths"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process is running, so we need to terminate it\n            proc.terminate()\n            time.sleep(1)  # Wait for the process to terminate\n            return f\"Process found. Restarting {process_name}.\"\n\n    # Process is not running, so we need to start it\n    subprocess.Popen(process_name, shell=True)\n    time.sleep(1)  # Wait for the process to start\n    return f\"Process not found. Starting {process_name}.\"", "raw_solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process is running, so we need to terminate it\n            proc.terminate()\n            time.sleep(1)  # Wait for the process to terminate\n            return f\"Process found. Restarting {process_name}.\"\n\n    # Process is not running, so we need to start it\n    subprocess.Popen(process_name, shell=True)\n    time.sleep(1)  # Wait for the process to start\n    return f\"Process not found. Starting {process_name}.\"\n\n# Test the function"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    \"\"\"\n    Zips all files (not including subdirectories) located in the specified directory and returns the path to the created zip file.\n    If the specified directory does not exist, it raises a FileNotFoundError.\n    If the directory does not contain any files, it returns None.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    files = glob.glob(os.path.join(directory, '*'))\n    files = [f for f in files if os.path.isfile(f)]  # Filter out directories\n\n    if not files:\n        return None\n\n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return zip_path", "raw_solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    \"\"\"\n    Zips all files (not including subdirectories) located in the specified directory and returns the path to the created zip file.\n    If the specified directory does not exist, it raises a FileNotFoundError.\n    If the directory does not contain any files, it returns None.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    files = glob.glob(os.path.join(directory, '*'))\n    files = [f for f in files if os.path.isfile(f)]  # Filter out directories\n\n    if not files:\n        return None\n\n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return zip_path"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    \"\"\"\n    Generates a word cloud from the given text after removing URLs.\n    \"\"\"\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Split the text into words\n    words = text.split()\n\n    # If there are no words available, raise an exception\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n\n    # Join the words back into a string\n    text = ' '.join(words)\n\n    # Generate a word cloud\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                min_font_size = 10).generate(text)\n\n    # Plot the word cloud\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 0)\n\n    plt.show()\n\n    return wordcloud", "raw_solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    \"\"\"\n    Generates a word cloud from the given text after removing URLs.\n    \"\"\"\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Split the text into words\n    words = text.split()\n\n    # If there are no words available, raise an exception\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n\n    # Join the words back into a string\n    text = ' '.join(words)\n\n    # Generate a word cloud\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                min_font_size = 10).generate(text)\n\n    # Plot the word cloud\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 0)\n\n    plt.show()\n\n    return wordcloud"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_column):\n    \"\"\"\n    Trains a random forest classifier on the input dataframe and plots the feature importance.\n    \"\"\"\n    # Split the dataframe into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train the random forest classifier\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X, y)\n\n    # Get the feature importances\n    importances = clf.feature_importances_\n    feature_names = X.columns\n\n    # Create a dataframe for feature importances\n    df_importances = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances\n    })\n\n    # Sort the dataframe by importance\n    df_importances = df_importances.sort_values('Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Feature', data=df_importances)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.tight_layout()\n\n    return clf, plt.gca()", "raw_solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, target_column):\n    \"\"\"\n    Trains a random forest classifier on the input dataframe and plots the feature importance.\n    \"\"\"\n    # Split the dataframe into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train the random forest classifier\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X, y)\n\n    # Get the feature importances\n    importances = clf.feature_importances_\n    feature_names = X.columns\n\n    # Create a dataframe for feature importances\n    df_importances = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances\n    })\n\n    # Sort the dataframe by importance\n    df_importances = df_importances.sort_values('Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Feature', data=df_importances)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.tight_layout()\n\n    return clf, plt.gca()"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash", "raw_solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nclass User(UserMixin):\n    def __init__(self, username, password):\n        self.username = username\n        self.password_hash = generate_password_hash(password)\n\n    def check_password(self, password):\n        return check_password_hash(self.password_hash, password)\n\nlogin_manager = LoginManager()\n\n@login_manager.user_loader\ndef load_user(user_id):\n    # This is where you would normally load a user from a database\n    return User('admin', 'password')\n\ndef create_app(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = User('admin', 'password')  # Normally, this would be loaded from a database\n            if user is not None and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('protected'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return 'Logged in as: ' + current_user.username\n\n    return app\n"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    It standardizes the column, calculates Z-scores, and removes data points where the Z-score exceeds a threshold.\n    The function also visualizes the data before and after outlier removal.\n\n    Parameters:\n    data (pandas.DataFrame): The dataset to be processed.\n    column (str): The column of the dataset to be processed.\n    outlier_z_score (float): The Z-score threshold for identifying outliers.\n\n    Returns:\n    tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Standardize the column\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[column].values.reshape(-1, 1))\n\n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data[column]))\n\n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove outliers\n    data_without_outliers = data.drop(outliers)\n\n    # Visualize data before and after outlier removal\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.scatter(range(len(data[column])), data[column])\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(range(len(data_without_outliers[column])), data_without_outliers[column])\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.tight_layout()\n    plt.show()\n\n    return data, data_without_outliers, outliers", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    It standardizes the column, calculates Z-scores, and removes data points where the Z-score exceeds a threshold.\n    The function also visualizes the data before and after outlier removal.\n\n    Parameters:\n    data (pandas.DataFrame): The dataset to be processed.\n    column (str): The column of the dataset to be processed.\n    outlier_z_score (float): The Z-score threshold for identifying outliers.\n\n    Returns:\n    tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Standardize the column\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[column].values.reshape(-1, 1))\n\n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data[column]))\n\n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove outliers\n    data_without_outliers = data.drop(outliers)\n\n    # Visualize data before and after outlier removal\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.scatter(range(len(data[column])), data[column])\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(range(len(data_without_outliers[column])), data_without_outliers[column])\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.tight_layout()\n    plt.show()\n\n    return data, data_without_outliers, outliers"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' should be an integer greater than 1\")\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels)\n\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X')\n\n    return labels, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' should be an integer greater than 1\")\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels)\n\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X')\n\n    return labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    transformed_df = pd.DataFrame(transformed_data, columns=[f'PC{i+1}' for i in range(n_components)])\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n\n    return transformed_df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    transformed_df = pd.DataFrame(transformed_data, columns=[f'PC{i+1}' for i in range(n_components)])\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n\n    return transformed_df, ax"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target\n\n    # Set global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create pair plot\n    g = sns.pairplot(iris_df, hue='species', diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'k'})\n    g.fig.suptitle(\"Iris Dataset Pair Plot\", y=1.08)\n\n    return g.fig", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Load iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target\n\n    # Set global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create pair plot\n    g = sns.pairplot(iris_df, hue='species', diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'k'})\n    g.fig.suptitle(\"Iris Dataset Pair Plot\", y=1.08)\n\n    return g.fig"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\nfrom typing import Tuple\ndef task_func(seed=42) -> Tuple[plt.axes.Axes, pd.DataFrame]:\n    \"\"\"\n    Generates a plot of random time series data for the past 30 days with reproducibility controlled by an optional seed parameter.\n    The plot is styled with Arial font for better readability.\n    \"\"\"\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate a DataFrame with random data\n        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]\n        values = [random.randint(0, 100) for _ in range(30)]\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        df['Date'] = pd.to_datetime(df['Date'])\n\n        # Create the plot\n        plt.figure(figsize=(10, 6))\n        plt.plot(df['Date'], df['Value'])\n        plt.title('Random Time Series Data', fontname=\"Arial\")\n        plt.xlabel('Date', fontname=\"Arial\")\n        plt.ylabel('Value', fontname=\"Arial\")\n        plt.xticks(rotation=45, fontname=\"Arial\")\n        plt.yticks(fontname=\"Arial\")\n        plt.grid(True)\n\n        return plt.gca(), df\n\n    except Exception as e:\n        raise ValueError(f\"There was an issue generating the data or plot: {e}\")", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\nfrom typing import Tuple\n\ndef task_func(seed=42) -> Tuple[plt.axes.Axes, pd.DataFrame]:\n    \"\"\"\n    Generates a plot of random time series data for the past 30 days with reproducibility controlled by an optional seed parameter.\n    The plot is styled with Arial font for better readability.\n    \"\"\"\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate a DataFrame with random data\n        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]\n        values = [random.randint(0, 100) for _ in range(30)]\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        df['Date'] = pd.to_datetime(df['Date'])\n\n        # Create the plot\n        plt.figure(figsize=(10, 6))\n        plt.plot(df['Date'], df['Value'])\n        plt.title('Random Time Series Data', fontname=\"Arial\")\n        plt.xlabel('Date', fontname=\"Arial\")\n        plt.ylabel('Value', fontname=\"Arial\")\n        plt.xticks(rotation=45, fontname=\"Arial\")\n        plt.yticks(fontname=\"Arial\")\n        plt.grid(True)\n\n        return plt.gca(), df\n\n    except Exception as e:\n        raise ValueError(f\"There was an issue generating the data or plot: {e}\")"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.datasets import load_boston\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_to=None):\n    \"\"\"\n    Draws the correlation heatmap of the Boston Housing dataset using Seaborn.\n    If `save_to` is provided, saves the plot to the specified file.\n    \"\"\"\n    try:\n        # Load the dataset\n        boston = load_boston()\n        df = pd.DataFrame(boston.data, columns=boston.feature_names)\n\n        # Compute the correlation matrix\n        corr = df.corr()\n\n        # Generate a mask for the upper triangle\n        mask = np.triu(np.ones_like(corr, dtype=bool))\n\n        # Set up the matplotlib figure\n        f, ax = plt.subplots(figsize=(11, 9))\n\n        # Generate a custom diverging colormap\n        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n        if save_to:\n            plt.savefig(save_to)\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"Error in generating or saving the plot: {e}\")", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.datasets import load_boston\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_to=None):\n    \"\"\"\n    Draws the correlation heatmap of the Boston Housing dataset using Seaborn.\n    If `save_to` is provided, saves the plot to the specified file.\n    \"\"\"\n    try:\n        # Load the dataset\n        boston = load_boston()\n        df = pd.DataFrame(boston.data, columns=boston.feature_names)\n\n        # Compute the correlation matrix\n        corr = df.corr()\n\n        # Generate a mask for the upper triangle\n        mask = np.triu(np.ones_like(corr, dtype=bool))\n\n        # Set up the matplotlib figure\n        f, ax = plt.subplots(figsize=(11, 9))\n\n        # Generate a custom diverging colormap\n        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n        if save_to:\n            plt.savefig(save_to)\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"Error in generating or saving the plot: {e}\")"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n\n    # Check if df has 'value' column\n    if 'value' not in df.columns:\n        raise ValueError(\"'df' should have a 'value' column\")\n\n    # Check if 'freq' is a valid frequency string\n    try:\n        pd.to_datetime(df.index, freq=freq)\n    except ValueError:\n        raise ValueError(\"'freq' should be a valid frequency string\")\n\n    # Check if 'decomposition_model' is 'additive' or 'multiplicative'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' should be 'additive' or 'multiplicative'\")\n\n    # Decompose the time series\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, freq=freq)\n\n    # Plot the decomposition\n    fig, ax = plt.subplots(4, 1, figsize=(10, 10))\n    decomposition.trend.plot(ax=ax[0], legend=False)\n    ax[0].set_title('Trend')\n    decomposition.seasonal.plot(ax=ax[1], legend=False)\n    ax[1].set_title('Seasonality')\n    decomposition.resid.plot(ax=ax[2], legend=False)\n    ax[2].set_title('Residuals')\n    plt.tight_layout()\n\n    return (decomposition, ax[2])", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n\n    # Check if df has 'value' column\n    if 'value' not in df.columns:\n        raise ValueError(\"'df' should have a 'value' column\")\n\n    # Check if 'freq' is a valid frequency string\n    try:\n        pd.to_datetime(df.index, freq=freq)\n    except ValueError:\n        raise ValueError(\"'freq' should be a valid frequency string\")\n\n    # Check if 'decomposition_model' is 'additive' or 'multiplicative'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' should be 'additive' or 'multiplicative'\")\n\n    # Decompose the time series\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, freq=freq)\n\n    # Plot the decomposition\n    fig, ax = plt.subplots(4, 1, figsize=(10, 10))\n    decomposition.trend.plot(ax=ax[0], legend=False)\n    ax[0].set_title('Trend')\n    decomposition.seasonal.plot(ax=ax[1], legend=False)\n    ax[1].set_title('Seasonality')\n    decomposition.resid.plot(ax=ax[2], legend=False)\n    ax[2].set_title('Residuals')\n    plt.tight_layout()\n\n    return (decomposition, ax[2])"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generates a pandas Series of random dates within a specified date range, including both start_date and end_date, with an optional seed for reproducibility.\n    \"\"\"\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"'start_date' and 'end_date' must be datetime.datetime instances\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' must be earlier than 'end_date'\")\n\n    random_seed(seed)\n\n    time_delta = end_date - start_date\n    days_range = [start_date + timedelta(days=i) for i in range(time_delta.days + 1)]\n\n    random_dates = pd.Series(days_range).sample(frac=1, random_state=seed)\n\n    return random_dates\nstart_date = datetime(2020, 1, 1)\nend_date = datetime(2020, 12, 31)\nseed = 42", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generates a pandas Series of random dates within a specified date range, including both start_date and end_date, with an optional seed for reproducibility.\n    \"\"\"\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"'start_date' and 'end_date' must be datetime.datetime instances\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' must be earlier than 'end_date'\")\n\n    random_seed(seed)\n\n    time_delta = end_date - start_date\n    days_range = [start_date + timedelta(days=i) for i in range(time_delta.days + 1)]\n\n    random_dates = pd.Series(days_range).sample(frac=1, random_state=seed)\n\n    return random_dates\n\n# Testing the function\nstart_date = datetime(2020, 1, 1)\nend_date = datetime(2020, 12, 31)\nseed = 42\n\nrandom_dates = task_func(start_date, end_date, seed)"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    Modifies a list by adding the element '12', then concatenates a number of CSV files from a directory into a single DataFrame.\n    The number of files concatenated is determined by the sum of the numbers in the list.\n    \"\"\"\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list.\")\n    my_list.append('12')\n\n    # Get a list of all CSV files in the directory\n    file_list = glob.glob(os.path.join(file_dir, f\"*{file_ext}\"))\n    if not file_list:\n        raise FileNotFoundError(f\"No files found in the directory {file_dir} with extension {file_ext}.\")\n\n    # Concatenate the CSV files\n    df_list = []\n    for file in file_list[:sum(my_list)]:\n        df = pd.read_csv(file)\n        df_list.append(df)\n    df_concat = pd.concat(df_list, ignore_index=True)\n\n    return df_concat", "raw_solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    Modifies a list by adding the element '12', then concatenates a number of CSV files from a directory into a single DataFrame.\n    The number of files concatenated is determined by the sum of the numbers in the list.\n    \"\"\"\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list.\")\n    my_list.append('12')\n\n    # Get a list of all CSV files in the directory\n    file_list = glob.glob(os.path.join(file_dir, f\"*{file_ext}\"))\n    if not file_list:\n        raise FileNotFoundError(f\"No files found in the directory {file_dir} with extension {file_ext}.\")\n\n    # Concatenate the CSV files\n    df_list = []\n    for file in file_list[:sum(my_list)]:\n        df = pd.read_csv(file)\n        df_list.append(df)\n    df_concat = pd.concat(df_list, ignore_index=True)\n\n    return df_concat"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"All elements in 'my_list' should be numeric (int or float)\")\n\n    my_list.append(12)\n    total_sum = sum(my_list)\n    if total_sum > size:\n        raise ValueError(\"Sum of elements in 'my_list' should not exceed 'size'\")\n\n    random_seed(seed)\n    start_time = time.time()\n    random_list = [randint(1, 100) for _ in range(total_sum)]\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 101), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return (time_taken, ax)", "raw_solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"All elements in 'my_list' should be numeric (int or float)\")\n\n    my_list.append(12)\n    total_sum = sum(my_list)\n    if total_sum > size:\n        raise ValueError(\"Sum of elements in 'my_list' should not exceed 'size'\")\n\n    random_seed(seed)\n    start_time = time.time()\n    random_list = [randint(1, 100) for _ in range(total_sum)]\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 101), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return (time_taken, ax)"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom requests.exceptions import ConnectionError, HTTPError\ndef task_func(url='http://example.com'):\n    \"\"\"\n    Scrapes the first table from a web page and extracts data into a Pandas DataFrame.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except (ConnectionError, HTTPError) as e:\n        raise e\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find_all('table')\n\n    if not table:\n        raise ValueError(\"No table data found on the page\")\n\n    table = table[0]\n    rows = table.find_all('tr')\n\n    if not rows:\n        raise ValueError(\"No table data found on the page\")\n\n    header = [th.text.strip() for th in rows[0].find_all('th')]\n    data = []\n\n    for row in rows[1:]:\n        cols = row.find_all('td')\n        cols = [col.text.strip() for col in cols]\n        data.append(cols)\n\n    df = pd.DataFrame(data, columns=header)\n\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom requests.exceptions import ConnectionError, HTTPError\n\ndef task_func(url='http://example.com'):\n    \"\"\"\n    Scrapes the first table from a web page and extracts data into a Pandas DataFrame.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except (ConnectionError, HTTPError) as e:\n        raise e\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find_all('table')\n\n    if not table:\n        raise ValueError(\"No table data found on the page\")\n\n    table = table[0]\n    rows = table.find_all('tr')\n\n    if not rows:\n        raise ValueError(\"No table data found on the page\")\n\n    header = [th.text.strip() for th in rows[0].find_all('th')]\n    data = []\n\n    for row in rows[1:]:\n        cols = row.find_all('td')\n        cols = [col.text.strip() for col in cols]\n        data.append(cols)\n\n    df = pd.DataFrame(data, columns=header)\n\n    return df"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input should be a non-empty DataFrame\")\n\n    numeric_cols = df.select_dtypes(include=['int', 'float']).columns\n    if not numeric_cols.any():\n        raise ValueError(\"No numeric columns in the DataFrame\")\n\n    hist_plots = []\n    for col in numeric_cols:\n        plt.figure()\n        df[col].hist(bins=10)\n        plt.title(col)\n        plt.xlabel('Value')\n        plt.ylabel('Frequency')\n        hist_plots.append(plt.gca())\n\n    return hist_plots", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input should be a non-empty DataFrame\")\n\n    numeric_cols = df.select_dtypes(include=['int', 'float']).columns\n    if not numeric_cols.any():\n        raise ValueError(\"No numeric columns in the DataFrame\")\n\n    hist_plots = []\n    for col in numeric_cols:\n        plt.figure()\n        df[col].hist(bins=10)\n        plt.title(col)\n        plt.xlabel('Value')\n        plt.ylabel('Frequency')\n        hist_plots.append(plt.gca())\n\n    return hist_plots"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef check_port(ip, port):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.settimeout(5)\n    result = sock.connect_ex((str(ip), port))\n    sock.close()\n    return result == 0\ndef task_func(ip_range, port):\n    ip_network = IPv4Network(ip_range)\n    ip_list = [str(ip) for ip in ip_network.hosts()]\n    results = {}\n\n    def worker(ip):\n        results[ip] = check_port(ip, port)\n\n    threads = [Thread(target=worker, args=(ip,)) for ip in ip_list]\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n\n    return results", "raw_solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef check_port(ip, port):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.settimeout(5)\n    result = sock.connect_ex((str(ip), port))\n    sock.close()\n    return result == 0\n\ndef task_func(ip_range, port):\n    ip_network = IPv4Network(ip_range)\n    ip_list = [str(ip) for ip in ip_network.hosts()]\n    results = {}\n\n    def worker(ip):\n        results[ip] = check_port(ip, port)\n\n    threads = [Thread(target=worker, args=(ip,)) for ip in ip_list]\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n\n    return results\n\n# Test the function"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information such as message type, timestamp, and the message itself from a log file and stores the data in a CSV format.\n    \"\"\"\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(.*?)\\] - (.*)'\n\n    # Initialize lists to store the extracted data\n    types = []\n    timestamps = []\n    messages = []\n\n    # Open the log file and read each line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Search for a match in the line using the regular expression\n            match = re.search(pattern, line)\n\n            # If a match is found, extract the data and append it to the corresponding lists\n            if match:\n                types.append(match.group(1))\n                timestamps.append(match.group(2))\n                messages.append(match.group(3))\n\n    # If no valid log entries are found, raise a ValueError\n    if not types:\n        raise ValueError('No valid log entries found')\n\n    # Convert the timestamps to datetime objects\n    for i in range(len(timestamps)):\n        try:\n            timestamps[i] = datetime.strptime(timestamps[i], '%Y-%m-%d %H:%M:%S')\n        except ValueError:\n            raise ValueError(f'Invalid timestamp found at line {i+1}')\n\n    # Create a pandas DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Type': types,\n        'Timestamp': timestamps,\n        'Message': messages\n    })\n\n    # Save the DataFrame to a CSV file\n    csv_file = log_file.replace('.log', '.csv')\n    df.to_csv(csv_file, index=False)\n\n    return csv_file", "raw_solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information such as message type, timestamp, and the message itself from a log file and stores the data in a CSV format.\n    \"\"\"\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(.*?)\\] - (.*)'\n\n    # Initialize lists to store the extracted data\n    types = []\n    timestamps = []\n    messages = []\n\n    # Open the log file and read each line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Search for a match in the line using the regular expression\n            match = re.search(pattern, line)\n\n            # If a match is found, extract the data and append it to the corresponding lists\n            if match:\n                types.append(match.group(1))\n                timestamps.append(match.group(2))\n                messages.append(match.group(3))\n\n    # If no valid log entries are found, raise a ValueError\n    if not types:\n        raise ValueError('No valid log entries found')\n\n    # Convert the timestamps to datetime objects\n    for i in range(len(timestamps)):\n        try:\n            timestamps[i] = datetime.strptime(timestamps[i], '%Y-%m-%d %H:%M:%S')\n        except ValueError:\n            raise ValueError(f'Invalid timestamp found at line {i+1}')\n\n    # Create a pandas DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Type': types,\n        'Timestamp': timestamps,\n        'Message': messages\n    })\n\n    # Save the DataFrame to a CSV file\n    csv_file = log_file.replace('.log', '.csv')\n    df.to_csv(csv_file, index=False)\n\n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a text.\n    The function generates a histogram subplot, which facilitates the understanding of how word lengths vary within the provided text.\n    \"\"\"\n    # Filter out non-word characters and split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n\n    # Create a histogram of word lengths\n    bins = np.arange(min(word_lengths), max(word_lengths) + 2, 1) - 0.5\n    ax = plt.hist(word_lengths, bins=bins, rwidth=rwidth, color='g', zorder=2)\n\n    # Set the x-axis label and title\n    plt.xlabel('Word Length')\n    plt.title('Distribution of Word Lengths')\n\n    return ax", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a text.\n    The function generates a histogram subplot, which facilitates the understanding of how word lengths vary within the provided text.\n    \"\"\"\n    # Filter out non-word characters and split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n\n    # Create a histogram of word lengths\n    bins = np.arange(min(word_lengths), max(word_lengths) + 2, 1) - 0.5\n    ax = plt.hist(word_lengths, bins=bins, rwidth=rwidth, color='g', zorder=2)\n\n    # Set the x-axis label and title\n    plt.xlabel('Word Length')\n    plt.title('Distribution of Word Lengths')\n\n    return ax"}
{"task_id": "BigCodeBench/177", "solution": "import pandas as pd\nimport re\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize\nfrom string import punctuation\ndef task_func(df):\n    \"\"\"\n    Extracts articles whose titles contain specific case-insensitive keywords (\"like\" or \"what\") from a DataFrame and analyzes the frequency of each word in the content of these articles, excluding punctuation.\n    \"\"\"\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"The DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    # Extract articles with titles containing \"like\" or \"what\"\n    articles = df[df['Title'].str.contains(r'(?i)like|what')]\n\n    # Extract words from the content of the articles\n    words = word_tokenize(articles['Content'].str.cat(sep=' '))\n\n    # Remove punctuation\n    words = [''.join(c for c in w if c not in punctuation) for w in words]\n\n    # Count the frequency of each word\n    word_freq = dict(Counter(words))\n\n    return word_freq", "raw_solution": "import pandas as pd\nimport re\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize\nfrom string import punctuation\n\ndef task_func(df):\n    \"\"\"\n    Extracts articles whose titles contain specific case-insensitive keywords (\"like\" or \"what\") from a DataFrame and analyzes the frequency of each word in the content of these articles, excluding punctuation.\n    \"\"\"\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"The DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    # Extract articles with titles containing \"like\" or \"what\"\n    articles = df[df['Title'].str.contains(r'(?i)like|what')]\n\n    # Extract words from the content of the articles\n    words = word_tokenize(articles['Content'].str.cat(sep=' '))\n\n    # Remove punctuation\n    words = [''.join(c for c in w if c not in punctuation) for w in words]\n\n    # Count the frequency of each word\n    word_freq = dict(Counter(words))\n\n    return word_freq"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    \"\"\"\n    Prepares and transforms text data from a specified column in a DataFrame by removing stopwords, numbers, and punctuation,\n    and subsequently applying a vectorization process to convert text into a numeric format suitable for analysis.\n    \"\"\"\n    # Remove stopwords, numbers, and punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join(\n        [word for word in x.split() if word not in STOPWORDS and not word.isdigit() and word.isalpha()]))\n\n    # Apply vectorization\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert to DataFrame\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    \"\"\"\n    Prepares and transforms text data from a specified column in a DataFrame by removing stopwords, numbers, and punctuation,\n    and subsequently applying a vectorization process to convert text into a numeric format suitable for analysis.\n    \"\"\"\n    # Remove stopwords, numbers, and punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join(\n        [word for word in x.split() if word not in STOPWORDS and not word.isdigit() and word.isalpha()]))\n\n    # Apply vectorization\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert to DataFrame\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not isinstance(dic, dict):\n        raise ValueError(\"Input 'dic' should be a dictionary.\")\n    if not isinstance(cities, list):\n        raise ValueError(\"Input 'cities' should be a list.\")\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"'Lon' and 'Lat' keys are required in the dictionary.\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"'Lon' and 'Lat' values should be tuples.\")\n\n    lon_range, lat_range = dic['Lon'], dic['Lat']\n    coordinates = [(np.random.uniform(lon_range[0], lon_range[1]), np.random.uniform(lat_range[0], lat_range[1])) for _ in range(len(cities))]\n\n    geometry = [Point(xy) for xy in coordinates]\n    df = gpd.GeoDataFrame(cities, geometry=geometry, crs=4326)\n    df.columns = ['City', 'Coordinates']\n\n    return df", "raw_solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not isinstance(dic, dict):\n        raise ValueError(\"Input 'dic' should be a dictionary.\")\n    if not isinstance(cities, list):\n        raise ValueError(\"Input 'cities' should be a list.\")\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"'Lon' and 'Lat' keys are required in the dictionary.\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"'Lon' and 'Lat' values should be tuples.\")\n\n    lon_range, lat_range = dic['Lon'], dic['Lat']\n    coordinates = [(np.random.uniform(lon_range[0], lon_range[1]), np.random.uniform(lat_range[0], lat_range[1])) for _ in range(len(cities))]\n\n    geometry = [Point(xy) for xy in coordinates]\n    df = gpd.GeoDataFrame(cities, geometry=geometry, crs=4326)\n    df.columns = ['City', 'Coordinates']\n\n    return df"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    Generate a weather report for specified cities at a given UTC datetime.\n    \"\"\"\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n\n    set_seed(seed)\n\n    data = []\n    for city in cities:\n        tz = pytz.timezone(timezones[city])\n        local_datetime = utc_datetime.astimezone(tz)\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        data.append([city, local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z'), weather_condition])\n\n    df = pd.DataFrame(data, columns=['City', 'Local Time', 'Weather Condition'])\n    return df", "raw_solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    Generate a weather report for specified cities at a given UTC datetime.\n    \"\"\"\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n\n    set_seed(seed)\n\n    data = []\n    for city in cities:\n        tz = pytz.timezone(timezones[city])\n        local_datetime = utc_datetime.astimezone(tz)\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        data.append([city, local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z'), weather_condition])\n\n    df = pd.DataFrame(data, columns=['City', 'Local Time', 'Weather Condition'])\n    return df"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    np.random.seed(seed)\n    random_walk = np.random.choice([-1, 1], size=elements)\n    random_walk_cumulative = np.cumsum(random_walk)\n\n    stats = {\n        'count': elements,\n        'mean': np.mean(random_walk_cumulative),\n        'std': np.std(random_walk_cumulative),\n        'min': np.min(random_walk_cumulative),\n        '5th percentile': np.percentile(random_walk_cumulative, 5),\n        '25th percentile': np.percentile(random_walk_cumulative, 25),\n        'median': np.median(random_walk_cumulative),\n        '75th percentile': np.percentile(random_walk_cumulative, 75),\n        '95th percentile': np.percentile(random_walk_cumulative, 95),\n        'max': np.max(random_walk_cumulative),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(random_walk_cumulative)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Cumulative Sum')\n\n    return stats, ax.figure", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    np.random.seed(seed)\n    random_walk = np.random.choice([-1, 1], size=elements)\n    random_walk_cumulative = np.cumsum(random_walk)\n\n    stats = {\n        'count': elements,\n        'mean': np.mean(random_walk_cumulative),\n        'std': np.std(random_walk_cumulative),\n        'min': np.min(random_walk_cumulative),\n        '5th percentile': np.percentile(random_walk_cumulative, 5),\n        '25th percentile': np.percentile(random_walk_cumulative, 25),\n        'median': np.median(random_walk_cumulative),\n        '75th percentile': np.percentile(random_walk_cumulative, 75),\n        '95th percentile': np.percentile(random_walk_cumulative, 95),\n        'max': np.max(random_walk_cumulative),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(random_walk_cumulative)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Cumulative Sum')\n\n    return stats, ax.figure"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\nfrom io import BytesIO\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Downloads a zip file from a URL, extracts its contents to a specified directory, and returns a list of the extracted files.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n\n    # Raise an exception if the GET request was unsuccessful\n    response.raise_for_status()\n\n    # Create a zipfile object from the response content\n    zip_file = zipfile.ZipFile(BytesIO(response.content))\n\n    # Extract all files in the zip file to the destination directory\n    zip_file.extractall(destination_directory)\n\n    # Get a list of all files in the destination directory\n    extracted_files = os.listdir(destination_directory)\n\n    return extracted_files", "raw_solution": "import requests\nimport os\nimport zipfile\nfrom io import BytesIO\n\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Downloads a zip file from a URL, extracts its contents to a specified directory, and returns a list of the extracted files.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n\n    # Raise an exception if the GET request was unsuccessful\n    response.raise_for_status()\n\n    # Create a zipfile object from the response content\n    zip_file = zipfile.ZipFile(BytesIO(response.content))\n\n    # Extract all files in the zip file to the destination directory\n    zip_file.extractall(destination_directory)\n\n    # Get a list of all files in the destination directory\n    extracted_files = os.listdir(destination_directory)\n\n    return extracted_files"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low should be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size)\n\n    fig, ax = plt.subplots(1, 1)\n    ax.imshow(image)\n    ax.axis('off')\n    plt.show()\n\n    return ax, image", "raw_solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low should be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size)\n\n    fig, ax = plt.subplots(1, 1)\n    ax.imshow(image)\n    ax.axis('off')\n    plt.show()\n\n    return ax, image"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from a list L, normalizes it based on the sound pressure level (SPL) of a specified audio file, and generates a spectrogram from the matrix.\n    \"\"\"\n    # Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"The specified audio file {audio_file} does not exist.\")\n\n    # Read the audio file\n    data, sample_rate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10 ** (spl / 20))\n\n    # Generate the spectrogram\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(normalized_matrix.flatten())), ref=np.max)\n\n    # Create the figure for the spectrogram\n    plt.figure(figsize=(14, 5))\n    librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Spectrogram')\n\n    return normalized_matrix, plt.gcf()", "raw_solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from a list L, normalizes it based on the sound pressure level (SPL) of a specified audio file, and generates a spectrogram from the matrix.\n    \"\"\"\n    # Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"The specified audio file {audio_file} does not exist.\")\n\n    # Read the audio file\n    data, sample_rate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10 ** (spl / 20))\n\n    # Generate the spectrogram\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(normalized_matrix.flatten())), ref=np.max)\n\n    # Create the figure for the spectrogram\n    plt.figure(figsize=(14, 5))\n    librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Spectrogram')\n\n    return normalized_matrix, plt.gcf()"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values\n    numeric_values = [val for sublist in original for val in sublist if isinstance(val, (int, float))]\n    numeric_values = np.array(numeric_values)\n\n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n\n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n\n    # Generate PDF\n    x_values = np.linspace(stats_dict['min'], stats_dict['max'], 1000)\n    pdf_values = stats.norm.pdf(x_values, stats_dict['mean'], stats_dict['std_dev'])\n\n    ax.plot(x_values, pdf_values, label='PDF')\n    ax.legend()\n\n    return numeric_values, stats_dict, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extract numeric values\n    numeric_values = [val for sublist in original for val in sublist if isinstance(val, (int, float))]\n    numeric_values = np.array(numeric_values)\n\n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n\n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n\n    # Generate PDF\n    x_values = np.linspace(stats_dict['min'], stats_dict['max'], 1000)\n    pdf_values = stats.norm.pdf(x_values, stats_dict['mean'], stats_dict['std_dev'])\n\n    ax.plot(x_values, pdf_values, label='PDF')\n    ax.legend()\n\n    return numeric_values, stats_dict, ax"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array], norm='l2')[0]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the original and normalized arrays\n    ax.plot(original_array, label='Original Data')\n    ax.plot(normalized_array, label='Normalized Data')\n\n    # Set the title and labels\n    ax.set_title('Original vs. Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    # Add a legend\n    ax.legend()\n\n    # Return the numpy arrays and the axes object\n    return original_array, normalized_array, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array], norm='l2')[0]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the original and normalized arrays\n    ax.plot(original_array, label='Original Data')\n    ax.plot(normalized_array, label='Normalized Data')\n\n    # Set the title and labels\n    ax.set_title('Original vs. Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    # Add a legend\n    ax.legend()\n\n    # Return the numpy arrays and the axes object\n    return original_array, normalized_array, ax"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # 1. Adds a new key \"a\" with the value 1 to the dictionary.\n    data['a'] = 1\n\n    # 2. Generates a signal based on the values in \"data\".\n    signal = list(data.values())\n\n    # 3. Runs a Fast Fourier Transform (FFT) on the signal.\n    fft_signal = fftpack.fft(signal)\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n\n    # 4. Plots and returns the FFT of the signal with a title of 'FFT of the signal'.\n    plt.figure(figsize=(10, 5))\n    plt.plot(freqs, np.abs(fft_signal))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n    plt.show()\n\n    return (freqs, np.abs(fft_signal))", "raw_solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # 1. Adds a new key \"a\" with the value 1 to the dictionary.\n    data['a'] = 1\n\n    # 2. Generates a signal based on the values in \"data\".\n    signal = list(data.values())\n\n    # 3. Runs a Fast Fourier Transform (FFT) on the signal.\n    fft_signal = fftpack.fft(signal)\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n\n    # 4. Plots and returns the FFT of the signal with a title of 'FFT of the signal'.\n    plt.figure(figsize=(10, 5))\n    plt.plot(freqs, np.abs(fft_signal))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n    plt.show()\n\n    return (freqs, np.abs(fft_signal))"}
{"task_id": "BigCodeBench/273", "solution": "import http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nclass HTTPRequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n\n        if self.headers['Content-Type'] != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Content-Type header is not application/json'\n            }).encode())\n            return\n\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Invalid JSON'\n            }).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'No data key in request'\n            }).encode())\n            return\n\n        self.send_response(200)\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\ndef task_func():\n    \"\"\"\n    This function creates an HTTP server that listens on port 8000 and handles incoming POST requests.\n    \"\"\"\n    server_address = ('', 8000)\n    httpd = http.server.HTTPServer(server_address, HTTPRequestHandler)\n    print('Starting server on port 8000...')\n    httpd.serve_forever()", "raw_solution": "import http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\nclass HTTPRequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n\n        if self.headers['Content-Type'] != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Content-Type header is not application/json'\n            }).encode())\n            return\n\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Invalid JSON'\n            }).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'No data key in request'\n            }).encode())\n            return\n\n        self.send_response(200)\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\n\ndef task_func():\n    \"\"\"\n    This function creates an HTTP server that listens on port 8000 and handles incoming POST requests.\n    \"\"\"\n    server_address = ('', 8000)\n    httpd = http.server.HTTPServer(server_address, HTTPRequestHandler)\n    print('Starting server on port 8000...')\n    httpd.serve_forever()"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\nfrom json import JSONDecodeError\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            try:\n                # Parse the request body as JSON\n                content_length = int(self.headers['Content-Length'])\n                post_data = self.rfile.read(content_length)\n                email_data = json.loads(post_data)\n\n                # Extract the email data\n                subject = email_data['subject']\n                message = email_data['message']\n                to = email_data['to']\n\n                # Create the email\n                msg = MIMEText(message)\n                msg['Subject'] = subject\n                msg['From'] = smtp_username\n                msg['To'] = to\n\n                # Send the email\n                smtp = smtplib.SMTP(smtp_server, smtp_port)\n                smtp.login(smtp_username, smtp_password)\n                smtp.send_message(msg)\n                smtp.quit()\n\n                # Send a 200 OK response\n                self.send_response(200)\n                self.end_headers()\n                response = 'Email sent successfully'.encode()\n                self.wfile.write(response)\n\n            except JSONDecodeError:\n                # Send a 400 Bad Request response if the email data is not valid JSON\n                self.send_response(400)\n                self.end_headers()\n                response = 'Invalid JSON'.encode()\n                self.wfile.write(response)\n\n            except KeyError:\n                # Send a 400 Bad Request response if the 'subject', 'message', or 'to' keys are missing from the email data\n                self.send_response(400)\n                self.end_headers()\n                response = 'Missing email data'.encode()\n                self.wfile.write(response)\n\n            except smtplib.SMTPAuthenticationError:\n                # Send a 535 Authentication Failed response if there is an authentication issue with the SMTP server\n                self.send_response(535)\n                self.end_headers()\n                response = 'SMTP Authentication Error'.encode()\n                self.wfile.write(response)\n\n    return EmailHandler", "raw_solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\nfrom json import JSONDecodeError\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            try:\n                # Parse the request body as JSON\n                content_length = int(self.headers['Content-Length'])\n                post_data = self.rfile.read(content_length)\n                email_data = json.loads(post_data)\n\n                # Extract the email data\n                subject = email_data['subject']\n                message = email_data['message']\n                to = email_data['to']\n\n                # Create the email\n                msg = MIMEText(message)\n                msg['Subject'] = subject\n                msg['From'] = smtp_username\n                msg['To'] = to\n\n                # Send the email\n                smtp = smtplib.SMTP(smtp_server, smtp_port)\n                smtp.login(smtp_username, smtp_password)\n                smtp.send_message(msg)\n                smtp.quit()\n\n                # Send a 200 OK response\n                self.send_response(200)\n                self.end_headers()\n                response = 'Email sent successfully'.encode()\n                self.wfile.write(response)\n\n            except JSONDecodeError:\n                # Send a 400 Bad Request response if the email data is not valid JSON\n                self.send_response(400)\n                self.end_headers()\n                response = 'Invalid JSON'.encode()\n                self.wfile.write(response)\n\n            except KeyError:\n                # Send a 400 Bad Request response if the 'subject', 'message', or 'to' keys are missing from the email data\n                self.send_response(400)\n                self.end_headers()\n                response = 'Missing email data'.encode()\n                self.wfile.write(response)\n\n            except smtplib.SMTPAuthenticationError:\n                # Send a 535 Authentication Failed response if there is an authentication issue with the SMTP server\n                self.send_response(535)\n                self.end_headers()\n                response = 'SMTP Authentication Error'.encode()\n                self.wfile.write(response)\n\n    return EmailHandler"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(directory):\n    \"\"\"\n    Count the number of words in .txt files within a specified directory, export the counts to a JSON file, and then return the total number of words.\n    \"\"\"\n    word_counts = Counter()\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                words = file.read().split()\n                word_counts.update(words)\n\n    # Export the counts to a JSON file\n    with open('word_counts.json', 'w') as file:\n        json.dump(word_counts, file)\n\n    # Return the total number of words\n    return sum(word_counts.values())", "raw_solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(directory):\n    \"\"\"\n    Count the number of words in .txt files within a specified directory, export the counts to a JSON file, and then return the total number of words.\n    \"\"\"\n    word_counts = Counter()\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                words = file.read().split()\n                word_counts.update(words)\n\n    # Export the counts to a JSON file\n    with open('word_counts.json', 'w') as file:\n        json.dump(word_counts, file)\n\n    # Return the total number of words\n    return sum(word_counts.values())"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns, calculates the Pearson correlation\n    coefficient between these columns, and optionally visualizes the correlation matrix using a heatmap.\n    \"\"\"\n    if df.empty or 'Value' not in df.columns:\n        raise ValueError(\"Invalid DataFrame input. It should not be empty and contain a 'Value' column.\")\n\n    # Split lists in the 'Value' column into separate columns\n    df_split = pd.DataFrame(df['Value'].to_list())\n    df_split.columns = [f'Value_{i}' for i in range(df_split.shape[1])]\n    df = pd.concat([df.drop('Value', axis=1), df_split], axis=1)\n\n    # Calculate Pearson correlation coefficients\n    corr_matrix = df.corr(method='pearson')\n\n    if plot:\n        # Visualize the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n\n    return corr_matrix", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns, calculates the Pearson correlation\n    coefficient between these columns, and optionally visualizes the correlation matrix using a heatmap.\n    \"\"\"\n    if df.empty or 'Value' not in df.columns:\n        raise ValueError(\"Invalid DataFrame input. It should not be empty and contain a 'Value' column.\")\n\n    # Split lists in the 'Value' column into separate columns\n    df_split = pd.DataFrame(df['Value'].to_list())\n    df_split.columns = [f'Value_{i}' for i in range(df_split.shape[1])]\n    df = pd.concat([df.drop('Value', axis=1), df_split], axis=1)\n\n    # Calculate Pearson correlation coefficients\n    corr_matrix = df.corr(method='pearson')\n\n    if plot:\n        # Visualize the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n\n    return corr_matrix"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func():\n    # Generate random grades for each student in each subject\n    random.seed(0)\n    data = {field: [random.randint(0, 100) for _ in range(100)] for field in FIELDS}\n    data['Student'] = STUDENTS\n\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data)\n    df.set_index('Student', inplace=True)\n\n    # Calculate the average grade for each student\n    df['Average'] = df.mean(axis=1)\n\n    # Calculate the average grade for each subject\n    subject_averages = df.mean()\n\n    # Append the subject averages to the DataFrame\n    df = df.append(subject_averages, ignore_index=True)\n\n    # Rename the last row to 'Average'\n    df.rename(index={len(df)-1: 'Average'}, inplace=True)\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func():\n    # Generate random grades for each student in each subject\n    random.seed(0)\n    data = {field: [random.randint(0, 100) for _ in range(100)] for field in FIELDS}\n    data['Student'] = STUDENTS\n\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data)\n    df.set_index('Student', inplace=True)\n\n    # Calculate the average grade for each student\n    df['Average'] = df.mean(axis=1)\n\n    # Calculate the average grade for each subject\n    subject_averages = df.mean()\n\n    # Append the subject averages to the DataFrame\n    df = df.append(subject_averages, ignore_index=True)\n\n    # Rename the last row to 'Average'\n    df.rename(index={len(df)-1: 'Average'}, inplace=True)\n\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Generate data\n    data = []\n    for i in range(PEOPLE_COUNT):\n        name = f'Person{i+1}'\n        age = random.randint(18, 80)\n        height = round(random.gauss(170, 10), 2)\n        weight = round(random.gauss(70, 10), 2)\n        data.append([name, age, height, weight])\n\n    # Calculate averages\n    avg_age = round(mean(row[1] for row in data), 2)\n    avg_height = round(mean(row[2] for row in data), 2)\n    avg_weight = round(mean(row[3] for row in data), 2)\n    data.append([f'Average', avg_age, avg_height, avg_weight])\n\n    # Write to CSV\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    return filename", "raw_solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    # Generate data\n    data = []\n    for i in range(PEOPLE_COUNT):\n        name = f'Person{i+1}'\n        age = random.randint(18, 80)\n        height = round(random.gauss(170, 10), 2)\n        weight = round(random.gauss(70, 10), 2)\n        data.append([name, age, height, weight])\n\n    # Calculate averages\n    avg_age = round(mean(row[1] for row in data), 2)\n    avg_height = round(mean(row[2] for row in data), 2)\n    avg_weight = round(mean(row[3] for row in data), 2)\n    data.append([f'Average', avg_age, avg_height, avg_weight])\n\n    # Write to CSV\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    return filename"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom typing import Tuple, Dict\ndef task_func(directory: str) -> Tuple[str, Dict[str, list]]:\n    \"\"\"\n    Organizes files in a directory based on the first text that is not enclosed in square brackets.\n    Moves the files to subdirectories named after this text. If no matching text is found, the file is not moved.\n    \"\"\"\n    # Create a dictionary to store the files for each subdirectory\n    subdir_files = {}\n\n    # Iterate over each file in the directory\n    for filename in os.listdir(directory):\n        # Use a regular expression to find the first text that is not enclosed in square brackets\n        match = re.search(r'\\[([^\\]]*)\\]', filename)\n        if match:\n            # If a match is found, get the text and create a subdirectory if it doesn't exist\n            subdir = os.path.join(directory, match.group(1))\n            if not os.path.exists(subdir):\n                os.mkdir(subdir)\n            # Move the file to the subdirectory\n            shutil.move(os.path.join(directory, filename), subdir)\n            # Add the file to the list of files for the subdirectory\n            if subdir in subdir_files:\n                subdir_files[subdir].append(filename)\n            else:\n                subdir_files[subdir] = [filename]\n\n    # Return the directory path and the dictionary of subdirectories and files\n    return directory, subdir_files", "raw_solution": "import re\nimport os\nimport shutil\nfrom typing import Tuple, Dict\n\ndef task_func(directory: str) -> Tuple[str, Dict[str, list]]:\n    \"\"\"\n    Organizes files in a directory based on the first text that is not enclosed in square brackets.\n    Moves the files to subdirectories named after this text. If no matching text is found, the file is not moved.\n    \"\"\"\n    # Create a dictionary to store the files for each subdirectory\n    subdir_files = {}\n\n    # Iterate over each file in the directory\n    for filename in os.listdir(directory):\n        # Use a regular expression to find the first text that is not enclosed in square brackets\n        match = re.search(r'\\[([^\\]]*)\\]', filename)\n        if match:\n            # If a match is found, get the text and create a subdirectory if it doesn't exist\n            subdir = os.path.join(directory, match.group(1))\n            if not os.path.exists(subdir):\n                os.mkdir(subdir)\n            # Move the file to the subdirectory\n            shutil.move(os.path.join(directory, filename), subdir)\n            # Add the file to the list of files for the subdirectory\n            if subdir in subdir_files:\n                subdir_files[subdir].append(filename)\n            else:\n                subdir_files[subdir] = [filename]\n\n    # Return the directory path and the dictionary of subdirectories and files\n    return directory, subdir_files"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\nfrom typing import List\ndef task_func(file_list: List[str]) -> List[int]:\n    \"\"\"\n    Run files from list of files as subprocesses at the same time.\n    \"\"\"\n    exit_codes = [0] * len(file_list)\n    threads = []\n\n    def run_file(i, file):\n        \"\"\"\n        Helper function to run a file as a subprocess.\n        \"\"\"\n        try:\n            exit_codes[i] = subprocess.run([\"python\", file], check=True).returncode\n        except subprocess.CalledProcessError as e:\n            exit_codes[i] = e.returncode\n\n    for i, file in enumerate(file_list):\n        thread = threading.Thread(target=run_file, args=(i, file))\n        thread.start()\n        threads.append(thread)\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes", "raw_solution": "import subprocess\nimport time\nimport threading\nfrom typing import List\n\ndef task_func(file_list: List[str]) -> List[int]:\n    \"\"\"\n    Run files from list of files as subprocesses at the same time.\n    \"\"\"\n    exit_codes = [0] * len(file_list)\n    threads = []\n\n    def run_file(i, file):\n        \"\"\"\n        Helper function to run a file as a subprocess.\n        \"\"\"\n        try:\n            exit_codes[i] = subprocess.run([\"python\", file], check=True).returncode\n        except subprocess.CalledProcessError as e:\n            exit_codes[i] = e.returncode\n\n    for i, file in enumerate(file_list):\n        thread = threading.Thread(target=run_file, args=(i, file))\n        thread.start()\n        threads.append(thread)\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    \"\"\"\n    Finds and runs all .bat files in a given directory, returning their file names and exit codes.\n    \"\"\"\n    # Get all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n\n    # Initialize an empty list to store the file names and exit codes\n    results = []\n\n    # Iterate over each .bat file\n    for file in bat_files:\n        # Try to run the file\n        try:\n            # Run the file and capture its output\n            output = subprocess.run(file, capture_output=True, text=True, shell=True, check=True)\n\n            # Get the exit code\n            exit_code = output.returncode\n        except subprocess.CalledProcessError as e:\n            # If the file could not be executed, set the exit code to None\n            exit_code = None\n\n        # Add the file name and exit code to the results list\n        results.append((file, exit_code))\n\n    # Return the results list\n    return results", "raw_solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    Finds and runs all .bat files in a given directory, returning their file names and exit codes.\n    \"\"\"\n    # Get all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n\n    # Initialize an empty list to store the file names and exit codes\n    results = []\n\n    # Iterate over each .bat file\n    for file in bat_files:\n        # Try to run the file\n        try:\n            # Run the file and capture its output\n            output = subprocess.run(file, capture_output=True, text=True, shell=True, check=True)\n\n            # Get the exit code\n            exit_code = output.returncode\n        except subprocess.CalledProcessError as e:\n            # If the file could not be executed, set the exit code to None\n            exit_code = None\n\n        # Add the file name and exit code to the results list\n        results.append((file, exit_code))\n\n    # Return the results list\n    return results"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"The input df must be DataFrame, not be empty\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the column {col}\")\n\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n    if df[col].dtype in ['int64', 'float64']:\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.countplot(y=df[col], ax=axes[0])\n    axes[0].set_title(f'Histogram of {col}')\n\n    sns.boxplot(y=df[col], ax=axes[1])\n    axes[1].set_title(f'Box plot of {col}')\n\n    plt.tight_layout()\n\n    return fig", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"The input df must be DataFrame, not be empty\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the column {col}\")\n\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n    if df[col].dtype in ['int64', 'float64']:\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.countplot(y=df[col], ax=axes[0])\n    axes[0].set_title(f'Histogram of {col}')\n\n    sns.boxplot(y=df[col], ax=axes[1])\n    axes[1].set_title(f'Box plot of {col}')\n\n    plt.tight_layout()\n\n    return fig"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Runs a Python script as a process with predefined arguments.\n    By default, waits for the process to complete. If wait is False, the function returns None.\n    Raises:\n        - ValueError: If the script does not exist.\n        - subprocess.CalledProcessError: If the script raises an exception.\n    Returns:\n        int: The return code of the subprocess. If 'wait' is False, returns None.\n    \"\"\"\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n\n    try:\n        if wait:\n            result = subprocess.run([sys.executable, script_path] + list(args), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            return result.returncode\n        else:\n            subprocess.Popen([sys.executable, script_path] + list(args))\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output, stderr=e.stderr)", "raw_solution": "import subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Runs a Python script as a process with predefined arguments.\n    By default, waits for the process to complete. If wait is False, the function returns None.\n    Raises:\n        - ValueError: If the script does not exist.\n        - subprocess.CalledProcessError: If the script raises an exception.\n    Returns:\n        int: The return code of the subprocess. If 'wait' is False, returns None.\n    \"\"\"\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n\n    try:\n        if wait:\n            result = subprocess.run([sys.executable, script_path] + list(args), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            return result.returncode\n        else:\n            subprocess.Popen([sys.executable, script_path] + list(args))\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output, stderr=e.stderr)\n\n# Test the function"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    Loads data from an Excel spreadsheet (.xlsx), calculates the mean and standard deviation of each column,\n    and draws a bar chart.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(file_location):\n        raise FileNotFoundError(f\"The file {file_location} does not exist.\")\n\n    # Load data from Excel\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except KeyError:\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the workbook.\")\n\n    # Calculate mean and standard deviation\n    stats = df.describe().T.astype(float)\n    stats['mean'] = stats['mean'].round(2)\n    stats['std'] = stats['std'].round(2)\n    stats_dict = stats.to_dict()\n\n    # Draw bar chart\n    fig, ax = plt.subplots()\n    ax.bar(stats.index, stats['mean'], yerr=stats['std'], align='center', alpha=0.5, ecolor='black', capsize=10)\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n\n    return stats_dict, fig", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    Loads data from an Excel spreadsheet (.xlsx), calculates the mean and standard deviation of each column,\n    and draws a bar chart.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(file_location):\n        raise FileNotFoundError(f\"The file {file_location} does not exist.\")\n\n    # Load data from Excel\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except KeyError:\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the workbook.\")\n\n    # Calculate mean and standard deviation\n    stats = df.describe().T.astype(float)\n    stats['mean'] = stats['mean'].round(2)\n    stats['std'] = stats['std'].round(2)\n    stats_dict = stats.to_dict()\n\n    # Draw bar chart\n    fig, ax = plt.subplots()\n    ax.bar(stats.index, stats['mean'], yerr=stats['std'], align='center', alpha=0.5, ecolor='black', capsize=10)\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n\n    return stats_dict, fig"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects\")\n\n    # Create a dictionary to store the count of activities for each day of the week\n    activity_counts = defaultdict(int)\n    for activity in activities:\n        activity_counts[activity.strftime(\"%A\")] += 1\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(activity_counts.keys(), activity_counts.values())\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    return ax", "raw_solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects\")\n\n    # Create a dictionary to store the count of activities for each day of the week\n    activity_counts = defaultdict(int)\n    for activity in activities:\n        activity_counts[activity.strftime(\"%A\")] += 1\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(activity_counts.keys(), activity_counts.values())\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    return ax"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n\n    # Filter out directories\n    files = [f for f in files if os.path.isfile(os.path.join(src_dir, f))]\n\n    # If there are no files in the source directory, return an empty string\n    if not files:\n        return \"\"\n\n    # Select a random file\n    file_name = random.choice(files)\n\n    # Construct the full path to the file\n    src_path = os.path.join(src_dir, file_name)\n    dest_path = os.path.join(dest_dir, file_name)\n\n    # Move the file\n    shutil.move(src_path, dest_path)\n\n    # Return the name of the file moved\n    return file_name", "raw_solution": "import os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n\n    # Filter out directories\n    files = [f for f in files if os.path.isfile(os.path.join(src_dir, f))]\n\n    # If there are no files in the source directory, return an empty string\n    if not files:\n        return \"\"\n\n    # Select a random file\n    file_name = random.choice(files)\n\n    # Construct the full path to the file\n    src_path = os.path.join(src_dir, file_name)\n    dest_path = os.path.join(dest_dir, file_name)\n\n    # Move the file\n    shutil.move(src_path, dest_path)\n\n    # Return the name of the file moved\n    return file_name"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Protects all double quotes in all Excel (.xlsx) files in the specified directory by prefixing them with a double backslash.\n    \"\"\"\n    # Get all .xlsx files in the directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Initialize counter\n    num_files_processed = 0\n\n    # Process each file\n    for file in xlsx_files:\n        # Load workbook\n        wb = load_workbook(filename=file)\n\n        # Iterate over each worksheet\n        for sheet in wb:\n            # Iterate over each cell\n            for row in sheet:\n                for cell in row:\n                    # If cell contains double quotes, replace them with double backslashes\n                    if cell and '\"' in cell.value:\n                        cell.value = re.sub(r'\"', '\\\\\"', cell.value)\n\n        # Save workbook\n        wb.save(file)\n\n        # Increment counter\n        num_files_processed += 1\n\n    return num_files_processed", "raw_solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Protects all double quotes in all Excel (.xlsx) files in the specified directory by prefixing them with a double backslash.\n    \"\"\"\n    # Get all .xlsx files in the directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Initialize counter\n    num_files_processed = 0\n\n    # Process each file\n    for file in xlsx_files:\n        # Load workbook\n        wb = load_workbook(filename=file)\n\n        # Iterate over each worksheet\n        for sheet in wb:\n            # Iterate over each cell\n            for row in sheet:\n                for cell in row:\n                    # If cell contains double quotes, replace them with double backslashes\n                    if cell and '\"' in cell.value:\n                        cell.value = re.sub(r'\"', '\\\\\"', cell.value)\n\n        # Save workbook\n        wb.save(file)\n\n        # Increment counter\n        num_files_processed += 1\n\n    return num_files_processed"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n\n    x = np.linspace(0, 2*np.pi, sample_size)\n    y_sin = np.sin(frequency * x)\n    y_cos = np.cos(frequency * x)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y_sin, label='Sine')\n    ax.plot(x, y_cos, label='Cosine')\n    ax.legend()\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Sine and Cosine Waves')\n    return fig, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n\n    x = np.linspace(0, 2*np.pi, sample_size)\n    y_sin = np.sin(frequency * x)\n    y_cos = np.cos(frequency * x)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y_sin, label='Sine')\n    ax.plot(x, y_cos, label='Cosine')\n    ax.legend()\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Sine and Cosine Waves')\n    return fig, ax"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    \"\"\"\n    Initializes a Flask-Mail instance for sending emails using the generated Flask application with the specified app_name.\n    \"\"\"\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables, use defaults if not available\n    app.config['MAIL_SERVER'] = os.environ.get('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.environ.get('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = os.environ.get('MAIL_USE_TLS', False) in ['True', 'true', '1']\n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD', None)\n\n    mail = Mail(app)\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']", "raw_solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    \"\"\"\n    Initializes a Flask-Mail instance for sending emails using the generated Flask application with the specified app_name.\n    \"\"\"\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables, use defaults if not available\n    app.config['MAIL_SERVER'] = os.environ.get('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.environ.get('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = os.environ.get('MAIL_USE_TLS', False) in ['True', 'true', '1']\n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD', None)\n\n    mail = Mail(app)\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculates the mean, median, and standard deviation of the data from a specific column in an Excel file.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(os.path.join(excel_file_path, file_name)):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path.\")\n\n    # Load the Excel file\n    df = pd.read_excel(os.path.join(excel_file_path, file_name))\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file.\")\n\n    # Calculate the mean, median, and standard deviation\n    mean = df[column_name].mean()\n    median = df[column_name].median()\n    std_dev = df[column_name].std()\n\n    # Return the results in a dictionary\n    return {\n        'mean': mean,\n        'median': median,\n        'standard deviation': std_dev\n    }", "raw_solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculates the mean, median, and standard deviation of the data from a specific column in an Excel file.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(os.path.join(excel_file_path, file_name)):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path.\")\n\n    # Load the Excel file\n    df = pd.read_excel(os.path.join(excel_file_path, file_name))\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file.\")\n\n    # Calculate the mean, median, and standard deviation\n    mean = df[column_name].mean()\n    median = df[column_name].median()\n    std_dev = df[column_name].std()\n\n    # Return the results in a dictionary\n    return {\n        'mean': mean,\n        'median': median,\n        'standard deviation': std_dev\n    }"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y, learning_rate=0.01):\n    \"\"\"\n    Trains a simple neural network on given input data and target labels.\n    \"\"\"\n    # Split the data into a training set (75%) and a test set (25%)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct a Sequential model with one dense hidden layer and a sigmoid activation function\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model using binary cross-entropy loss and SGD optimizer with a specified learning rate\n    sgd = SGD(lr=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # Fit the model to the training data (without verbose output), also evaluating it on the test set as validation data\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plots the model's training and validation loss over epochs\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    return model, plt.gca()", "raw_solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y, learning_rate=0.01):\n    \"\"\"\n    Trains a simple neural network on given input data and target labels.\n    \"\"\"\n    # Split the data into a training set (75%) and a test set (25%)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct a Sequential model with one dense hidden layer and a sigmoid activation function\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model using binary cross-entropy loss and SGD optimizer with a specified learning rate\n    sgd = SGD(lr=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # Fit the model to the training data (without verbose output), also evaluating it on the test set as validation data\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plots the model's training and validation loss over epochs\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_dim=X_train.shape[1], activation='sigmoid'),\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.001), metrics=['AUC'])\n\n    # Fit the model\n    history = model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and ROC area for each class\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model, plt.gca()", "raw_solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_dim=X_train.shape[1], activation='sigmoid'),\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.001), metrics=['AUC'])\n\n    # Fit the model\n    history = model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and ROC area for each class\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    Reads an RGB image, applies K-means clustering to segment the image into 'n_clusters' regions,\n    and saves each region as a separate image. The function returns numpy arrays of the original\n    and segmented images.\n    \"\"\"\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"Image file {image_path} does not exist.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' must be a positive integer.\")\n\n    # Read the image\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to be a list of RGB pixels\n    img_list = img.reshape((-1, 3))\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(img_list)\n\n    # Get the cluster labels for each pixel\n    labels = kmeans.predict(img_list)\n\n    # Create the segmented image\n    segmented_img = kmeans.cluster_centers_[labels].reshape(img.shape)\n\n    # Convert the segmented image to uint8\n    segmented_img = np.uint8(segmented_img)\n\n    return img, segmented_img", "raw_solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    Reads an RGB image, applies K-means clustering to segment the image into 'n_clusters' regions,\n    and saves each region as a separate image. The function returns numpy arrays of the original\n    and segmented images.\n    \"\"\"\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"Image file {image_path} does not exist.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' must be a positive integer.\")\n\n    # Read the image\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to be a list of RGB pixels\n    img_list = img.reshape((-1, 3))\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(img_list)\n\n    # Get the cluster labels for each pixel\n    labels = kmeans.predict(img_list)\n\n    # Create the segmented image\n    segmented_img = kmeans.cluster_centers_[labels].reshape(img.shape)\n\n    # Convert the segmented image to uint8\n    segmented_img = np.uint8(segmented_img)\n\n    return img, segmented_img"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculates the product of a matrix 'P' and a 3D tensor 'T', flattens the result, applies KMeans clustering to the flattened data, and visualizes it.\n    \"\"\"\n    # Calculate the product of P and T\n    product = np.matmul(P, T)\n\n    # Flatten the result\n    flattened = product.flatten()\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n\n    # Visualize the KMeans clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result)\n    ax.set_title('KMeans Clustering Visualization')\n\n    return cluster_result, ax", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculates the product of a matrix 'P' and a 3D tensor 'T', flattens the result, applies KMeans clustering to the flattened data, and visualizes it.\n    \"\"\"\n    # Calculate the product of P and T\n    product = np.matmul(P, T)\n\n    # Flatten the result\n    flattened = product.flatten()\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n\n    # Visualize the KMeans clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result)\n    ax.set_title('KMeans Clustering Visualization')\n\n    return cluster_result, ax"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculates the Voronoi diagram for a number of points in 2D and plots it.\n    Jittering is applied prior to plotting.\n    \"\"\"\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"'points' must be a numpy array\")\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"'points' must be a 2D array\")\n    if not isinstance(seed, int):\n        raise TypeError(\"'seed' must be an integer\")\n\n    np.random.seed(seed)\n    jitter = np.random.uniform(-1, 1, points.shape)\n    points += jitter\n\n    vor = Voronoi(points)\n\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    ax.scatter(points[:, 0], points[:, 1], color='r')\n\n    return vor, ax", "raw_solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculates the Voronoi diagram for a number of points in 2D and plots it.\n    Jittering is applied prior to plotting.\n    \"\"\"\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"'points' must be a numpy array\")\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"'points' must be a 2D array\")\n    if not isinstance(seed, int):\n        raise TypeError(\"'seed' must be an integer\")\n\n    np.random.seed(seed)\n    jitter = np.random.uniform(-1, 1, points.shape)\n    points += jitter\n\n    vor = Voronoi(points)\n\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    ax.scatter(points[:, 0], points[:, 1], color='r')\n\n    return vor, ax"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n    If a file with the same name already exists in the destination directory, it is not moved.\n    \"\"\"\n    if not os.path.exists(src_dir) or not os.path.exists(dest_dir):\n        raise FileNotFoundError(\"Either source or destination directory does not exist\")\n\n    moved_files = []\n    for file in glob.glob(os.path.join(src_dir, f'*.{ext}')):\n        filename = os.path.basename(file)\n        dest_file = os.path.join(dest_dir, filename)\n        if not os.path.exists(dest_file):\n            shutil.move(file, dest_dir)\n            moved_files.append(file)\n\n    return moved_files", "raw_solution": "import os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n    If a file with the same name already exists in the destination directory, it is not moved.\n    \"\"\"\n    if not os.path.exists(src_dir) or not os.path.exists(dest_dir):\n        raise FileNotFoundError(\"Either source or destination directory does not exist\")\n\n    moved_files = []\n    for file in glob.glob(os.path.join(src_dir, f'*.{ext}')):\n        filename = os.path.basename(file)\n        dest_file = os.path.join(dest_dir, filename)\n        if not os.path.exists(dest_file):\n            shutil.move(file, dest_dir)\n            moved_files.append(file)\n\n    return moved_files"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    \"\"\"\n    Loads a JSON string into a dictionary, normalizes the dictionary by doubling the numerical values,\n    and then creates a Pandas DataFrame from the dictionary.\n    \"\"\"\n    # Check if the JSON string is empty\n    if not json_str:\n        return pd.DataFrame()\n\n    # Load the JSON string into a dictionary\n    data_dict = json.loads(json_str)\n\n    # Normalize the dictionary by doubling the numerical values\n    def normalize_value(value):\n        if isinstance(value, (int, float)):\n            return value * 2\n        elif isinstance(value, str):\n            # Extract numerical values from the string\n            numbers = re.findall(r'[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?[\\d]*(?:[eE][-+]?\\d+)?', value)\n            if numbers:\n                return float(numbers[0]) * 2\n        elif isinstance(value, list):\n            return [normalize_value(v) for v in value]\n        else:\n            return value\n\n    data_dict = {key: normalize_value(value) for key, value in data_dict.items()}\n\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(data_dict)\n\n    return df", "raw_solution": "import json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    \"\"\"\n    Loads a JSON string into a dictionary, normalizes the dictionary by doubling the numerical values,\n    and then creates a Pandas DataFrame from the dictionary.\n    \"\"\"\n    # Check if the JSON string is empty\n    if not json_str:\n        return pd.DataFrame()\n\n    # Load the JSON string into a dictionary\n    data_dict = json.loads(json_str)\n\n    # Normalize the dictionary by doubling the numerical values\n    def normalize_value(value):\n        if isinstance(value, (int, float)):\n            return value * 2\n        elif isinstance(value, str):\n            # Extract numerical values from the string\n            numbers = re.findall(r'[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?[\\d]*(?:[eE][-+]?\\d+)?', value)\n            if numbers:\n                return float(numbers[0]) * 2\n        elif isinstance(value, list):\n            return [normalize_value(v) for v in value]\n        else:\n            return value\n\n    data_dict = {key: normalize_value(value) for key, value in data_dict.items()}\n\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(data_dict)\n\n    return df"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n\n    start_time = time.time()\n    process = subprocess.Popen(['python', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    cpu_percent = 0\n    memory_usage = 0\n    while True:\n        if time.time() - start_time > timeout:\n            process.terminate()\n            break\n\n        if psutil.pid_exists(process.pid):\n            cpu_percent += psutil.cpu_percent(interval=1)\n            memory_usage += psutil.Process(process.pid).memory_info().rss\n        else:\n            break\n\n    return {'CPU Usage': cpu_percent, 'Memory Usage': memory_usage}", "raw_solution": "import subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n\n    start_time = time.time()\n    process = subprocess.Popen(['python', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    cpu_percent = 0\n    memory_usage = 0\n    while True:\n        if time.time() - start_time > timeout:\n            process.terminate()\n            break\n\n        if psutil.pid_exists(process.pid):\n            cpu_percent += psutil.cpu_percent(interval=1)\n            memory_usage += psutil.Process(process.pid).memory_info().rss\n        else:\n            break\n\n    return {'CPU Usage': cpu_percent, 'Memory Usage': memory_usage}\n\n# Test the function"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n\n    # Generate DataFrame\n    df = pd.DataFrame({\n        \"x\": np.random.rand(N),\n        \"y\": np.random.rand(N),\n    })\n\n    # Generate categories\n    if N >= len(CATEGORIES):\n        df[\"category\"] = np.random.choice(CATEGORIES, N)\n    else:\n        df[\"category\"] = np.random.choice(CATEGORIES, N, replace=False)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    for category in CATEGORIES:\n        df_category = df[df[\"category\"] == category]\n        ax.scatter(df_category[\"x\"], df_category[\"y\"], label=category)\n    ax.legend()\n\n    return df, ax.figure", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n\n    # Generate DataFrame\n    df = pd.DataFrame({\n        \"x\": np.random.rand(N),\n        \"y\": np.random.rand(N),\n    })\n\n    # Generate categories\n    if N >= len(CATEGORIES):\n        df[\"category\"] = np.random.choice(CATEGORIES, N)\n    else:\n        df[\"category\"] = np.random.choice(CATEGORIES, N, replace=False)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    for category in CATEGORIES:\n        df_category = df[df[\"category\"] == category]\n        ax.scatter(df_category[\"x\"], df_category[\"y\"], label=category)\n    ax.legend()\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    np.random.seed(seed)\n    start_time = pd.to_datetime(start_time)\n    end_time = pd.to_datetime(end_time)\n    time_range = pd.date_range(start=start_time, end=end_time, freq=step)\n    values = np.random.normal(size=len(time_range)) + trend * np.arange(len(time_range))\n    df = pd.DataFrame({'Time': time_range, 'Value': values})\n    ax = df.plot(x='Time', y='Value', kind='line')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    return ax\nstart_time = '2022-01-01'\nend_time = '2022-01-31'\nstep = 'D'\ntrend = 1", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    np.random.seed(seed)\n    start_time = pd.to_datetime(start_time)\n    end_time = pd.to_datetime(end_time)\n    time_range = pd.date_range(start=start_time, end=end_time, freq=step)\n    values = np.random.normal(size=len(time_range)) + trend * np.arange(len(time_range))\n    df = pd.DataFrame({'Time': time_range, 'Value': values})\n    ax = df.plot(x='Time', y='Value', kind='line')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    return ax\n\n# Test the function\nstart_time = '2022-01-01'\nend_time = '2022-01-31'\nstep = 'D'\ntrend = 1\nax = task_func(start_time, end_time, step, trend)\nplt.show()"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generates sales data for five products from a given epoch time up to the current time.\n    \"\"\"\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or not isinstance(random_seed, int) or not isinstance(products, list):\n        raise ValueError(\"Invalid input type\")\n    if len(products) != 5:\n        raise ValueError(\"Exactly 5 products are required\")\n    for product in products:\n        if not isinstance(product, str):\n            raise ValueError(\"Product names must be strings\")\n\n    # Set random seed\n    random.seed(random_seed)\n\n    # Generate sales data\n    data = []\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n    delta = end_date - start_date\n    for day in range(delta.days + 1):\n        for product in products:\n            data.append({\n                \"Product\": product,\n                \"Date\": start_date + timedelta(days=day),\n                \"Sales\": random.randint(10, 50),\n            })\n\n    return pd.DataFrame(data)", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generates sales data for five products from a given epoch time up to the current time.\n    \"\"\"\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or not isinstance(random_seed, int) or not isinstance(products, list):\n        raise ValueError(\"Invalid input type\")\n    if len(products) != 5:\n        raise ValueError(\"Exactly 5 products are required\")\n    for product in products:\n        if not isinstance(product, str):\n            raise ValueError(\"Product names must be strings\")\n\n    # Set random seed\n    random.seed(random_seed)\n\n    # Generate sales data\n    data = []\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n    delta = end_date - start_date\n    for day in range(delta.days + 1):\n        for product in products:\n            data.append({\n                \"Product\": product,\n                \"Date\": start_date + timedelta(days=day),\n                \"Sales\": random.randint(10, 50),\n            })\n\n    return pd.DataFrame(data)\n\n# Test the function"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\nfrom pandas.io.json import json_normalize\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Converts JSON strings to an Excel file.\n    \"\"\"\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    try:\n        data = json.loads(json_str)\n    except ValueError:\n        raise ValueError(\"json_str is not valid JSON\")\n\n    if not isinstance(data, list):\n        data = [data]\n\n    df = pd.json_normalize(data)\n\n    try:\n        writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n        df.to_excel(writer, sheet_name=sheet_name, index=False)\n        writer.save()\n    except Exception as e:\n        raise Exception(\"Error writing to file: {}\".format(e))\n\n    return os.path.abspath(filename)", "raw_solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\nfrom pandas.io.json import json_normalize\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Converts JSON strings to an Excel file.\n    \"\"\"\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    try:\n        data = json.loads(json_str)\n    except ValueError:\n        raise ValueError(\"json_str is not valid JSON\")\n\n    if not isinstance(data, list):\n        data = [data]\n\n    df = pd.json_normalize(data)\n\n    try:\n        writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n        df.to_excel(writer, sheet_name=sheet_name, index=False)\n        writer.save()\n    except Exception as e:\n        raise Exception(\"Error writing to file: {}\".format(e))\n\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n\n    # Define activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n\n    # Generate data\n    data = []\n    for i in range(days_in_past):\n        date = datetime.now() - timedelta(days=i)\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append([date, activity, duration])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    df[\"Date\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n\n    # Create lineplot\n    ax = sns.lineplot(x=\"Date\", y=\"Duration\", hue=\"Activity\", data=df)\n\n    return (ax, df)", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n\n    # Define activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n\n    # Generate data\n    data = []\n    for i in range(days_in_past):\n        date = datetime.now() - timedelta(days=i)\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append([date, activity, duration])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    df[\"Date\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n\n    # Create lineplot\n    ax = sns.lineplot(x=\"Date\", y=\"Duration\", hue=\"Activity\", data=df)\n\n    return (ax, df)\n\n# Test the function\nax, df = task_func(7)\nplt.show()"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    np.random.seed(random_seed)\n    dates = pd.date_range(start=datetime.now() - timedelta(days=days_in_past), periods=days_in_past).tolist()\n    prices = np.random.rand(days_in_past, len(stock_names)).tolist()\n    data = {stock_names[i]: prices[j] for i in range(len(stock_names)) for j in range(days_in_past)}\n    df = pd.DataFrame(data, index=dates)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    np.random.seed(random_seed)\n    dates = pd.date_range(start=datetime.now() - timedelta(days=days_in_past), periods=days_in_past).tolist()\n    prices = np.random.rand(days_in_past, len(stock_names)).tolist()\n    data = {stock_names[i]: prices[j] for i in range(len(stock_names)) for j in range(days_in_past)}\n    df = pd.DataFrame(data, index=dates)\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r') as file1, open(file_path2, 'r') as file2:\n            lines1 = file1.readlines()\n            lines2 = file2.readlines()\n\n        if not lines1 or not lines2:\n            raise ValueError(\"One or both of the files are empty.\")\n\n        diff_lines = list(ndiff(lines1, lines2))\n\n        diff_data = []\n        for i, line in enumerate(diff_lines, start=1):\n            status = line[0]\n            content = line[2:]\n            diff_data.append([i, status, content])\n\n        df = pd.DataFrame(diff_data, columns=['Line Number', 'Status', 'Content'])\n\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files cannot be found.\")\n    except Exception as e:\n        raise Exception(\"An error occurred: {}\".format(e))", "raw_solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r') as file1, open(file_path2, 'r') as file2:\n            lines1 = file1.readlines()\n            lines2 = file2.readlines()\n\n        if not lines1 or not lines2:\n            raise ValueError(\"One or both of the files are empty.\")\n\n        diff_lines = list(ndiff(lines1, lines2))\n\n        diff_data = []\n        for i, line in enumerate(diff_lines, start=1):\n            status = line[0]\n            content = line[2:]\n            diff_data.append([i, status, content])\n\n        df = pd.DataFrame(diff_data, columns=['Line Number', 'Status', 'Content'])\n\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files cannot be found.\")\n    except Exception as e:\n        raise Exception(\"An error occurred: {}\".format(e))"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not data:\n        return ({'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}, None)\n\n    df = pd.DataFrame(data)\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in data\")\n\n    column_data = df[column]\n    summary_stats = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max(),\n    }\n\n    fig, ax = plt.subplots()\n    column_data.value_counts().plot(kind='pie', ax=ax)\n    ax.set_title(f\"Distribution of {column}\")\n\n    return (summary_stats, ax)", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if not data:\n        return ({'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}, None)\n\n    df = pd.DataFrame(data)\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in data\")\n\n    column_data = df[column]\n    summary_stats = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max(),\n    }\n\n    fig, ax = plt.subplots()\n    column_data.value_counts().plot(kind='pie', ax=ax)\n    ax.set_title(f\"Distribution of {column}\")\n\n    return (summary_stats, ax)"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    \"\"\"\n    Analyze a list of fitness data, calculate the sum, the mean, the minimum, the maximum of a certain column and draw a line chart.\n    Additionally, validate that the numeric values for steps, calories burned, and distance walked are non-negative.\n    \"\"\"\n    # Create a pandas DataFrame from the data\n    df = pd.DataFrame(data, columns=['Date', 'Steps', 'Calories Burned', 'Distance Walked'])\n\n    # Validate that the data list is not empty\n    if df.empty:\n        raise ValueError(\"The data list is empty.\")\n\n    # Validate that the specified column is valid\n    if column not in df.columns:\n        raise KeyError(f\"The specified column '{column}' is not valid.\")\n\n    # Validate that the numeric values for steps, calories burned, and distance walked are non-negative\n    for col in ['Steps', 'Calories Burned', 'Distance Walked']:\n        if (df[col] < 0).any():\n            raise ValueError(f\"The numeric value for '{col}' cannot be negative.\")\n\n    # Calculate the sum, mean, min, max of the column\n    summary_stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Plot a line chart\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column])\n    ax.set_title(f'Line Chart of {column}')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n\n    return summary_stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    \"\"\"\n    Analyze a list of fitness data, calculate the sum, the mean, the minimum, the maximum of a certain column and draw a line chart.\n    Additionally, validate that the numeric values for steps, calories burned, and distance walked are non-negative.\n    \"\"\"\n    # Create a pandas DataFrame from the data\n    df = pd.DataFrame(data, columns=['Date', 'Steps', 'Calories Burned', 'Distance Walked'])\n\n    # Validate that the data list is not empty\n    if df.empty:\n        raise ValueError(\"The data list is empty.\")\n\n    # Validate that the specified column is valid\n    if column not in df.columns:\n        raise KeyError(f\"The specified column '{column}' is not valid.\")\n\n    # Validate that the numeric values for steps, calories burned, and distance walked are non-negative\n    for col in ['Steps', 'Calories Burned', 'Distance Walked']:\n        if (df[col] < 0).any():\n            raise ValueError(f\"The numeric value for '{col}' cannot be negative.\")\n\n    # Calculate the sum, mean, min, max of the column\n    summary_stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Plot a line chart\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column])\n    ax.set_title(f'Line Chart of {column}')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n\n    return summary_stats, ax"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom typing import List, Dict\ndef task_func(input_file: str = \"data.json\") -> pd.DataFrame:\n    \"\"\"\n    Read a list of dictionaries from a JSON file, calculate the mean and median for each key (ignoring non-numeric or missing values), and convert the results into a Pandas DataFrame.\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize defaultdicts to store mean and median values\n    mean_dict = defaultdict(list)\n    median_dict = defaultdict(list)\n\n    # Iterate over each dictionary in the list\n    for d in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in d.items():\n            # Ignore non-numeric or missing values\n            if isinstance(value, (int, float)):\n                mean_dict[key].append(value)\n\n    # Calculate mean and median for each key\n    for key, values in mean_dict.items():\n        mean_dict[key] = np.mean(values)\n        median_dict[key] = np.median(values)\n\n    # Convert the dictionaries to DataFrames and concatenate them\n    mean_df = pd.DataFrame(list(mean_dict.items()), columns=['key', 'mean'])\n    median_df = pd.DataFrame(list(median_dict.items()), columns=['key', 'median'])\n    df = pd.merge(mean_df, median_df, on='key').set_index('key')\n\n    return df", "raw_solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom typing import List, Dict\n\ndef task_func(input_file: str = \"data.json\") -> pd.DataFrame:\n    \"\"\"\n    Read a list of dictionaries from a JSON file, calculate the mean and median for each key (ignoring non-numeric or missing values), and convert the results into a Pandas DataFrame.\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize defaultdicts to store mean and median values\n    mean_dict = defaultdict(list)\n    median_dict = defaultdict(list)\n\n    # Iterate over each dictionary in the list\n    for d in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in d.items():\n            # Ignore non-numeric or missing values\n            if isinstance(value, (int, float)):\n                mean_dict[key].append(value)\n\n    # Calculate mean and median for each key\n    for key, values in mean_dict.items():\n        mean_dict[key] = np.mean(values)\n        median_dict[key] = np.median(values)\n\n    # Convert the dictionaries to DataFrames and concatenate them\n    mean_df = pd.DataFrame(list(mean_dict.items()), columns=['key', 'mean'])\n    median_df = pd.DataFrame(list(median_dict.items()), columns=['key', 'median'])\n    df = pd.merge(mean_df, median_df, on='key').set_index('key')\n\n    return df"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File should be in .csv format\")\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        rows = list(reader)\n\n    duplicates = [item for item in rows if rows.count(item) > 1]\n    duplicates_dict = dict(Counter(map(tuple, duplicates)))\n\n    df = pd.DataFrame(list(duplicates_dict.items()), columns=['Duplicates', 'Counts'])\n    df.set_index('Duplicates', inplace=True)\n\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Counts')\n    ax.set_title('Count of Duplicate Rows')\n\n    return duplicates_dict, ax", "raw_solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File should be in .csv format\")\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        rows = list(reader)\n\n    duplicates = [item for item in rows if rows.count(item) > 1]\n    duplicates_dict = dict(Counter(map(tuple, duplicates)))\n\n    df = pd.DataFrame(list(duplicates_dict.items()), columns=['Duplicates', 'Counts'])\n    df.set_index('Duplicates', inplace=True)\n\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Counts')\n    ax.set_title('Count of Duplicate Rows')\n\n    return duplicates_dict, ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    df['age'] = df['age'].apply(np.floor)\n    df = df[df['age'] >= 0]\n    if df.empty:\n        raise ValueError(\"All ages are negative\")\n    age_distribution = df[df.duplicated(subset='name', keep=False)]['age'].value_counts()\n    if age_distribution.empty:\n        return (Counter(), None)\n    min_age, max_age = age_distribution.index.min(), age_distribution.index.max()\n    bins = np.arange(min_age - .5, max_age + .5)\n    hist, bins = np.histogram(age_distribution.index, bins=bins)\n    age_distribution = Counter(dict(zip(bins[:-1], hist)))\n    ax = sns.barplot(x=bins[:-1], y=hist)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age Distribution for Duplicate Names')\n    return (age_distribution, ax.figure)", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    df['age'] = df['age'].apply(np.floor)\n    df = df[df['age'] >= 0]\n    if df.empty:\n        raise ValueError(\"All ages are negative\")\n    age_distribution = df[df.duplicated(subset='name', keep=False)]['age'].value_counts()\n    if age_distribution.empty:\n        return (Counter(), None)\n    min_age, max_age = age_distribution.index.min(), age_distribution.index.max()\n    bins = np.arange(min_age - .5, max_age + .5)\n    hist, bins = np.histogram(age_distribution.index, bins=bins)\n    age_distribution = Counter(dict(zip(bins[:-1], hist)))\n    ax = sns.barplot(x=bins[:-1], y=hist)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age Distribution for Duplicate Names')\n    return (age_distribution, ax.figure)"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, bins=4):\n    # Identify and count duplicate values in 'value' column\n    counter = Counter(df['value'])\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, color='green', alpha=0.6, edgecolor='black')\n\n    # Fit a normal distribution to the data\n    mu, sigma = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n\n    # Overlay the normal distribution curve\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set plot title and labels\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return counter, ax", "raw_solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, bins=4):\n    # Identify and count duplicate values in 'value' column\n    counter = Counter(df['value'])\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, color='green', alpha=0.6, edgecolor='black')\n\n    # Fit a normal distribution to the data\n    mu, sigma = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n\n    # Overlay the normal distribution curve\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set plot title and labels\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return counter, ax"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b', and plots it as a bar chart.\n    List 'a' sets the DataFrame's row indices, while the length of list 'b' determines the number of columns.\n    The function returns the Axes object of the plotted bar chart.\n    \"\"\"\n    # Generate random values for DataFrame\n    np.random.seed(0)\n    df = pd.DataFrame(np.random.randint(0,10,size=(len(a), len(b))), index=a, columns=COLUMNS[:len(b)])\n\n    # Plot DataFrame as bar chart\n    ax = df.plot(kind='bar')\n\n    return ax\na = ['Row1', 'Row2', 'Row3']\nb = [1, 2, 3, 4, 5]", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b', and plots it as a bar chart.\n    List 'a' sets the DataFrame's row indices, while the length of list 'b' determines the number of columns.\n    The function returns the Axes object of the plotted bar chart.\n    \"\"\"\n    # Generate random values for DataFrame\n    np.random.seed(0)\n    df = pd.DataFrame(np.random.randint(0,10,size=(len(a), len(b))), index=a, columns=COLUMNS[:len(b)])\n\n    # Plot DataFrame as bar chart\n    ax = df.plot(kind='bar')\n\n    return ax\n\n# Test the function\na = ['Row1', 'Row2', 'Row3']\nb = [1, 2, 3, 4, 5]\nax = task_func(a, b)\nplt.show()"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): A pandas DataFrame with 'date' and 'value' columns.\n\n    Returns:\n    Axes: A matplotlib.axes.Axes object representing the plot.\n    \"\"\"\n    # Ensure that 'date' column is in datetime format\n    data['date'] = pd.to_datetime(data['date'])\n\n    # Extract year and month from 'date' column\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n\n    # Group data by month and calculate mean value\n    grouped_data = data.groupby('month')['value'].mean().reset_index()\n\n    # Create a bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(grouped_data['month'], grouped_data['value'], color='blue')\n\n    # Set title and labels\n    ax.set_title(f\"Monthly Data for {grouped_data['year'].iloc[0]}\")\n    ax.set_xlabel(\"Month\")\n    ax.set_ylabel(\"Value\")\n\n    return ax", "raw_solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): A pandas DataFrame with 'date' and 'value' columns.\n\n    Returns:\n    Axes: A matplotlib.axes.Axes object representing the plot.\n    \"\"\"\n    # Ensure that 'date' column is in datetime format\n    data['date'] = pd.to_datetime(data['date'])\n\n    # Extract year and month from 'date' column\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n\n    # Group data by month and calculate mean value\n    grouped_data = data.groupby('month')['value'].mean().reset_index()\n\n    # Create a bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(grouped_data['month'], grouped_data['value'], color='blue')\n\n    # Set title and labels\n    ax.set_title(f\"Monthly Data for {grouped_data['year'].iloc[0]}\")\n    ax.set_xlabel(\"Month\")\n    ax.set_ylabel(\"Value\")\n\n    return ax"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    This function draws a histogram to visualize the frequency distribution of numeric values provided in a string format.\n    \"\"\"\n    # Convert the string data to a list of floats\n    data = [float(x) for x in data.split()]\n    \n    # Convert the list to a numpy array\n    data = np.array(data)\n    \n    # Calculate the bins\n    bins = np.arange(data.min(), data.max()+2) - 0.5\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, edgecolor='black')\n    \n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    This function draws a histogram to visualize the frequency distribution of numeric values provided in a string format.\n    \"\"\"\n    # Convert the string data to a list of floats\n    data = [float(x) for x in data.split()]\n    \n    # Convert the list to a numpy array\n    data = np.array(data)\n    \n    # Calculate the bins\n    bins = np.arange(data.min(), data.max()+2) - 0.5\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, edgecolor='black')\n    \n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    return ax"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    def sine_func(x, a, b, c):\n        return a * np.sin(b * x) + c\n\n    x = np.linspace(0, 2 * np.pi, array_length)\n    y = np.sin(x) + np.random.normal(0, noise_level, array_length)\n\n    popt, _ = curve_fit(sine_func, x, y)\n\n    y_fit = sine_func(x, *popt)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, 'o', label='data')\n    ax.plot(x, y_fit, '-', label='fit')\n    ax.legend()\n    return ax", "raw_solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    def sine_func(x, a, b, c):\n        return a * np.sin(b * x) + c\n\n    x = np.linspace(0, 2 * np.pi, array_length)\n    y = np.sin(x) + np.random.normal(0, noise_level, array_length)\n\n    popt, _ = curve_fit(sine_func, x, y)\n\n    y_fit = sine_func(x, *popt)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, 'o', label='data')\n    ax.plot(x, y_fit, '-', label='fit')\n    ax.legend()\n    return ax\n\n# Test the function\nax = task_func(100, 0.2)\nplt.show()"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r') as file:\n            reader = csv.reader(file)\n            data = [row[0] for row in reader]\n    except FileNotFoundError:\n        return \"File not found\"\n    except IOError:\n        return \"Error reading file\"\n\n    # Normalize text to ASCII\n    data = [unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') for text in data]\n\n    # Tokenize text\n    words = ' '.join(data).split()\n\n    # Count words\n    word_counts = Counter(words)\n\n    # Get 10 most common words\n    common_words = word_counts.most_common(10)\n\n    # Create bar plot\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('10 Most Common Words')\n\n    return (ax, common_words)", "raw_solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r') as file:\n            reader = csv.reader(file)\n            data = [row[0] for row in reader]\n    except FileNotFoundError:\n        return \"File not found\"\n    except IOError:\n        return \"Error reading file\"\n\n    # Normalize text to ASCII\n    data = [unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') for text in data]\n\n    # Tokenize text\n    words = ' '.join(data).split()\n\n    # Count words\n    word_counts = Counter(words)\n\n    # Get 10 most common words\n    common_words = word_counts.most_common(10)\n\n    # Create bar plot\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('10 Most Common Words')\n\n    return (ax, common_words)"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    \"\"\"\n    Generates a list of normally distributed random numbers, plots their histogram and PDF, and returns the figure.\n    \"\"\"\n    # Generate normally distributed random numbers\n    data = np.random.normal(size=size)\n\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots(2, 1, figsize=(8, 6))\n\n    # Plot histogram\n    ax[0].hist(data, bins=30, density=True, edgecolor='black')\n    ax[0].set_title('Histogram')\n    ax[0].set_xlabel('Value')\n    ax[0].set_ylabel('Frequency')\n\n    # Plot PDF\n    x = np.linspace(-4, 4, 1000)\n    pdf = stats.norm.pdf(x)\n    ax[1].plot(x, pdf, label='PDF')\n    ax[1].set_title('Probability Density Function')\n    ax[1].set_xlabel('Value')\n    ax[1].set_ylabel('Probability')\n\n    # Adjust layout and display plot\n    plt.tight_layout()\n    plt.show()\n\n    return fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    \"\"\"\n    Generates a list of normally distributed random numbers, plots their histogram and PDF, and returns the figure.\n    \"\"\"\n    # Generate normally distributed random numbers\n    data = np.random.normal(size=size)\n\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots(2, 1, figsize=(8, 6))\n\n    # Plot histogram\n    ax[0].hist(data, bins=30, density=True, edgecolor='black')\n    ax[0].set_title('Histogram')\n    ax[0].set_xlabel('Value')\n    ax[0].set_ylabel('Frequency')\n\n    # Plot PDF\n    x = np.linspace(-4, 4, 1000)\n    pdf = stats.norm.pdf(x)\n    ax[1].plot(x, pdf, label='PDF')\n    ax[1].set_title('Probability Density Function')\n    ax[1].set_xlabel('Value')\n    ax[1].set_ylabel('Probability')\n\n    # Adjust layout and display plot\n    plt.tight_layout()\n    plt.show()\n\n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\nfrom Crypto.Util.Padding import pad, unpad\nfrom Crypto.Random import get_random_bytes\ndef task_func():\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Generate random bytes for filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Generate AES key and nonce\n    aes_key = get_random_bytes(32)\n    aes_nonce = get_random_bytes(16)\n\n    # Encrypt the private key with AES\n    cipher = AES.new(aes_key, AES.MODE_EAX)\n    ciphertext, aes_tag = cipher.encrypt_and_digest(rsa.key.exportKey('PEM'))\n\n    # Save the encrypted private key to file\n    with open(filename, 'wb') as file:\n        [file.write(x) for x in (cipher.nonce, aes_tag, ciphertext)]\n\n    return pubkey, filename, aes_key, aes_nonce", "raw_solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\nfrom Crypto.Util.Padding import pad, unpad\nfrom Crypto.Random import get_random_bytes\n\ndef task_func():\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Generate random bytes for filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Generate AES key and nonce\n    aes_key = get_random_bytes(32)\n    aes_nonce = get_random_bytes(16)\n\n    # Encrypt the private key with AES\n    cipher = AES.new(aes_key, AES.MODE_EAX)\n    ciphertext, aes_tag = cipher.encrypt_and_digest(rsa.key.exportKey('PEM'))\n\n    # Save the encrypted private key to file\n    with open(filename, 'wb') as file:\n        [file.write(x) for x in (cipher.nonce, aes_tag, ciphertext)]\n\n    return pubkey, filename, aes_key, aes_nonce\n\n# Testing\npubkey, filename, aes_key, aes_nonce = task_func()"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Read file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # AES encryption\n    aes_key = os.urandom(32)  # 256-bit key\n    cipher = Cipher(algorithms.AES(aes_key), modes.ECB())\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    padded_content = padder.update(file_content) + padder.finalize()\n    encrypted_file_content = encryptor.update(padded_content) + encryptor.finalize()\n\n    # Save encrypted file\n    encrypted_file_path = 'encrypted_file.bin'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_file_content)\n\n    # Encrypt AES key with RSA\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save encrypted AES key\n    encrypted_aes_key_path = 'encrypted_aes_key.bin'\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pubkey, encrypted_file_path, encrypted_aes_key_path", "raw_solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Read file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # AES encryption\n    aes_key = os.urandom(32)  # 256-bit key\n    cipher = Cipher(algorithms.AES(aes_key), modes.ECB())\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    padded_content = padder.update(file_content) + padder.finalize()\n    encrypted_file_content = encryptor.update(padded_content) + encryptor.finalize()\n\n    # Save encrypted file\n    encrypted_file_path = 'encrypted_file.bin'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_file_content)\n\n    # Encrypt AES key with RSA\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save encrypted AES key\n    encrypted_aes_key_path = 'encrypted_aes_key.bin'\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pubkey, encrypted_file_path, encrypted_aes_key_path"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL is invalid or empty\")\n    try:\n        response = urllib.request.urlopen(url)\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"Network connectivity or server issue\") from e\n    html = response.read()\n    doc = pq(html)\n    data = []\n    for a in doc('a'):\n        text = a.text\n        href = a.attrib['href']\n        data.append({'text': text, 'href': href})\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    return df", "raw_solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL is invalid or empty\")\n    try:\n        response = urllib.request.urlopen(url)\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"Network connectivity or server issue\") from e\n    html = response.read()\n    doc = pq(html)\n    data = []\n    for a in doc('a'):\n        text = a.text\n        href = a.attrib['href']\n        data.append({'text': text, 'href': href})\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    return df"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates sensor data for the specified number of hours and saves it in a CSV file.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate data\n    data = []\n    start_time = datetime.now()\n    for _ in range(hours):\n        for sensor in SENSORS:\n            data.append({\n                'Time': (start_time + timedelta(hours=_)).strftime('%Y-%m-%d %H:%M:%S'),\n                sensor: randint(0, 100)  # Random values for simplicity\n            })\n\n    # Save data to CSV file\n    csv_file = os.path.join(output_dir, 'sensor_data.csv')\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Temperature', 'Humidity', 'Pressure'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    return csv_file", "raw_solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates sensor data for the specified number of hours and saves it in a CSV file.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate data\n    data = []\n    start_time = datetime.now()\n    for _ in range(hours):\n        for sensor in SENSORS:\n            data.append({\n                'Time': (start_time + timedelta(hours=_)).strftime('%Y-%m-%d %H:%M:%S'),\n                sensor: randint(0, 100)  # Random values for simplicity\n            })\n\n    # Save data to CSV file\n    csv_file = os.path.join(output_dir, 'sensor_data.csv')\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Temperature', 'Humidity', 'Pressure'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    return csv_file\n\n# Test the function\ncsv_file = task_func(24)"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate traffic data\n    traffic_data = []\n    current_time = datetime.now()\n    for i in range(hours):\n        traffic_counts = {vehicle_type: randint(0, 100) for vehicle_type in VEHICLE_TYPES}\n        traffic_data.append({'Time': current_time, **traffic_counts})\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV file\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=['Time', *VEHICLE_TYPES])\n        writer.writeheader()\n        for data in traffic_data:\n            writer.writerow(data)\n\n    # Load data from CSV file and plot\n    df = pd.read_csv(csv_file_path)\n    df.set_index('Time', inplace=True)\n    ax = df.plot(kind='line', figsize=(10, 6))\n\n    return csv_file_path, ax.figure", "raw_solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate traffic data\n    traffic_data = []\n    current_time = datetime.now()\n    for i in range(hours):\n        traffic_counts = {vehicle_type: randint(0, 100) for vehicle_type in VEHICLE_TYPES}\n        traffic_data.append({'Time': current_time, **traffic_counts})\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV file\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=['Time', *VEHICLE_TYPES])\n        writer.writeheader()\n        for data in traffic_data:\n            writer.writerow(data)\n\n    # Load data from CSV file and plot\n    df = pd.read_csv(csv_file_path)\n    df.set_index('Time', inplace=True)\n    ax = df.plot(kind='line', figsize=(10, 6))\n\n    return csv_file_path, ax.figure"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate weather data\n    weather_data = []\n    start_time = datetime.now()\n    for i in range(hours):\n        time = (start_time + timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        weather_data.append([time, condition])\n\n    # Save weather data to CSV file\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file to a backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    shutil.copy(csv_file_path, backup_dir)\n\n    return csv_file_path", "raw_solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate weather data\n    weather_data = []\n    start_time = datetime.now()\n    for i in range(hours):\n        time = (start_time + timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        weather_data.append([time, condition])\n\n    # Save weather data to CSV file\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file to a backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    shutil.copy(csv_file_path, backup_dir)\n\n    return csv_file_path"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    \"\"\"\n    Generates a DataFrame of football match results for multiple teams, with random goals and penalties.\n    Penalties are converted into fines according to penalty costs.\n    \"\"\"\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Team': [TEAMS[randint(0, len(TEAMS)-1)] for _ in range(goals+penalties)],\n        'Result': ['Goal' if randint(0, 1) == 0 else 'Penalty' for _ in range(goals+penalties)]\n    })\n\n    # Add Goals and Penalty Costs\n    df.loc[df['Result'] == 'Goal', 'Goals'] = 1\n    df.loc[df['Result'] == 'Penalty', 'Penalty Cost'] = PENALTY_COST\n\n    # Calculate total fines\n    df['Fines'] = df['Penalty Cost'].fillna(0)\n\n    # Create plots\n    fig1, ax1 = plt.subplots()\n    sns.countplot(x='Team', data=df, hue='Result', ax=ax1)\n    ax1.set_title('Goals and Penalties by Team')\n\n    fig2, ax2 = plt.subplots()\n    df.groupby('Team')['Fines'].sum().plot(kind='bar', ax=ax2)\n    ax2.set_title('Total Fines by Team')\n\n    # Return DataFrame and plot objects\n    return df, [fig1, fig2]", "raw_solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    \"\"\"\n    Generates a DataFrame of football match results for multiple teams, with random goals and penalties.\n    Penalties are converted into fines according to penalty costs.\n    \"\"\"\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Team': [TEAMS[randint(0, len(TEAMS)-1)] for _ in range(goals+penalties)],\n        'Result': ['Goal' if randint(0, 1) == 0 else 'Penalty' for _ in range(goals+penalties)]\n    })\n\n    # Add Goals and Penalty Costs\n    df.loc[df['Result'] == 'Goal', 'Goals'] = 1\n    df.loc[df['Result'] == 'Penalty', 'Penalty Cost'] = PENALTY_COST\n\n    # Calculate total fines\n    df['Fines'] = df['Penalty Cost'].fillna(0)\n\n    # Create plots\n    fig1, ax1 = plt.subplots()\n    sns.countplot(x='Team', data=df, hue='Result', ax=ax1)\n    ax1.set_title('Goals and Penalties by Team')\n\n    fig2, ax2 = plt.subplots()\n    df.groupby('Team')['Fines'].sum().plot(kind='bar', ax=ax2)\n    ax2.set_title('Total Fines by Team')\n\n    # Return DataFrame and plot objects\n    return df, [fig1, fig2]"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    \"\"\"\n    Creates a Pandas DataFrame with random integer values between 0 and 9 for a given number of rows.\n    Counts the non-zero values in each column and visualizes this information using a bar plot.\n    \"\"\"\n    # Create DataFrame with random integer values\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.ne(0).sum()\n\n    # Create bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Non-zero count')\n    ax.set_title('Count of non-zero values in each column')\n\n    return (df, ax)", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    \"\"\"\n    Creates a Pandas DataFrame with random integer values between 0 and 9 for a given number of rows.\n    Counts the non-zero values in each column and visualizes this information using a bar plot.\n    \"\"\"\n    # Create DataFrame with random integer values\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.ne(0).sum()\n\n    # Create bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Non-zero count')\n    ax.set_title('Count of non-zero values in each column')\n\n    return (df, ax)\n\n# Test the function\ndf, ax = task_func(100)\nplt.show()"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students):\n    np.random.seed(0)\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    grades = np.random.randint(50, 100, (num_students, len(courses)))\n    students = [f'Student_{i}' for i in range(1, num_students + 1)]\n    df = pd.DataFrame(grades, index=students, columns=courses)\n\n    avg_grades = df.mean()\n    passing_grades = (df >= 60).sum()\n\n    df_output = pd.DataFrame({\n        'Average Grade': avg_grades,\n        'Passing Grades': passing_grades\n    })\n\n    fig, ax = plt.subplots()\n    df_output.plot(kind='bar', ax=ax)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.legend(loc='upper left')\n\n    return df_output, ax", "raw_solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students):\n    np.random.seed(0)\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    grades = np.random.randint(50, 100, (num_students, len(courses)))\n    students = [f'Student_{i}' for i in range(1, num_students + 1)]\n    df = pd.DataFrame(grades, index=students, columns=courses)\n\n    avg_grades = df.mean()\n    passing_grades = (df >= 60).sum()\n\n    df_output = pd.DataFrame({\n        'Average Grade': avg_grades,\n        'Passing Grades': passing_grades\n    })\n\n    fig, ax = plt.subplots()\n    df_output.plot(kind='bar', ax=ax)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.legend(loc='upper left')\n\n    return df_output, ax"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef exponential_decay(t, a, b, c):\n    \"\"\"\n    Exponential decay function.\n    \"\"\"\n    return a * np.exp(-b * t) + c\ndef task_func(array, target_value):\n    \"\"\"\n    Fit an exponential decay function to the indices in the array where the first column matches the target value.\n    \"\"\"\n    # Get indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n\n    # Fit the exponential decay function to the indices\n    popt, _ = optimize.curve_fit(exponential_decay, indices, array[indices, 1])\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot the original data\n    ax.scatter(indices, array[indices, 1], label='Data')\n\n    # Plot the fitted function\n    ax.plot(indices, exponential_decay(indices, *popt), label='Fit')\n\n    ax.legend()\n\n    return popt, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef exponential_decay(t, a, b, c):\n    \"\"\"\n    Exponential decay function.\n    \"\"\"\n    return a * np.exp(-b * t) + c\n\ndef task_func(array, target_value):\n    \"\"\"\n    Fit an exponential decay function to the indices in the array where the first column matches the target value.\n    \"\"\"\n    # Get indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n\n    # Fit the exponential decay function to the indices\n    popt, _ = optimize.curve_fit(exponential_decay, indices, array[indices, 1])\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot the original data\n    ax.scatter(indices, array[indices, 1], label='Data')\n\n    # Plot the fitted function\n    ax.plot(indices, exponential_decay(indices, *popt), label='Fit')\n\n    ax.legend()\n\n    return popt, ax"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef preprocess(text):\n    # Remove non-alphanumeric characters, convert to lowercase, and remove stopwords\n    text = ALPHANUMERIC.sub(' ', text.lower())\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n    return ' '.join(words)\ndef task_func(texts, num_topics):\n    # Preprocess the texts\n    texts = [preprocess(text) for text in texts]\n\n    # Vectorize the texts using TF-IDF\n    vectorizer = TfidfVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(texts)\n\n    # Apply NMF to extract the specified number of topics\n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(X)\n\n    # Get the most significant words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_features = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n        topics.append(top_features)\n\n    return topics", "raw_solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef preprocess(text):\n    # Remove non-alphanumeric characters, convert to lowercase, and remove stopwords\n    text = ALPHANUMERIC.sub(' ', text.lower())\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n    return ' '.join(words)\n\ndef task_func(texts, num_topics):\n    # Preprocess the texts\n    texts = [preprocess(text) for text in texts]\n\n    # Vectorize the texts using TF-IDF\n    vectorizer = TfidfVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(texts)\n\n    # Apply NMF to extract the specified number of topics\n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(X)\n\n    # Get the most significant words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_features = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n        topics.append(top_features)\n\n    return topics"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nALPHANUMERIC = re.compile('[\\W_]+')\ndef clean_text(text, stopwords=None):\n    \"\"\"\n    Clean a text by removing all non-alphanumeric characters except space, lowercased, and stop words are removed.\n    \"\"\"\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove non-alphanumeric characters\n    text = ALPHANUMERIC.sub(' ', text)\n\n    # Tokenize the text\n    words = text.split()\n\n    # Remove stopwords\n    if stopwords is not None:\n        words = [word for word in words if word not in stopwords]\n\n    return words\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generate word vectors from a list of texts using the gensim Word2Vec model and nltk.corpus.stopwords.\n    \"\"\"\n    # Clean the texts\n    cleaned_texts = [clean_text(text, stopwords) for text in texts]\n\n    # Train the Word2Vec model\n    model = Word2Vec(cleaned_texts)\n\n    return model", "raw_solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef clean_text(text, stopwords=None):\n    \"\"\"\n    Clean a text by removing all non-alphanumeric characters except space, lowercased, and stop words are removed.\n    \"\"\"\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove non-alphanumeric characters\n    text = ALPHANUMERIC.sub(' ', text)\n\n    # Tokenize the text\n    words = text.split()\n\n    # Remove stopwords\n    if stopwords is not None:\n        words = [word for word in words if word not in stopwords]\n\n    return words\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generate word vectors from a list of texts using the gensim Word2Vec model and nltk.corpus.stopwords.\n    \"\"\"\n    # Clean the texts\n    cleaned_texts = [clean_text(text, stopwords) for text in texts]\n\n    # Train the Word2Vec model\n    model = Word2Vec(cleaned_texts)\n\n    return model"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in a directory. The function reads each JSON file alphabetically into a DataFrame and inserts a \"Source\" column that specifies the filename. The processed files are then moved to a \"processed\" subdirectory. The path may not exist initially.\n    \"\"\"\n    # Create the \"processed\" subdirectory if it doesn't exist\n    if not os.path.exists(os.path.join(path, \"processed\")):\n        os.makedirs(os.path.join(path, \"processed\"))\n\n    # Get a list of all JSON files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    json_files.sort()  # Sort the files alphabetically\n\n    # Initialize an empty DataFrame to store the data from all files\n    df = pd.DataFrame()\n\n    # Process each JSON file\n    for file in json_files:\n        # Read the JSON file into a DataFrame\n        with open(os.path.join(path, file), 'r') as f:\n            data = json.load(f)\n        temp_df = pd.json_normalize(data)\n\n        # Add a \"Source\" column that specifies the filename\n        temp_df['Source'] = file\n\n        # Append the data from the current file to the DataFrame\n        df = df.append(temp_df, ignore_index=True)\n\n        # Move the processed file to the \"processed\" subdirectory\n        shutil.move(os.path.join(path, file), os.path.join(path, \"processed\", file))\n\n    return df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in a directory. The function reads each JSON file alphabetically into a DataFrame and inserts a \"Source\" column that specifies the filename. The processed files are then moved to a \"processed\" subdirectory. The path may not exist initially.\n    \"\"\"\n    # Create the \"processed\" subdirectory if it doesn't exist\n    if not os.path.exists(os.path.join(path, \"processed\")):\n        os.makedirs(os.path.join(path, \"processed\"))\n\n    # Get a list of all JSON files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    json_files.sort()  # Sort the files alphabetically\n\n    # Initialize an empty DataFrame to store the data from all files\n    df = pd.DataFrame()\n\n    # Process each JSON file\n    for file in json_files:\n        # Read the JSON file into a DataFrame\n        with open(os.path.join(path, file), 'r') as f:\n            data = json.load(f)\n        temp_df = pd.json_normalize(data)\n\n        # Add a \"Source\" column that specifies the filename\n        temp_df['Source'] = file\n\n        # Append the data from the current file to the DataFrame\n        df = df.append(temp_df, ignore_index=True)\n\n        # Move the processed file to the \"processed\" subdirectory\n        shutil.move(os.path.join(path, file), os.path.join(path, \"processed\", file))\n\n    return df"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Create the directory if it doesn't exist\n    if not os.path.exists('task_func_data'):\n        os.makedirs('task_func_data')\n\n    # Create the CSV file\n    file_path = 'task_func_data/Output.txt'\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Date', 'Temperature', 'Humidity'])\n\n        # Generate random data and write it to the CSV file\n        for _ in range(10):\n            date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            temperature = random.uniform(0, 100)\n            humidity = random.uniform(0, 100)\n            writer.writerow([date, temperature, humidity])\n\n    # Return the path to the CSV file\n    print(f'CSV file created at: {file_path}')\n\n    # Delete the CSV file\n    os.remove(file_path)\n    print(f'CSV file deleted at: {file_path}')\n\n    return file_path", "raw_solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Create the directory if it doesn't exist\n    if not os.path.exists('task_func_data'):\n        os.makedirs('task_func_data')\n\n    # Create the CSV file\n    file_path = 'task_func_data/Output.txt'\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Date', 'Temperature', 'Humidity'])\n\n        # Generate random data and write it to the CSV file\n        for _ in range(10):\n            date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            temperature = random.uniform(0, 100)\n            humidity = random.uniform(0, 100)\n            writer.writerow([date, temperature, humidity])\n\n    # Return the path to the CSV file\n    print(f'CSV file created at: {file_path}')\n\n    # Delete the CSV file\n    os.remove(file_path)\n    print(f'CSV file deleted at: {file_path}')\n\n    return file_path\n\n# Test the function\nfile_path = task_func()"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Scrapes data from a given URL and saves the scraped data to a CSV file.\n    \"\"\"\n    # Send a request to the website\n    response = urllib.request.urlopen(url)\n    # Parse the HTML content of the page with BeautifulSoup\n    soup = BeautifulSoup(response, 'html.parser')\n\n    # Find the table in the HTML\n    table = soup.find('table')\n\n    # Find all rows in the table\n    rows = table.find_all('tr')\n\n    # Open the CSV file for writing\n    with open(CSV_FILE_PATH, 'w', newline='') as csvfile:\n        # Create a CSV writer\n        writer = csv.writer(csvfile)\n\n        # Write the headers\n        headers = [header.text for header in rows[0].find_all('th')]\n        writer.writerow(headers)\n\n        # Write the data\n        for row in rows[1:]:\n            data = [cell.text for cell in row.find_all('td')]\n            writer.writerow(data)\n\n    return CSV_FILE_PATH", "raw_solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    Scrapes data from a given URL and saves the scraped data to a CSV file.\n    \"\"\"\n    # Send a request to the website\n    response = urllib.request.urlopen(url)\n    # Parse the HTML content of the page with BeautifulSoup\n    soup = BeautifulSoup(response, 'html.parser')\n\n    # Find the table in the HTML\n    table = soup.find('table')\n\n    # Find all rows in the table\n    rows = table.find_all('tr')\n\n    # Open the CSV file for writing\n    with open(CSV_FILE_PATH, 'w', newline='') as csvfile:\n        # Create a CSV writer\n        writer = csv.writer(csvfile)\n\n        # Write the headers\n        headers = [header.text for header in rows[0].find_all('th')]\n        writer.writerow(headers)\n\n        # Write the data\n        for row in rows[1:]:\n            data = [cell.text for cell in row.find_all('td')]\n            writer.writerow(data)\n\n    return CSV_FILE_PATH"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data is not a DataFrame.\")\n    if data.empty:\n        raise ValueError(\"Data is empty.\")\n    if target_column not in data.columns:\n        raise ValueError(f\"{target_column} is not a column of data.\")\n    if not np.issubdtype(data.dtypes.object, np.number).all():\n        raise ValueError(\"Data contains values that are not numeric.\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state is not an integer.\")\n    if not 0 <= test_size <= 1:\n        raise ValueError(\"Test size is not between 0 and 1.\")\n\n    X = data.drop([target_column], axis=1)\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.score(X_test, y_test)", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data is not a DataFrame.\")\n    if data.empty:\n        raise ValueError(\"Data is empty.\")\n    if target_column not in data.columns:\n        raise ValueError(f\"{target_column} is not a column of data.\")\n    if not np.issubdtype(data.dtypes.object, np.number).all():\n        raise ValueError(\"Data contains values that are not numeric.\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state is not an integer.\")\n    if not 0 <= test_size <= 1:\n        raise ValueError(\"Test size is not between 0 and 1.\")\n\n    X = data.drop([target_column], axis=1)\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport random\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    random.seed(rng_seed)\n    np.random.seed(rng_seed)\n\n    # Generate random data\n    ids = list(range(1, 101))\n    names = latin_names + other_names\n    random.shuffle(names)\n    dobs = [datetime.datetime(random.randint(start_year, end_year), random.randint(1, 12), random.randint(1, 28)) for _ in range(100)]\n    emails = [f\"{name.lower()}{dob.year}@{email_domain}\" for name, dob in zip(names, dobs)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dobs,\n        'Email': emails\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport random\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    random.seed(rng_seed)\n    np.random.seed(rng_seed)\n\n    # Generate random data\n    ids = list(range(1, 101))\n    names = latin_names + other_names\n    random.shuffle(names)\n    dobs = [datetime.datetime(random.randint(start_year, end_year), random.randint(1, 12), random.randint(1, 28)) for _ in range(100)]\n    emails = [f\"{name.lower()}{dob.year}@{email_domain}\" for name, dob in zip(names, dobs)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dobs,\n        'Email': emails\n    })\n\n    return df\n\n# Test the function\ndf = task_func(start_year=1980, end_year=2000, email_domain='test.at', rng_seed=3)"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Load data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize dictionaries to store mean and median values\n    mean_dict = defaultdict(float)\n    median_dict = defaultdict(float)\n\n    # Calculate mean and median for each key\n    for d in data:\n        for key, value in d.items():\n            mean_dict[key] += value\n            median_dict[key] += value\n\n    # Calculate mean and median\n    for key in mean_dict.keys():\n        mean_dict[key] /= len(data)\n        median_dict[key] = np.median([d[key] for d in data])\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for key in mean_dict.keys():\n            writer.writerow([key, mean_dict[key], median_dict[key]])\n\n    # Return dictionary with mean and median values\n    return {'mean': dict(mean_dict), 'median': dict(median_dict)}", "raw_solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\ndef task_func(input_file, output_file):\n    # Load data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize dictionaries to store mean and median values\n    mean_dict = defaultdict(float)\n    median_dict = defaultdict(float)\n\n    # Calculate mean and median for each key\n    for d in data:\n        for key, value in d.items():\n            mean_dict[key] += value\n            median_dict[key] += value\n\n    # Calculate mean and median\n    for key in mean_dict.keys():\n        mean_dict[key] /= len(data)\n        median_dict[key] = np.median([d[key] for d in data])\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for key in mean_dict.keys():\n            writer.writerow([key, mean_dict[key], median_dict[key]])\n\n    # Return dictionary with mean and median values\n    return {'mean': dict(mean_dict), 'median': dict(median_dict)}"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(file_dict, target_dir=\"non_none_files\"):\n    \"\"\"\n    Process files from a dictionary by checking if the file exists, and if it has content, then copies it to a target directory.\n    \"\"\"\n    # Create target directory if it doesn't exist\n    Path(target_dir).mkdir(parents=True, exist_ok=True)\n\n    copied_files = []\n\n    for file_name, file_path in file_dict.items():\n        # Check if file exists\n        if os.path.isfile(file_path):\n            # Check if file is not empty\n            if os.path.getsize(file_path) > 0:\n                # Copy file to target directory\n                shutil.copy2(file_path, target_dir)\n                # Append full file path to list\n                copied_files.append(os.path.join(target_dir, file_name))\n\n    return copied_files", "raw_solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(file_dict, target_dir=\"non_none_files\"):\n    \"\"\"\n    Process files from a dictionary by checking if the file exists, and if it has content, then copies it to a target directory.\n    \"\"\"\n    # Create target directory if it doesn't exist\n    Path(target_dir).mkdir(parents=True, exist_ok=True)\n\n    copied_files = []\n\n    for file_name, file_path in file_dict.items():\n        # Check if file exists\n        if os.path.isfile(file_path):\n            # Check if file is not empty\n            if os.path.getsize(file_path) > 0:\n                # Copy file to target directory\n                shutil.copy2(file_path, target_dir)\n                # Append full file path to list\n                copied_files.append(os.path.join(target_dir, file_name))\n\n    return copied_files"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in a directory based on a specified pattern and creates new files with altered names while preserving the content.\n    \"\"\"\n    # Initialize an empty list to store the names of the new files\n    new_files = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        match = re.match(pattern, filename)\n        if match:\n            # If the file matches the pattern, create a new filename\n            new_filename = match.group(1) + '.csv'\n            new_files.append(new_filename)\n\n            # Read the content of the original file\n            with open(os.path.join(directory, filename), 'r') as file:\n                reader = csv.reader(file)\n                content = list(reader)\n\n            # Write the content to the new file\n            with open(os.path.join(directory, new_filename), 'w', newline='') as file:\n                writer = csv.writer(file)\n                writer.writerows(content)\n\n    # Return the list of new filenames\n    return new_files", "raw_solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in a directory based on a specified pattern and creates new files with altered names while preserving the content.\n    \"\"\"\n    # Initialize an empty list to store the names of the new files\n    new_files = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        match = re.match(pattern, filename)\n        if match:\n            # If the file matches the pattern, create a new filename\n            new_filename = match.group(1) + '.csv'\n            new_files.append(new_filename)\n\n            # Read the content of the original file\n            with open(os.path.join(directory, filename), 'r') as file:\n                reader = csv.reader(file)\n                content = list(reader)\n\n            # Write the content to the new file\n            with open(os.path.join(directory, new_filename), 'w', newline='') as file:\n                writer = csv.writer(file)\n                writer.writerows(content)\n\n    # Return the list of new filenames\n    return new_files"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    for filename in os.listdir(directory):\n        match = re.match(pattern, filename)\n        if match:\n            prefix = match.group(1)\n            if not os.path.exists(os.path.join(directory, prefix)):\n                os.mkdir(os.path.join(directory, prefix))\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(os.path.join(directory, prefix))\n            extracted_dirs.append(prefix)\n    return extracted_dirs", "raw_solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    for filename in os.listdir(directory):\n        match = re.match(pattern, filename)\n        if match:\n            prefix = match.group(1)\n            if not os.path.exists(os.path.join(directory, prefix)):\n                os.mkdir(os.path.join(directory, prefix))\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(os.path.join(directory, prefix))\n            extracted_dirs.append(prefix)\n    return extracted_dirs"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nimport shutil\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    \"\"\"\n    Archive all files that match a particular pattern and then delete the original files.\n    \"\"\"\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Get a list of files that match the pattern\n    files = glob.glob(pattern)\n\n    # Archive each file\n    for file in files:\n        # Get the base name of the file\n        base_name = os.path.basename(file)\n\n        # Create the archive file path\n        archive_file = os.path.join(ARCHIVE_DIR, base_name)\n\n        # Archive the file\n        shutil.move(file, archive_file)\n\n    # Return the archive file path\n    return archive_file", "raw_solution": "import subprocess\nimport os\nimport glob\nimport shutil\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    Archive all files that match a particular pattern and then delete the original files.\n    \"\"\"\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Get a list of files that match the pattern\n    files = glob.glob(pattern)\n\n    # Archive each file\n    for file in files:\n        # Get the base name of the file\n        base_name = os.path.basename(file)\n\n        # Create the archive file path\n        archive_file = os.path.join(ARCHIVE_DIR, base_name)\n\n        # Archive the file\n        shutil.move(file, archive_file)\n\n    # Return the archive file path\n    return archive_file"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Count the total number of goals and penalties from a CSV file and update it with the given goals and penalties.\n    The function should output with:\n        count (Counter.collections): A Counter object with total counts of goals and penalties.\n    \"\"\"\n    # Initialize counters\n    total_goals = 0\n    total_penalties = 0\n\n    # Open the CSV file\n    with open(csv_file_path, 'r') as csv_file:\n        reader = csv.reader(csv_file)\n        next(reader)  # Skip the header\n\n        # Iterate over the rows in the CSV file\n        for row in reader:\n            # Add the goals and penalties from the current row to the total counts\n            total_goals += int(row[1])\n            total_penalties += int(row[2])\n\n    # Add the given goals and penalties to the total counts\n    total_goals += goals\n    total_penalties += penalties\n\n    # Create a Counter object with the total counts of goals and penalties\n    count = Counter({'goals': total_goals, 'penalties': total_penalties})\n\n    return count", "raw_solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Count the total number of goals and penalties from a CSV file and update it with the given goals and penalties.\n    The function should output with:\n        count (Counter.collections): A Counter object with total counts of goals and penalties.\n    \"\"\"\n    # Initialize counters\n    total_goals = 0\n    total_penalties = 0\n\n    # Open the CSV file\n    with open(csv_file_path, 'r') as csv_file:\n        reader = csv.reader(csv_file)\n        next(reader)  # Skip the header\n\n        # Iterate over the rows in the CSV file\n        for row in reader:\n            # Add the goals and penalties from the current row to the total counts\n            total_goals += int(row[1])\n            total_penalties += int(row[2])\n\n    # Add the given goals and penalties to the total counts\n    total_goals += goals\n    total_penalties += penalties\n\n    # Create a Counter object with the total counts of goals and penalties\n    count = Counter({'goals': total_goals, 'penalties': total_penalties})\n\n    return count"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Moves files from the source directory to the target directory based on a specified pattern.\n    \"\"\"\n    if not os.path.exists(source_dir) or not os.path.exists(target_dir):\n        raise ValueError(\"Source or target directory does not exist.\")\n\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Moves files from the source directory to the target directory based on a specified pattern.\n    \"\"\"\n    if not os.path.exists(source_dir) or not os.path.exists(target_dir):\n        raise ValueError(\"Source or target directory does not exist.\")\n\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef clean_text(text):\n    \"\"\"\n    Clean the text by removing all non-alphanumeric characters except spaces and converting to lowercase.\n    \"\"\"\n    return ALPHANUMERIC.sub(' ', text).lower()\ndef cosine_similarity(text1, text2):\n    \"\"\"\n    Calculate the cosine similarity between two texts.\n    \"\"\"\n    vector1 = Counter(clean_text(text1).split())\n    vector2 = Counter(clean_text(text2).split())\n\n    intersection = set(vector1.keys()) & set(vector2.keys())\n    numerator = sum([vector1[word] * vector2[word] for word in intersection])\n\n    sum1 = sum([vector1[word] ** 2 for word in vector1.keys()])\n    sum2 = sum([vector2[word] ** 2 for word in vector2.keys()])\n    denominator = np.sqrt(sum1) * np.sqrt(sum2)\n\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator\ndef levenshtein_ratio(text1, text2):\n    \"\"\"\n    Calculate the Levenshtein ratio between two texts.\n    \"\"\"\n    return ratio(clean_text(text1), clean_text(text2))\ndef task_func(text1, text2):\n    \"\"\"\n    Calculate the cosine similarity and Levenshtein ratio between two texts.\n    \"\"\"\n    cosine_similarity_value = cosine_similarity(text1, text2)\n    levenshtein_ratio_value = levenshtein_ratio(text1, text2)\n\n    return (cosine_similarity_value, levenshtein_ratio_value)", "raw_solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef clean_text(text):\n    \"\"\"\n    Clean the text by removing all non-alphanumeric characters except spaces and converting to lowercase.\n    \"\"\"\n    return ALPHANUMERIC.sub(' ', text).lower()\n\ndef cosine_similarity(text1, text2):\n    \"\"\"\n    Calculate the cosine similarity between two texts.\n    \"\"\"\n    vector1 = Counter(clean_text(text1).split())\n    vector2 = Counter(clean_text(text2).split())\n\n    intersection = set(vector1.keys()) & set(vector2.keys())\n    numerator = sum([vector1[word] * vector2[word] for word in intersection])\n\n    sum1 = sum([vector1[word] ** 2 for word in vector1.keys()])\n    sum2 = sum([vector2[word] ** 2 for word in vector2.keys()])\n    denominator = np.sqrt(sum1) * np.sqrt(sum2)\n\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator\n\ndef levenshtein_ratio(text1, text2):\n    \"\"\"\n    Calculate the Levenshtein ratio between two texts.\n    \"\"\"\n    return ratio(clean_text(text1), clean_text(text2))\n\ndef task_func(text1, text2):\n    \"\"\"\n    Calculate the cosine similarity and Levenshtein ratio between two texts.\n    \"\"\"\n    cosine_similarity_value = cosine_similarity(text1, text2)\n    levenshtein_ratio_value = levenshtein_ratio(text1, text2)\n\n    return (cosine_similarity_value, levenshtein_ratio_value)"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input should be a list of integers\")\n    if not all(isinstance(i, int) for i in numbers):\n        raise TypeError(\"Input should be a list of integers\")\n    if any(i < 0 for i in numbers):\n        raise ValueError(\"Input should not contain negative numbers\")\n\n    fac = [math.factorial(i) for i in numbers]\n    perm = list(permutations(numbers, len(numbers)))\n\n    return fac, list(perm)", "raw_solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input should be a list of integers\")\n    if not all(isinstance(i, int) for i in numbers):\n        raise TypeError(\"Input should be a list of integers\")\n    if any(i < 0 for i in numbers):\n        raise ValueError(\"Input should not contain negative numbers\")\n\n    fac = [math.factorial(i) for i in numbers]\n    perm = list(permutations(numbers, len(numbers)))\n\n    return fac, list(perm)"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfer files from one directory to another based on the specified file extensions.\n    \"\"\"\n    transferred_files = []\n    for extension in EXTENSIONS:\n        source_files = glob.glob(os.path.join(SOURCE_DIR, f'*.{extension}'))\n        for file in source_files:\n            try:\n                shutil.move(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Could not transfer file {file} due to error: {str(e)}\")\n    return transferred_files", "raw_solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfer files from one directory to another based on the specified file extensions.\n    \"\"\"\n    transferred_files = []\n    for extension in EXTENSIONS:\n        source_files = glob.glob(os.path.join(SOURCE_DIR, f'*.{extension}'))\n        for file in source_files:\n            try:\n                shutil.move(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Could not transfer file {file} due to error: {str(e)}\")\n    return transferred_files"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    \"\"\"\n    Takes a list of tuples containing elements and their respective counts and weights.\n    Normalizes the counts using z-score normalization and the weights using min-max scaling.\n    Returns a pandas DataFrame with the items, normalized counts, and normalized weights.\n    \"\"\"\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n\n    # Normalize the counts using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n\n    # Normalize the weights using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    return df[['Item', 'Normalized Count', 'Normalized Weight']]\ndata = [('Item1', 10, 20), ('Item2', 20, 30), ('Item3', 30, 40)]", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    \"\"\"\n    Takes a list of tuples containing elements and their respective counts and weights.\n    Normalizes the counts using z-score normalization and the weights using min-max scaling.\n    Returns a pandas DataFrame with the items, normalized counts, and normalized weights.\n    \"\"\"\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n\n    # Normalize the counts using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n\n    # Normalize the weights using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    return df[['Item', 'Normalized Count', 'Normalized Weight']]\n\n# Test the function\ndata = [('Item1', 10, 20), ('Item2', 20, 30), ('Item3', 30, 40)]"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_list=[('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize an empty list to store the mean values\n    mean_values = []\n\n    # Iterate over each tuple in the list\n    for i in range(len(data_list[0])):\n        # Initialize a list to store the numerical values at the current position\n        num_values = []\n\n        # Iterate over each tuple in the list\n        for j in range(len(data_list)):\n            # Check if the current position in the tuple is a number\n            if isinstance(data_list[j][i], (int, float)):\n                num_values.append(data_list[j][i])\n\n        # Calculate the mean of the numerical values at the current position\n        mean_value = np.mean(num_values) if num_values else np.nan\n\n        # Append the mean value to the list of mean values\n        mean_values.append(mean_value)\n\n    # Create a pandas DataFrame with the mean values and the corresponding positions as the index\n    df = pd.DataFrame(mean_values, index=['Position ' + str(i) for i in range(len(mean_values))], columns=['Mean Value'])\n\n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data_list=[('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize an empty list to store the mean values\n    mean_values = []\n\n    # Iterate over each tuple in the list\n    for i in range(len(data_list[0])):\n        # Initialize a list to store the numerical values at the current position\n        num_values = []\n\n        # Iterate over each tuple in the list\n        for j in range(len(data_list)):\n            # Check if the current position in the tuple is a number\n            if isinstance(data_list[j][i], (int, float)):\n                num_values.append(data_list[j][i])\n\n        # Calculate the mean of the numerical values at the current position\n        mean_value = np.mean(num_values) if num_values else np.nan\n\n        # Append the mean value to the list of mean values\n        mean_values.append(mean_value)\n\n    # Create a pandas DataFrame with the mean values and the corresponding positions as the index\n    df = pd.DataFrame(mean_values, index=['Position ' + str(i) for i in range(len(mean_values))], columns=['Mean Value'])\n\n    return df\n\n# Test the function\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    \"\"\"\n    Performs a chi-square test of independence of variables in a contingency table.\n    \"\"\"\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"'data' is empty\")\n\n    # Check if col1 and col2 are in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"One or both of the columns are not in 'data'\")\n\n    # Check if col1 and col2 have multiple categories\n    if data[col1].nunique() == 1 or data[col2].nunique() == 1:\n        raise ValueError(\"One or both of the columns do not have multiple categories\")\n\n    # Check if any categories have less than 5 observations\n    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\n        raise ValueError(\"Some categories have less than 5 observations\")\n\n    # Check if col1 and col2 are categorical\n    if not pd.api.types.is_categorical_dtype(data[col1]) or not pd.api.types.is_categorical_dtype(data[col2]):\n        raise TypeError(\"One or both of the columns contain non-categorical data\")\n\n    # Construct contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n\n    # Perform chi-square test of independence\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n\n    return p_value", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    \"\"\"\n    Performs a chi-square test of independence of variables in a contingency table.\n    \"\"\"\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"'data' is empty\")\n\n    # Check if col1 and col2 are in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"One or both of the columns are not in 'data'\")\n\n    # Check if col1 and col2 have multiple categories\n    if data[col1].nunique() == 1 or data[col2].nunique() == 1:\n        raise ValueError(\"One or both of the columns do not have multiple categories\")\n\n    # Check if any categories have less than 5 observations\n    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\n        raise ValueError(\"Some categories have less than 5 observations\")\n\n    # Check if col1 and col2 are categorical\n    if not pd.api.types.is_categorical_dtype(data[col1]) or not pd.api.types.is_categorical_dtype(data[col2]):\n        raise TypeError(\"One or both of the columns contain non-categorical data\")\n\n    # Construct contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n\n    # Perform chi-square test of independence\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n\n    return p_value"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulates a number of dice rolls and returns the frequency of each outcome.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    frequencies = np.bincount(results)\n\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequencies)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return frequencies, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulates a number of dice rolls and returns the frequency of each outcome.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    frequencies = np.bincount(results)\n\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequencies)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return frequencies, ax"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from a source directory to a target directory.\n    The function identifies processed files by the '_processed' suffix in the filename.\n    \"\"\"\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(source_dir)\n\n    # Filter the list to only include files with the '_processed' suffix\n    processed_files = [file for file in files if file.endswith('_processed')]\n\n    # Create the archive file\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as archive:\n        # Add each processed file to the archive\n        for file in processed_files:\n            file_path = os.path.join(source_dir, file)\n            archive.write(file_path, arcname=file)\n\n    # Return the path to the created archive\n    return archive_path", "raw_solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from a source directory to a target directory.\n    The function identifies processed files by the '_processed' suffix in the filename.\n    \"\"\"\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(source_dir)\n\n    # Filter the list to only include files with the '_processed' suffix\n    processed_files = [file for file in files if file.endswith('_processed')]\n\n    # Create the archive file\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as archive:\n        # Add each processed file to the archive\n        for file in processed_files:\n            file_path = os.path.join(source_dir, file)\n            archive.write(file_path, arcname=file)\n\n    # Return the path to the created archive\n    return archive_path"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    \"\"\"\n    Predicts the stock closing prices for the next 7 days using simple linear regression and plots the data.\n    \"\"\"\n    # Convert the 'Date' column to datetime format\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Create a new column 'Time' which represents the time in seconds since the start of the data\n    df['Time'] = (df.index - df.index[0]).total_seconds()\n\n    # Reshape the 'Time' column to a 2D array\n    time_array = df['Time'].values.reshape(-1, 1)\n\n    # Create a Linear Regression model\n    model = LinearRegression()\n\n    # Fit the model to the data\n    model.fit(time_array, df['Close'])\n\n    # Generate future timestamps\n    future_timestamps = pd.date_range(start=df.index[-1], periods=7, freq='D')\n    future_timestamps = (future_timestamps - df.index[0]).total_seconds()\n\n    # Predict the closing prices for the next 7 days\n    future_time_array = np.array(future_timestamps).reshape(-1, 1)\n    predictions = model.predict(future_time_array)\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df['Close'], label='Actual')\n    plt.plot(future_timestamps, predictions, label='Predicted')\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Prices Prediction')\n    plt.legend()\n    plt.grid(True)\n\n    return (predictions.tolist(), plt.gca())", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    \"\"\"\n    Predicts the stock closing prices for the next 7 days using simple linear regression and plots the data.\n    \"\"\"\n    # Convert the 'Date' column to datetime format\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Create a new column 'Time' which represents the time in seconds since the start of the data\n    df['Time'] = (df.index - df.index[0]).total_seconds()\n\n    # Reshape the 'Time' column to a 2D array\n    time_array = df['Time'].values.reshape(-1, 1)\n\n    # Create a Linear Regression model\n    model = LinearRegression()\n\n    # Fit the model to the data\n    model.fit(time_array, df['Close'])\n\n    # Generate future timestamps\n    future_timestamps = pd.date_range(start=df.index[-1], periods=7, freq='D')\n    future_timestamps = (future_timestamps - df.index[0]).total_seconds()\n\n    # Predict the closing prices for the next 7 days\n    future_time_array = np.array(future_timestamps).reshape(-1, 1)\n    predictions = model.predict(future_time_array)\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df['Close'], label='Actual')\n    plt.plot(future_timestamps, predictions, label='Predicted')\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Prices Prediction')\n    plt.legend()\n    plt.grid(True)\n\n    return (predictions.tolist(), plt.gca())"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n    df['z_score'] = zscore(df['closing_price'])\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    plot = df.plot(kind='scatter', x='closing_price', y='z_score', color='red')\n    plt.axhline(y=z_threshold, color='r', linestyle='-')\n    plt.axhline(y=-z_threshold, color='r', linestyle='-')\n    plt.title('Outliers in Closing Prices')\n    plt.xlabel('Closing Price')\n    plt.ylabel('Z-Score')\n    return outliers, plot", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    df['z_score'] = zscore(df['closing_price'])\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    plot = df.plot(kind='scatter', x='closing_price', y='z_score', color='red')\n    plt.axhline(y=z_threshold, color='r', linestyle='-')\n    plt.axhline(y=-z_threshold, color='r', linestyle='-')\n    plt.title('Outliers in Closing Prices')\n    plt.xlabel('Closing Price')\n    plt.ylabel('Z-Score')\n    return outliers, plot"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualizes the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \"\"\"\n    # Create a new figure\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n\n    # Box plot\n    sns.boxplot(df['Close'], ax=ax1)\n    ax1.set_title('Box Plot of Closing Prices')\n\n    # Histogram\n    sns.histplot(df['Close'], ax=ax2, kde=True)\n    ax2.set_title('Histogram of Closing Prices')\n\n    # Return the axes\n    return (ax1, ax2)", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualizes the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \"\"\"\n    # Create a new figure\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n\n    # Box plot\n    sns.boxplot(df['Close'], ax=ax1)\n    ax1.set_title('Box Plot of Closing Prices')\n\n    # Histogram\n    sns.histplot(df['Close'], ax=ax2, kde=True)\n    ax2.set_title('Histogram of Closing Prices')\n\n    # Return the axes\n    return (ax1, ax2)"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts the share closing prices for the next 7 days using the ARIMA model and plots the forecast.\n    \"\"\"\n    # Fit the model\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit(disp=0)\n\n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot the forecast\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(df['Close'])\n    ax.plot(forecast, color='red')\n    ax.set_title('Share Price Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n\n    return list(forecast), ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts the share closing prices for the next 7 days using the ARIMA model and plots the forecast.\n    \"\"\"\n    # Fit the model\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit(disp=0)\n\n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot the forecast\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(df['Close'])\n    ax.plot(forecast, color='red')\n    ax.set_title('Share Price Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n\n    return list(forecast), ax"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Create a dictionary containing all possible two-letter combinations of the lowercase English alphabets.\n    The dictionary values represent the frequency of these two-letter combinations in the given word.\n    If a combination does not appear in the word, its value will be 0.\n    \"\"\"\n    # Generate all two-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=2)]\n\n    # Create a dictionary with all combinations as keys and 0 as values\n    result = {combo: 0 for combo in combinations}\n\n    # Count the frequency of each two-letter combination in the word\n    word_counter = Counter(word[i:i+2] for i in range(len(word)))\n\n    # Update the values in the result dictionary with the counts from the word\n    for combo, count in word_counter.items():\n        if combo in result:\n            result[combo] = count\n\n    return result", "raw_solution": "from collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Create a dictionary containing all possible two-letter combinations of the lowercase English alphabets.\n    The dictionary values represent the frequency of these two-letter combinations in the given word.\n    If a combination does not appear in the word, its value will be 0.\n    \"\"\"\n    # Generate all two-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=2)]\n\n    # Create a dictionary with all combinations as keys and 0 as values\n    result = {combo: 0 for combo in combinations}\n\n    # Count the frequency of each two-letter combination in the word\n    word_counter = Counter(word[i:i+2] for i in range(len(word)))\n\n    # Update the values in the result dictionary with the counts from the word\n    for combo, count in word_counter.items():\n        if combo in result:\n            result[combo] = count\n\n    return result"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create a DataFrame with dates and categories\n    df = pd.DataFrame({'Date': dates})\n    df['Category'] = np.random.choice(categories, size=periods)\n\n    # Generate random sales data\n    df['Sales'] = np.random.randint(100, 1000, size=periods)\n\n    # Plot the sales data\n    ax = df.groupby('Category')['Sales'].sum().plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Sales Report by Category')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Sales')\n\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create a DataFrame with dates and categories\n    df = pd.DataFrame({'Date': dates})\n    df['Category'] = np.random.choice(categories, size=periods)\n\n    # Generate random sales data\n    df['Sales'] = np.random.randint(100, 1000, size=periods)\n\n    # Plot the sales data\n    ax = df.groupby('Category')['Sales'].sum().plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Sales Report by Category')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Sales')\n\n    return df, ax.figure\n\n# Test the function\ndf, fig = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    if sales_data is None:\n        sales_data = pd.date_range(start=start_date, periods=periods, freq=freq)\n        sales_data = pd.Series(np.random.randint(1, 100, len(sales_data)), index=sales_data)\n    else:\n        sales_data = pd.Series(sales_data, index=pd.date_range(start=start_date, periods=len(sales_data), freq=freq))\n\n    X = np.array(range(0, len(sales_data))).reshape(-1, 1)\n    Y = sales_data.values.reshape(-1, 1)\n\n    linear_regressor = LinearRegression()\n    linear_regressor.fit(X, Y)\n\n    future_dates = pd.date_range(start=sales_data.index[-1], periods=len(sales_data), freq=freq)\n    future_X = np.array(range(len(sales_data), len(sales_data) + len(future_dates))).reshape(-1, 1)\n\n    future_sales = linear_regressor.predict(future_X)\n\n    return future_sales.flatten()", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    if sales_data is None:\n        sales_data = pd.date_range(start=start_date, periods=periods, freq=freq)\n        sales_data = pd.Series(np.random.randint(1, 100, len(sales_data)), index=sales_data)\n    else:\n        sales_data = pd.Series(sales_data, index=pd.date_range(start=start_date, periods=len(sales_data), freq=freq))\n\n    X = np.array(range(0, len(sales_data))).reshape(-1, 1)\n    Y = sales_data.values.reshape(-1, 1)\n\n    linear_regressor = LinearRegression()\n    linear_regressor.fit(X, Y)\n\n    future_dates = pd.date_range(start=sales_data.index[-1], periods=len(sales_data), freq=freq)\n    future_X = np.array(range(len(sales_data), len(sales_data) + len(future_dates))).reshape(-1, 1)\n\n    future_sales = linear_regressor.predict(future_X)\n\n    return future_sales.flatten()"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    Randomly assigns a specified number of tasks to employees with a due date of the current day.\n    \"\"\"\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    tasks = [task.replace(' ', '_') for task in task_list]\n    random.shuffle(tasks)\n\n    assigned_tasks = tasks[:n_tasks]\n    random.shuffle(employees)\n\n    assigned_to = [random.choice(employees) for _ in range(n_tasks)]\n    due_date = datetime.now().date()\n\n    df = pd.DataFrame({\n        \"Task Name\": assigned_tasks,\n        \"Assigned To\": assigned_to,\n        \"Due Date\": due_date,\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    Randomly assigns a specified number of tasks to employees with a due date of the current day.\n    \"\"\"\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    tasks = [task.replace(' ', '_') for task in task_list]\n    random.shuffle(tasks)\n\n    assigned_tasks = tasks[:n_tasks]\n    random.shuffle(employees)\n\n    assigned_to = [random.choice(employees) for _ in range(n_tasks)]\n    due_date = datetime.now().date()\n\n    df = pd.DataFrame({\n        \"Task Name\": assigned_tasks,\n        \"Assigned To\": assigned_to,\n        \"Due Date\": due_date,\n    })\n\n    return df"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    \"\"\"\n    Replaces spaces in given words with underscores, then plots the frequency of each unique word.\n    \"\"\"\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n\n    # Replace spaces with underscores\n    text = text.lower()\n    text = re.sub(r'\\s', '_', text)\n\n    # Split the text into words\n    words = text.split('_')\n\n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(range(len(word_counts)), list(word_counts.values()), align='center')\n    ax.set_xticks(range(len(word_counts)))\n    ax.set_xticklabels(list(word_counts.keys()))\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of each unique word')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    \"\"\"\n    Replaces spaces in given words with underscores, then plots the frequency of each unique word.\n    \"\"\"\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n\n    # Replace spaces with underscores\n    text = text.lower()\n    text = re.sub(r'\\s', '_', text)\n\n    # Split the text into words\n    words = text.split('_')\n\n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(range(len(word_counts)), list(word_counts.values()), align='center')\n    ax.set_xticks(range(len(word_counts)))\n    ax.set_xticklabels(list(word_counts.keys()))\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of each unique word')\n\n    return ax"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory and save it as a zip file saved to a target directory.\n    \"\"\"\n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n\n    # Create target directory if it does not exist\n    if not os.path.isdir(target_directory):\n        os.makedirs(target_directory)\n\n    # Create zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through source directory\n        for folder_name, subfolders, filenames in os.walk(source_directory):\n            for filename in filenames:\n                # Check if file has a valid extension\n                if any(filename.endswith(ext) for ext in valid_extensions):\n                    # Create full file path\n                    file_path = os.path.join(folder_name, filename)\n                    # Add file to zip\n                    zipf.write(file_path, arcname=filename)\n\n    return zip_path", "raw_solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory and save it as a zip file saved to a target directory.\n    \"\"\"\n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n\n    # Create target directory if it does not exist\n    if not os.path.isdir(target_directory):\n        os.makedirs(target_directory)\n\n    # Create zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through source directory\n        for folder_name, subfolders, filenames in os.walk(source_directory):\n            for filename in filenames:\n                # Check if file has a valid extension\n                if any(filename.endswith(ext) for ext in valid_extensions):\n                    # Create full file path\n                    file_path = os.path.join(folder_name, filename)\n                    # Add file to zip\n                    zipf.write(file_path, arcname=filename)\n\n    return zip_path"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\nimport openpyxl\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files and saves them in a target directory.\n    Each file's text content is captured and stored in a CSV with a single 'Text' column and no row indices.\n    This function will overwrite existing files in the target directory if they have the same names as the converted files.\n    If the source directory does not exist, this function raises a FileNotFoundError.\n    This function returns the number of files successfully converted to CSV.\n    \"\"\"\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    count = 0\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".txt\"):\n            with open(os.path.join(source_directory, filename), 'r') as file:\n                data = file.read()\n            df = pd.DataFrame([data], columns=['Text'])\n            df.to_csv(os.path.join(target_directory, f\"{filename.split('.')[0]}.csv\"), index=False)\n            count += 1\n\n        elif filename.endswith(\".docx\"):\n            doc = docx.Document(os.path.join(source_directory, filename))\n            data = ' '.join([p.text for p in doc.paragraphs])\n            df = pd.DataFrame([data], columns=['Text'])\n            df.to_csv(os.path.join(target_directory, f\"{filename.split('.')[0]}.csv\"), index=False)\n            count += 1\n\n        elif filename.endswith(\".xlsx\"):\n            xls = pd.read_excel(os.path.join(source_directory, filename))\n            data = xls.to_csv(index=False)\n            with open(os.path.join(target_directory, f\"{filename.split('.')[0]}.csv\"), 'w') as file:\n                file.write(data)\n            count += 1\n\n        elif filename.endswith(\".csv\"):\n            # Copy the CSV file to the target directory\n            os.system(f\"cp {os.path.join(source_directory, filename)} {os.path.join(target_directory, filename)}\")\n            count += 1\n\n    return count", "raw_solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\nimport openpyxl\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files and saves them in a target directory.\n    Each file's text content is captured and stored in a CSV with a single 'Text' column and no row indices.\n    This function will overwrite existing files in the target directory if they have the same names as the converted files.\n    If the source directory does not exist, this function raises a FileNotFoundError.\n    This function returns the number of files successfully converted to CSV.\n    \"\"\"\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    count = 0\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".txt\"):\n            with open(os.path.join(source_directory, filename), 'r') as file:\n                data = file.read()\n            df = pd.DataFrame([data], columns=['Text'])\n            df.to_csv(os.path.join(target_directory, f\"{filename.split('.')[0]}.csv\"), index=False)\n            count += 1\n\n        elif filename.endswith(\".docx\"):\n            doc = docx.Document(os.path.join(source_directory, filename))\n            data = ' '.join([p.text for p in doc.paragraphs])\n            df = pd.DataFrame([data], columns=['Text'])\n            df.to_csv(os.path.join(target_directory, f\"{filename.split('.')[0]}.csv\"), index=False)\n            count += 1\n\n        elif filename.endswith(\".xlsx\"):\n            xls = pd.read_excel(os.path.join(source_directory, filename))\n            data = xls.to_csv(index=False)\n            with open(os.path.join(target_directory, f\"{filename.split('.')[0]}.csv\"), 'w') as file:\n                file.write(data)\n            count += 1\n\n        elif filename.endswith(\".csv\"):\n            # Copy the CSV file to the target directory\n            os.system(f\"cp {os.path.join(source_directory, filename)} {os.path.join(target_directory, filename)}\")\n            count += 1\n\n    return count"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame cannot be empty\")\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame cannot contain NaN values\")\n    if not df.select_dtypes(include=[np.number]).empty:\n        raise TypeError(\"DataFrame must contain only numeric data types\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    df_cumulative = df_normalized.cumsum(axis=0)\n\n    return df_cumulative", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame cannot be empty\")\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame cannot contain NaN values\")\n    if not df.select_dtypes(include=[np.number]).empty:\n        raise TypeError(\"DataFrame must contain only numeric data types\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    df_cumulative = df_normalized.cumsum(axis=0)\n\n    return df_cumulative"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n    \"\"\"\n    if not os.path.isdir(directory_path):\n        raise ValueError(\"Provided directory does not exist.\")\n\n    result = []\n    for file_name in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, file_name)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path)\n            creation_time = os.path.getctime(file_path)\n            modification_time = os.path.getmtime(file_path)\n            creation_time_iso = datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat()\n            modification_time_iso = datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat()\n            result.append((file_name, file_size, creation_time_iso, modification_time_iso))\n\n    return result", "raw_solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n    \"\"\"\n    if not os.path.isdir(directory_path):\n        raise ValueError(\"Provided directory does not exist.\")\n\n    result = []\n    for file_name in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, file_name)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path)\n            creation_time = os.path.getctime(file_path)\n            modification_time = os.path.getmtime(file_path)\n            creation_time_iso = datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat()\n            modification_time_iso = datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat()\n            result.append((file_name, file_size, creation_time_iso, modification_time_iso))\n\n    return result"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on provided JSON data.\n    \"\"\"\n    # Check if the JSON data is malformed\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n\n    # Check if the JSON data is empty\n    if not data:\n        raise ValueError(\"The JSON data is empty.\")\n\n    # Create a list to store the population data\n    population_data = []\n\n    # Iterate over the data\n    for country, population in data.items():\n        # Check if the country name is a string\n        if not isinstance(country, str):\n            raise ValueError(\"Non-string country names are not allowed.\")\n\n        # Check if the population is a numeric and non-negative\n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Non-numeric or negative populations are not allowed.\")\n\n        # Round down the population to the nearest integer\n        population = math.floor(population)\n\n        # Append the data to the list\n        population_data.append({\"Country\": country, \"Population\": population})\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(population_data)\n\n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Create the file path\n    file_path = os.path.join(output_dir, file_name)\n\n    # Write the DataFrame to a CSV file\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"The file cannot be written to the specified directory.\")\n\n    return file_path, df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on provided JSON data.\n    \"\"\"\n    # Check if the JSON data is malformed\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n\n    # Check if the JSON data is empty\n    if not data:\n        raise ValueError(\"The JSON data is empty.\")\n\n    # Create a list to store the population data\n    population_data = []\n\n    # Iterate over the data\n    for country, population in data.items():\n        # Check if the country name is a string\n        if not isinstance(country, str):\n            raise ValueError(\"Non-string country names are not allowed.\")\n\n        # Check if the population is a numeric and non-negative\n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Non-numeric or negative populations are not allowed.\")\n\n        # Round down the population to the nearest integer\n        population = math.floor(population)\n\n        # Append the data to the list\n        population_data.append({\"Country\": country, \"Population\": population})\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(population_data)\n\n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Create the file path\n    file_path = os.path.join(output_dir, file_name)\n\n    # Write the DataFrame to a CSV file\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"The file cannot be written to the specified directory.\")\n\n    return file_path, df"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n    \"\"\"\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(\"Specified directory does not exist or is not a directory.\")\n\n    result = {}\n    for item in os.listdir(dir_path):\n        item_path = os.path.join(dir_path, item)\n        for predicate in predicates:\n            if re.search(predicate, item):\n                result[item] = True\n                break\n        else:\n            result[item] = False\n\n    return result", "raw_solution": "import os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n    \"\"\"\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(\"Specified directory does not exist or is not a directory.\")\n\n    result = {}\n    for item in os.listdir(dir_path):\n        item_path = os.path.join(dir_path, item)\n        for predicate in predicates:\n            if re.search(predicate, item):\n                result[item] = True\n                break\n        else:\n            result[item] = False\n\n    return result"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    # Convert hexadecimal to bytes\n    bytes_obj = binascii.unhexlify(hex_string)\n\n    # Convert bytes to UTF-8 string\n    utf8_str = bytes_obj.decode('utf-8')\n\n    # Initialize dictionary to store results\n    result = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_obj).decode('utf-8'),\n        'utf-8': utf8_str,\n        'utf-16': utf8_str,\n        'utf-32': utf8_str,\n        'ASCII': 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_str),\n        'ROT13': codecs.encode(utf8_str, 'rot_13')\n    }\n\n    # Check if UTF-16, UTF-32, ASCII representations are possible\n    for encoding in ['utf-16', 'utf-32', 'ASCII']:\n        try:\n            result[encoding] = utf8_str.encode(encoding).decode(encoding)\n        except UnicodeEncodeError:\n            result[encoding] = 'Not representable in ' + encoding.upper()\n\n    return result", "raw_solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    # Convert hexadecimal to bytes\n    bytes_obj = binascii.unhexlify(hex_string)\n\n    # Convert bytes to UTF-8 string\n    utf8_str = bytes_obj.decode('utf-8')\n\n    # Initialize dictionary to store results\n    result = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_obj).decode('utf-8'),\n        'utf-8': utf8_str,\n        'utf-16': utf8_str,\n        'utf-32': utf8_str,\n        'ASCII': 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_str),\n        'ROT13': codecs.encode(utf8_str, 'rot_13')\n    }\n\n    # Check if UTF-16, UTF-32, ASCII representations are possible\n    for encoding in ['utf-16', 'utf-32', 'ASCII']:\n        try:\n            result[encoding] = utf8_str.encode(encoding).decode(encoding)\n        except UnicodeEncodeError:\n            result[encoding] = 'Not representable in ' + encoding.upper()\n\n    return result\n\n# Test the function"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Failed to download file: {e}\")\n        return False\n\n    # Calculate the MD5 checksum of the downloaded file\n    md5_hash = hashlib.md5()\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(byte_block)\n    calculated_md5_checksum = md5_hash.hexdigest()\n\n    # Check if the calculated checksum matches the expected checksum\n    if calculated_md5_checksum != EXPECTED_MD5_CHECKSUM:\n        print(\"Checksum does not match. Deleting the file.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the contents of the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Failed to extract file: {e}\")\n        return False\n\n    # Remove the downloaded file\n    os.remove(TARGET_TAR_FILE)\n\n    return True", "raw_solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Failed to download file: {e}\")\n        return False\n\n    # Calculate the MD5 checksum of the downloaded file\n    md5_hash = hashlib.md5()\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(byte_block)\n    calculated_md5_checksum = md5_hash.hexdigest()\n\n    # Check if the calculated checksum matches the expected checksum\n    if calculated_md5_checksum != EXPECTED_MD5_CHECKSUM:\n        print(\"Checksum does not match. Deleting the file.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the contents of the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Failed to extract file: {e}\")\n        return False\n\n    # Remove the downloaded file\n    os.remove(TARGET_TAR_FILE)\n\n    return True"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\nfrom typing import Dict\ndef task_func(url: str, column_name: str, csv_file_path: str) -> Dict[str, int]:\n    \"\"\"\n    Downloads a CSV file from a given URL, saves it to a specified path, and counts the occurrences of each value in a particular column.\n    \"\"\"\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except urllib.error.URLError:\n        print(f\"Failed to download file from URL: {url}\")\n        return\n\n    if not os.path.exists(csv_file_path):\n        print(f\"File does not exist at path: {csv_file_path}\")\n        return\n\n    try:\n        with open(csv_file_path, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            column_values = [row[column_name] for row in csv_reader if column_name in row]\n    except KeyError:\n        print(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n        os.remove(csv_file_path)\n        raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n\n    count_dict = collections.Counter(column_values)\n    os.remove(csv_file_path)\n    return count_dict", "raw_solution": "import urllib.request\nimport os\nimport csv\nimport collections\nfrom typing import Dict\n\ndef task_func(url: str, column_name: str, csv_file_path: str) -> Dict[str, int]:\n    \"\"\"\n    Downloads a CSV file from a given URL, saves it to a specified path, and counts the occurrences of each value in a particular column.\n    \"\"\"\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except urllib.error.URLError:\n        print(f\"Failed to download file from URL: {url}\")\n        return\n\n    if not os.path.exists(csv_file_path):\n        print(f\"File does not exist at path: {csv_file_path}\")\n        return\n\n    try:\n        with open(csv_file_path, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            column_values = [row[column_name] for row in csv_reader if column_name in row]\n    except KeyError:\n        print(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n        os.remove(csv_file_path)\n        raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n\n    count_dict = collections.Counter(column_values)\n    os.remove(csv_file_path)\n    return count_dict"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\nimport io\ndef task_func(url):\n    try:\n        response = urllib.request.urlopen(url)\n        content = response.read()\n        parser = etree.XMLParser(recover=True)\n        tree = etree.fromstring(content, parser=parser)\n    except (urllib.error.URLError, etree.XMLSyntaxError) as e:\n        raise ValueError(f\"Error fetching or parsing XML: {str(e)}\")\n\n    items = tree.xpath('//item')\n    if not items:\n        raise ValueError(\"XML structure does not match expected format\")\n\n    data = []\n    for item in items:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        data.append(item_data)\n\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\nimport io\n\ndef task_func(url):\n    try:\n        response = urllib.request.urlopen(url)\n        content = response.read()\n        parser = etree.XMLParser(recover=True)\n        tree = etree.fromstring(content, parser=parser)\n    except (urllib.error.URLError, etree.XMLSyntaxError) as e:\n        raise ValueError(f\"Error fetching or parsing XML: {str(e)}\")\n\n    items = tree.xpath('//item')\n    if not items:\n        raise ValueError(\"XML structure does not match expected format\")\n\n    data = []\n    for item in items:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        data.append(item_data)\n\n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    \"\"\"\n    Downloads a text file from a specified URL, processes the text to count the frequency of each word,\n    and then plots a bar chart showing the ten most frequently occurring words.\n    \"\"\"\n    # Download the file\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n\n    # Process the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    counter = Counter(words)\n\n    # Plot the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values())\n    ax.set_title('Word Frequencies')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    plt.xticks(rotation=90)\n\n    return counter, ax", "raw_solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    \"\"\"\n    Downloads a text file from a specified URL, processes the text to count the frequency of each word,\n    and then plots a bar chart showing the ten most frequently occurring words.\n    \"\"\"\n    # Download the file\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n\n    # Process the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    counter = Counter(words)\n\n    # Plot the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values())\n    ax.set_title('Word Frequencies')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    plt.xticks(rotation=90)\n\n    return counter, ax"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Unable to download the file from the provided URL. {str(e)}\"\n\n    if response.headers.get('content-type') != 'application/zip':\n        return \"Error: The URL does not point to a ZIP file.\"\n\n    try:\n        os.makedirs(download_path, exist_ok=True)\n        file_path = os.path.join(download_path, \"downloaded.zip\")\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n    except Exception as e:\n        return f\"Error: Unable to write to the download path. {str(e)}\"\n\n    try:\n        with ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.testzip()\n    except BadZipFile as e:\n        os.remove(file_path)\n        return \"Error: The downloaded file is not a valid ZIP file.\"\n    except Exception as e:\n        os.remove(file_path)\n        return f\"Error: Unable to extract the ZIP file. {str(e)}\"\n\n    return file_path", "raw_solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Unable to download the file from the provided URL. {str(e)}\"\n\n    if response.headers.get('content-type') != 'application/zip':\n        return \"Error: The URL does not point to a ZIP file.\"\n\n    try:\n        os.makedirs(download_path, exist_ok=True)\n        file_path = os.path.join(download_path, \"downloaded.zip\")\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n    except Exception as e:\n        return f\"Error: Unable to write to the download path. {str(e)}\"\n\n    try:\n        with ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.testzip()\n    except BadZipFile as e:\n        os.remove(file_path)\n        return \"Error: The downloaded file is not a valid ZIP file.\"\n    except Exception as e:\n        os.remove(file_path)\n        return f\"Error: Unable to extract the ZIP file. {str(e)}\"\n\n    return file_path"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    \"\"\"\n    Extracts and converts data from a specified HTML table based on the given 'table_id' on a webpage into a Pandas DataFrame.\n    If the table is present but contains no data rows (i.e., no <tr> tags), the function returns an empty DataFrame.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        print (\"HTTP Error:\",errh)\n        return\n    except requests.exceptions.ConnectionError as errc:\n        print (\"Error Connecting:\",errc)\n        return\n    except requests.exceptions.Timeout as errt:\n        print (\"Timeout Error:\",errt)\n        return\n    except requests.exceptions.RequestException as err:\n        print (\"Something went wrong\",err)\n        return\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if table is None:\n        raise ValueError('Table with the specified ID not found.')\n\n    csv = pd.read_html(str(table))[0].to_csv(index=False)\n    df = pd.read_csv(StringIO(csv))\n\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(url, table_id):\n    \"\"\"\n    Extracts and converts data from a specified HTML table based on the given 'table_id' on a webpage into a Pandas DataFrame.\n    If the table is present but contains no data rows (i.e., no <tr> tags), the function returns an empty DataFrame.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        print (\"HTTP Error:\",errh)\n        return\n    except requests.exceptions.ConnectionError as errc:\n        print (\"Error Connecting:\",errc)\n        return\n    except requests.exceptions.Timeout as errt:\n        print (\"Timeout Error:\",errt)\n        return\n    except requests.exceptions.RequestException as err:\n        print (\"Something went wrong\",err)\n        return\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if table is None:\n        raise ValueError('Table with the specified ID not found.')\n\n    csv = pd.read_html(str(table))[0].to_csv(index=False)\n    df = pd.read_csv(StringIO(csv))\n\n    return df"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nimport os\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    \"\"\"\n    Downloads and extracts a zip file from a specified URL.\n    \"\"\"\n    # Create download and zip directories if they don't exist\n    DOWNLOAD_DIR.mkdir(exist_ok=True)\n    ZIP_DIR.mkdir(exist_ok=True)\n\n    # Construct full path for the file\n    file_path = DOWNLOAD_DIR / filename\n\n    try:\n        # Download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n\n        # Save the file\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception occurred: {}\".format(str(e)), [])\n\n    try:\n        # Extract the zip file\n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # Get the list of filenames in the unzipped directory\n        unzipped_files = os.listdir(ZIP_DIR)\n\n        return (\"Success\", unzipped_files)\n\n    except zipfile.BadZipFile as e:\n        return (\"Error: BadZipFile exception occurred: {}\".format(str(e)), [])\n    except zipfile.LargeZipFile as e:\n        return (\"Error: LargeZipFile exception occurred: {}\".format(str(e)), [])\n    except Exception as e:\n        return (\"Error: File-related exception occurred: {}\".format(str(e)), [])", "raw_solution": "import requests\nfrom pathlib import Path\nimport zipfile\nimport os\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    \"\"\"\n    Downloads and extracts a zip file from a specified URL.\n    \"\"\"\n    # Create download and zip directories if they don't exist\n    DOWNLOAD_DIR.mkdir(exist_ok=True)\n    ZIP_DIR.mkdir(exist_ok=True)\n\n    # Construct full path for the file\n    file_path = DOWNLOAD_DIR / filename\n\n    try:\n        # Download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n\n        # Save the file\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception occurred: {}\".format(str(e)), [])\n\n    try:\n        # Extract the zip file\n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # Get the list of filenames in the unzipped directory\n        unzipped_files = os.listdir(ZIP_DIR)\n\n        return (\"Success\", unzipped_files)\n\n    except zipfile.BadZipFile as e:\n        return (\"Error: BadZipFile exception occurred: {}\".format(str(e)), [])\n    except zipfile.LargeZipFile as e:\n        return (\"Error: LargeZipFile exception occurred: {}\".format(str(e)), [])\n    except Exception as e:\n        return (\"Error: File-related exception occurred: {}\".format(str(e)), [])"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Parse the content of the response with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all the <a> tags in the HTML\n    links = soup.find_all('a')\n\n    # Initialize a set to store unique absolute links\n    unique_links = set()\n\n    # Iterate over the links\n    for link in links:\n        # Get the href attribute of the link\n        href = link.get('href')\n\n        # If the href attribute exists and is not None, join it with the base URL to get the absolute URL\n        if href and href != '#':\n            absolute_url = urljoin(base_url, href)\n\n            # Add the absolute URL to the set of unique links\n            unique_links.add(absolute_url)\n\n    # Open the CSV file in write mode\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write each unique link to a new row in the CSV file\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique links\n    return len(unique_links)", "raw_solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Parse the content of the response with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all the <a> tags in the HTML\n    links = soup.find_all('a')\n\n    # Initialize a set to store unique absolute links\n    unique_links = set()\n\n    # Iterate over the links\n    for link in links:\n        # Get the href attribute of the link\n        href = link.get('href')\n\n        # If the href attribute exists and is not None, join it with the base URL to get the absolute URL\n        if href and href != '#':\n            absolute_url = urljoin(base_url, href)\n\n            # Add the absolute URL to the set of unique links\n            unique_links.add(absolute_url)\n\n    # Open the CSV file in write mode\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write each unique link to a new row in the CSV file\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique links\n    return len(unique_links)"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\nfrom typing import Union\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> Union[int, None]:\n    \"\"\"\n    This function parses HTML table data from a specified URL or local file and stores it into an SQLite database.\n    It handles different scenarios for fetching, processing, and storing data.\n    \"\"\"\n    try:\n        # Fetch HTML content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error: {e}\")\n        return\n\n    # Parse HTML content\n    try:\n        parsed_content = html.fromstring(response.content)\n        table_element = parsed_content.xpath('//table')\n        if not table_element:\n            print(\"No table found in the HTML content.\")\n            return 0\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return\n\n    # Convert HTML table to pandas DataFrame\n    try:\n        table_data = pd.read_html(webpage_url)[0]\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return\n\n    # Store DataFrame into SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n        table_data.to_sql(\"my_table\", conn, if_exists='replace', index=False)\n        conn.close()\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return\n\n    return table_data.shape[0]", "raw_solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\nfrom typing import Union\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> Union[int, None]:\n    \"\"\"\n    This function parses HTML table data from a specified URL or local file and stores it into an SQLite database.\n    It handles different scenarios for fetching, processing, and storing data.\n    \"\"\"\n    try:\n        # Fetch HTML content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error: {e}\")\n        return\n\n    # Parse HTML content\n    try:\n        parsed_content = html.fromstring(response.content)\n        table_element = parsed_content.xpath('//table')\n        if not table_element:\n            print(\"No table found in the HTML content.\")\n            return 0\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return\n\n    # Convert HTML table to pandas DataFrame\n    try:\n        table_data = pd.read_html(webpage_url)[0]\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return\n\n    # Store DataFrame into SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n        table_data.to_sql(\"my_table\", conn, if_exists='replace', index=False)\n        conn.close()\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return\n\n    return table_data.shape[0]"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nfrom PIL import Image\nfrom PIL.ExifTags import TAGS\nIMAGE_PATH = \"image.png\"\ndef get_image_comment(image_path):\n    image = Image.open(image_path)\n    exif_data = image._getexif()\n    if exif_data is not None:\n        for tag, value in exif_data.items():\n            tag_name = TAGS.get(tag, tag)\n            if tag_name == 'Artist':\n                return value\n    return \"\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        ocr_text = pytesseract.image_to_string(Image.open(filename), lang='eng')\n        decoded_text = codecs.decode(ocr_text, from_encoding)\n        encoded_text = codecs.encode(decoded_text, to_encoding)\n        return encoded_text.decode()\n    except (ValueError, LookupError) as e:\n        try:\n            comment = get_image_comment(filename)\n            decoded_comment = codecs.decode(comment, from_encoding)\n            encoded_comment = codecs.encode(decoded_comment, to_encoding)\n            return encoded_comment.decode()\n        except (ValueError, LookupError):\n            return \"\"", "raw_solution": "from PIL import Image\nimport codecs\nimport pytesseract\nfrom PIL import Image\nfrom PIL.ExifTags import TAGS\n\nIMAGE_PATH = \"image.png\"\n\ndef get_image_comment(image_path):\n    image = Image.open(image_path)\n    exif_data = image._getexif()\n    if exif_data is not None:\n        for tag, value in exif_data.items():\n            tag_name = TAGS.get(tag, tag)\n            if tag_name == 'Artist':\n                return value\n    return \"\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        ocr_text = pytesseract.image_to_string(Image.open(filename), lang='eng')\n        decoded_text = codecs.decode(ocr_text, from_encoding)\n        encoded_text = codecs.encode(decoded_text, to_encoding)\n        return encoded_text.decode()\n    except (ValueError, LookupError) as e:\n        try:\n            comment = get_image_comment(filename)\n            decoded_comment = codecs.decode(comment, from_encoding)\n            encoded_comment = codecs.encode(decoded_comment, to_encoding)\n            return encoded_comment.decode()\n        except (ValueError, LookupError):\n            return \"\""}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        content = response.content\n\n        if not content:\n            return {}\n\n        if not from_encoding:\n            from_encoding = chardet.detect(content)['encoding']\n\n        if not from_encoding:\n            raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        decoded_content = content.decode(from_encoding)\n        reencoded_content = decoded_content.encode(to_encoding)\n        json_data = json.loads(reencoded_content)\n\n        return json_data\n    except Exception as e:\n        raise e", "raw_solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        content = response.content\n\n        if not content:\n            return {}\n\n        if not from_encoding:\n            from_encoding = chardet.detect(content)['encoding']\n\n        if not from_encoding:\n            raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        decoded_content = content.decode(from_encoding)\n        reencoded_content = decoded_content.encode(to_encoding)\n        json_data = json.loads(reencoded_content)\n\n        return json_data\n    except Exception as e:\n        raise e"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads a CSV file and processes its date-related data.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"File {csv_file_path} not found.\")\n\n    # Read CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n\n    # Check if column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"Column {column_name} not found in the CSV file.\")\n\n    # Convert date values to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n\n    # Filter rows based on the current date\n    current_date = datetime.now()\n    df = df[df[column_name] < current_date]\n\n    # Sort the resulting data\n    df = df.sort_values(by=[column_name])\n\n    return df", "raw_solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads a CSV file and processes its date-related data.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"File {csv_file_path} not found.\")\n\n    # Read CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n\n    # Check if column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"Column {column_name} not found in the CSV file.\")\n\n    # Convert date values to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n\n    # Filter rows based on the current date\n    current_date = datetime.now()\n    df = df[df[column_name] < current_date]\n\n    # Sort the resulting data\n    df = df.sort_values(by=[column_name])\n\n    return df"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    context.load_cert_chain(cert_file, key_file)\n\n    secure_sock = context.wrap_socket(client_socket, server_side=True)\n\n    try:\n        secure_sock.do_handshake()\n    except Exception as e:\n        return f\"Handshake error: {str(e)}\"\n\n    try:\n        file_path = secure_sock.recv(buffer_size).decode().strip()\n    except Exception as e:\n        return f\"Receiving file path error: {str(e)}\"\n\n    if not os.path.exists(file_path):\n        return 'File not found'\n\n    try:\n        with open(file_path, 'rb') as file:\n            sha256 = hashlib.sha256()\n            while True:\n                data = file.read(buffer_size)\n                if not data:\n                    break\n                sha256.update(data)\n        return sha256.hexdigest()\n    except Exception as e:\n        return f\"Hashing file error: {str(e)}\"", "raw_solution": "import ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    context.load_cert_chain(cert_file, key_file)\n\n    secure_sock = context.wrap_socket(client_socket, server_side=True)\n\n    try:\n        secure_sock.do_handshake()\n    except Exception as e:\n        return f\"Handshake error: {str(e)}\"\n\n    try:\n        file_path = secure_sock.recv(buffer_size).decode().strip()\n    except Exception as e:\n        return f\"Receiving file path error: {str(e)}\"\n\n    if not os.path.exists(file_path):\n        return 'File not found'\n\n    try:\n        with open(file_path, 'rb') as file:\n            sha256 = hashlib.sha256()\n            while True:\n                data = file.read(buffer_size)\n                if not data:\n                    break\n                sha256.update(data)\n        return sha256.hexdigest()\n    except Exception as e:\n        return f\"Hashing file error: {str(e)}\""}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5):\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # List for sockets and their corresponding buffers\n    sockets_list = [server_socket]\n    clients_list = {}\n    start_time = datetime.now()\n\n    print(f\"Server started at {start_time}\")\n\n    while (datetime.now() - start_time).seconds < run_duration:\n        read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list)\n\n        # Handle received data\n        for notified_socket in read_sockets:\n            if notified_socket == server_socket:\n                client_socket, client_address = server_socket.accept()\n                sockets_list.append(client_socket)\n                clients_list[client_socket] = queue.Queue()\n            else:\n                data = notified_socket.recv(buffer_size)\n                if data:\n                    # Append server's current time to received data\n                    response = data + str.encode(str(datetime.now()))\n                    clients_list[notified_socket].put(response)\n                else:\n                    # If no data received, close the connection\n                    sockets_list.remove(notified_socket)\n                    del clients_list[notified_socket]\n\n        # Handle exceptions\n        for notified_socket in exception_sockets:\n            sockets_list.remove(notified_socket)\n            del clients_list[notified_socket]\n\n        # Send data to clients\n        for notified_socket in read_sockets:\n            if notified_socket in clients_list:\n                while not clients_list[notified_socket].empty():\n                    notified_socket.send(clients_list[notified_socket].get())\n\n    # Close the server socket\n    server_socket.close()\n\n    end_time = datetime.now()\n    print(f\"Server stopped at {end_time}\")\n    print(f\"Server operation duration: {end_time - start_time}\")\n\n    return f\"Server started at {start_time} and stopped at {end_time}, operation duration: {end_time - start_time}\"", "raw_solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5):\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # List for sockets and their corresponding buffers\n    sockets_list = [server_socket]\n    clients_list = {}\n    start_time = datetime.now()\n\n    print(f\"Server started at {start_time}\")\n\n    while (datetime.now() - start_time).seconds < run_duration:\n        read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list)\n\n        # Handle received data\n        for notified_socket in read_sockets:\n            if notified_socket == server_socket:\n                client_socket, client_address = server_socket.accept()\n                sockets_list.append(client_socket)\n                clients_list[client_socket] = queue.Queue()\n            else:\n                data = notified_socket.recv(buffer_size)\n                if data:\n                    # Append server's current time to received data\n                    response = data + str.encode(str(datetime.now()))\n                    clients_list[notified_socket].put(response)\n                else:\n                    # If no data received, close the connection\n                    sockets_list.remove(notified_socket)\n                    del clients_list[notified_socket]\n\n        # Handle exceptions\n        for notified_socket in exception_sockets:\n            sockets_list.remove(notified_socket)\n            del clients_list[notified_socket]\n\n        # Send data to clients\n        for notified_socket in read_sockets:\n            if notified_socket in clients_list:\n                while not clients_list[notified_socket].empty():\n                    notified_socket.send(clients_list[notified_socket].get())\n\n    # Close the server socket\n    server_socket.close()\n\n    end_time = datetime.now()\n    print(f\"Server stopped at {end_time}\")\n    print(f\"Server operation duration: {end_time - start_time}\")\n\n    return f\"Server started at {start_time} and stopped at {end_time}, operation duration: {end_time - start_time}\"\n\n# Test the function"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    \"\"\"\n    Receive a message from a client socket and send it as an email via an SMTP server.\n    \"\"\"\n    # Receive message from client\n    data = client_socket.recv(1024).decode()\n\n    # Prompt for sender's email, recipient's email, and password\n    sender_email = input(\"Enter your email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    password = getpass.getpass(\"Enter your password: \")\n\n    # Create an email message\n    msg = EmailMessage()\n    msg.set_content(data)\n    msg['Subject'] = 'Message from client'\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n\n    # Send email\n    try:\n        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n        server.starttls()\n        server.login(sender_email, password)\n        server.send_message(msg)\n        server.quit()\n    except Exception as e:\n        print(f\"Failed to send email: {str(e)}\")\n        return\n\n    print(\"Email sent successfully.\")", "raw_solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    \"\"\"\n    Receive a message from a client socket and send it as an email via an SMTP server.\n    \"\"\"\n    # Receive message from client\n    data = client_socket.recv(1024).decode()\n\n    # Prompt for sender's email, recipient's email, and password\n    sender_email = input(\"Enter your email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    password = getpass.getpass(\"Enter your password: \")\n\n    # Create an email message\n    msg = EmailMessage()\n    msg.set_content(data)\n    msg['Subject'] = 'Message from client'\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n\n    # Send email\n    try:\n        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n        server.starttls()\n        server.login(sender_email, password)\n        server.send_message(msg)\n        server.quit()\n    except Exception as e:\n        print(f\"Failed to send email: {str(e)}\")\n        return\n\n    print(\"Email sent successfully.\")"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File {file_path} not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    # Check if the CSV file has a header\n    if 'Text' in df.columns:\n        text_data = df['Text']\n    else:\n        text_data = df[df.columns[0]]\n\n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n\n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n\n    # Get the word counts\n    word_counts = vectorizer.get_feature_names_out()\n    count_values = X.toarray().sum(axis=0)\n\n    # Create a DataFrame for word counts\n    df_word_counts = pd.DataFrame(list(zip(word_counts, count_values)), columns=['Word', 'Count'])\n\n    # Get the top 10 most common words\n    top_10_words = df_word_counts.nlargest(10, 'Count')\n\n    # Plot the histogram\n    plt.figure(figsize=(10, 5))\n    plt.bar(top_10_words['Word'], top_10_words['Count'])\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the top 10 most common words')\n    plt.xticks(rotation=90)\n\n    if save_path is not None:\n        # Save the plot to a file\n        plt.savefig(save_path)\n        return None\n    else:\n        # Display the plot\n        plt.show()\n        return plt.gca()", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File {file_path} not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    # Check if the CSV file has a header\n    if 'Text' in df.columns:\n        text_data = df['Text']\n    else:\n        text_data = df[df.columns[0]]\n\n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n\n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n\n    # Get the word counts\n    word_counts = vectorizer.get_feature_names_out()\n    count_values = X.toarray().sum(axis=0)\n\n    # Create a DataFrame for word counts\n    df_word_counts = pd.DataFrame(list(zip(word_counts, count_values)), columns=['Word', 'Count'])\n\n    # Get the top 10 most common words\n    top_10_words = df_word_counts.nlargest(10, 'Count')\n\n    # Plot the histogram\n    plt.figure(figsize=(10, 5))\n    plt.bar(top_10_words['Word'], top_10_words['Count'])\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the top 10 most common words')\n    plt.xticks(rotation=90)\n\n    if save_path is not None:\n        # Save the plot to a file\n        plt.savefig(save_path)\n        return None\n    else:\n        # Display the plot\n        plt.show()\n        return plt.gca()"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    if animals is None:\n        animals = ['Dog', 'Cat', 'Bird']\n    if foods is None:\n        foods = ['Bread', 'Milk', 'Fish']\n    if not animals and not foods:\n        return pd.DataFrame()\n\n    combinations = list(itertools.product(animals, foods))\n    np.random.shuffle(combinations)\n    df = pd.DataFrame(combinations, columns=foods)\n    df.index = animals\n    df = df.astype(str) + ':' + df\n    return df", "raw_solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    if animals is None:\n        animals = ['Dog', 'Cat', 'Bird']\n    if foods is None:\n        foods = ['Bread', 'Milk', 'Fish']\n    if not animals and not foods:\n        return pd.DataFrame()\n\n    combinations = list(itertools.product(animals, foods))\n    np.random.shuffle(combinations)\n    df = pd.DataFrame(combinations, columns=foods)\n    df.index = animals\n    df = df.astype(str) + ':' + df\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert all timestamps to the specified timezone\n    converted_times = []\n    for time_string in time_strings:\n        dt = datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S')\n        converted_times.append(pytz.timezone(timezone).localize(dt))\n\n    # Calculate the time difference in seconds between each consecutive pair of timestamps\n    time_diffs = []\n    for i in range(1, len(converted_times)):\n        diff = converted_times[i] - converted_times[i-1]\n        time_diffs.append(abs(diff.total_seconds()))\n\n    # Return the average time difference\n    return np.mean(time_diffs)", "raw_solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert all timestamps to the specified timezone\n    converted_times = []\n    for time_string in time_strings:\n        dt = datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S')\n        converted_times.append(pytz.timezone(timezone).localize(dt))\n\n    # Calculate the time difference in seconds between each consecutive pair of timestamps\n    time_diffs = []\n    for i in range(1, len(converted_times)):\n        diff = converted_times[i] - converted_times[i-1]\n        time_diffs.append(abs(diff.total_seconds()))\n\n    # Return the average time difference\n    return np.mean(time_diffs)\n\n# Test the function"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Split into words\n    words = text.split()\n    # Count the frequency of each word\n    word_counts = Counter(words)\n    # Get the 10 most common words\n    top_10_words = word_counts.most_common(10)\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.barh([word[0] for word in top_10_words], [word[1] for word in top_10_words])\n    ax.set_xlabel('Count')\n    ax.set_ylabel('Word')\n    ax.set_title('Top 10 most common words')\n    plt.tight_layout()\n    return top_10_words, ax", "raw_solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Split into words\n    words = text.split()\n    # Count the frequency of each word\n    word_counts = Counter(words)\n    # Get the 10 most common words\n    top_10_words = word_counts.most_common(10)\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.barh([word[0] for word in top_10_words], [word[1] for word in top_10_words])\n    ax.set_xlabel('Count')\n    ax.set_ylabel('Word')\n    ax.set_title('Top 10 most common words')\n    plt.tight_layout()\n    return top_10_words, ax"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Extract URL from the string\n    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    if not url:\n        return \"No valid URL found in the provided string.\"\n    url = url[0]\n\n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n    except requests.exceptions.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # Parse the content and extract the title\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.find('title')\n    if not title:\n        return \"No title tag found in the webpage.\"\n    return title.text", "raw_solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Extract URL from the string\n    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    if not url:\n        return \"No valid URL found in the provided string.\"\n    url = url[0]\n\n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n    except requests.exceptions.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # Parse the content and extract the title\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.find('title')\n    if not title:\n        return \"No title tag found in the webpage.\"\n    return title.text"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with a specified key, downloads the file from the URL,\n    and saves it with a timestamped filename. The filename format is '{unknown_key}_{timestamp}.txt',\n    where 'timestamp' is formatted as '%Y%m%d%H%M%S%f' to include the date and time down to microseconds.\n    The file is saved in the specified directory or in the current working directory by default.\n    \"\"\"\n    data = json.loads(json_data)\n    url = data[unknown_key]\n\n    if save_dir is None:\n        save_dir = os.getcwd()\n\n    response = requests.get(url)\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    filepath = os.path.join(save_dir, filename)\n\n    with open(filepath, 'wb') as f:\n        f.write(response.content)\n\n    return os.path.abspath(filepath)", "raw_solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with a specified key, downloads the file from the URL,\n    and saves it with a timestamped filename. The filename format is '{unknown_key}_{timestamp}.txt',\n    where 'timestamp' is formatted as '%Y%m%d%H%M%S%f' to include the date and time down to microseconds.\n    The file is saved in the specified directory or in the current working directory by default.\n    \"\"\"\n    data = json.loads(json_data)\n    url = data[unknown_key]\n\n    if save_dir is None:\n        save_dir = os.getcwd()\n\n    response = requests.get(url)\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    filepath = os.path.join(save_dir, filename)\n\n    with open(filepath, 'wb') as f:\n        f.write(response.content)\n\n    return os.path.abspath(filepath)"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n    \"\"\"\n    # Define the phone number regex pattern\n    phone_pattern = re.compile(r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')\n\n    # If the URL starts with 'http', fetch the content, otherwise assume it's a local file\n    if url.startswith('http'):\n        response = requests.get(url)\n        content = response.text\n    else:\n        with open(url, 'r') as file:\n            content = file.read()\n\n    # Use BeautifulSoup to parse the content and find all matches of the phone number pattern\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    phone_numbers = [match.group() for match in phone_pattern.finditer(soup.text)]\n\n    # Save the phone numbers to the output file in JSON format\n    with open(output_path, 'w') as file:\n        json.dump(phone_numbers, file)\n\n    return phone_numbers", "raw_solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n    \"\"\"\n    # Define the phone number regex pattern\n    phone_pattern = re.compile(r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')\n\n    # If the URL starts with 'http', fetch the content, otherwise assume it's a local file\n    if url.startswith('http'):\n        response = requests.get(url)\n        content = response.text\n    else:\n        with open(url, 'r') as file:\n            content = file.read()\n\n    # Use BeautifulSoup to parse the content and find all matches of the phone number pattern\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    phone_numbers = [match.group() for match in phone_pattern.finditer(soup.text)]\n\n    # Save the phone numbers to the output file in JSON format\n    with open(output_path, 'w') as file:\n        json.dump(phone_numbers, file)\n\n    return phone_numbers"}
