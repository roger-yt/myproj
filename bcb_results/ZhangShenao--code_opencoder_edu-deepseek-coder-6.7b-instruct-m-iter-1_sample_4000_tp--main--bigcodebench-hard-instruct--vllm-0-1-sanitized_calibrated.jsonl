{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    try:\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n\n    try:\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    file_list = []\n    try:\n        ftp.retrlines('LIST', file_list.append)\n    except Exception as e:\n        raise Exception(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    file_names = [file.split()[-1] for file in file_list]\n\n    for file_name in file_names:\n        try:\n            with open(file_name, 'wb') as file:\n                ftp.retrbinary('RETR ' + file_name, file.write)\n        except Exception as e:\n            print(f\"Failed to download file {file_name} from server {ftp_server}: {str(e)}\")\n\n    ftp.quit()\n\n    return file_names", "raw_solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    try:\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n\n    try:\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    file_list = []\n    try:\n        ftp.retrlines('LIST', file_list.append)\n    except Exception as e:\n        raise Exception(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    file_names = [file.split()[-1] for file in file_list]\n\n    for file_name in file_names:\n        try:\n            with open(file_name, 'wb') as file:\n                ftp.retrbinary('RETR ' + file_name, file.write)\n        except Exception as e:\n            print(f\"Failed to download file {file_name} from server {ftp_server}: {str(e)}\")\n\n    ftp.quit()\n\n    return file_names"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"File {commands_file_path} does not exist.\")\n\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_file_paths = []\n\n    with open(commands_file_path, 'r') as file:\n        reader = csv.reader(file)\n        for i, row in enumerate(reader):\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{i}_output.txt')\n            output_file_paths.append(output_file_path)\n\n            try:\n                result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n            except subprocess.CalledProcessError as e:\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(f\"Command failed with exit code {e.returncode}.\\nError message: {e.output}\")\n\n    return output_file_paths", "raw_solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"File {commands_file_path} does not exist.\")\n\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_file_paths = []\n\n    with open(commands_file_path, 'r') as file:\n        reader = csv.reader(file)\n        for i, row in enumerate(reader):\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{i}_output.txt')\n            output_file_paths.append(output_file_path)\n\n            try:\n                result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n            except subprocess.CalledProcessError as e:\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(f\"Command failed with exit code {e.returncode}.\\nError message: {e.output}\")\n\n    return output_file_paths"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # If process is found, kill it\n            subprocess.run(['taskkill', '/F', '/IM', process_name])\n            return f\"Process found. Restarting {process_name}.\"\n\n    # If process is not found, start it\n    subprocess.Popen(process_name)\n    time.sleep(1)  # Wait for the process to start\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            return f\"Process not found. Starting {process_name}.\"", "raw_solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # If process is found, kill it\n            subprocess.run(['taskkill', '/F', '/IM', process_name])\n            return f\"Process found. Restarting {process_name}.\"\n\n    # If process is not found, start it\n    subprocess.Popen(process_name)\n    time.sleep(1)  # Wait for the process to start\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            return f\"Process not found. Starting {process_name}.\"\n\n# Test cases"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    \"\"\"\n    Zips all files (not including subdirectories) located in the specified directory and returns the path to the created zip file.\n    If the specified directory does not exist, it raises a FileNotFoundError.\n    If the directory does not contain any files, it returns None.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    files = glob.glob(os.path.join(directory, \"*\"))\n    files = [f for f in files if os.path.isfile(f)]\n\n    if not files:\n        return None\n\n    zip_path = os.path.join(directory, \"files.zip\")\n    with zipfile.ZipFile(zip_path, \"w\") as zipf:\n        for file in files:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return zip_path", "raw_solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    \"\"\"\n    Zips all files (not including subdirectories) located in the specified directory and returns the path to the created zip file.\n    If the specified directory does not exist, it raises a FileNotFoundError.\n    If the directory does not contain any files, it returns None.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    files = glob.glob(os.path.join(directory, \"*\"))\n    files = [f for f in files if os.path.isfile(f)]\n\n    if not files:\n        return None\n\n    zip_path = os.path.join(directory, \"files.zip\")\n    with zipfile.ZipFile(zip_path, \"w\") as zipf:\n        for file in files:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return zip_path"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = url_pattern.sub('', text)\n\n    # Split the text into words\n    words = text.split()\n\n    # If there are no words available, raise an exception\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n\n    # Generate a word cloud from the words\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = None, \n                min_font_size = 10).generate(str(words))\n\n    # Plot the word cloud\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show()\n\n    return wordcloud", "raw_solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs from the text\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = url_pattern.sub('', text)\n\n    # Split the text into words\n    words = text.split()\n\n    # If there are no words available, raise an exception\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n\n    # Generate a word cloud from the words\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = None, \n                min_font_size = 10).generate(str(words))\n\n    # Plot the word cloud\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show()\n\n    return wordcloud"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, target_column):\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train a random forest classifier\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X, y)\n\n    # Get feature importances\n    importances = clf.feature_importances_\n    features = X.columns\n\n    # Create a dataframe for feature importances\n    df_importances = pd.DataFrame({'Feature': features, 'Importance': importances})\n\n    # Sort the dataframe by importance\n    df_importances = df_importances.sort_values('Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Feature', data=df_importances)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.tight_layout()\n\n    return clf, plt.gca()", "raw_solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_column):\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train a random forest classifier\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X, y)\n\n    # Get feature importances\n    importances = clf.feature_importances_\n    features = X.columns\n\n    # Create a dataframe for feature importances\n    df_importances = pd.DataFrame({'Feature': features, 'Importance': importances})\n\n    # Sort the dataframe by importance\n    df_importances = df_importances.sort_values('Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Feature', data=df_importances)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.tight_layout()\n\n    return clf, plt.gca()"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash", "raw_solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass User(UserMixin):\n    def __init__(self, username, password):\n        self.username = username\n        self.password = generate_password_hash(password)\n\n    def check_password(self, password):\n        return check_password_hash(self.password, password)\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\n@login_manager.user_loader\ndef load_user(user_id):\n    for user in users:\n        if user.username == user_id:\n            return user\n    return None\n\n@login_manager.unauthorized_handler\ndef unauthorized_handler():\n    return 'Unauthorized', 401\n\ndef create_app(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n\n    users = [User('admin', 'password')]\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            for user in users:\n                if user.username == form.username.data and user.check_password(form.password.data):\n                    login_user(user)\n                    return redirect(url_for('protected'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return 'Logged in as: ' + current_user.username\n\n    return app"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    It standardizes the column, calculates Z-scores, and removes data points where the Z-score exceeds a threshold.\n    The function also visualizes the data before and after outlier removal.\n\n    Parameters:\n    data (pandas.DataFrame): The dataset to be processed.\n    column (str): The column in the dataset to be processed.\n    outlier_z_score (float): The Z-score threshold for identifying outliers.\n\n    Returns:\n    tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Standardize the column\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[column].values.reshape(-1, 1))\n\n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data[column]))\n\n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove outliers\n    data_without_outliers = data.drop(outliers)\n\n    # Visualize data before and after outlier removal\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.scatter(range(len(data[column])), data[column])\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(range(len(data_without_outliers[column])), data_without_outliers[column])\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.tight_layout()\n    plt.show()\n\n    return data, data_without_outliers, outliers", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    It standardizes the column, calculates Z-scores, and removes data points where the Z-score exceeds a threshold.\n    The function also visualizes the data before and after outlier removal.\n\n    Parameters:\n    data (pandas.DataFrame): The dataset to be processed.\n    column (str): The column in the dataset to be processed.\n    outlier_z_score (float): The Z-score threshold for identifying outliers.\n\n    Returns:\n    tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Standardize the column\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[column].values.reshape(-1, 1))\n\n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data[column]))\n\n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove outliers\n    data_without_outliers = data.drop(outliers)\n\n    # Visualize data before and after outlier removal\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.scatter(range(len(data[column])), data[column])\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(range(len(data_without_outliers[column])), data_without_outliers[column])\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.tight_layout()\n    plt.show()\n\n    return data, data_without_outliers, outliers"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' should be an integer greater than 1\")\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels)\n\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=169, linewidths=3, color='r', zorder=10)\n\n    plt.title('K-Means Clustering')\n    plt.xlabel(data.columns[0])\n    plt.ylabel(data.columns[1])\n\n    return labels, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' should be an integer greater than 1\")\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels)\n\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=169, linewidths=3, color='r', zorder=10)\n\n    plt.title('K-Means Clustering')\n    plt.xlabel(data.columns[0])\n    plt.ylabel(data.columns[1])\n\n    return labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    transformed_df = pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('2D PCA Scatter Plot')\n\n    return transformed_df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    transformed_df = pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('2D PCA Scatter Plot')\n\n    return transformed_df, ax"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Generate the pair plot\n    g = sns.pairplot(iris_df, hue='species', palette='Dark2', diag_kind='kde', height=2.5)\n    g.fig.suptitle(\"Iris Dataset Pair Plot\", y=1.08)\n\n    return g.figure", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Generate the pair plot\n    g = sns.pairplot(iris_df, hue='species', palette='Dark2', diag_kind='kde', height=2.5)\n    g.fig.suptitle(\"Iris Dataset Pair Plot\", y=1.08)\n\n    return g.figure"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate random data\n        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]\n        values = [random.randint(0, 100) for _ in range(30)]\n\n        # Create a DataFrame\n        df = pd.DataFrame({\n            'Date': dates,\n            'Value': values\n        })\n\n        # Convert 'Date' to datetime\n        df['Date'] = pd.to_datetime(df['Date'])\n\n        # Set the plot style\n        plt.style.use('seaborn')\n\n        # Create the plot\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.plot(df['Date'], df['Value'], color='blue')\n\n        # Set labels and title\n        ax.set_xlabel('Date', fontsize=12, fontname='Arial')\n        ax.set_ylabel('Value', fontsize=12, fontname='Arial')\n        ax.set_title('Random Time Series Data', fontsize=14, fontname='Arial')\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"There was an issue generating the data or plot: {e}\")", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate random data\n        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]\n        values = [random.randint(0, 100) for _ in range(30)]\n\n        # Create a DataFrame\n        df = pd.DataFrame({\n            'Date': dates,\n            'Value': values\n        })\n\n        # Convert 'Date' to datetime\n        df['Date'] = pd.to_datetime(df['Date'])\n\n        # Set the plot style\n        plt.style.use('seaborn')\n\n        # Create the plot\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.plot(df['Date'], df['Value'], color='blue')\n\n        # Set labels and title\n        ax.set_xlabel('Date', fontsize=12, fontname='Arial')\n        ax.set_ylabel('Value', fontsize=12, fontname='Arial')\n        ax.set_title('Random Time Series Data', fontsize=14, fontname='Arial')\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"There was an issue generating the data or plot: {e}\")"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_to=None):\n    \"\"\"\n    Draws the correlation heatmap of the Boston Housing dataset using Seaborn.\n    If `save_to` is provided, saves the plot to a file.\n    Raises ValueError if an error occurs in generating or saving the plot.\n    Returns the Axes object containing the heatmap plot.\n    \"\"\"\n    np.random.seed(seed)\n\n    # Load the dataset\n    try:\n        data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    except Exception as e:\n        raise ValueError(f\"Failed to load dataset: {e}\")\n\n    # Define column names\n    column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n    data.columns = column_names\n\n    # Calculate correlation matrix\n    correlation_matrix = data.corr().round(2)\n\n    # Generate heatmap\n    try:\n        fig, ax = plt.subplots(figsize=(10, 10))\n        sns.heatmap(data=correlation_matrix, annot=True, ax=ax)\n    except Exception as e:\n        raise ValueError(f\"Failed to generate heatmap: {e}\")\n\n    # Save plot to file if specified\n    if save_to is not None:\n        try:\n            plt.savefig(save_to)\n        except Exception as e:\n            raise ValueError(f\"Failed to save plot: {e}\")\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_to=None):\n    \"\"\"\n    Draws the correlation heatmap of the Boston Housing dataset using Seaborn.\n    If `save_to` is provided, saves the plot to a file.\n    Raises ValueError if an error occurs in generating or saving the plot.\n    Returns the Axes object containing the heatmap plot.\n    \"\"\"\n    np.random.seed(seed)\n\n    # Load the dataset\n    try:\n        data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    except Exception as e:\n        raise ValueError(f\"Failed to load dataset: {e}\")\n\n    # Define column names\n    column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n    data.columns = column_names\n\n    # Calculate correlation matrix\n    correlation_matrix = data.corr().round(2)\n\n    # Generate heatmap\n    try:\n        fig, ax = plt.subplots(figsize=(10, 10))\n        sns.heatmap(data=correlation_matrix, annot=True, ax=ax)\n    except Exception as e:\n        raise ValueError(f\"Failed to generate heatmap: {e}\")\n\n    # Save plot to file if specified\n    if save_to is not None:\n        try:\n            plt.savefig(save_to)\n        except Exception as e:\n            raise ValueError(f\"Failed to save plot: {e}\")\n\n    return ax"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.seasonal import DecomposeResult\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n\n    # Check if df has required columns\n    if 'value' not in df.columns:\n        raise ValueError(\"'df' should have a 'value' column\")\n\n    # Check if df has valid data types\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column should have numeric data types\")\n\n    # Check if freq is a valid frequency string\n    if freq not in ['D', 'M', 'Q', 'Y']:\n        raise ValueError(\"'freq' should be a valid frequency string\")\n\n    # Check if decomposition_model is valid\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' should be 'additive' or 'multiplicative'\")\n\n    # Decompose the time series\n    result = seasonal_decompose(df['value'], model=decomposition_model, freq=freq)\n\n    # Plot the decomposition\n    fig, ax = plt.subplots(4, 1, figsize=(10, 10))\n    result.plot(ax=ax)\n    plt.tight_layout()\n\n    return (result, ax)", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.seasonal import DecomposeResult\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n\n    # Check if df has required columns\n    if 'value' not in df.columns:\n        raise ValueError(\"'df' should have a 'value' column\")\n\n    # Check if df has valid data types\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column should have numeric data types\")\n\n    # Check if freq is a valid frequency string\n    if freq not in ['D', 'M', 'Q', 'Y']:\n        raise ValueError(\"'freq' should be a valid frequency string\")\n\n    # Check if decomposition_model is valid\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' should be 'additive' or 'multiplicative'\")\n\n    # Decompose the time series\n    result = seasonal_decompose(df['value'], model=decomposition_model, freq=freq)\n\n    # Plot the decomposition\n    fig, ax = plt.subplots(4, 1, figsize=(10, 10))\n    result.plot(ax=ax)\n    plt.tight_layout()\n\n    return (result, ax)"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"'start_date' and 'end_date' must be datetime.datetime instances\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' must be earlier than 'end_date'\")\n\n    random_seed(seed)\n    time_between_dates = end_date - start_date\n    days_between_dates = time_between_dates.days\n\n    random_day_offsets = [randint(0, days_between_dates) for _ in range(days_between_dates)]\n    random_dates = [start_date + timedelta(days=day) for day in random_day_offsets]\n\n    return pd.Series(random_dates)", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"'start_date' and 'end_date' must be datetime.datetime instances\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' must be earlier than 'end_date'\")\n\n    random_seed(seed)\n    time_between_dates = end_date - start_date\n    days_between_dates = time_between_dates.days\n\n    random_day_offsets = [randint(0, days_between_dates) for _ in range(days_between_dates)]\n    random_dates = [start_date + timedelta(days=day) for day in random_day_offsets]\n\n    return pd.Series(random_dates)"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n    if not os.path.exists(file_dir):\n        raise FileNotFoundError(f\"No files found in the directory: {file_dir}\")\n\n    my_list.append('12')\n    file_list = glob.glob(os.path.join(file_dir, f\"*{file_ext}\"))\n    if not file_list:\n        raise FileNotFoundError(f\"No files found with extension: {file_ext} in the directory: {file_dir}\")\n\n    df = pd.DataFrame()\n    for file in file_list[:sum(my_list)]:\n        df = pd.concat([df, pd.read_csv(file)])\n\n    return df", "raw_solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n    if not os.path.exists(file_dir):\n        raise FileNotFoundError(f\"No files found in the directory: {file_dir}\")\n\n    my_list.append('12')\n    file_list = glob.glob(os.path.join(file_dir, f\"*{file_ext}\"))\n    if not file_list:\n        raise FileNotFoundError(f\"No files found with extension: {file_ext} in the directory: {file_dir}\")\n\n    df = pd.DataFrame()\n    for file in file_list[:sum(my_list)]:\n        df = pd.concat([df, pd.read_csv(file)])\n\n    return df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' must be a list\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"'my_list' must contain only numeric elements\")\n\n    random_seed(seed)\n    start_time = time.time()\n\n    my_list.append(12)\n    total_sum = sum(my_list)\n    random_list = [randint(1, 100) for _ in range(min(total_sum, size))]\n\n    elapsed_time = time.time() - start_time\n\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 101), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return elapsed_time, ax", "raw_solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' must be a list\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"'my_list' must contain only numeric elements\")\n\n    random_seed(seed)\n    start_time = time.time()\n\n    my_list.append(12)\n    total_sum = sum(my_list)\n    random_list = [randint(1, 100) for _ in range(min(total_sum, size))]\n\n    elapsed_time = time.time() - start_time\n\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 101), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return elapsed_time, ax"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom requests.exceptions import ConnectionError, HTTPError\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except (ConnectionError, HTTPError) as err:\n        raise SystemExit(err)\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find_all('table')\n\n    if not table:\n        raise ValueError(\"No table data found on the page\")\n\n    table = table[0]\n    rows = table.find_all('tr')\n\n    if not rows:\n        raise ValueError(\"No table rows found\")\n\n    header = [th.text for th in rows[0].find_all('th')]\n    data = []\n\n    for row in rows[1:]:\n        cols = row.find_all('td')\n        cols = [col.text.strip() for col in cols]\n        data.append(cols)\n\n    df = pd.DataFrame(data, columns=header)\n\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom requests.exceptions import ConnectionError, HTTPError\n\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except (ConnectionError, HTTPError) as err:\n        raise SystemExit(err)\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find_all('table')\n\n    if not table:\n        raise ValueError(\"No table data found on the page\")\n\n    table = table[0]\n    rows = table.find_all('tr')\n\n    if not rows:\n        raise ValueError(\"No table rows found\")\n\n    header = [th.text for th in rows[0].find_all('th')]\n    data = []\n\n    for row in rows[1:]:\n        cols = row.find_all('td')\n        cols = [col.text.strip() for col in cols]\n        data.append(cols)\n\n    df = pd.DataFrame(data, columns=header)\n\n    return df"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input should be a non-empty DataFrame\")\n\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if not numeric_cols.any():\n        raise ValueError(\"There are no numeric columns in the DataFrame\")\n\n    figs, axes = plt.subplots(len(numeric_cols), figsize=(10, 10))\n    if len(numeric_cols) == 1:\n        axes = [axes]\n\n    for col, ax in zip(numeric_cols, axes):\n        ax.hist(df[col], bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n\n    plt.tight_layout()\n    return axes", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input should be a non-empty DataFrame\")\n\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if not numeric_cols.any():\n        raise ValueError(\"There are no numeric columns in the DataFrame\")\n\n    figs, axes = plt.subplots(len(numeric_cols), figsize=(10, 10))\n    if len(numeric_cols) == 1:\n        axes = [axes]\n\n    for col, ax in zip(numeric_cols, axes):\n        ax.hist(df[col], bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n\n    plt.tight_layout()\n    return axes"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    def check_port(ip, port):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((str(ip), port))\n        sock.close()\n        return result == 0\n\n    ip_network = IPv4Network(ip_range)\n    ip_list = [str(ip) for ip in ip_network.hosts()]\n\n    results = {}\n    for ip in ip_list:\n        results[ip] = check_port(ip, port)\n\n    return results\nip_range = \"192.168.1.0/24\"\nport = 80", "raw_solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    def check_port(ip, port):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((str(ip), port))\n        sock.close()\n        return result == 0\n\n    ip_network = IPv4Network(ip_range)\n    ip_list = [str(ip) for ip in ip_network.hosts()]\n\n    results = {}\n    for ip in ip_list:\n        results[ip] = check_port(ip, port)\n\n    return results\n\n# Test the function\nip_range = \"192.168.1.0/24\"\nport = 80"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information such as message type, timestamp, and the message itself from a log file and stores the data in a CSV format.\n    \"\"\"\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(.*?)\\] - (.*)'\n\n    # Initialize an empty list to store the log entries\n    log_entries = []\n\n    # Open the log file and read each line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Use the regular expression to match the log entry pattern\n            match = re.match(pattern, line)\n\n            # If a match is found, extract the message type, timestamp, and message\n            if match:\n                message_type, timestamp, message = match.groups()\n\n                # Validate the timestamp\n                try:\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                except ValueError:\n                    raise ValueError(f'Invalid timestamp in log entry: {line}')\n\n                # Append the log entry to the list\n                log_entries.append({\n                    'message_type': message_type,\n                    'timestamp': timestamp,\n                    'message': message\n                })\n\n    # If no valid log entries are found, raise an exception\n    if not log_entries:\n        raise ValueError('No valid log entries found')\n\n    # Convert the log entries to a pandas DataFrame\n    df = pd.DataFrame(log_entries)\n\n    # Generate a file path for the CSV file\n    csv_file = f'{log_file}.csv'\n\n    # Write the DataFrame to the CSV file\n    df.to_csv(csv_file, index=False)\n\n    # Return the file path to the CSV file\n    return csv_file", "raw_solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information such as message type, timestamp, and the message itself from a log file and stores the data in a CSV format.\n    \"\"\"\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(.*?)\\] - (.*)'\n\n    # Initialize an empty list to store the log entries\n    log_entries = []\n\n    # Open the log file and read each line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Use the regular expression to match the log entry pattern\n            match = re.match(pattern, line)\n\n            # If a match is found, extract the message type, timestamp, and message\n            if match:\n                message_type, timestamp, message = match.groups()\n\n                # Validate the timestamp\n                try:\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                except ValueError:\n                    raise ValueError(f'Invalid timestamp in log entry: {line}')\n\n                # Append the log entry to the list\n                log_entries.append({\n                    'message_type': message_type,\n                    'timestamp': timestamp,\n                    'message': message\n                })\n\n    # If no valid log entries are found, raise an exception\n    if not log_entries:\n        raise ValueError('No valid log entries found')\n\n    # Convert the log entries to a pandas DataFrame\n    df = pd.DataFrame(log_entries)\n\n    # Generate a file path for the CSV file\n    csv_file = f'{log_file}.csv'\n\n    # Write the DataFrame to the CSV file\n    df.to_csv(csv_file, index=False)\n\n    # Return the file path to the CSV file\n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a text.\n    The function generates a histogram subplot, which facilitates the understanding of how word lengths vary within the provided text.\n    If there are no words in the input text, or all words are filtered out, the histogram will be empty as no bins will be created.\n    \"\"\"\n    # Filter out non-word characters and split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # If there are no words in the input text, return an empty histogram\n    if not words:\n        return plt.hist([], bins=0)\n\n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n\n    # Generate a histogram of word lengths\n    bins = np.arange(min(word_lengths), max(word_lengths) + 2, 1) - 0.5\n    ax = plt.hist(word_lengths, bins=bins, rwidth=rwidth)\n\n    # Set the x-axis label and title\n    plt.xlabel('Word Length')\n    plt.title('Distribution of Word Lengths')\n\n    return ax", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a text.\n    The function generates a histogram subplot, which facilitates the understanding of how word lengths vary within the provided text.\n    If there are no words in the input text, or all words are filtered out, the histogram will be empty as no bins will be created.\n    \"\"\"\n    # Filter out non-word characters and split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # If there are no words in the input text, return an empty histogram\n    if not words:\n        return plt.hist([], bins=0)\n\n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n\n    # Generate a histogram of word lengths\n    bins = np.arange(min(word_lengths), max(word_lengths) + 2, 1) - 0.5\n    ax = plt.hist(word_lengths, bins=bins, rwidth=rwidth)\n\n    # Set the x-axis label and title\n    plt.xlabel('Word Length')\n    plt.title('Distribution of Word Lengths')\n\n    return ax"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\ndef task_func(df):\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    df = df[df['Title'].str.contains('like|what', case=False)]\n\n    content = df['Content'].str.replace('[^\\w\\s]','').str.lower()\n    words = nltk.word_tokenize(content)\n    words = [word for word in words if word not in punctuation]\n\n    word_freq = dict(Counter(words))\n\n    return word_freq", "raw_solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\ndef task_func(df):\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    df = df[df['Title'].str.contains('like|what', case=False)]\n\n    content = df['Content'].str.replace('[^\\w\\s]','').str.lower()\n    words = nltk.word_tokenize(content)\n    words = [word for word in words if word not in punctuation]\n\n    word_freq = dict(Counter(words))\n\n    return word_freq"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    \"\"\"\n    Prepares and transforms text data from a specified column in a DataFrame by removing stopwords, numbers, and punctuation,\n    and subsequently applying a vectorization process to convert text into a numeric format suitable for analysis.\n    \"\"\"\n    # Remove stopwords, numbers, and punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS]))\n    dataframe[text_column] = dataframe[text_column].str.replace('[^\\w\\s]', '')\n    dataframe[text_column] = dataframe[text_column].str.replace('\\d+', '')\n\n    # Apply vectorization\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert to DataFrame\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    \"\"\"\n    Prepares and transforms text data from a specified column in a DataFrame by removing stopwords, numbers, and punctuation,\n    and subsequently applying a vectorization process to convert text into a numeric format suitable for analysis.\n    \"\"\"\n    # Remove stopwords, numbers, and punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS]))\n    dataframe[text_column] = dataframe[text_column].str.replace('[^\\w\\s]', '')\n    dataframe[text_column] = dataframe[text_column].str.replace('\\d+', '')\n\n    # Apply vectorization\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert to DataFrame\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n\n    return df"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not all(key in dic for key in ('Lon', 'Lat')):\n        raise ValueError(\"'Lon' and 'Lat' keys are required in the dictionary\")\n    if not all(isinstance(dic[key], tuple) and len(dic[key]) == 2 for key in ('Lon', 'Lat')):\n        raise ValueError(\"'Lon' and 'Lat' values should be tuples with 2 elements\")\n\n    lon_range, lat_range = dic['Lon'], dic['Lat']\n    coordinates = [(np.random.uniform(lon_range[0], lon_range[1]), np.random.uniform(lat_range[0], lat_range[1])) for _ in cities]\n    points = [Point(x, y) for x, y in coordinates]\n    df = gpd.GeoDataFrame(cities, geometry=points, crs='epsg:4326')\n    df.rename(columns={0: 'City', 1: 'Coordinates'}, inplace=True)\n    return df", "raw_solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not all(key in dic for key in ('Lon', 'Lat')):\n        raise ValueError(\"'Lon' and 'Lat' keys are required in the dictionary\")\n    if not all(isinstance(dic[key], tuple) and len(dic[key]) == 2 for key in ('Lon', 'Lat')):\n        raise ValueError(\"'Lon' and 'Lat' values should be tuples with 2 elements\")\n\n    lon_range, lat_range = dic['Lon'], dic['Lat']\n    coordinates = [(np.random.uniform(lon_range[0], lon_range[1]), np.random.uniform(lat_range[0], lat_range[1])) for _ in cities]\n    points = [Point(x, y) for x, y in coordinates]\n    df = gpd.GeoDataFrame(cities, geometry=points, crs='epsg:4326')\n    df.rename(columns={0: 'City', 1: 'Coordinates'}, inplace=True)\n    return df"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n\n    set_seed(seed)\n\n    data = []\n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"No timezone found for city {city}\")\n\n        local_tz = pytz.timezone(timezones[city])\n        local_datetime = utc_datetime.astimezone(local_tz)\n\n        data.append({\n            'City': city,\n            'Local Time': local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather_conditions[randint(0, len(weather_conditions) - 1)]\n        })\n\n    return pd.DataFrame(data)", "raw_solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n\n    set_seed(seed)\n\n    data = []\n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"No timezone found for city {city}\")\n\n        local_tz = pytz.timezone(timezones[city])\n        local_datetime = utc_datetime.astimezone(local_tz)\n\n        data.append({\n            'City': city,\n            'Local Time': local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather_conditions[randint(0, len(weather_conditions) - 1)]\n        })\n\n    return pd.DataFrame(data)"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    np.random.seed(seed)\n    random_walk = np.random.choice([-1, 1], size=elements)\n    random_walk_cumulative = np.cumsum(random_walk)\n\n    stats = {\n        'count': elements,\n        'mean': np.mean(random_walk_cumulative),\n        'std': np.std(random_walk_cumulative),\n        'min': np.min(random_walk_cumulative),\n        '5th_percentile': np.percentile(random_walk_cumulative, 5),\n        '25th_percentile': np.percentile(random_walk_cumulative, 25),\n        'median': np.median(random_walk_cumulative),\n        '75th_percentile': np.percentile(random_walk_cumulative, 75),\n        '95th_percentile': np.percentile(random_walk_cumulative, 95),\n        'max': np.max(random_walk_cumulative),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(random_walk_cumulative)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Cumulative Sum')\n\n    return stats, ax.figure", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    np.random.seed(seed)\n    random_walk = np.random.choice([-1, 1], size=elements)\n    random_walk_cumulative = np.cumsum(random_walk)\n\n    stats = {\n        'count': elements,\n        'mean': np.mean(random_walk_cumulative),\n        'std': np.std(random_walk_cumulative),\n        'min': np.min(random_walk_cumulative),\n        '5th_percentile': np.percentile(random_walk_cumulative, 5),\n        '25th_percentile': np.percentile(random_walk_cumulative, 25),\n        'median': np.median(random_walk_cumulative),\n        '75th_percentile': np.percentile(random_walk_cumulative, 75),\n        '95th_percentile': np.percentile(random_walk_cumulative, 95),\n        'max': np.max(random_walk_cumulative),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(random_walk_cumulative)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Cumulative Sum')\n\n    return stats, ax.figure"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\nfrom io import BytesIO\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Downloads a zip file from a URL, extracts its contents to a specified directory,\n    and returns a list of the extracted files.\n\n    Args:\n        url (str): The URL of the zip file to download.\n        destination_directory (str): The directory to extract the zip file contents to.\n        headers (dict, optional): Headers to be sent with the HTTP request. Defaults to None.\n\n    Returns:\n        list: A list of filenames of the extracted files.\n    \"\"\"\n    # Make sure the destination directory exists\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    zip_file = zipfile.ZipFile(BytesIO(response.content))\n\n    # Extract the zip file contents\n    zip_file.extractall(destination_directory)\n\n    # Return the list of extracted files\n    return zip_file.namelist()", "raw_solution": "import requests\nimport os\nimport zipfile\nfrom io import BytesIO\n\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Downloads a zip file from a URL, extracts its contents to a specified directory,\n    and returns a list of the extracted files.\n\n    Args:\n        url (str): The URL of the zip file to download.\n        destination_directory (str): The directory to extract the zip file contents to.\n        headers (dict, optional): Headers to be sent with the HTTP request. Defaults to None.\n\n    Returns:\n        list: A list of filenames of the extracted files.\n    \"\"\"\n    # Make sure the destination directory exists\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    zip_file = zipfile.ZipFile(BytesIO(response.content))\n\n    # Extract the zip file contents\n    zip_file.extractall(destination_directory)\n\n    # Return the list of extracted files\n    return zip_file.namelist()"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low should be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size)\n\n    fig, ax = plt.subplots(1, 1)\n    ax.imshow(image)\n    ax.axis('off')\n    plt.show()\n\n    return ax, image", "raw_solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low should be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size)\n\n    fig, ax = plt.subplots(1, 1)\n    ax.imshow(image)\n    ax.axis('off')\n    plt.show()\n\n    return ax, image"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from a list L, normalizes it based on the sound pressure level (SPL) of a specified audio file, and generates a spectrogram from the matrix.\n    \"\"\"\n    # Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"The specified audio file {audio_file} does not exist.\")\n\n    # Read the audio file\n    data, sample_rate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    SPL = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10 ** (SPL / 20))\n\n    # Generate the spectrogram\n    spectrogram = np.abs(librosa.stft(normalized_matrix.flatten()))\n\n    # Create the figure for the spectrogram\n    fig, ax = plt.subplots()\n\n    # Display the spectrogram with a logarithmic scale for frequency and a linear scale for time\n    ax.imshow(librosa.power_to_db(spectrogram, ref=np.max), aspect='auto', origin='lower', extent=[0, len(data)/sample_rate, 0, sample_rate/2])\n\n    return normalized_matrix, fig", "raw_solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from a list L, normalizes it based on the sound pressure level (SPL) of a specified audio file, and generates a spectrogram from the matrix.\n    \"\"\"\n    # Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"The specified audio file {audio_file} does not exist.\")\n\n    # Read the audio file\n    data, sample_rate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    SPL = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10 ** (SPL / 20))\n\n    # Generate the spectrogram\n    spectrogram = np.abs(librosa.stft(normalized_matrix.flatten()))\n\n    # Create the figure for the spectrogram\n    fig, ax = plt.subplots()\n\n    # Display the spectrogram with a logarithmic scale for frequency and a linear scale for time\n    ax.imshow(librosa.power_to_db(spectrogram, ref=np.max), aspect='auto', origin='lower', extent=[0, len(data)/sample_rate, 0, sample_rate/2])\n\n    return normalized_matrix, fig"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values\n    numeric_values = [val for sublist in original for val in sublist if isinstance(val, (int, float))]\n    numeric_values = np.array(numeric_values)\n\n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n\n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n\n    # Generate PDF\n    x = np.linspace(stats_dict['min'], stats_dict['max'], 1000)\n    pdf = stats.norm.pdf(x, stats_dict['mean'], stats_dict['std_dev'])\n\n    # Plot PDF\n    ax.plot(x, pdf, label='PDF')\n    ax.legend(loc='upper right')\n\n    return numeric_values, stats_dict, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extract numeric values\n    numeric_values = [val for sublist in original for val in sublist if isinstance(val, (int, float))]\n    numeric_values = np.array(numeric_values)\n\n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n\n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n\n    # Generate PDF\n    x = np.linspace(stats_dict['min'], stats_dict['max'], 1000)\n    pdf = stats.norm.pdf(x, stats_dict['mean'], stats_dict['std_dev'])\n\n    # Plot PDF\n    ax.plot(x, pdf, label='PDF')\n    ax.legend(loc='upper right')\n\n    return numeric_values, stats_dict, ax"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array], norm='l2')[0]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the original and normalized arrays\n    ax.plot(original_array, label='Original Data')\n    ax.plot(normalized_array, label='Normalized Data')\n\n    # Set the title and legend\n    ax.set_title('Original vs. Normalized Data')\n    ax.legend()\n\n    # Return the numpy arrays and the axes object\n    return original_array, normalized_array, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array], norm='l2')[0]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the original and normalized arrays\n    ax.plot(original_array, label='Original Data')\n    ax.plot(normalized_array, label='Normalized Data')\n\n    # Set the title and legend\n    ax.set_title('Original vs. Normalized Data')\n    ax.legend()\n\n    # Return the numpy arrays and the axes object\n    return original_array, normalized_array, ax"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # Generate a signal based on the values in \"data\"\n    signal = list(data.values())\n\n    # Run a Fast Fourier Transform (FFT) on the signal\n    fft_signal = fftpack.fft(signal)\n\n    # Generate the frequency axis\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n\n    # Plot and return the FFT of the signal with a title of 'FFT of the signal'\n    plt.figure(figsize=(10, 5))\n    plt.plot(freqs, np.abs(fft_signal))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n\n    return (fft_signal, plt.gca())", "raw_solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # Generate a signal based on the values in \"data\"\n    signal = list(data.values())\n\n    # Run a Fast Fourier Transform (FFT) on the signal\n    fft_signal = fftpack.fft(signal)\n\n    # Generate the frequency axis\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n\n    # Plot and return the FFT of the signal with a title of 'FFT of the signal'\n    plt.figure(figsize=(10, 5))\n    plt.plot(freqs, np.abs(fft_signal))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n\n    return (fft_signal, plt.gca())"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nclass JsonPostHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n\n        if self.headers['Content-Type'] != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Content-Type header is not application/json'\n            }).encode())\n            return\n\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Invalid JSON'\n            }).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'No data key in request'\n            }).encode())\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\ndef task_func():\n    \"\"\"\n    This function returns a class that handles HTTP POST requests and validates incoming data.\n    \"\"\"\n    return JsonPostHandler", "raw_solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\nclass JsonPostHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n\n        if self.headers['Content-Type'] != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Content-Type header is not application/json'\n            }).encode())\n            return\n\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Invalid JSON'\n            }).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'No data key in request'\n            }).encode())\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\n\ndef task_func():\n    \"\"\"\n    This function returns a class that handles HTTP POST requests and validates incoming data.\n    \"\"\"\n    return JsonPostHandler"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\nfrom json import JSONDecodeError\nclass EmailHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        try:\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            email_data = json.loads(post_data)\n\n            if 'subject' not in email_data or 'message' not in email_data or 'to' not in email_data:\n                raise ValueError(\"Missing 'subject', 'message', or 'to' keys in the email data\")\n\n            msg = MIMEText(email_data['message'])\n            msg['Subject'] = email_data['subject']\n            msg['From'] = smtp_username\n            msg['To'] = email_data['to']\n\n            smtp = smtplib.SMTP(smtp_server, smtp_port)\n            smtp.login(smtp_username, smtp_password)\n            smtp.send_message(msg)\n            smtp.quit()\n\n            self.send_response(200)\n            self.end_headers()\n            self.wfile.write(b'Email sent successfully')\n\n        except JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Invalid JSON')\n\n        except ValueError as e:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(str(e).encode())\n\n        except smtplib.SMTPAuthenticationError:\n            self.send_response(535)\n            self.end_headers()\n            self.wfile.write(b'Authentication Failed')\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    server_address = ('', 8000)\n    httpd = http.server.HTTPServer(server_address, EmailHandler)\n    httpd.serve_forever()", "raw_solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\nfrom json import JSONDecodeError\n\nclass EmailHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        try:\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            email_data = json.loads(post_data)\n\n            if 'subject' not in email_data or 'message' not in email_data or 'to' not in email_data:\n                raise ValueError(\"Missing 'subject', 'message', or 'to' keys in the email data\")\n\n            msg = MIMEText(email_data['message'])\n            msg['Subject'] = email_data['subject']\n            msg['From'] = smtp_username\n            msg['To'] = email_data['to']\n\n            smtp = smtplib.SMTP(smtp_server, smtp_port)\n            smtp.login(smtp_username, smtp_password)\n            smtp.send_message(msg)\n            smtp.quit()\n\n            self.send_response(200)\n            self.end_headers()\n            self.wfile.write(b'Email sent successfully')\n\n        except JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Invalid JSON')\n\n        except ValueError as e:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(str(e).encode())\n\n        except smtplib.SMTPAuthenticationError:\n            self.send_response(535)\n            self.end_headers()\n            self.wfile.write(b'Authentication Failed')\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    server_address = ('', 8000)\n    httpd = http.server.HTTPServer(server_address, EmailHandler)\n    httpd.serve_forever()"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(directory):\n    \"\"\"\n    Counts the number of words in .txt files within a specified directory,\n    exports the counts to a JSON file, and then returns the total number of words.\n    \"\"\"\n    word_counts = Counter()\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                words = file.read().split()\n                word_counts.update(words)\n\n    # Export the counts to a JSON file\n    with open('word_counts.json', 'w') as file:\n        json.dump(word_counts, file)\n\n    # Return the total number of words\n    return sum(word_counts.values())", "raw_solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(directory):\n    \"\"\"\n    Counts the number of words in .txt files within a specified directory,\n    exports the counts to a JSON file, and then returns the total number of words.\n    \"\"\"\n    word_counts = Counter()\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                words = file.read().split()\n                word_counts.update(words)\n\n    # Export the counts to a JSON file\n    with open('word_counts.json', 'w') as file:\n        json.dump(word_counts, file)\n\n    # Return the total number of words\n    return sum(word_counts.values())"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally visualizes\n    the correlation matrix using a heatmap.\n    \"\"\"\n    if df.empty or 'Value' not in df.columns:\n        raise ValueError(\"Invalid DataFrame input. It should not be empty and contain 'Value' column.\")\n\n    # Split lists in the 'Value' column into separate columns\n    df[df['Value'].apply(lambda x: isinstance(x, list))][COLUMNS[1]] = df[df['Value'].apply(lambda x: isinstance(x, list))][COLUMNS[1]].apply(pd.Series)\n    df = df.dropna(subset=[COLUMNS[1]])\n\n    # Calculate Pearson correlation coefficient\n    corr_matrix = df[df.columns[1:]].corr()\n\n    if plot:\n        # Visualize the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n\n    return corr_matrix", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally visualizes\n    the correlation matrix using a heatmap.\n    \"\"\"\n    if df.empty or 'Value' not in df.columns:\n        raise ValueError(\"Invalid DataFrame input. It should not be empty and contain 'Value' column.\")\n\n    # Split lists in the 'Value' column into separate columns\n    df[df['Value'].apply(lambda x: isinstance(x, list))][COLUMNS[1]] = df[df['Value'].apply(lambda x: isinstance(x, list))][COLUMNS[1]].apply(pd.Series)\n    df = df.dropna(subset=[COLUMNS[1]])\n\n    # Calculate Pearson correlation coefficient\n    corr_matrix = df[df.columns[1:]].corr()\n\n    if plot:\n        # Visualize the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n\n    return corr_matrix"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n    # Generate random grades for each student in each subject\n    data = {field: [random.randint(0, 100) for _ in range(100)] for field in FIELDS}\n    data['Students'] = STUDENTS\n\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Calculate the average grade for each student\n    df['Average'] = df[FIELDS].mean(axis=1)\n\n    # Calculate the average grade for each subject\n    subject_averages = df[FIELDS].mean()\n\n    # Add the subject averages as a new row to the DataFrame\n    df = df.append(subject_averages, ignore_index=True)\n\n    # Rename the last row to 'Average Grade'\n    df.loc[len(df)-1, 'Students'] = 'Average Grade'\n\n    return df", "raw_solution": "import pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    # Generate random grades for each student in each subject\n    data = {field: [random.randint(0, 100) for _ in range(100)] for field in FIELDS}\n    data['Students'] = STUDENTS\n\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Calculate the average grade for each student\n    df['Average'] = df[FIELDS].mean(axis=1)\n\n    # Calculate the average grade for each subject\n    subject_averages = df[FIELDS].mean()\n\n    # Add the subject averages as a new row to the DataFrame\n    df = df.append(subject_averages, ignore_index=True)\n\n    # Rename the last row to 'Average Grade'\n    df.loc[len(df)-1, 'Students'] = 'Average Grade'\n\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Generate data\n    data = []\n    for i in range(PEOPLE_COUNT):\n        name = f'Person{i+1}'\n        age = random.randint(18, 80)\n        height = round(random.uniform(1.5, 2.2), 2)\n        weight = round(random.uniform(50, 200), 2)\n        data.append([name, age, height, weight])\n\n    # Calculate averages\n    avg_age = round(mean(row[1] for row in data), 2)\n    avg_height = round(mean(row[2] for row in data), 2)\n    avg_weight = round(mean(row[3] for row in data), 2)\n\n    # Append averages to data\n    data.append([f'Average', avg_age, avg_height, avg_weight])\n\n    # Write to CSV file\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    return filename", "raw_solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    # Generate data\n    data = []\n    for i in range(PEOPLE_COUNT):\n        name = f'Person{i+1}'\n        age = random.randint(18, 80)\n        height = round(random.uniform(1.5, 2.2), 2)\n        weight = round(random.uniform(50, 200), 2)\n        data.append([name, age, height, weight])\n\n    # Calculate averages\n    avg_age = round(mean(row[1] for row in data), 2)\n    avg_height = round(mean(row[2] for row in data), 2)\n    avg_weight = round(mean(row[3] for row in data), 2)\n\n    # Append averages to data\n    data.append([f'Average', avg_age, avg_height, avg_weight])\n\n    # Write to CSV file\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    return filename"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    \"\"\"\n    Organizes files in a directory based on the first text that is not enclosed in square brackets.\n    Moves the files to subdirectories named after this text. If no matching text is found, the file is not moved.\n    \"\"\"\n    # Initialize an empty dictionary to store the subdirectories and their corresponding files\n    subdirs = {}\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Use a regular expression to find the first text that is not enclosed in square brackets\n        match = re.search(r'\\[([^\\]]+)\\]', filename)\n\n        # If a match is found, create a subdirectory with the name of the match\n        if match:\n            subdir_name = match.group(1)\n            subdir_path = os.path.join(directory, subdir_name)\n\n            # If the subdirectory does not exist, create it\n            if not os.path.exists(subdir_path):\n                os.makedirs(subdir_path)\n\n            # Add the file to the list of files in the subdirectory\n            if subdir_name in subdirs:\n                subdirs[subdir_name].append(filename)\n            else:\n                subdirs[subdir_name] = [filename]\n\n            # Move the file to the subdirectory\n            shutil.move(os.path.join(directory, filename), subdir_path)\n\n    # Return the directory path and the dictionary of subdirectories and their files\n    return directory, subdirs", "raw_solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    \"\"\"\n    Organizes files in a directory based on the first text that is not enclosed in square brackets.\n    Moves the files to subdirectories named after this text. If no matching text is found, the file is not moved.\n    \"\"\"\n    # Initialize an empty dictionary to store the subdirectories and their corresponding files\n    subdirs = {}\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Use a regular expression to find the first text that is not enclosed in square brackets\n        match = re.search(r'\\[([^\\]]+)\\]', filename)\n\n        # If a match is found, create a subdirectory with the name of the match\n        if match:\n            subdir_name = match.group(1)\n            subdir_path = os.path.join(directory, subdir_name)\n\n            # If the subdirectory does not exist, create it\n            if not os.path.exists(subdir_path):\n                os.makedirs(subdir_path)\n\n            # Add the file to the list of files in the subdirectory\n            if subdir_name in subdirs:\n                subdirs[subdir_name].append(filename)\n            else:\n                subdirs[subdir_name] = [filename]\n\n            # Move the file to the subdirectory\n            shutil.move(os.path.join(directory, filename), subdir_path)\n\n    # Return the directory path and the dictionary of subdirectories and their files\n    return directory, subdirs"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    \"\"\"\n    Runs files from a list of files as subprocesses at the same time.\n    Returns a list of exit codes of the subprocesses.\n    \"\"\"\n    exit_codes = []\n    threads = []\n\n    def run_file(file):\n        \"\"\"\n        Runs a file as a subprocess and appends the exit code to the exit_codes list.\n        \"\"\"\n        exit_codes.append(subprocess.call([\"python\", file]))\n\n    # Start a new thread for each file\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes", "raw_solution": "import subprocess\nimport time\nimport threading\n\ndef task_func(file_list):\n    \"\"\"\n    Runs files from a list of files as subprocesses at the same time.\n    Returns a list of exit codes of the subprocesses.\n    \"\"\"\n    exit_codes = []\n    threads = []\n\n    def run_file(file):\n        \"\"\"\n        Runs a file as a subprocess and appends the exit code to the exit_codes list.\n        \"\"\"\n        exit_codes.append(subprocess.call([\"python\", file]))\n\n    # Start a new thread for each file\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    \"\"\"\n    Finds and runs all .bat files in a given directory, returning their file names and exit codes.\n    If a file cannot be executed, its exit code is None.\n    \"\"\"\n    # Get a list of all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n\n    # Initialize an empty list to store the file names and exit codes\n    results = []\n\n    # Iterate over each .bat file\n    for file in bat_files:\n        try:\n            # Run the file and capture its exit code\n            exit_code = subprocess.call([file], shell=True)\n        except Exception as e:\n            # If an error occurs, set the exit code to None\n            exit_code = None\n\n        # Append the file name and exit code to the results list\n        results.append((file, exit_code))\n\n    return results", "raw_solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    Finds and runs all .bat files in a given directory, returning their file names and exit codes.\n    If a file cannot be executed, its exit code is None.\n    \"\"\"\n    # Get a list of all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n\n    # Initialize an empty list to store the file names and exit codes\n    results = []\n\n    # Iterate over each .bat file\n    for file in bat_files:\n        try:\n            # Run the file and capture its exit code\n            exit_code = subprocess.call([file], shell=True)\n        except Exception as e:\n            # If an error occurs, set the exit code to None\n            exit_code = None\n\n        # Append the file name and exit code to the results list\n        results.append((file, exit_code))\n\n    return results"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"The input df must be DataFrame, not be empty\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the specified column: {col}\")\n\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n    if df[col].dtype in ['int64', 'float64']:\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.countplot(df[col], ax=axes[0])\n\n    sns.boxplot(df[col], ax=axes[1])\n\n    plt.tight_layout()\n    return fig", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"The input df must be DataFrame, not be empty\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the specified column: {col}\")\n\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n    if df[col].dtype in ['int64', 'float64']:\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.countplot(df[col], ax=axes[0])\n\n    sns.boxplot(df[col], ax=axes[1])\n\n    plt.tight_layout()\n    return fig"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Runs a Python script as a process with predefined arguments.\n    By default, waits for the process to complete. If wait is False, the function returns None.\n    Raises:\n        - ValueError: If the script does not exist.\n        - subprocess.CalledProcessError: If the script raises an exception.\n    Returns:\n        int: The return code of the subprocess. If 'wait' is False, returns None.\n    \"\"\"\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n\n    try:\n        if wait:\n            result = subprocess.run([sys.executable, script_path] + list(args), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            return result.returncode\n        else:\n            subprocess.Popen([sys.executable, script_path] + list(args))\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output)", "raw_solution": "import subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Runs a Python script as a process with predefined arguments.\n    By default, waits for the process to complete. If wait is False, the function returns None.\n    Raises:\n        - ValueError: If the script does not exist.\n        - subprocess.CalledProcessError: If the script raises an exception.\n    Returns:\n        int: The return code of the subprocess. If 'wait' is False, returns None.\n    \"\"\"\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n\n    try:\n        if wait:\n            result = subprocess.run([sys.executable, script_path] + list(args), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            return result.returncode\n        else:\n            subprocess.Popen([sys.executable, script_path] + list(args))\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output)"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(\"The specified file does not exist.\")\n    if not pd.ExcelFile(file_location).sheet_names.count(sheet_name):\n        raise ValueError(\"The specified sheet does not exist in the workbook.\")\n\n    # Load data from Excel\n    df = pd.read_excel(file_location, sheet_name=sheet_name)\n\n    # Calculate mean and standard deviation\n    stats_dict = {}\n    for col in df.columns:\n        stats_dict[col] = {'mean': df[col].mean(), 'std': df[col].std()}\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(len(stats_dict)), list(stats_dict.values()), tick_label=list(stats_dict.keys()))\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n\n    return stats_dict, fig", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(\"The specified file does not exist.\")\n    if not pd.ExcelFile(file_location).sheet_names.count(sheet_name):\n        raise ValueError(\"The specified sheet does not exist in the workbook.\")\n\n    # Load data from Excel\n    df = pd.read_excel(file_location, sheet_name=sheet_name)\n\n    # Calculate mean and standard deviation\n    stats_dict = {}\n    for col in df.columns:\n        stats_dict[col] = {'mean': df[col].mean(), 'std': df[col].std()}\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(len(stats_dict)), list(stats_dict.values()), tick_label=list(stats_dict.keys()))\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n\n    return stats_dict, fig"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"Activities must be datetime objects\")\n\n    day_of_week_counts = defaultdict(int)\n    for activity in activities:\n        day_of_week = activity.strftime(\"%A\")\n        day_of_week_counts[day_of_week] += 1\n\n    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [day_of_week_counts[day] for day in days_of_week]\n\n    plt.bar(days_of_week, counts)\n    plt.xlabel('Day of the Week')\n    plt.ylabel('Number of Activities')\n    plt.title('Weekly Activity')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return plt.gca()", "raw_solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"Activities must be datetime objects\")\n\n    day_of_week_counts = defaultdict(int)\n    for activity in activities:\n        day_of_week = activity.strftime(\"%A\")\n        day_of_week_counts[day_of_week] += 1\n\n    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [day_of_week_counts[day] for day in days_of_week]\n\n    plt.bar(days_of_week, counts)\n    plt.xlabel('Day of the Week')\n    plt.ylabel('Number of Activities')\n    plt.title('Weekly Activity')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n    The function returns the name of the file moved.\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n\n    # If there are no files in the source directory, return an empty string\n    if not files:\n        return \"\"\n\n    # Select a random file from the list of files\n    file_name = random.choice(files)\n\n    # Construct the full path to the file\n    src_path = os.path.join(src_dir, file_name)\n\n    # Move the file to the destination directory\n    shutil.move(src_path, dest_dir)\n\n    # Return the name of the file moved\n    return file_name", "raw_solution": "import os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n    The function returns the name of the file moved.\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n\n    # If there are no files in the source directory, return an empty string\n    if not files:\n        return \"\"\n\n    # Select a random file from the list of files\n    file_name = random.choice(files)\n\n    # Construct the full path to the file\n    src_path = os.path.join(src_dir, file_name)\n\n    # Move the file to the destination directory\n    shutil.move(src_path, dest_dir)\n\n    # Return the name of the file moved\n    return file_name"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    # Get all .xlsx files in the directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Initialize counter\n    count = 0\n\n    # Process each .xlsx file\n    for file in xlsx_files:\n        # Load the workbook\n        wb = load_workbook(filename=file)\n\n        # Iterate over each worksheet\n        for sheet in wb:\n            # Iterate over each cell\n            for row in sheet.iter_rows():\n                for cell in row:\n                    # If the cell contains a double quote, replace it with a double backslash\n                    if cell.value and '\"' in str(cell.value):\n                        cell.value = str(cell.value).replace('\"', '\\\\\"')\n\n        # Save the workbook\n        wb.save(file)\n\n        # Increment the counter\n        count += 1\n\n    return count", "raw_solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Get all .xlsx files in the directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Initialize counter\n    count = 0\n\n    # Process each .xlsx file\n    for file in xlsx_files:\n        # Load the workbook\n        wb = load_workbook(filename=file)\n\n        # Iterate over each worksheet\n        for sheet in wb:\n            # Iterate over each cell\n            for row in sheet.iter_rows():\n                for cell in row:\n                    # If the cell contains a double quote, replace it with a double backslash\n                    if cell.value and '\"' in str(cell.value):\n                        cell.value = str(cell.value).replace('\"', '\\\\\"')\n\n        # Save the workbook\n        wb.save(file)\n\n        # Increment the counter\n        count += 1\n\n    return count"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n\n    x = np.linspace(0, 1, sample_size, False)\n    y_sin = np.sin(2 * np.pi * frequency * x)\n    y_cos = np.cos(2 * np.pi * frequency * x)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y_sin, label='Sine Wave')\n    ax.plot(x, y_cos, label='Cosine Wave')\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency}')\n\n    return fig, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n\n    x = np.linspace(0, 1, sample_size, False)\n    y_sin = np.sin(2 * np.pi * frequency * x)\n    y_cos = np.cos(2 * np.pi * frequency * x)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y_sin, label='Sine Wave')\n    ax.plot(x, y_cos, label='Cosine Wave')\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency}')\n\n    return fig, ax"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables\n    app.config['MAIL_SERVER'] = os.environ.get('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.environ.get('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = os.environ.get('MAIL_USE_TLS', False) in ['True', 'true', '1']\n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD', None)\n\n    mail = Mail(app)\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']", "raw_solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables\n    app.config['MAIL_SERVER'] = os.environ.get('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.environ.get('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = os.environ.get('MAIL_USE_TLS', False) in ['True', 'true', '1']\n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD', None)\n\n    mail = Mail(app)\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculates the mean, median, and standard deviation of the data from a specific column in an Excel file.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.exists(os.path.join(excel_file_path, file_name)):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path.\")\n\n    # Read the Excel file\n    df = pd.read_excel(os.path.join(excel_file_path, file_name))\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file.\")\n\n    # Calculate the mean, median, and standard deviation\n    mean_val = np.mean(df[column_name])\n    median_val = np.median(df[column_name])\n    std_dev_val = np.std(df[column_name])\n\n    # Return the results in a dictionary\n    return {\n        'mean': mean_val,\n        'median': median_val,\n        'standard_deviation': std_dev_val\n    }", "raw_solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculates the mean, median, and standard deviation of the data from a specific column in an Excel file.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.exists(os.path.join(excel_file_path, file_name)):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path.\")\n\n    # Read the Excel file\n    df = pd.read_excel(os.path.join(excel_file_path, file_name))\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file.\")\n\n    # Calculate the mean, median, and standard deviation\n    mean_val = np.mean(df[column_name])\n    median_val = np.median(df[column_name])\n    std_dev_val = np.std(df[column_name])\n\n    # Return the results in a dictionary\n    return {\n        'mean': mean_val,\n        'median': median_val,\n        'standard_deviation': std_dev_val\n    }"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y, learning_rate=0.01):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct the Sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model\n    sgd = SGD(lr=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # Train the model\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plot the training and validation loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    return model, plt.gca()", "raw_solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y, learning_rate=0.01):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct the Sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model\n    sgd = SGD(lr=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # Train the model\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plot the training and validation loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_shape=(X_train.shape[1],), activation='sigmoid'),\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.01), metrics=['AUC'])\n\n    # Fit the model to the training data\n    history = model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities on the test set\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model, plt.gca()", "raw_solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_shape=(X_train.shape[1],), activation='sigmoid'),\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.01), metrics=['AUC'])\n\n    # Fit the model to the training data\n    history = model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities on the test set\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(\"Image file does not exist at the specified path.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' must be a positive integer.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Reshape the image to a 2D array of pixels and RGB components\n    image_array = np.reshape(image, (-1, 3))\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed).fit(image_array)\n\n    # Get the cluster labels for each pixel\n    labels = kmeans.labels_\n\n    # Replace each pixel's color with the centroid of the cluster it belongs to\n    segmented_array = kmeans.cluster_centers_[labels]\n\n    # Reshape the segmented array back to the original image shape\n    segmented_image = np.reshape(segmented_array, image.shape)\n\n    # Convert the image arrays to uint8 for OpenCV\n    image_array = image_array.astype(np.uint8)\n    segmented_image = segmented_image.astype(np.uint8)\n\n    return image_array, segmented_image", "raw_solution": "import cv2\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(\"Image file does not exist at the specified path.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' must be a positive integer.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Reshape the image to a 2D array of pixels and RGB components\n    image_array = np.reshape(image, (-1, 3))\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed).fit(image_array)\n\n    # Get the cluster labels for each pixel\n    labels = kmeans.labels_\n\n    # Replace each pixel's color with the centroid of the cluster it belongs to\n    segmented_array = kmeans.cluster_centers_[labels]\n\n    # Reshape the segmented array back to the original image shape\n    segmented_image = np.reshape(segmented_array, image.shape)\n\n    # Convert the image arrays to uint8 for OpenCV\n    image_array = image_array.astype(np.uint8)\n    segmented_image = segmented_image.astype(np.uint8)\n\n    return image_array, segmented_image"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculates the product of a matrix 'P' and a 3D tensor 'T', flattens the result, applies KMeans clustering to the flattened data, and visualizes it.\n    \"\"\"\n    # Calculate the product of P and T\n    product = np.matmul(P, T)\n\n    # Flatten the result\n    flattened = product.flatten()\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n\n    # Visualize the KMeans clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result)\n    ax.set_title('KMeans Clustering Visualization')\n\n    return cluster_result, ax", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculates the product of a matrix 'P' and a 3D tensor 'T', flattens the result, applies KMeans clustering to the flattened data, and visualizes it.\n    \"\"\"\n    # Calculate the product of P and T\n    product = np.matmul(P, T)\n\n    # Flatten the result\n    flattened = product.flatten()\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n\n    # Visualize the KMeans clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result)\n    ax.set_title('KMeans Clustering Visualization')\n\n    return cluster_result, ax"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculates the Voronoi diagram for a number of points in 2D and plots it.\n    Jittering is applied prior to plotting.\n\n    Args:\n    points (numpy.ndarray): A 2D array of shape (n, 2) representing the points.\n    seed (int): The seed for the random number generator.\n\n    Returns:\n    tuple: A tuple containing:\n        vor (Voronoi): A Voronoi object representing the Voronoi diagram of the points.\n        ax (Axes): The axes of the plotted Voronoi diagram.\n\n    Raises:\n    ValueError: If `points` is not a 2D numpy array.\n    \"\"\"\n    if not isinstance(points, np.ndarray) or points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"`points` must be a 2D numpy array\")\n\n    np.random.seed(seed)\n    jitter = np.random.uniform(-0.01, 0.01, points.shape)\n    points += jitter\n\n    vor = Voronoi(points)\n\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n\n    return vor, ax", "raw_solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculates the Voronoi diagram for a number of points in 2D and plots it.\n    Jittering is applied prior to plotting.\n\n    Args:\n    points (numpy.ndarray): A 2D array of shape (n, 2) representing the points.\n    seed (int): The seed for the random number generator.\n\n    Returns:\n    tuple: A tuple containing:\n        vor (Voronoi): A Voronoi object representing the Voronoi diagram of the points.\n        ax (Axes): The axes of the plotted Voronoi diagram.\n\n    Raises:\n    ValueError: If `points` is not a 2D numpy array.\n    \"\"\"\n    if not isinstance(points, np.ndarray) or points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"`points` must be a 2D numpy array\")\n\n    np.random.seed(seed)\n    jitter = np.random.uniform(-0.01, 0.01, points.shape)\n    points += jitter\n\n    vor = Voronoi(points)\n\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n\n    return vor, ax"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n    If a file with the same name already exists in the destination directory, it is not moved.\n    Raises FileNotFoundError if either the source or destination directory does not exist.\n    Returns a list of the full paths of files that were successfully moved.\n    \"\"\"\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory {src_dir} does not exist\")\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"Destination directory {dest_dir} does not exist\")\n\n    moved_files = []\n    for file in glob.glob(os.path.join(src_dir, f\"*.{ext}\")):\n        filename = os.path.basename(file)\n        dest_file = os.path.join(dest_dir, filename)\n        if not os.path.exists(dest_file):\n            shutil.move(file, dest_dir)\n            moved_files.append(dest_file)\n\n    return moved_files", "raw_solution": "import os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n    If a file with the same name already exists in the destination directory, it is not moved.\n    Raises FileNotFoundError if either the source or destination directory does not exist.\n    Returns a list of the full paths of files that were successfully moved.\n    \"\"\"\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory {src_dir} does not exist\")\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"Destination directory {dest_dir} does not exist\")\n\n    moved_files = []\n    for file in glob.glob(os.path.join(src_dir, f\"*.{ext}\")):\n        filename = os.path.basename(file)\n        dest_file = os.path.join(dest_dir, filename)\n        if not os.path.exists(dest_file):\n            shutil.move(file, dest_dir)\n            moved_files.append(dest_file)\n\n    return moved_files"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    if not json_str:\n        return pd.DataFrame()\n\n    data_dict = json.loads(json_str)\n\n    def normalize_value(value):\n        if isinstance(value, list):\n            return [float(i)*2 if isinstance(i, (int, float)) else float(i)*2 if re.match(r'^-?\\d+(?:\\.\\d+)?$', i) else i for i in value]\n        elif isinstance(value, str):\n            return float(value)*2 if re.match(r'^-?\\d+(?:\\.\\d+)?$', value) else value\n        elif isinstance(value, (int, float)):\n            return float(value)*2\n        else:\n            return value\n\n    data_dict = {key: normalize_value(value) for key, value in data_dict.items()}\n\n    return pd.DataFrame(data_dict)", "raw_solution": "import json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    if not json_str:\n        return pd.DataFrame()\n\n    data_dict = json.loads(json_str)\n\n    def normalize_value(value):\n        if isinstance(value, list):\n            return [float(i)*2 if isinstance(i, (int, float)) else float(i)*2 if re.match(r'^-?\\d+(?:\\.\\d+)?$', i) else i for i in value]\n        elif isinstance(value, str):\n            return float(value)*2 if re.match(r'^-?\\d+(?:\\.\\d+)?$', value) else value\n        elif isinstance(value, (int, float)):\n            return float(value)*2\n        else:\n            return value\n\n    data_dict = {key: normalize_value(value) for key, value in data_dict.items()}\n\n    return pd.DataFrame(data_dict)"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n\n    start_time = time.time()\n    process = subprocess.Popen(['python', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    cpu_usage = 0\n    memory_usage = 0\n\n    while True:\n        if time.time() - start_time > timeout:\n            process.terminate()\n            break\n\n        if psutil.pid_exists(process.pid):\n            process_info = psutil.Process(process.pid)\n            cpu_usage += process_info.cpu_percent(interval=1)\n            memory_usage += process_info.memory_info().rss\n        else:\n            break\n\n    return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}", "raw_solution": "import subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n\n    start_time = time.time()\n    process = subprocess.Popen(['python', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    cpu_usage = 0\n    memory_usage = 0\n\n    while True:\n        if time.time() - start_time > timeout:\n            process.terminate()\n            break\n\n        if psutil.pid_exists(process.pid):\n            process_info = psutil.Process(process.pid)\n            cpu_usage += process_info.cpu_percent(interval=1)\n            memory_usage += process_info.memory_info().rss\n        else:\n            break\n\n    return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n\n    # Generate DataFrame\n    df = pd.DataFrame({\n        \"x\": np.random.rand(N),\n        \"y\": np.random.rand(N),\n    })\n\n    # Generate categories\n    if N >= len(CATEGORIES):\n        df[\"category\"] = np.random.choice(CATEGORIES, N)\n    else:\n        df[\"category\"] = np.random.choice(CATEGORIES, N, replace=False)\n\n    # Scatter plot\n    fig, ax = plt.subplots()\n    for category in CATEGORIES:\n        df_category = df[df[\"category\"] == category]\n        ax.scatter(df_category[\"x\"], df_category[\"y\"], label=category)\n    ax.legend()\n\n    return (df, ax)", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n\n    # Generate DataFrame\n    df = pd.DataFrame({\n        \"x\": np.random.rand(N),\n        \"y\": np.random.rand(N),\n    })\n\n    # Generate categories\n    if N >= len(CATEGORIES):\n        df[\"category\"] = np.random.choice(CATEGORIES, N)\n    else:\n        df[\"category\"] = np.random.choice(CATEGORIES, N, replace=False)\n\n    # Scatter plot\n    fig, ax = plt.subplots()\n    for category in CATEGORIES:\n        df_category = df[df[\"category\"] == category]\n        ax.scatter(df_category[\"x\"], df_category[\"y\"], label=category)\n    ax.legend()\n\n    return (df, ax)"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    np.random.seed(seed)\n\n    # Generate timestamps\n    start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n    end_time = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n    time_range = pd.date_range(start=start_time, end=end_time, freq=f'{step}S')\n\n    # Generate values from normal distribution\n    values = np.random.normal(0, 1, len(time_range))\n\n    # Add trend\n    time_range_seconds = (time_range - time_range[0]).total_seconds()\n    values += trend * time_range_seconds\n\n    # Create DataFrame\n    df = pd.DataFrame({'Time': time_range, 'Value': values})\n\n    # Plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Time'], df['Value'])\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n\n    return ax", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    np.random.seed(seed)\n\n    # Generate timestamps\n    start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n    end_time = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n    time_range = pd.date_range(start=start_time, end=end_time, freq=f'{step}S')\n\n    # Generate values from normal distribution\n    values = np.random.normal(0, 1, len(time_range))\n\n    # Add trend\n    time_range_seconds = (time_range - time_range[0]).total_seconds()\n    values += trend * time_range_seconds\n\n    # Create DataFrame\n    df = pd.DataFrame({'Time': time_range, 'Value': values})\n\n    # Plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Time'], df['Value'])\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n\n    return ax"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or not isinstance(random_seed, int) or not isinstance(products, list):\n        raise ValueError(\"Invalid input type\")\n    if not 0 <= random_seed <= 1000000:\n        raise ValueError(\"Invalid random seed value\")\n    if not 1000000000000 <= epoch_milliseconds <= int(time.time() * 1000):\n        raise ValueError(\"Invalid epoch milliseconds value\")\n    if not 5 <= len(products) <= 50:\n        raise ValueError(\"Invalid number of products\")\n\n    # Set random seed\n    random.seed(random_seed)\n\n    # Convert epoch time to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n\n    # Generate sales data\n    data = []\n    for day_num in range((end_date - start_date).days + 1):\n        day = start_date + timedelta(days=day_num)\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append([product, day, sales])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Product\", \"Date\", \"Sales\"])\n\n    return df", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or not isinstance(random_seed, int) or not isinstance(products, list):\n        raise ValueError(\"Invalid input type\")\n    if not 0 <= random_seed <= 1000000:\n        raise ValueError(\"Invalid random seed value\")\n    if not 1000000000000 <= epoch_milliseconds <= int(time.time() * 1000):\n        raise ValueError(\"Invalid epoch milliseconds value\")\n    if not 5 <= len(products) <= 50:\n        raise ValueError(\"Invalid number of products\")\n\n    # Set random seed\n    random.seed(random_seed)\n\n    # Convert epoch time to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n\n    # Generate sales data\n    data = []\n    for day_num in range((end_date - start_date).days + 1):\n        day = start_date + timedelta(days=day_num)\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append([product, day, sales])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Product\", \"Date\", \"Sales\"])\n\n    return df"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Converts a JSON string to an Excel file.\n    If the JSON string represents an empty array, the function creates an Excel file with no data rows.\n    \"\"\"\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"json_str is not valid JSON\")\n\n    if not isinstance(data, list):\n        raise ValueError(\"json_str must represent a JSON array\")\n\n    df = pd.DataFrame(data)\n\n    try:\n        writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n        df.to_excel(writer, sheet_name=sheet_name, index=False)\n        writer.save()\n    except Exception as e:\n        raise Exception(\"Error writing to file: {}\".format(e))\n\n    return os.path.abspath(filename)", "raw_solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Converts a JSON string to an Excel file.\n    If the JSON string represents an empty array, the function creates an Excel file with no data rows.\n    \"\"\"\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"json_str is not valid JSON\")\n\n    if not isinstance(data, list):\n        raise ValueError(\"json_str must represent a JSON array\")\n\n    df = pd.DataFrame(data)\n\n    try:\n        writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n        df.to_excel(writer, sheet_name=sheet_name, index=False)\n        writer.save()\n    except Exception as e:\n        raise Exception(\"Error writing to file: {}\".format(e))\n\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_in_past)]\n\n    data = []\n    for date in dates:\n        for activity in activities:\n            data.append([date, activity, random.randint(0, 120)])\n\n    df = pd.DataFrame(data, columns=['Date', 'Activity', 'Duration'])\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', ax=ax)\n    ax.set_title('Daily Activity Durations')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Duration')\n\n    return (ax, df)", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_in_past)]\n\n    data = []\n    for date in dates:\n        for activity in activities:\n            data.append([date, activity, random.randint(0, 120)])\n\n    df = pd.DataFrame(data, columns=['Date', 'Activity', 'Duration'])\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', ax=ax)\n    ax.set_title('Daily Activity Durations')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Duration')\n\n    return (ax, df)"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    np.random.seed(random_seed)\n    dates = pd.date_range(start=datetime.now() - timedelta(days=days_in_past), periods=days_in_past).tolist()\n    prices = np.random.rand(days_in_past, len(stock_names)).tolist()\n    df = pd.DataFrame(prices, index=dates, columns=stock_names)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    np.random.seed(random_seed)\n    dates = pd.date_range(start=datetime.now() - timedelta(days=days_in_past), periods=days_in_past).tolist()\n    prices = np.random.rand(days_in_past, len(stock_names)).tolist()\n    df = pd.DataFrame(prices, index=dates, columns=stock_names)\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r') as file1, open(file_path2, 'r') as file2:\n            lines1 = file1.readlines()\n            lines2 = file2.readlines()\n\n        if not lines1 or not lines2:\n            raise ValueError(\"One or both of the files are empty.\")\n\n        diff_lines = list(ndiff(lines1, lines2))\n\n        diff_data = []\n        for i, line in enumerate(diff_lines, start=1):\n            status = line[0]\n            content = line[2:]\n            diff_data.append([i, status, content])\n\n        df = pd.DataFrame(diff_data, columns=['Line Number', 'Status', 'Content'])\n\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files cannot be found.\")\n    except Exception as e:\n        raise Exception(\"An error occurred: {}\".format(e))", "raw_solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r') as file1, open(file_path2, 'r') as file2:\n            lines1 = file1.readlines()\n            lines2 = file2.readlines()\n\n        if not lines1 or not lines2:\n            raise ValueError(\"One or both of the files are empty.\")\n\n        diff_lines = list(ndiff(lines1, lines2))\n\n        diff_data = []\n        for i, line in enumerate(diff_lines, start=1):\n            status = line[0]\n            content = line[2:]\n            diff_data.append([i, status, content])\n\n        df = pd.DataFrame(diff_data, columns=['Line Number', 'Status', 'Content'])\n\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files cannot be found.\")\n    except Exception as e:\n        raise Exception(\"An error occurred: {}\".format(e))"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if data.empty:\n        stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n        fig, ax = plt.subplots()\n        ax.pie([], labels=[])\n        return stats, ax\n\n    stats = {\n        'sum': data[column].sum(),\n        'mean': data[column].mean(),\n        'min': data[column].min(),\n        'max': data[column].max()\n    }\n\n    fig, ax = plt.subplots()\n    ax.pie(data[column].value_counts(), labels=data[column].value_counts().index)\n    ax.set_title(f'{column} distribution')\n\n    return stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if data.empty:\n        stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n        fig, ax = plt.subplots()\n        ax.pie([], labels=[])\n        return stats, ax\n\n    stats = {\n        'sum': data[column].sum(),\n        'mean': data[column].mean(),\n        'min': data[column].min(),\n        'max': data[column].max()\n    }\n\n    fig, ax = plt.subplots()\n    ax.pie(data[column].value_counts(), labels=data[column].value_counts().index)\n    ax.set_title(f'{column} distribution')\n\n    return stats, ax"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not data:\n        raise ValueError(\"Data list is empty\")\n\n    df = pd.DataFrame(data)\n\n    if column not in df.columns:\n        raise KeyError(f\"Specified column '{column}' is not valid\")\n\n    if df[column].min() < 0:\n        raise ValueError(f\"Negative value found in column '{column}'\")\n\n    summary_stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max(),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column])\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    ax.set_title(f'Line Chart of {column}')\n\n    return (summary_stats, ax)", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if not data:\n        raise ValueError(\"Data list is empty\")\n\n    df = pd.DataFrame(data)\n\n    if column not in df.columns:\n        raise KeyError(f\"Specified column '{column}' is not valid\")\n\n    if df[column].min() < 0:\n        raise ValueError(f\"Negative value found in column '{column}'\")\n\n    summary_stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max(),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column])\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    ax.set_title(f'Line Chart of {column}')\n\n    return (summary_stats, ax)"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom typing import List, Dict, Any\ndef task_func(input_file: str = \"data.json\") -> pd.DataFrame:\n    \"\"\"\n    Reads a list of dictionaries from a JSON file, calculates the mean and median for each key (ignoring non-numeric or missing values),\n    and converts the results into a Pandas DataFrame.\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n\n    # Initialize defaultdict to store mean and median values\n    stats = defaultdict(list)\n\n    # Iterate over each dictionary in the list\n    for item in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in item.items():\n            # Ignore non-numeric or missing values\n            if isinstance(value, (int, float)):\n                stats[key].append(value)\n\n    # Calculate mean and median for each key\n    for key, values in stats.items():\n        stats[key] = [np.mean(values), np.median(values)]\n\n    # Convert the results into a Pandas DataFrame\n    df = pd.DataFrame.from_dict(stats, orient='index', columns=['mean', 'median'])\n    df.index.name = 'variable'\n\n    # Sort the DataFrame by the variable names\n    df = df.sort_index()\n\n    return df", "raw_solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom typing import List, Dict, Any\n\ndef task_func(input_file: str = \"data.json\") -> pd.DataFrame:\n    \"\"\"\n    Reads a list of dictionaries from a JSON file, calculates the mean and median for each key (ignoring non-numeric or missing values),\n    and converts the results into a Pandas DataFrame.\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n\n    # Initialize defaultdict to store mean and median values\n    stats = defaultdict(list)\n\n    # Iterate over each dictionary in the list\n    for item in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in item.items():\n            # Ignore non-numeric or missing values\n            if isinstance(value, (int, float)):\n                stats[key].append(value)\n\n    # Calculate mean and median for each key\n    for key, values in stats.items():\n        stats[key] = [np.mean(values), np.median(values)]\n\n    # Convert the results into a Pandas DataFrame\n    df = pd.DataFrame.from_dict(stats, orient='index', columns=['mean', 'median'])\n    df.index.name = 'variable'\n\n    # Sort the DataFrame by the variable names\n    df = df.sort_index()\n\n    return df"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError('File must be a CSV file')\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    duplicates = [row for row in data if data.count(row) > 1]\n    duplicate_counts = Counter(tuple(row) for row in duplicates)\n\n    # Convert duplicated rows into a pandas DataFrame\n    df = pd.DataFrame(list(duplicate_counts.items()), columns=['Row', 'Count'])\n    df['Row'] = df['Row'].apply(lambda x: list(x))\n\n    # Plot using matplotlib\n    fig, ax = plt.subplots()\n    df['Count'].plot(kind='bar', ax=ax)\n    ax.set_xlabel('Duplicate Row')\n    ax.set_ylabel('Count')\n    ax.set_title('Count of Duplicate Rows')\n\n    return duplicate_counts, ax", "raw_solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError('File must be a CSV file')\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    duplicates = [row for row in data if data.count(row) > 1]\n    duplicate_counts = Counter(tuple(row) for row in duplicates)\n\n    # Convert duplicated rows into a pandas DataFrame\n    df = pd.DataFrame(list(duplicate_counts.items()), columns=['Row', 'Count'])\n    df['Row'] = df['Row'].apply(lambda x: list(x))\n\n    # Plot using matplotlib\n    fig, ax = plt.subplots()\n    df['Count'].plot(kind='bar', ax=ax)\n    ax.set_xlabel('Duplicate Row')\n    ax.set_ylabel('Count')\n    ax.set_title('Count of Duplicate Rows')\n\n    return duplicate_counts, ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age cannot be negative\")\n\n    df['age'] = df['age'].apply(np.floor)\n    df['age'] = df['age'].astype(int)\n\n    duplicate_names = df[df.duplicated(subset='name', keep=False)]\n    if duplicate_names.empty:\n        return Counter(), None\n\n    age_distribution = duplicate_names['age'].value_counts()\n    min_age, max_age = duplicate_names['age'].min(), duplicate_names['age'].max()\n\n    plt.figure(figsize=(10, 6))\n    sns.histplot(duplicate_names['age'], bins=range(min_age, max_age + 2), color='skyblue')\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Age Distribution for Duplicate Names')\n    plt.grid(True)\n\n    return age_distribution, plt.gca()", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age cannot be negative\")\n\n    df['age'] = df['age'].apply(np.floor)\n    df['age'] = df['age'].astype(int)\n\n    duplicate_names = df[df.duplicated(subset='name', keep=False)]\n    if duplicate_names.empty:\n        return Counter(), None\n\n    age_distribution = duplicate_names['age'].value_counts()\n    min_age, max_age = duplicate_names['age'].min(), duplicate_names['age'].max()\n\n    plt.figure(figsize=(10, 6))\n    sns.histplot(duplicate_names['age'], bins=range(min_age, max_age + 2), color='skyblue')\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Age Distribution for Duplicate Names')\n    plt.grid(True)\n\n    return age_distribution, plt.gca()"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    # Identify and count duplicate values in the 'value' column\n    counts = Counter(df['value'])\n\n    # Plot a histogram for all values in the 'value' column\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, color='green', alpha=0.6, edgecolor='black')\n\n    # Overlay a normal distribution curve on the histogram\n    mu, sigma = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set plot title and labels\n    ax.set_title(\"Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return counts, ax", "raw_solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=4):\n    # Identify and count duplicate values in the 'value' column\n    counts = Counter(df['value'])\n\n    # Plot a histogram for all values in the 'value' column\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, color='green', alpha=0.6, edgecolor='black')\n\n    # Overlay a normal distribution curve on the histogram\n    mu, sigma = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set plot title and labels\n    ax.set_title(\"Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return counts, ax"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b', and plots it as a bar chart.\n    'a' sets the DataFrame's row indices, while the length of 'b' determines the number of columns.\n    The function returns the Axes object of the plotted bar chart.\n    \"\"\"\n    # Generate random values\n    np.random.seed(0)\n    data = np.random.randint(0, 100, size=(len(a), len(b)))\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:len(b)])\n\n    # Plot DataFrame as a bar chart\n    ax = df.plot(kind='bar')\n\n    return ax\na = ['Row1', 'Row2', 'Row3']\nb = [1, 2, 3, 4, 5]", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b', and plots it as a bar chart.\n    'a' sets the DataFrame's row indices, while the length of 'b' determines the number of columns.\n    The function returns the Axes object of the plotted bar chart.\n    \"\"\"\n    # Generate random values\n    np.random.seed(0)\n    data = np.random.randint(0, 100, size=(len(a), len(b)))\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:len(b)])\n\n    # Plot DataFrame as a bar chart\n    ax = df.plot(kind='bar')\n\n    return ax\n\n# Test the function\na = ['Row1', 'Row2', 'Row3']\nb = [1, 2, 3, 4, 5]\nax = task_func(a, b)\nplt.show()"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): A DataFrame with 'date' and 'value' columns. 'date' should be in 'yyyy-mm-dd' format.\n\n    Returns:\n    Axes object: A matplotlib.axes.Axes object representing the plot.\n    \"\"\"\n    # Convert 'date' column to datetime\n    data['date'] = pd.to_datetime(data['date'])\n\n    # Extract year and month from 'date'\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n\n    # Group by 'year' and 'month', and calculate the mean 'value'\n    grouped_data = data.groupby(['year', 'month']).mean().reset_index()\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(grouped_data['month'], grouped_data['value'])\n\n    # Set title and labels\n    ax.set_title(f\"Monthly Data for {grouped_data['year'].iloc[0]}\")\n    ax.set_xlabel(\"Month\")\n    ax.set_ylabel(\"Value\")\n\n    return ax", "raw_solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): A DataFrame with 'date' and 'value' columns. 'date' should be in 'yyyy-mm-dd' format.\n\n    Returns:\n    Axes object: A matplotlib.axes.Axes object representing the plot.\n    \"\"\"\n    # Convert 'date' column to datetime\n    data['date'] = pd.to_datetime(data['date'])\n\n    # Extract year and month from 'date'\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n\n    # Group by 'year' and 'month', and calculate the mean 'value'\n    grouped_data = data.groupby(['year', 'month']).mean().reset_index()\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(grouped_data['month'], grouped_data['value'])\n\n    # Set title and labels\n    ax.set_title(f\"Monthly Data for {grouped_data['year'].iloc[0]}\")\n    ax.set_xlabel(\"Month\")\n    ax.set_ylabel(\"Value\")\n\n    return ax"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    This function draws a histogram to visualize the frequency distribution of numeric values provided in a string format.\n    The function takes a string of numeric values separated by commas as input, converts it to a numpy array, and then uses\n    matplotlib to create a histogram. The histogram uses bins calculated as `np.arange(data.min(), data.max()+2) - 0.5`.\n    The function returns the Axes object of the created histogram.\n    \"\"\"\n    # Convert the string of numeric values to a numpy array\n    data = np.array([float(x) for x in data.split(',')])\n\n    # Calculate the bins\n    bins = np.arange(data.min(), data.max()+2) - 0.5\n\n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, edgecolor='black')\n\n    # Set the title and labels\n    ax.set_title('Histogram of Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    This function draws a histogram to visualize the frequency distribution of numeric values provided in a string format.\n    The function takes a string of numeric values separated by commas as input, converts it to a numpy array, and then uses\n    matplotlib to create a histogram. The histogram uses bins calculated as `np.arange(data.min(), data.max()+2) - 0.5`.\n    The function returns the Axes object of the created histogram.\n    \"\"\"\n    # Convert the string of numeric values to a numpy array\n    data = np.array([float(x) for x in data.split(',')])\n\n    # Calculate the bins\n    bins = np.arange(data.min(), data.max()+2) - 0.5\n\n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, edgecolor='black')\n\n    # Set the title and labels\n    ax.set_title('Histogram of Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    def sine_func(x, a, b, c):\n        return a * np.sin(b * x) + c\n\n    x = np.linspace(0, 1, array_length)\n    y = sine_func(x, 1, 2 * np.pi, 0) + np.random.normal(0, noise_level, array_length)\n\n    popt, _ = curve_fit(sine_func, x, y)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y, 'b-', label='Noisy Sine Wave')\n    plt.plot(x, sine_func(x, *popt), 'r-', label='Adjusted Curve')\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Sine Wave with Noise and Adjusted Curve')\n    return plt.gca()", "raw_solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    def sine_func(x, a, b, c):\n        return a * np.sin(b * x) + c\n\n    x = np.linspace(0, 1, array_length)\n    y = sine_func(x, 1, 2 * np.pi, 0) + np.random.normal(0, noise_level, array_length)\n\n    popt, _ = curve_fit(sine_func, x, y)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y, 'b-', label='Noisy Sine Wave')\n    plt.plot(x, sine_func(x, *popt), 'r-', label='Adjusted Curve')\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Sine Wave with Noise and Adjusted Curve')\n    return plt.gca()"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            data = [row[0] for row in reader]\n    except FileNotFoundError:\n        raise FileNotFoundError(\"File not found at the specified path.\")\n    except IOError:\n        raise IOError(\"There was an error in reading the file.\")\n\n    # Normalize the text to ASCII\n    data = [unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') for text in data]\n\n    # Split the text into words\n    words = ' '.join(data).split()\n\n    # Count the words\n    word_counts = Counter(words)\n\n    # Get the 10 most common words\n    common_words = word_counts.most_common(10)\n\n    # Create a bar plot\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('10 Most Common Words')\n\n    return (ax, common_words)", "raw_solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            data = [row[0] for row in reader]\n    except FileNotFoundError:\n        raise FileNotFoundError(\"File not found at the specified path.\")\n    except IOError:\n        raise IOError(\"There was an error in reading the file.\")\n\n    # Normalize the text to ASCII\n    data = [unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') for text in data]\n\n    # Split the text into words\n    words = ' '.join(data).split()\n\n    # Count the words\n    word_counts = Counter(words)\n\n    # Get the 10 most common words\n    common_words = word_counts.most_common(10)\n\n    # Create a bar plot\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('10 Most Common Words')\n\n    return (ax, common_words)"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(size=size)\n\n    # Create a figure\n    fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n\n    # Plot histogram\n    ax[0].hist(data, bins=30, density=True)\n    ax[0].set_title('Histogram')\n\n    # Plot PDF\n    x = np.linspace(-4, 4, 1000)\n    pdf = stats.norm.pdf(x)\n    ax[1].plot(x, pdf)\n    ax[1].set_title('Probability Density Function')\n\n    return fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(size=size)\n\n    # Create a figure\n    fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n\n    # Plot histogram\n    ax[0].hist(data, bins=30, density=True)\n    ax[0].set_title('Histogram')\n\n    # Plot PDF\n    x = np.linspace(-4, 4, 1000)\n    pdf = stats.norm.pdf(x)\n    ax[1].plot(x, pdf)\n    ax[1].set_title('Probability Density Function')\n\n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\nfrom Crypto.Util.Padding import pad, unpad", "raw_solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\nfrom Crypto.Util.Padding import pad, unpad\n\ndef generate_rsa_keys_and_encrypt_private_key(password: bytes) -> (rsa.PublicKey, str, bytes, bytes):\n    \"\"\"\n    Generates an RSA public and private key pair and saves the private key in a file after encrypting it with a password using AES encryption.\n    Returns the public key and the filename of the encrypted private key, along with encryption details for testing.\n    \"\"\"\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Convert private key to bytes\n    privkey_bytes = rsa.key.exportKey('PEM')\n\n    # Generate random 8 bytes for filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Encrypt private key with AES\n    cipher = AES.new(password, AES.MODE_EAX)\n    ciphertext, tag = cipher.encrypt_and_digest(privkey_bytes)\n\n    # Save encrypted private key to file\n    with open(filename, 'wb') as file:\n        [file.write(x) for x in (cipher.nonce, ciphertext, tag)]\n\n    return pubkey, filename, cipher.nonce, tag\n\ndef test_generate_rsa_keys_and_encrypt_private_key():\n    password = b'password'\n    pubkey, filename, nonce, tag = generate_rsa_keys_and_encrypt_private_key(password)\n\n    # Test if the file exists\n    assert os.path.isfile(filename)\n\n    # Test if the file contains the correct data\n    with open(filename, 'rb') as file:\n        nonce, ciphertext, tag = [file.read(x) for x in (16, -1, 16)]\n        cipher = AES.new(password, AES.MODE_EAX, nonce=nonce)\n        privkey_bytes = unpad(cipher.decrypt(ciphertext), AES.block_size)\n        assert privkey_bytes == rsa.key.exportKey('PEM')\n\n    # Test if the function returns the correct values\n    assert isinstance(pubkey, rsa.key.PublicKey)\n    assert isinstance(filename, str)\n    assert isinstance(nonce, bytes)\n    assert isinstance(tag, bytes)\n\n    # Clean up\n    os.remove(filename)\n\ntest_generate_rsa_keys_and_encrypt_private_key()"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Read file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # AES encryption\n    aes_key = os.urandom(32)  # 256-bit key\n    cipher = Cipher(algorithms.AES(aes_key), modes.ECB())\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    padded_content = padder.update(file_content) + padder.finalize()\n    encrypted_file_content = encryptor.update(padded_content) + encryptor.finalize()\n\n    # Save encrypted file\n    encrypted_file_path = 'encrypted_file.bin'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_file_content)\n\n    # Encrypt AES key with RSA\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save encrypted AES key\n    encrypted_aes_key_path = 'encrypted_aes_key.bin'\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pubkey, encrypted_file_path, encrypted_aes_key_path", "raw_solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Read file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # AES encryption\n    aes_key = os.urandom(32)  # 256-bit key\n    cipher = Cipher(algorithms.AES(aes_key), modes.ECB())\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    padded_content = padder.update(file_content) + padder.finalize()\n    encrypted_file_content = encryptor.update(padded_content) + encryptor.finalize()\n\n    # Save encrypted file\n    encrypted_file_path = 'encrypted_file.bin'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_file_content)\n\n    # Encrypt AES key with RSA\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save encrypted AES key\n    encrypted_aes_key_path = 'encrypted_aes_key.bin'\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pubkey, encrypted_file_path, encrypted_aes_key_path"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL is invalid or empty\")\n\n    try:\n        response = urllib.request.urlopen(url)\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"Network connectivity or server issue\") from e\n\n    html = response.read().decode('utf-8')\n    doc = pq(html)\n\n    data = []\n    for tag in doc('a'):\n        text = tag.text\n        href = tag.attrib['href']\n        data.append({'text': text, 'href': href})\n\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    return df", "raw_solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL is invalid or empty\")\n\n    try:\n        response = urllib.request.urlopen(url)\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"Network connectivity or server issue\") from e\n\n    html = response.read().decode('utf-8')\n    doc = pq(html)\n\n    data = []\n    for tag in doc('a'):\n        text = tag.text\n        href = tag.attrib['href']\n        data.append({'text': text, 'href': href})\n\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    return df"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates sensor data for the specified number of hours and saves it in a CSV file.\n    The function outputs with:\n        hours (int): Number of hours to generate data for.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate data\n    data = []\n    current_time = datetime.now()\n    for _ in range(hours):\n        row = {}\n        row['Time'] = current_time.strftime('%Y-%m-%d %H:%M:%S')\n        for sensor in SENSORS:\n            row[sensor] = randint(0, 100)\n        data.append(row)\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV file\n    file_path = os.path.join(output_dir, 'sensor_data.csv')\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Temperature', 'Humidity', 'Pressure'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    return file_path", "raw_solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates sensor data for the specified number of hours and saves it in a CSV file.\n    The function outputs with:\n        hours (int): Number of hours to generate data for.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate data\n    data = []\n    current_time = datetime.now()\n    for _ in range(hours):\n        row = {}\n        row['Time'] = current_time.strftime('%Y-%m-%d %H:%M:%S')\n        for sensor in SENSORS:\n            row[sensor] = randint(0, 100)\n        data.append(row)\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV file\n    file_path = os.path.join(output_dir, 'sensor_data.csv')\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Temperature', 'Humidity', 'Pressure'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    return file_path"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate traffic data\n    data = []\n    for i in range(hours):\n        row = {'Time': datetime.now() + timedelta(hours=i)}\n        for vehicle in VEHICLE_TYPES:\n            row[vehicle] = randint(0, 100)\n        data.append(row)\n\n    # Save data to CSV file\n    csv_file = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Car', 'Bus', 'Truck', 'Bike'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    # Load data from CSV file\n    df = pd.read_csv(csv_file)\n    df['Time'] = pd.to_datetime(df['Time'])\n\n    # Plot data\n    fig, ax = plt.subplots()\n    for vehicle in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle], label=vehicle)\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n\n    return csv_file, ax", "raw_solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate traffic data\n    data = []\n    for i in range(hours):\n        row = {'Time': datetime.now() + timedelta(hours=i)}\n        for vehicle in VEHICLE_TYPES:\n            row[vehicle] = randint(0, 100)\n        data.append(row)\n\n    # Save data to CSV file\n    csv_file = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Car', 'Bus', 'Truck', 'Bike'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    # Load data from CSV file\n    df = pd.read_csv(csv_file)\n    df['Time'] = pd.to_datetime(df['Time'])\n\n    # Plot data\n    fig, ax = plt.subplots()\n    for vehicle in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle], label=vehicle)\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n\n    return csv_file, ax"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate weather data\n    weather_data = []\n    current_time = datetime.now()\n    for i in range(hours):\n        time = (current_time + timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        weather_data.append([time, condition])\n\n    # Save weather data to CSV file\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file to a backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    shutil.copy2(csv_file_path, backup_dir)\n\n    return csv_file_path", "raw_solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate weather data\n    weather_data = []\n    current_time = datetime.now()\n    for i in range(hours):\n        time = (current_time + timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        weather_data.append([time, condition])\n\n    # Save weather data to CSV file\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file to a backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    shutil.copy2(csv_file_path, backup_dir)\n\n    return csv_file_path"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    \"\"\"\n    Generates a DataFrame of football match results for multiple teams, with random goals and penalties.\n    The function also visualizes the goals and penalty costs using seaborn.\n\n    Args:\n    goals (int): The number of goals per team.\n    penalties (int): The number of penalties per team.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing match results.\n    list: A list containing two seaborn plot objects (Axes) for goals and penalty costs.\n    \"\"\"\n    # Generate random goals and penalties\n    data = {\n        'Team': TEAMS,\n        'Goals': [randint(0, goals) for _ in range(len(TEAMS))],\n        'Penalty Cost': [randint(0, penalties) * PENALTY_COST for _ in range(len(TEAMS))]\n    }\n    df = pd.DataFrame(data)\n\n    # Create plots\n    fig, axes = plt.subplots(2, 1, figsize=(8, 10))\n\n    # Plot goals\n    sns.barplot(x='Goals', y='Team', data=df, ax=axes[0])\n    axes[0].set_title('Goals per Team')\n    axes[0].set_xlabel('Goals')\n    axes[0].set_ylabel('Team')\n\n    # Plot penalty costs\n    sns.barplot(x='Penalty Cost', y='Team', data=df, ax=axes[1])\n    axes[1].set_title('Penalty Costs per Team')\n    axes[1].set_xlabel('Penalty Cost (in dollars)')\n    axes[1].set_ylabel('Team')\n\n    plt.tight_layout()\n\n    return df, axes", "raw_solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    \"\"\"\n    Generates a DataFrame of football match results for multiple teams, with random goals and penalties.\n    The function also visualizes the goals and penalty costs using seaborn.\n\n    Args:\n    goals (int): The number of goals per team.\n    penalties (int): The number of penalties per team.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing match results.\n    list: A list containing two seaborn plot objects (Axes) for goals and penalty costs.\n    \"\"\"\n    # Generate random goals and penalties\n    data = {\n        'Team': TEAMS,\n        'Goals': [randint(0, goals) for _ in range(len(TEAMS))],\n        'Penalty Cost': [randint(0, penalties) * PENALTY_COST for _ in range(len(TEAMS))]\n    }\n    df = pd.DataFrame(data)\n\n    # Create plots\n    fig, axes = plt.subplots(2, 1, figsize=(8, 10))\n\n    # Plot goals\n    sns.barplot(x='Goals', y='Team', data=df, ax=axes[0])\n    axes[0].set_title('Goals per Team')\n    axes[0].set_xlabel('Goals')\n    axes[0].set_ylabel('Team')\n\n    # Plot penalty costs\n    sns.barplot(x='Penalty Cost', y='Team', data=df, ax=axes[1])\n    axes[1].set_title('Penalty Costs per Team')\n    axes[1].set_xlabel('Penalty Cost (in dollars)')\n    axes[1].set_ylabel('Team')\n\n    plt.tight_layout()\n\n    return df, axes"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    \"\"\"\n    Generates a DataFrame with random integer values between 0 and 9 for a given number of rows.\n    Counts the non-zero values in each column and visualizes this information using a bar plot.\n    \"\"\"\n    # Generate DataFrame with random integer values\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.ne(0).sum()\n\n    # Create bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Non-zero count')\n    ax.set_title('Non-zero counts in each column')\n\n    return (df, ax)", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    \"\"\"\n    Generates a DataFrame with random integer values between 0 and 9 for a given number of rows.\n    Counts the non-zero values in each column and visualizes this information using a bar plot.\n    \"\"\"\n    # Generate DataFrame with random integer values\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.ne(0).sum()\n\n    # Create bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Non-zero count')\n    ax.set_title('Non-zero counts in each column')\n\n    return (df, ax)\n\n# Test the function\ndf, ax = task_func(100)\nplt.show()"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students):\n    np.random.seed(0)\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    grades = np.random.randint(50, 100, (num_students, len(courses)))\n    students = [f'Student_{i}' for i in range(1, num_students + 1)]\n\n    df = pd.DataFrame(grades, index=students, columns=courses)\n\n    avg_grades = df.mean()\n    passing_grades = (df >= 60).sum()\n\n    df_stats = pd.DataFrame({\n        'Average Grade': avg_grades,\n        'Passing Grades': passing_grades\n    })\n\n    fig, ax = plt.subplots()\n    df_stats.plot(kind='bar', ax=ax)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.legend(loc='upper left')\n\n    return df_stats, ax", "raw_solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students):\n    np.random.seed(0)\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    grades = np.random.randint(50, 100, (num_students, len(courses)))\n    students = [f'Student_{i}' for i in range(1, num_students + 1)]\n\n    df = pd.DataFrame(grades, index=students, columns=courses)\n\n    avg_grades = df.mean()\n    passing_grades = (df >= 60).sum()\n\n    df_stats = pd.DataFrame({\n        'Average Grade': avg_grades,\n        'Passing Grades': passing_grades\n    })\n\n    fig, ax = plt.subplots()\n    df_stats.plot(kind='bar', ax=ax)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.legend(loc='upper left')\n\n    return df_stats, ax"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    \"\"\"\n    Fits an exponential decay function to the indices in the array where the first column matches the target value.\n    \"\"\"\n    # Extract the indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n\n    # Define the exponential decay function\n    def decay_func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Perform the curve fitting\n    popt, _ = optimize.curve_fit(decay_func, indices, array[indices, 1])\n\n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.plot(indices, array[indices, 1], 'o', label='Data')\n    ax.plot(indices, decay_func(indices, *popt), '-', label='Fit')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return (popt, ax)", "raw_solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    \"\"\"\n    Fits an exponential decay function to the indices in the array where the first column matches the target value.\n    \"\"\"\n    # Extract the indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n\n    # Define the exponential decay function\n    def decay_func(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Perform the curve fitting\n    popt, _ = optimize.curve_fit(decay_func, indices, array[indices, 1])\n\n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.plot(indices, array[indices, 1], 'o', label='Data')\n    ax.plot(indices, decay_func(indices, *popt), '-', label='Fit')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return (popt, ax)"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords", "raw_solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    # Remove non-alphanumeric characters, convert to lowercase, and remove stopwords\n    words = ALPHANUMERIC.sub(' ', text).lower().split()\n    words = [word for word in words if word not in STOPWORDS]\n    return ' '.join(words)\n\ndef topic_extraction(texts, num_topics):\n    # Preprocess the texts\n    texts = [preprocess_text(text) for text in texts]\n\n    # Vectorize the texts using TF-IDF\n    vectorizer = TfidfVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(texts)\n\n    # Apply NMF to extract topics\n    nmf = NMF(n_components=num_topics, random_state=42, alpha=.1, l1_ratio=.5, init='nndsvd').fit(X)\n\n    # Get the most significant words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_features_ind = topic.argsort()[:-num_topics - 1:-1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        topics.append(top_features)\n\n    return topics"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generates word vectors from a list of texts using the gensim Word2Vec model and nltk.corpus.stopwords.\n    The texts are first cleaned by removing all non-alphanumeric characters except space, lowercased, and stop words are removed.\n    \"\"\"\n    if stopwords is None:\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n\n    # Clean the texts\n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space\n        text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase\n        text = text.lower()\n        # Remove stop words\n        text = ' '.join([word for word in text.split() if word not in stopwords])\n        cleaned_texts.append(text.split())\n\n    # Train the Word2Vec model\n    model = Word2Vec(cleaned_texts, min_count=1)\n\n    return model", "raw_solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generates word vectors from a list of texts using the gensim Word2Vec model and nltk.corpus.stopwords.\n    The texts are first cleaned by removing all non-alphanumeric characters except space, lowercased, and stop words are removed.\n    \"\"\"\n    if stopwords is None:\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n\n    # Clean the texts\n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space\n        text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase\n        text = text.lower()\n        # Remove stop words\n        text = ' '.join([word for word in text.split() if word not in stopwords])\n        cleaned_texts.append(text.split())\n\n    # Train the Word2Vec model\n    model = Word2Vec(cleaned_texts, min_count=1)\n\n    return model"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in a directory. The function reads each JSON file alphabetically into a DataFrame and inserts a \"Source\" column that specifies the filename. The processed files are then moved to a \"processed\" subdirectory. The path may not exist initially.\n    \"\"\"\n    # Create the \"processed\" subdirectory if it doesn't exist\n    if not os.path.exists(os.path.join(path, 'processed')):\n        os.makedirs(os.path.join(path, 'processed'))\n\n    # Get a list of JSON files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    json_files.sort()  # Sort the files alphabetically\n\n    # Initialize an empty DataFrame to store the data from all files\n    df = pd.DataFrame()\n\n    # Read each JSON file into a DataFrame and insert a \"Source\" column\n    for file in json_files:\n        with open(os.path.join(path, file), 'r') as f:\n            data = json.load(f)\n            temp_df = pd.DataFrame(data)\n            temp_df['Source'] = file\n            df = df.append(temp_df, ignore_index=True)\n\n        # Move the processed file to the \"processed\" subdirectory\n        shutil.move(os.path.join(path, file), os.path.join(path, 'processed', file))\n\n    return df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in a directory. The function reads each JSON file alphabetically into a DataFrame and inserts a \"Source\" column that specifies the filename. The processed files are then moved to a \"processed\" subdirectory. The path may not exist initially.\n    \"\"\"\n    # Create the \"processed\" subdirectory if it doesn't exist\n    if not os.path.exists(os.path.join(path, 'processed')):\n        os.makedirs(os.path.join(path, 'processed'))\n\n    # Get a list of JSON files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    json_files.sort()  # Sort the files alphabetically\n\n    # Initialize an empty DataFrame to store the data from all files\n    df = pd.DataFrame()\n\n    # Read each JSON file into a DataFrame and insert a \"Source\" column\n    for file in json_files:\n        with open(os.path.join(path, file), 'r') as f:\n            data = json.load(f)\n            temp_df = pd.DataFrame(data)\n            temp_df['Source'] = file\n            df = df.append(temp_df, ignore_index=True)\n\n        # Move the processed file to the \"processed\" subdirectory\n        shutil.move(os.path.join(path, file), os.path.join(path, 'processed', file))\n\n    return df"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Create the directory if it doesn't exist\n    if not os.path.exists('task_func_data'):\n        os.makedirs('task_func_data')\n\n    # Generate random temperature and humidity data\n    temperature = [random.uniform(20, 30) for _ in range(10)]\n    humidity = [random.uniform(40, 80) for _ in range(10)]\n\n    # Create a CSV file in append mode\n    with open('task_func_data/Output.txt', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Timestamp', 'Temperature', 'Humidity'])\n        for i in range(10):\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            writer.writerow([timestamp, temperature[i], humidity[i]])\n\n    # Return the path to the CSV file\n    return os.path.abspath('task_func_data/Output.txt')", "raw_solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Create the directory if it doesn't exist\n    if not os.path.exists('task_func_data'):\n        os.makedirs('task_func_data')\n\n    # Generate random temperature and humidity data\n    temperature = [random.uniform(20, 30) for _ in range(10)]\n    humidity = [random.uniform(40, 80) for _ in range(10)]\n\n    # Create a CSV file in append mode\n    with open('task_func_data/Output.txt', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Timestamp', 'Temperature', 'Humidity'])\n        for i in range(10):\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            writer.writerow([timestamp, temperature[i], humidity[i]])\n\n    # Return the path to the CSV file\n    return os.path.abspath('task_func_data/Output.txt')\n\n# Test the function"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Scrapes data from a given URL and saves the scraped data to a CSV file.\n    \"\"\"\n    # Send a request to the URL\n    response = urllib.request.urlopen(url)\n\n    # Parse the HTML content\n    soup = BeautifulSoup(response, 'html.parser')\n\n    # Find the data you want to scrape\n    # This is a placeholder, you need to replace it with the actual HTML tags and attributes\n    data = soup.find_all('div', {'class': 'data'})\n\n    # Open a CSV file for writing\n    with open(CSV_FILE_PATH, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n\n        # Write the header row\n        writer.writerow(['Header 1', 'Header 2', 'Header 3'])\n\n        # Write the data rows\n        for item in data:\n            row = [item.text for item in item.find_all('span')]\n            writer.writerow(row)\n\n    return CSV_FILE_PATH", "raw_solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    Scrapes data from a given URL and saves the scraped data to a CSV file.\n    \"\"\"\n    # Send a request to the URL\n    response = urllib.request.urlopen(url)\n\n    # Parse the HTML content\n    soup = BeautifulSoup(response, 'html.parser')\n\n    # Find the data you want to scrape\n    # This is a placeholder, you need to replace it with the actual HTML tags and attributes\n    data = soup.find_all('div', {'class': 'data'})\n\n    # Open a CSV file for writing\n    with open(CSV_FILE_PATH, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n\n        # Write the header row\n        writer.writerow(['Header 1', 'Header 2', 'Header 3'])\n\n        # Write the data rows\n        for item in data:\n            row = [item.text for item in item.find_all('span')]\n            writer.writerow(row)\n\n    return CSV_FILE_PATH"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data is not a DataFrame.\")\n    if data.empty:\n        raise ValueError(\"Data is empty.\")\n    if target_column not in data.columns:\n        raise ValueError(f\"{target_column} is not a column of data.\")\n    if not all(data.dtypes == 'float64') and not all(data.dtypes == 'int64'):\n        raise ValueError(\"Data contains values that are not numeric.\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state is not an integer.\")\n    if not 0 <= test_size <= 1:\n        raise ValueError(\"Test size is not between 0 and 1.\")\n\n    X = data.drop([target_column], axis=1)\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.score(X_test, y_test)", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data is not a DataFrame.\")\n    if data.empty:\n        raise ValueError(\"Data is empty.\")\n    if target_column not in data.columns:\n        raise ValueError(f\"{target_column} is not a column of data.\")\n    if not all(data.dtypes == 'float64') and not all(data.dtypes == 'int64'):\n        raise ValueError(\"Data contains values that are not numeric.\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state is not an integer.\")\n    if not 0 <= test_size <= 1:\n        raise ValueError(\"Test size is not between 0 and 1.\")\n\n    X = data.drop([target_column], axis=1)\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport random\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    random.seed(rng_seed)\n    np.random.seed(rng_seed)\n\n    # Generate random data\n    ids = list(range(1, 101))\n    names = latin_names + other_names\n    random.shuffle(names)\n    dobs = [datetime.datetime(random.randint(start_year, end_year), random.randint(1, 12), random.randint(1, 28)) for _ in range(100)]\n    emails = [f\"{name}{dob.year}@{email_domain}\" for name, dob in zip(names, dobs)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dobs,\n        'Email': emails\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport random\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    random.seed(rng_seed)\n    np.random.seed(rng_seed)\n\n    # Generate random data\n    ids = list(range(1, 101))\n    names = latin_names + other_names\n    random.shuffle(names)\n    dobs = [datetime.datetime(random.randint(start_year, end_year), random.randint(1, 12), random.randint(1, 28)) for _ in range(100)]\n    emails = [f\"{name}{dob.year}@{email_domain}\" for name, dob in zip(names, dobs)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dobs,\n        'Email': emails\n    })\n\n    return df\n\n# Test the function\ndf = task_func(start_year=1980, end_year=2000, email_domain='test.at', rng_seed=3)"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Load data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize a dictionary to store the mean and median for each key\n    stats = defaultdict(lambda: {'mean': None, 'median': None})\n\n    # Calculate the mean and median for each key\n    for item in data:\n        for key, value in item.items():\n            if isinstance(value, list) and len(value) > 0:\n                values = np.array(value)\n                stats[key]['mean'] = np.mean(values)\n                stats[key]['median'] = np.median(values)\n\n    # Write the results to a CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for field, stat in stats.items():\n            writer.writerow([field, stat['mean'], stat['median']])\n\n    return stats", "raw_solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\ndef task_func(input_file, output_file):\n    # Load data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize a dictionary to store the mean and median for each key\n    stats = defaultdict(lambda: {'mean': None, 'median': None})\n\n    # Calculate the mean and median for each key\n    for item in data:\n        for key, value in item.items():\n            if isinstance(value, list) and len(value) > 0:\n                values = np.array(value)\n                stats[key]['mean'] = np.mean(values)\n                stats[key]['median'] = np.median(values)\n\n    # Write the results to a CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for field, stat in stats.items():\n            writer.writerow([field, stat['mean'], stat['median']])\n\n    return stats"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(file_dict, target_dir=\"non_none_files\"):\n    \"\"\"\n    Processes files from a dictionary by checking if the file exists and if it has content,\n    then copies it to a target directory.\n\n    Args:\n        file_dict (dict): A dictionary where keys are file names and values are file paths.\n        target_dir (str): The target directory where the files will be copied. Default is \"non_none_files\".\n\n    Returns:\n        copied_files (list): A list of full file paths that were copied.\n    \"\"\"\n    # Create target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    copied_files = []\n\n    for file_name, file_path in file_dict.items():\n        # Check if file exists and has content\n        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n            # Construct full target file path\n            target_file_path = os.path.join(target_dir, file_name)\n\n            # Copy file to target directory\n            shutil.copy2(file_path, target_file_path)\n\n            # Add full target file path to list\n            copied_files.append(target_file_path)\n\n    return copied_files", "raw_solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(file_dict, target_dir=\"non_none_files\"):\n    \"\"\"\n    Processes files from a dictionary by checking if the file exists and if it has content,\n    then copies it to a target directory.\n\n    Args:\n        file_dict (dict): A dictionary where keys are file names and values are file paths.\n        target_dir (str): The target directory where the files will be copied. Default is \"non_none_files\".\n\n    Returns:\n        copied_files (list): A list of full file paths that were copied.\n    \"\"\"\n    # Create target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    copied_files = []\n\n    for file_name, file_path in file_dict.items():\n        # Check if file exists and has content\n        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n            # Construct full target file path\n            target_file_path = os.path.join(target_dir, file_name)\n\n            # Copy file to target directory\n            shutil.copy2(file_path, target_file_path)\n\n            # Add full target file path to list\n            copied_files.append(target_file_path)\n\n    return copied_files"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in a directory based on a specified pattern and creates new files with altered names while preserving the content.\n    \"\"\"\n    # Initialize an empty list to store the names of the new files\n    new_files = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        if re.match(pattern, filename):\n            # If it does, create a new filename by replacing the number with a different one\n            new_filename = re.sub(r'-\\d+\\.csv$', '-999.csv', filename)\n            # Add the new filename to the list\n            new_files.append(new_filename)\n\n            # Read the content of the original file\n            with open(os.path.join(directory, filename), 'r') as file:\n                reader = csv.reader(file)\n                data = list(reader)\n\n            # Write the content to the new file\n            with open(os.path.join(directory, new_filename), 'w', newline='') as file:\n                writer = csv.writer(file)\n                writer.writerows(data)\n\n    # Return the list of new filenames\n    return new_files", "raw_solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in a directory based on a specified pattern and creates new files with altered names while preserving the content.\n    \"\"\"\n    # Initialize an empty list to store the names of the new files\n    new_files = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        if re.match(pattern, filename):\n            # If it does, create a new filename by replacing the number with a different one\n            new_filename = re.sub(r'-\\d+\\.csv$', '-999.csv', filename)\n            # Add the new filename to the list\n            new_files.append(new_filename)\n\n            # Read the content of the original file\n            with open(os.path.join(directory, filename), 'r') as file:\n                reader = csv.reader(file)\n                data = list(reader)\n\n            # Write the content to the new file\n            with open(os.path.join(directory, new_filename), 'w', newline='') as file:\n                writer = csv.writer(file)\n                writer.writerows(data)\n\n    # Return the list of new filenames\n    return new_files"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    for filename in os.listdir(directory):\n        match = re.match(pattern, filename)\n        if match:\n            prefix = match.group(1)\n            if not os.path.exists(os.path.join(directory, prefix)):\n                os.mkdir(os.path.join(directory, prefix))\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(os.path.join(directory, prefix))\n            extracted_dirs.append(prefix)\n    return extracted_dirs", "raw_solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    for filename in os.listdir(directory):\n        match = re.match(pattern, filename)\n        if match:\n            prefix = match.group(1)\n            if not os.path.exists(os.path.join(directory, prefix)):\n                os.mkdir(os.path.join(directory, prefix))\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(os.path.join(directory, prefix))\n            extracted_dirs.append(prefix)\n    return extracted_dirs"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nimport shutil\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    \"\"\"\n    Archives all files that match a particular pattern and then deletes the original files.\n    \"\"\"\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find files that match the pattern\n    files = glob.glob(pattern)\n\n    # Archive and delete each file\n    for file in files:\n        # Get the base name of the file\n        base_name = os.path.basename(file)\n\n        # Create the archive file path\n        archive_file = os.path.join(ARCHIVE_DIR, base_name)\n\n        # Archive the file\n        shutil.move(file, archive_file)\n\n    # Return the archive file path\n    return archive_file", "raw_solution": "import subprocess\nimport os\nimport glob\nimport shutil\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    Archives all files that match a particular pattern and then deletes the original files.\n    \"\"\"\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find files that match the pattern\n    files = glob.glob(pattern)\n\n    # Archive and delete each file\n    for file in files:\n        # Get the base name of the file\n        base_name = os.path.basename(file)\n\n        # Create the archive file path\n        archive_file = os.path.join(ARCHIVE_DIR, base_name)\n\n        # Archive the file\n        shutil.move(file, archive_file)\n\n    # Return the archive file path\n    return archive_file"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize counters\n    total_goals = 0\n    total_penalties = 0\n\n    # Read CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.reader(file)\n        next(reader)  # Skip header\n        for row in reader:\n            total_goals += int(row[1])\n            total_penalties += int(row[2])\n\n    # Update with given goals and penalties\n    total_goals += goals\n    total_penalties += penalties\n\n    # Return Counter object\n    return Counter({'goals': total_goals, 'penalties': total_penalties})", "raw_solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize counters\n    total_goals = 0\n    total_penalties = 0\n\n    # Read CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.reader(file)\n        next(reader)  # Skip header\n        for row in reader:\n            total_goals += int(row[1])\n            total_penalties += int(row[2])\n\n    # Update with given goals and penalties\n    total_goals += goals\n    total_penalties += penalties\n\n    # Return Counter object\n    return Counter({'goals': total_goals, 'penalties': total_penalties})\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\n# Test setup\nsetup_csv()\n\n# Test task_func"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(source_dir) or not os.path.exists(target_dir):\n        return \"Source or target directory does not exist.\"\n\n    moved_files_count = 0\n    for file_name in os.listdir(source_dir):\n        if re.match(file_pattern, file_name):\n            shutil.move(os.path.join(source_dir, file_name), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(source_dir) or not os.path.exists(target_dir):\n        return \"Source or target directory does not exist.\"\n\n    moved_files_count = 0\n    for file_name in os.listdir(source_dir):\n        if re.match(file_pattern, file_name):\n            shutil.move(os.path.join(source_dir, file_name), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    # Clean the texts\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Calculate term frequency\n    counter1 = Counter(text1.split())\n    counter2 = Counter(text2.split())\n\n    # Calculate cosine similarity\n    vec1 = np.array(list(counter1.values()))\n    vec2 = np.array(list(counter2.values()))\n    cosine_similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n\n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return (cosine_similarity, levenshtein_ratio)", "raw_solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    # Clean the texts\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Calculate term frequency\n    counter1 = Counter(text1.split())\n    counter2 = Counter(text2.split())\n\n    # Calculate cosine similarity\n    vec1 = np.array(list(counter1.values()))\n    vec2 = np.array(list(counter2.values()))\n    cosine_similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n\n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return (cosine_similarity, levenshtein_ratio)"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input should be a list of integers.\")\n    if not all(isinstance(i, int) for i in numbers):\n        raise TypeError(\"Input should be a list of integers.\")\n    if any(i < 0 for i in numbers):\n        raise ValueError(\"Input should not contain negative numbers.\")\n\n    fac = [math.factorial(i) for i in numbers]\n    perm = [i for i in permutations(numbers, len(numbers))]\n\n    return fac, perm", "raw_solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input should be a list of integers.\")\n    if not all(isinstance(i, int) for i in numbers):\n        raise TypeError(\"Input should be a list of integers.\")\n    if any(i < 0 for i in numbers):\n        raise ValueError(\"Input should not contain negative numbers.\")\n\n    fac = [math.factorial(i) for i in numbers]\n    perm = [i for i in permutations(numbers, len(numbers))]\n\n    return fac, perm"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfers files from one directory to another based on the specified file extensions.\n    \"\"\"\n    transferred_files = []\n    for extension in EXTENSIONS:\n        source_files = glob.glob(os.path.join(SOURCE_DIR, f'*.{extension}'))\n        for file in source_files:\n            try:\n                shutil.move(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Failed to transfer file {file}: {str(e)}\")\n    return transferred_files", "raw_solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfers files from one directory to another based on the specified file extensions.\n    \"\"\"\n    transferred_files = []\n    for extension in EXTENSIONS:\n        source_files = glob.glob(os.path.join(SOURCE_DIR, f'*.{extension}'))\n        for file in source_files:\n            try:\n                shutil.move(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Failed to transfer file {file}: {str(e)}\")\n    return transferred_files"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Convert the input data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n\n    # Normalize the counts using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n\n    # Normalize the weights using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    # Return the DataFrame with the normalized counts and weights\n    return df[['Item', 'Normalized Count', 'Normalized Weight']]", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Convert the input data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n\n    # Normalize the counts using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n\n    # Normalize the weights using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    # Return the DataFrame with the normalized counts and weights\n    return df[['Item', 'Normalized Count', 'Normalized Weight']]"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Filter out non-numeric values\n    filtered_data = [tuple(filter(lambda x: isinstance(x, (int, float)), t)) for t in data_list]\n\n    # Calculate means\n    means = [np.mean(list(iter(t))) for t in zip(*filtered_data)]\n\n    # Create DataFrame\n    df = pd.DataFrame(means, columns=['Mean Value'])\n    df.index = ['Position ' + str(i) for i in range(len(means))]\n\n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Filter out non-numeric values\n    filtered_data = [tuple(filter(lambda x: isinstance(x, (int, float)), t)) for t in data_list]\n\n    # Calculate means\n    means = [np.mean(list(iter(t))) for t in zip(*filtered_data)]\n\n    # Create DataFrame\n    df = pd.DataFrame(means, columns=['Mean Value'])\n    df.index = ['Position ' + str(i) for i in range(len(means))]\n\n    return df\n\n# Test the function\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    if data.empty:\n        raise ValueError(\"'data' is empty\")\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"'col1' or 'col2' are not in 'data'\")\n    if data[col1].nunique() == 1 or data[col2].nunique() == 1:\n        raise ValueError(\"One or both of the columns do not have multiple categories\")\n    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\n        raise ValueError(\"Some categories have less than 5 observations\")\n    if not np.issubdtype(data[col1].dtype, np.object_) or not np.issubdtype(data[col2].dtype, np.object_):\n        raise TypeError(\"One or both of the columns contain non-categorical data\")\n\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    if data.empty:\n        raise ValueError(\"'data' is empty\")\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"'col1' or 'col2' are not in 'data'\")\n    if data[col1].nunique() == 1 or data[col2].nunique() == 1:\n        raise ValueError(\"One or both of the columns do not have multiple categories\")\n    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\n        raise ValueError(\"Some categories have less than 5 observations\")\n    if not np.issubdtype(data[col1].dtype, np.object_) or not np.issubdtype(data[col2].dtype, np.object_):\n        raise TypeError(\"One or both of the columns contain non-categorical data\")\n\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulates a number of dice rolls and calculates the frequency of each result.\n    Returns a tuple containing:\n    - A numpy array with the frequency of each outcome.\n    - A matplotlib Axes object representing the histogram.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n\n    # Calculate frequency of each outcome\n    frequencies = np.bincount(results)\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequencies)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return frequencies, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulates a number of dice rolls and calculates the frequency of each result.\n    Returns a tuple containing:\n    - A numpy array with the frequency of each outcome.\n    - A matplotlib Axes object representing the histogram.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n\n    # Calculate frequency of each outcome\n    frequencies = np.bincount(results)\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequencies)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return frequencies, ax"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from a source directory to a target directory.\n    The function identifies processed files by the '_processed' suffix in the filename.\n    The function returns the path to the created archive.\n    \"\"\"\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(source_dir)\n\n    # Filter the list to only include files with the '_processed' suffix\n    processed_files = [file for file in files if re.search(r'_processed\\.', file)]\n\n    # Create the archive file\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as zipf:\n        for file in processed_files:\n            file_path = os.path.join(source_dir, file)\n            zipf.write(file_path, arcname=file)\n\n    return archive_path", "raw_solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from a source directory to a target directory.\n    The function identifies processed files by the '_processed' suffix in the filename.\n    The function returns the path to the created archive.\n    \"\"\"\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(source_dir)\n\n    # Filter the list to only include files with the '_processed' suffix\n    processed_files = [file for file in files if re.search(r'_processed\\.', file)]\n\n    # Create the archive file\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as zipf:\n        for file in processed_files:\n            file_path = os.path.join(source_dir, file)\n            zipf.write(file_path, arcname=file)\n\n    return archive_path"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    \"\"\"\n    Predicts the stock closing prices for the next 7 days using simple linear regression and plots the data.\n    \"\"\"\n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Generate future timestamps\n    future_timestamps = pd.date_range(start=df.index[-1], periods=7, freq='D')\n\n    # Reset the index\n    df.reset_index(inplace=True)\n\n    # Create a new DataFrame for the future timestamps\n    future_df = pd.DataFrame(future_timestamps, columns=['Date'])\n\n    # Merge the original DataFrame and the future DataFrame\n    df = pd.concat([df, future_df], ignore_index=True)\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Create a new column 'Time' that represents the time since the first day\n    df['Time'] = (df.index - df.index[0]).days\n\n    # Create a new column 'Time_Squared' that represents the square of the time\n    df['Time_Squared'] = df['Time'] ** 2\n\n    # Create a new column 'Time_Cubed' that represents the cube of the time\n    df['Time_Cubed'] = df['Time'] ** 3\n\n    # Create a new column 'Time_Fourth' that represents the fourth power of the time\n    df['Time_Fourth'] = df['Time'] ** 4\n\n    # Create a new column 'Time_Fifth' that represents the fifth power of the time\n    df['Time_Fifth'] = df['Time'] ** 5\n\n    # Create a new column 'Time_Sixth' that represents the sixth power of the time\n    df['Time_Sixth'] = df['Time'] ** 6\n\n    # Create a new column 'Time_Seventh' that represents the seventh power of the time\n    df['Time_Seventh'] = df['Time'] ** 7\n\n    # Create a new column 'Time_Eighth' that represents the eighth power of the time\n    df['Time_Eighth'] = df['Time'] ** 8\n\n    # Create a new column 'Time_Ninth' that represents the ninth power of the time\n    df['Time_Ninth'] = df['Time'] ** 9\n\n    # Create a new column 'Time_Tenth' that represents the tenth power of the time\n    df['Time_Tenth'] = df['Time'] ** 10\n\n    # Create a new column 'Time_Eleventh' that represents the eleventh power of the time\n    df['Time_Eleventh'] = df['Time'] ** 11\n\n    # Create a new column 'Time_Twelfth' that represents the twelfth power of the time\n    df['Time_Twelfth'] = df['Time'] ** 12\n\n    # Create a new column 'Time_Thirteenth' that represents the thirteenth power of the time\n    df['Time_Thirteenth'] = df['Time'] ** 13\n\n    # Create a new column 'Time_Fourteenth' that represents the fourteenth power of the time\n    df['Time_Fourteenth'] = df['Time'] ** 14\n\n    # Create a new column 'Time_Fifteenth' that represents the fifteenth power of the time\n    df['Time_Fifteenth'] = df['Time'] ** 15\n\n    # Create a new column 'Time_Sixteenth' that represents the sixteenth power of the time\n    df['Time_Sixteenth'] = df['Time'] ** 16\n\n    # Create a new column 'Time_Seventeenth' that represents the seventeenth power of the time", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    \"\"\"\n    Predicts the stock closing prices for the next 7 days using simple linear regression and plots the data.\n    \"\"\"\n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Generate future timestamps\n    future_timestamps = pd.date_range(start=df.index[-1], periods=7, freq='D')\n\n    # Reset the index\n    df.reset_index(inplace=True)\n\n    # Create a new DataFrame for the future timestamps\n    future_df = pd.DataFrame(future_timestamps, columns=['Date'])\n\n    # Merge the original DataFrame and the future DataFrame\n    df = pd.concat([df, future_df], ignore_index=True)\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Create a new column 'Time' that represents the time since the first day\n    df['Time'] = (df.index - df.index[0]).days\n\n    # Create a new column 'Time_Squared' that represents the square of the time\n    df['Time_Squared'] = df['Time'] ** 2\n\n    # Create a new column 'Time_Cubed' that represents the cube of the time\n    df['Time_Cubed'] = df['Time'] ** 3\n\n    # Create a new column 'Time_Fourth' that represents the fourth power of the time\n    df['Time_Fourth'] = df['Time'] ** 4\n\n    # Create a new column 'Time_Fifth' that represents the fifth power of the time\n    df['Time_Fifth'] = df['Time'] ** 5\n\n    # Create a new column 'Time_Sixth' that represents the sixth power of the time\n    df['Time_Sixth'] = df['Time'] ** 6\n\n    # Create a new column 'Time_Seventh' that represents the seventh power of the time\n    df['Time_Seventh'] = df['Time'] ** 7\n\n    # Create a new column 'Time_Eighth' that represents the eighth power of the time\n    df['Time_Eighth'] = df['Time'] ** 8\n\n    # Create a new column 'Time_Ninth' that represents the ninth power of the time\n    df['Time_Ninth'] = df['Time'] ** 9\n\n    # Create a new column 'Time_Tenth' that represents the tenth power of the time\n    df['Time_Tenth'] = df['Time'] ** 10\n\n    # Create a new column 'Time_Eleventh' that represents the eleventh power of the time\n    df['Time_Eleventh'] = df['Time'] ** 11\n\n    # Create a new column 'Time_Twelfth' that represents the twelfth power of the time\n    df['Time_Twelfth'] = df['Time'] ** 12\n\n    # Create a new column 'Time_Thirteenth' that represents the thirteenth power of the time\n    df['Time_Thirteenth'] = df['Time'] ** 13\n\n    # Create a new column 'Time_Fourteenth' that represents the fourteenth power of the time\n    df['Time_Fourteenth'] = df['Time'] ** 14\n\n    # Create a new column 'Time_Fifteenth' that represents the fifteenth power of the time\n    df['Time_Fifteenth'] = df['Time'] ** 15\n\n    # Create a new column 'Time_Sixteenth' that represents the sixteenth power of the time\n    df['Time_Sixteenth'] = df['Time'] ** 16\n\n    # Create a new column 'Time_Seventeenth' that represents the seventeenth power of the time\n    df['Time_Seventeenth'] = df['Time'] ** "}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n    df['z_score'] = zscore(df['closing_price'])\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n\n    fig, ax = plt.subplots()\n    ax.scatter(df.index, df['closing_price'])\n    ax.scatter(outliers.index, outliers['closing_price'], color='r')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n\n    return outliers, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    df['z_score'] = zscore(df['closing_price'])\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n\n    fig, ax = plt.subplots()\n    ax.scatter(df.index, df['closing_price'])\n    ax.scatter(outliers.index, outliers['closing_price'], color='r')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n\n    return outliers, ax"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualizes the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \"\"\"\n    # Create a new figure\n    fig, axes = plt.subplots(2, figsize=(10, 8))\n\n    # Box plot\n    sns.boxplot(data=df['Close'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n\n    # Histogram\n    sns.histplot(data=df['Close'], kde=True, ax=axes[1])\n    axes[1].set_title('Histogram of Closing Prices')\n\n    plt.tight_layout()\n\n    return axes[0], axes[1]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualizes the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \"\"\"\n    # Create a new figure\n    fig, axes = plt.subplots(2, figsize=(10, 8))\n\n    # Box plot\n    sns.boxplot(data=df['Close'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n\n    # Histogram\n    sns.histplot(data=df['Close'], kde=True, ax=axes[1])\n    axes[1].set_title('Histogram of Closing Prices')\n\n    plt.tight_layout()\n\n    return axes[0], axes[1]"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts the share closing prices for the next 7 days using the ARIMA model and plots the forecast.\n    \"\"\"\n    # Fit the ARIMA model\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit(disp=0)\n\n    # Make forecast\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot forecast\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(df['Date'], df['Close'], label='Historical Prices')\n    ax.plot(pd.date_range(start=df['Date'].iloc[-1], periods=8)[1:], forecast, label='Forecast Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    ax.legend()\n\n    return list(forecast), ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts the share closing prices for the next 7 days using the ARIMA model and plots the forecast.\n    \"\"\"\n    # Fit the ARIMA model\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit(disp=0)\n\n    # Make forecast\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot forecast\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(df['Date'], df['Close'], label='Historical Prices')\n    ax.plot(pd.date_range(start=df['Date'].iloc[-1], periods=8)[1:], forecast, label='Forecast Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    ax.legend()\n\n    return list(forecast), ax"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Counts the frequency of all two-letter combinations in a word.\n    \"\"\"\n    # Generate all two-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=2)]\n\n    # Count the frequency of each combination in the word\n    word_counter = Counter(word[i:i+2] for i in range(len(word)-1))\n\n    # Create a dictionary with all combinations and their counts\n    result = {combo: word_counter[combo] for combo in combinations}\n\n    return result", "raw_solution": "from collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Counts the frequency of all two-letter combinations in a word.\n    \"\"\"\n    # Generate all two-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=2)]\n\n    # Count the frequency of each combination in the word\n    word_counter = Counter(word[i:i+2] for i in range(len(word)-1))\n\n    # Create a dictionary with all combinations and their counts\n    result = {combo: word_counter[combo] for combo in combinations}\n\n    return result"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create a DataFrame with the date range and sales data\n    df = pd.DataFrame({\n        'Date': dates,\n        'Category': np.random.choice(categories, periods),\n        'Sales': np.random.randint(100, 1000, periods)\n    })\n\n    # Group by 'Category' and 'Date' and sum the 'Sales'\n    df = df.groupby(['Category', 'Date']).sum().reset_index()\n\n    # Plot the 'Sales' over 'Date' for each 'Category'\n    fig, ax = plt.subplots()\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category)\n\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.legend()\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create a DataFrame with the date range and sales data\n    df = pd.DataFrame({\n        'Date': dates,\n        'Category': np.random.choice(categories, periods),\n        'Sales': np.random.randint(100, 1000, periods)\n    })\n\n    # Group by 'Category' and 'Date' and sum the 'Sales'\n    df = df.groupby(['Category', 'Date']).sum().reset_index()\n\n    # Plot the 'Sales' over 'Date' for each 'Category'\n    fig, ax = plt.subplots()\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category)\n\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.legend()\n\n    return df, ax\n\n# Test the function\ndf, ax = task_func()"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    if sales_data is None:\n        sales_data = pd.date_range(start=start_date, periods=periods, freq=freq).to_series()\n        sales_data = sales_data.apply(lambda x: np.random.randint(1, 100))\n    else:\n        sales_data = pd.Series(sales_data, index=pd.date_range(start=start_date, periods=len(sales_data), freq=freq))\n\n    X = np.array(range(len(sales_data))).reshape(-1, 1)\n    Y = sales_data.values.reshape(-1, 1)\n\n    linear_regressor = LinearRegression()\n    linear_regressor.fit(X, Y)\n\n    future_dates = pd.date_range(start=start_date, periods=periods, freq=freq).to_series().index.values.reshape(-1, 1)\n    forecasted_sales = linear_regressor.predict(future_dates)\n\n    return forecasted_sales.flatten()", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    if sales_data is None:\n        sales_data = pd.date_range(start=start_date, periods=periods, freq=freq).to_series()\n        sales_data = sales_data.apply(lambda x: np.random.randint(1, 100))\n    else:\n        sales_data = pd.Series(sales_data, index=pd.date_range(start=start_date, periods=len(sales_data), freq=freq))\n\n    X = np.array(range(len(sales_data))).reshape(-1, 1)\n    Y = sales_data.values.reshape(-1, 1)\n\n    linear_regressor = LinearRegression()\n    linear_regressor.fit(X, Y)\n\n    future_dates = pd.date_range(start=start_date, periods=periods, freq=freq).to_series().index.values.reshape(-1, 1)\n    forecasted_sales = linear_regressor.predict(future_dates)\n\n    return forecasted_sales.flatten()"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    today = datetime.now().date()\n\n    assignments = []\n    for _ in range(n_tasks):\n        task = random.choice(task_list)\n        employee = random.choice(employees)\n        task_name = task.replace(\" \", \"_\")\n        assignments.append({\n            \"Task Name\": task_name,\n            \"Assigned To\": employee,\n            \"Due Date\": today,\n        })\n\n    return pd.DataFrame(assignments)", "raw_solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    today = datetime.now().date()\n\n    assignments = []\n    for _ in range(n_tasks):\n        task = random.choice(task_list)\n        employee = random.choice(employees)\n        task_name = task.replace(\" \", \"_\")\n        assignments.append({\n            \"Task Name\": task_name,\n            \"Assigned To\": employee,\n            \"Due Date\": today,\n        })\n\n    return pd.DataFrame(assignments)"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty\")\n\n    # Replace spaces with underscores\n    text = text.lower()\n    text = re.sub(r'\\s', '_', text)\n\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(word_counts.keys(), word_counts.values())\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Each Unique Word')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty\")\n\n    # Replace spaces with underscores\n    text = text.lower()\n    text = re.sub(r'\\s', '_', text)\n\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(word_counts.keys(), word_counts.values())\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Each Unique Word')\n\n    return ax"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory and save it as a zip file saved to a target directory.\n    The valid extensions are: ['.txt', '.docx', '.xlsx', '.csv'].\n    If the source_directory does not exist, it raises an OSError.\n    It returns the full path to the created zip file in the format \"/path/to/target_directory/zip_name.zip\".\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n\n    # Check if target directory exists, if not, create it\n    if not os.path.isdir(target_directory):\n        os.makedirs(target_directory)\n\n    # Define the valid file extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Get a list of all files with valid extensions in the source directory\n    files_to_zip = [file for file in glob.glob(f\"{source_directory}/*\") if os.path.splitext(file)[1] in valid_extensions]\n\n    # Create the zip file\n    zip_file_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n        for file in files_to_zip:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return zip_file_path", "raw_solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory and save it as a zip file saved to a target directory.\n    The valid extensions are: ['.txt', '.docx', '.xlsx', '.csv'].\n    If the source_directory does not exist, it raises an OSError.\n    It returns the full path to the created zip file in the format \"/path/to/target_directory/zip_name.zip\".\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n\n    # Check if target directory exists, if not, create it\n    if not os.path.isdir(target_directory):\n        os.makedirs(target_directory)\n\n    # Define the valid file extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Get a list of all files with valid extensions in the source directory\n    files_to_zip = [file for file in glob.glob(f\"{source_directory}/*\") if os.path.splitext(file)[1] in valid_extensions]\n\n    # Create the zip file\n    zip_file_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n        for file in files_to_zip:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return zip_file_path"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport openpyxl\nimport csv\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files and saves them in a target directory.\n    Each file's text content is captured and stored in a CSV with a single 'Text' column and no row indices.\n    This function will overwrite existing files in the target directory if they have the same names as the converted files.\n    If the source directory does not exist, this function raises a FileNotFoundError.\n    This function returns the number of files successfully converted to CSV.\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n\n    # Get a list of files in the source directory\n    files = os.listdir(source_directory)\n\n    # Initialize counter for successfully converted files\n    count = 0\n\n    # Loop through each file in the source directory\n    for file in files:\n        # Get the file extension\n        ext = Path(file).suffix\n\n        # Check if the file is a text file, Word document, Excel spreadsheet, or CSV file\n        if ext in ['.txt', '.docx', '.xlsx', '.csv']:\n            # Read the file content\n            if ext == '.txt':\n                with open(os.path.join(source_directory, file), 'r') as f:\n                    content = f.read()\n            elif ext == '.docx':\n                doc = docx.Document(os.path.join(source_directory, file))\n                content = ' '.join([p.text for p in doc.paragraphs])\n            elif ext == '.xlsx':\n                wb = openpyxl.load_workbook(os.path.join(source_directory, file))\n                content = ' '.join([str(cell.value) for row in wb.active.rows for cell in row])\n            else:  # ext == '.csv'\n                with open(os.path.join(source_directory, file), 'r') as f:\n                    reader = csv.reader(f)\n                    content = ' '.join([', '.join(row) for row in reader])\n\n            # Create a DataFrame with the file content\n            df = pd.DataFrame({'Text': [content]})\n\n            # Save the DataFrame as a CSV file in the target directory\n            df.to_csv(os.path.join(target_directory, f\"{Path(file).stem}.csv\"), index=False)\n\n            # Increment the counter\n            count += 1\n\n    # Return the number of successfully converted files\n    return count", "raw_solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport openpyxl\nimport csv\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files and saves them in a target directory.\n    Each file's text content is captured and stored in a CSV with a single 'Text' column and no row indices.\n    This function will overwrite existing files in the target directory if they have the same names as the converted files.\n    If the source directory does not exist, this function raises a FileNotFoundError.\n    This function returns the number of files successfully converted to CSV.\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n\n    # Get a list of files in the source directory\n    files = os.listdir(source_directory)\n\n    # Initialize counter for successfully converted files\n    count = 0\n\n    # Loop through each file in the source directory\n    for file in files:\n        # Get the file extension\n        ext = Path(file).suffix\n\n        # Check if the file is a text file, Word document, Excel spreadsheet, or CSV file\n        if ext in ['.txt', '.docx', '.xlsx', '.csv']:\n            # Read the file content\n            if ext == '.txt':\n                with open(os.path.join(source_directory, file), 'r') as f:\n                    content = f.read()\n            elif ext == '.docx':\n                doc = docx.Document(os.path.join(source_directory, file))\n                content = ' '.join([p.text for p in doc.paragraphs])\n            elif ext == '.xlsx':\n                wb = openpyxl.load_workbook(os.path.join(source_directory, file))\n                content = ' '.join([str(cell.value) for row in wb.active.rows for cell in row])\n            else:  # ext == '.csv'\n                with open(os.path.join(source_directory, file), 'r') as f:\n                    reader = csv.reader(f)\n                    content = ' '.join([', '.join(row) for row in reader])\n\n            # Create a DataFrame with the file content\n            df = pd.DataFrame({'Text': [content]})\n\n            # Save the DataFrame as a CSV file in the target directory\n            df.to_csv(os.path.join(target_directory, f\"{Path(file).stem}.csv\"), index=False)\n\n            # Increment the counter\n            count += 1\n\n    # Return the number of successfully converted files\n    return count"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame cannot be empty\")\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame cannot contain NaN values\")\n    if not df.select_dtypes(include=[np.number]).empty:\n        raise TypeError(\"DataFrame must contain only numeric data types\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_normalized.cumsum()", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame cannot be empty\")\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame cannot contain NaN values\")\n    if not df.select_dtypes(include=[np.number]).empty:\n        raise TypeError(\"DataFrame must contain only numeric data types\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_normalized.cumsum()"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n    \"\"\"\n    if not os.path.exists(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n\n    result = []\n    for filename in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, filename)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path)\n            creation_time = os.path.getctime(file_path)\n            modification_time = os.path.getmtime(file_path)\n            result.append((filename, file_size,\n                           datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat(),\n                           datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat()))\n\n    return result", "raw_solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n    \"\"\"\n    if not os.path.exists(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n\n    result = []\n    for filename in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, filename)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path)\n            creation_time = os.path.getctime(file_path)\n            modification_time = os.path.getmtime(file_path)\n            result.append((filename, file_size,\n                           datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat(),\n                           datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat()))\n\n    return result"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on provided JSON data.\n    \"\"\"\n    # Check if the JSON data is malformed, empty, or contains non-string country names, non-numeric or negative populations\n    try:\n        data = json.loads(json_data)\n        if not isinstance(data, dict):\n            raise ValueError(\"JSON data is malformed\")\n        if not data:\n            raise ValueError(\"JSON data is empty\")\n        for country, population in data.items():\n            if not isinstance(country, str):\n                raise ValueError(\"Non-string country names are not allowed\")\n            if not isinstance(population, (int, float)) or population < 0:\n                raise ValueError(\"Non-numeric or negative populations are not allowed\")\n    except json.JSONDecodeError:\n        raise ValueError(\"JSON data is malformed\")\n\n    # Create a DataFrame from the JSON data\n    df = pd.DataFrame(list(data.items()), columns=[\"Country\", \"Population\"])\n    df[\"Population\"] = df[\"Population\"].apply(lambda x: int(math.floor(x)))  # Round down to the nearest integer\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Write the DataFrame to a CSV file\n    file_path = os.path.join(output_dir, file_name)\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"Failed to write to the specified directory\")\n\n    return file_path, df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on provided JSON data.\n    \"\"\"\n    # Check if the JSON data is malformed, empty, or contains non-string country names, non-numeric or negative populations\n    try:\n        data = json.loads(json_data)\n        if not isinstance(data, dict):\n            raise ValueError(\"JSON data is malformed\")\n        if not data:\n            raise ValueError(\"JSON data is empty\")\n        for country, population in data.items():\n            if not isinstance(country, str):\n                raise ValueError(\"Non-string country names are not allowed\")\n            if not isinstance(population, (int, float)) or population < 0:\n                raise ValueError(\"Non-numeric or negative populations are not allowed\")\n    except json.JSONDecodeError:\n        raise ValueError(\"JSON data is malformed\")\n\n    # Create a DataFrame from the JSON data\n    df = pd.DataFrame(list(data.items()), columns=[\"Country\", \"Population\"])\n    df[\"Population\"] = df[\"Population\"].apply(lambda x: int(math.floor(x)))  # Round down to the nearest integer\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Write the DataFrame to a CSV file\n    file_path = os.path.join(output_dir, file_name)\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"Failed to write to the specified directory\")\n\n    return file_path, df"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n    \"\"\"\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The specified directory {dir_path} does not exist or is not a directory.\")\n\n    result = {}\n    for item in os.listdir(dir_path):\n        for predicate in predicates:\n            if re.search(predicate, item):\n                result[item] = True\n                break\n        else:\n            result[item] = False\n\n    return result", "raw_solution": "import os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n    \"\"\"\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The specified directory {dir_path} does not exist or is not a directory.\")\n\n    result = {}\n    for item in os.listdir(dir_path):\n        for predicate in predicates:\n            if re.search(predicate, item):\n                result[item] = True\n                break\n        else:\n            result[item] = False\n\n    return result"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    # Convert hexadecimal to bytes\n    bytes_obj = binascii.unhexlify(hex_string)\n\n    # Convert bytes to UTF-8 string\n    utf8_str = bytes_obj.decode('utf-8')\n\n    # Initialize dictionary to store encodings\n    encodings = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_obj).decode('utf-8'),\n        'utf-8': utf8_str,\n        'utf-16': utf8_str,\n        'utf-32': utf8_str,\n        'ASCII': 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_str),\n        'ROT13': codecs.encode(utf8_str, 'rot_13')\n    }\n\n    # Check if UTF-16, UTF-32, and ASCII encodings are possible\n    try:\n        encodings['utf-16'] = utf8_str.encode('utf-16').decode('utf-16')\n        encodings['utf-32'] = utf8_str.encode('utf-32').decode('utf-32')\n        encodings['ASCII'] = utf8_str\n        for char in utf8_str:\n            if ord(char) > 127:\n                encodings['ASCII'] = 'Not representable in ASCII'\n                break\n    except UnicodeEncodeError:\n        pass\n\n    return encodings", "raw_solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    # Convert hexadecimal to bytes\n    bytes_obj = binascii.unhexlify(hex_string)\n\n    # Convert bytes to UTF-8 string\n    utf8_str = bytes_obj.decode('utf-8')\n\n    # Initialize dictionary to store encodings\n    encodings = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_obj).decode('utf-8'),\n        'utf-8': utf8_str,\n        'utf-16': utf8_str,\n        'utf-32': utf8_str,\n        'ASCII': 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_str),\n        'ROT13': codecs.encode(utf8_str, 'rot_13')\n    }\n\n    # Check if UTF-16, UTF-32, and ASCII encodings are possible\n    try:\n        encodings['utf-16'] = utf8_str.encode('utf-16').decode('utf-16')\n        encodings['utf-32'] = utf8_str.encode('utf-32').decode('utf-32')\n        encodings['ASCII'] = utf8_str\n        for char in utf8_str:\n            if ord(char) > 127:\n                encodings['ASCII'] = 'Not representable in ASCII'\n                break\n    except UnicodeEncodeError:\n        pass\n\n    return encodings"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        return False\n\n    # Calculate the MD5 checksum of the downloaded file\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        md5 = hashlib.md5()\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            md5.update(chunk)\n    downloaded_md5_checksum = md5.hexdigest()\n\n    # Check if the checksum matches the expected value\n    if downloaded_md5_checksum != EXPECTED_MD5_CHECKSUM:\n        print(\"Checksum does not match. Deleting the file.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the contents of the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Extraction failed: {e}\")\n        return False\n\n    # Remove the downloaded file\n    os.remove(TARGET_TAR_FILE)\n\n    return True", "raw_solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        return False\n\n    # Calculate the MD5 checksum of the downloaded file\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        md5 = hashlib.md5()\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            md5.update(chunk)\n    downloaded_md5_checksum = md5.hexdigest()\n\n    # Check if the checksum matches the expected value\n    if downloaded_md5_checksum != EXPECTED_MD5_CHECKSUM:\n        print(\"Checksum does not match. Deleting the file.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the contents of the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Extraction failed: {e}\")\n        return False\n\n    # Remove the downloaded file\n    os.remove(TARGET_TAR_FILE)\n\n    return True"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except urllib.error.URLError:\n        print(\"Failed to download the file from the provided URL.\")\n        return\n\n    if not os.path.exists(csv_file_path):\n        print(\"Failed to download the file from the provided URL.\")\n        return\n\n    try:\n        with open(csv_file_path, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            column_values = [row[column_name] for row in csv_reader if column_name in row]\n\n        if not column_values:\n            raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n\n        count_dict = collections.Counter(column_values)\n\n        os.remove(csv_file_path)\n\n        return count_dict\n    except csv.Error:\n        print(\"Failed to read the CSV file.\")\n        os.remove(csv_file_path)\n        return", "raw_solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except urllib.error.URLError:\n        print(\"Failed to download the file from the provided URL.\")\n        return\n\n    if not os.path.exists(csv_file_path):\n        print(\"Failed to download the file from the provided URL.\")\n        return\n\n    try:\n        with open(csv_file_path, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            column_values = [row[column_name] for row in csv_reader if column_name in row]\n\n        if not column_values:\n            raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n\n        count_dict = collections.Counter(column_values)\n\n        os.remove(csv_file_path)\n\n        return count_dict\n    except csv.Error:\n        print(\"Failed to read the CSV file.\")\n        os.remove(csv_file_path)\n        return"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        response = urllib.request.urlopen(url)\n        xml_content = response.read()\n    except urllib.error.URLError:\n        raise ValueError(\"URL is invalid or XML file cannot be fetched from the URL.\")\n\n    try:\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError:\n        raise ValueError(\"XML file has invalid syntax.\")\n\n    items = root.findall('.//item')\n    if not items:\n        raise ValueError(\"XML structure does not match expected format.\")\n\n    data = []\n    for item in items:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        data.append(item_data)\n\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        response = urllib.request.urlopen(url)\n        xml_content = response.read()\n    except urllib.error.URLError:\n        raise ValueError(\"URL is invalid or XML file cannot be fetched from the URL.\")\n\n    try:\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError:\n        raise ValueError(\"XML file has invalid syntax.\")\n\n    items = root.findall('.//item')\n    if not items:\n        raise ValueError(\"XML structure does not match expected format.\")\n\n    data = []\n    for item in items:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        data.append(item_data)\n\n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Download the file\n    response = urllib.request.urlopen(url)\n    data = response.read()\n    text = data.decode('utf-8')\n\n    # Process the text to count the frequency of each word\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_counts = Counter(words)\n\n    # Plot a bar chart showing the ten most frequently occurring words\n    most_common_words = word_counts.most_common(10)\n    words, counts = zip(*most_common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Ten Most Common Words')\n\n    return word_counts, ax", "raw_solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Download the file\n    response = urllib.request.urlopen(url)\n    data = response.read()\n    text = data.decode('utf-8')\n\n    # Process the text to count the frequency of each word\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_counts = Counter(words)\n\n    # Plot a bar chart showing the ten most frequently occurring words\n    most_common_words = word_counts.most_common(10)\n    words, counts = zip(*most_common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Ten Most Common Words')\n\n    return word_counts, ax"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n\n        if response.headers.get('content-type') != 'application/zip':\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        file_name = url.split(\"/\")[-1]\n        file_path = os.path.join(download_path, file_name)\n\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n        try:\n            with ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.test_zip()\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        return file_path\n\n    except requests.exceptions.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n\n    except Exception as e:\n        return f\"Error: {str(e)}\"", "raw_solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n\n        if response.headers.get('content-type') != 'application/zip':\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        file_name = url.split(\"/\")[-1]\n        file_path = os.path.join(download_path, file_name)\n\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n        try:\n            with ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.test_zip()\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        return file_path\n\n    except requests.exceptions.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n\n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\nfrom requests.exceptions import HTTPError\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except HTTPError as http_err:\n        raise HTTPError(f'HTTP error occurred: {http_err}')\n    except Exception as err:\n        raise Exception(f'Other error occurred: {err}')\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if table is None:\n        raise ValueError('Table with the specified ID not found.')\n\n    csv = StringIO()\n    writer = csv.write\n    for tr in table.find_all('tr'):\n        for td in tr.find_all('td'):\n            writer(td.text.strip() + ',')\n        writer('\\n')\n    csv.seek(0)\n\n    df = pd.read_csv(csv)\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\nfrom requests.exceptions import HTTPError\n\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except HTTPError as http_err:\n        raise HTTPError(f'HTTP error occurred: {http_err}')\n    except Exception as err:\n        raise Exception(f'Other error occurred: {err}')\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if table is None:\n        raise ValueError('Table with the specified ID not found.')\n\n    csv = StringIO()\n    writer = csv.write\n    for tr in table.find_all('tr'):\n        for td in tr.find_all('td'):\n            writer(td.text.strip() + ',')\n        writer('\\n')\n    csv.seek(0)\n\n    df = pd.read_csv(csv)\n    return df"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nimport io\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    \"\"\"\n    Downloads and extracts a zip file from a specified URL.\n    \"\"\"\n    try:\n        # Create download directory if it doesn't exist\n        DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()\n\n        # Save the file\n        file_path = DOWNLOAD_DIR / filename\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n        # Extract the zip file\n        ZIP_DIR.mkdir(parents=True, exist_ok=True)\n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # Get a list of filenames in the unzipped directory\n        unzipped_files = [file.name for file in ZIP_DIR.iterdir() if file.is_file()]\n\n        return (\"Success\", unzipped_files)\n\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception\", [])\n\n    except Exception as e:\n        return (\"Error: File-related exception\", [])", "raw_solution": "import requests\nfrom pathlib import Path\nimport zipfile\nimport io\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    \"\"\"\n    Downloads and extracts a zip file from a specified URL.\n    \"\"\"\n    try:\n        # Create download directory if it doesn't exist\n        DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()\n\n        # Save the file\n        file_path = DOWNLOAD_DIR / filename\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n        # Extract the zip file\n        ZIP_DIR.mkdir(parents=True, exist_ok=True)\n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # Get a list of filenames in the unzipped directory\n        unzipped_files = [file.name for file in ZIP_DIR.iterdir() if file.is_file()]\n\n        return (\"Success\", unzipped_files)\n\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception\", [])\n\n    except Exception as e:\n        return (\"Error: File-related exception\", [])"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\n    It returns the number of unique absolute links scraped from the webpage.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Parse the content of the response with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all the <a> tags\n    links = soup.find_all('a')\n\n    # Set to store unique absolute URLs\n    unique_links = set()\n\n    # Iterate over the links\n    for link in links:\n        # Get the href attribute\n        href = link.get('href')\n\n        # If the href attribute exists and is not None\n        if href:\n            # Join the base URL and the href to get the absolute URL\n            absolute_url = urljoin(base_url, href)\n\n            # Add the absolute URL to the set\n            unique_links.add(absolute_url)\n\n    # Open the CSV file in write mode\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write the absolute URLs to the CSV file\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique absolute links\n    return len(unique_links)", "raw_solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\n    It returns the number of unique absolute links scraped from the webpage.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Parse the content of the response with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all the <a> tags\n    links = soup.find_all('a')\n\n    # Set to store unique absolute URLs\n    unique_links = set()\n\n    # Iterate over the links\n    for link in links:\n        # Get the href attribute\n        href = link.get('href')\n\n        # If the href attribute exists and is not None\n        if href:\n            # Join the base URL and the href to get the absolute URL\n            absolute_url = urljoin(base_url, href)\n\n            # Add the absolute URL to the set\n            unique_links.add(absolute_url)\n\n    # Open the CSV file in write mode\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write the absolute URLs to the CSV file\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique absolute links\n    return len(unique_links)"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\nfrom typing import Union\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> Union[int, None]:\n    try:\n        # Fetch the HTML content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Parse the HTML content\n    try:\n        parsed_content = html.fromstring(response.content)\n        table_content = parsed_content.xpath('//table')\n        if not table_content:\n            return 0\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Convert the HTML table to a pandas DataFrame\n    try:\n        df = pd.read_html(html.tostring(table_content[0]))[0]\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Connect to the SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Replace the table with new data\n    try:\n        df.to_sql(\"my_table\", conn, if_exists='replace', index=False)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Close the connection\n    conn.close()\n\n    # Return the number of rows in the parsed HTML table\n    return df.shape[0]", "raw_solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\nfrom typing import Union\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> Union[int, None]:\n    try:\n        # Fetch the HTML content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Parse the HTML content\n    try:\n        parsed_content = html.fromstring(response.content)\n        table_content = parsed_content.xpath('//table')\n        if not table_content:\n            return 0\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Convert the HTML table to a pandas DataFrame\n    try:\n        df = pd.read_html(html.tostring(table_content[0]))[0]\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Connect to the SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Replace the table with new data\n    try:\n        df.to_sql(\"my_table\", conn, if_exists='replace', index=False)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Close the connection\n    conn.close()\n\n    # Return the number of rows in the parsed HTML table\n    return df.shape[0]"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        image = Image.open(filename)\n\n        # Extract text from the image using OCR\n        ocr_text = pytesseract.image_to_string(image, lang='eng')\n\n        # If OCR extraction fails, try to extract text from the image comment\n        if not ocr_text:\n            ocr_text = image.getexif().get(306)\n\n        # If both OCR extraction and comment processing fail, return an empty string\n        if not ocr_text:\n            return \"\"\n\n        # Convert the text encoding\n        ocr_text = ocr_text.encode(from_encoding).decode(to_encoding)\n\n        return ocr_text\n\n    except (ValueError, LookupError) as e:\n        raise ValueError(f\"Error during conversion: {e}\")", "raw_solution": "from PIL import Image\nimport codecs\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        image = Image.open(filename)\n\n        # Extract text from the image using OCR\n        ocr_text = pytesseract.image_to_string(image, lang='eng')\n\n        # If OCR extraction fails, try to extract text from the image comment\n        if not ocr_text:\n            ocr_text = image.getexif().get(306)\n\n        # If both OCR extraction and comment processing fail, return an empty string\n        if not ocr_text:\n            return \"\"\n\n        # Convert the text encoding\n        ocr_text = ocr_text.encode(from_encoding).decode(to_encoding)\n\n        return ocr_text\n\n    except (ValueError, LookupError) as e:\n        raise ValueError(f\"Error during conversion: {e}\")\n\n# Test the function"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        content = response.content\n\n        if not content:\n            return {}\n\n        if not from_encoding:\n            from_encoding = chardet.detect(content)['encoding']\n\n        if not from_encoding:\n            raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        decoded_content = content.decode(from_encoding)\n        reencoded_content = decoded_content.encode(to_encoding)\n        json_data = json.loads(reencoded_content)\n\n        return json_data\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: {e}\")\n        return {}\n\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        raise", "raw_solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        content = response.content\n\n        if not content:\n            return {}\n\n        if not from_encoding:\n            from_encoding = chardet.detect(content)['encoding']\n\n        if not from_encoding:\n            raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        decoded_content = content.decode(from_encoding)\n        reencoded_content = decoded_content.encode(to_encoding)\n        json_data = json.loads(reencoded_content)\n\n        return json_data\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: {e}\")\n        return {}\n\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        raise"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads a CSV file and processes its date-related data.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    # Read the CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the CSV file.\")\n\n    # Convert date values to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n\n    # Filter rows based on the current date\n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n\n    # Sort the resulting data\n    df = df.sort_values(by=column_name)\n\n    return df", "raw_solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads a CSV file and processes its date-related data.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    # Read the CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the CSV file.\")\n\n    # Convert date values to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n\n    # Filter rows based on the current date\n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n\n    # Sort the resulting data\n    df = df.sort_values(by=column_name)\n\n    return df"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n\n    secure_socket = context.wrap_socket(client_socket, server_side=True)\n\n    data = secure_socket.recv(buffer_size).decode()\n\n    if os.path.isfile(data):\n        with open(data, 'rb') as file:\n            file_data = file.read()\n            file_hash = hashlib.sha256(file_data).hexdigest()\n            return file_hash\n    else:\n        return 'File not found'\n\n    secure_socket.close()", "raw_solution": "import ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n\n    secure_socket = context.wrap_socket(client_socket, server_side=True)\n\n    data = secure_socket.recv(buffer_size).decode()\n\n    if os.path.isfile(data):\n        with open(data, 'rb') as file:\n            file_data = file.read()\n            file_hash = hashlib.sha256(file_data).hexdigest()\n            return file_hash\n    else:\n        return 'File not found'\n\n    secure_socket.close()"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # List for sockets and their corresponding buffers\n    sockets_list = [server_socket]\n    clients_list = {}\n    start_time = datetime.now()\n\n    print(f\"Echo server started at {start_time}\")\n\n    while (datetime.now() - start_time).seconds < run_duration:\n        read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list)\n\n        # Handle received data\n        for notified_socket in read_sockets:\n            if notified_socket == server_socket:\n                client_socket, client_address = server_socket.accept()\n                sockets_list.append(client_socket)\n                clients_list[client_socket] = queue.Queue()\n            else:\n                data = notified_socket.recv(buffer_size)\n                if data:\n                    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    message = f\"{data.decode('utf-8')} - {current_time}\"\n                    notified_socket.send(message.encode('utf-8'))\n                else:\n                    sockets_list.remove(notified_socket)\n                    del clients_list[notified_socket]\n\n        # Handle exceptions\n        for notified_socket in exception_sockets:\n            sockets_list.remove(notified_socket)\n            del clients_list[notified_socket]\n\n    server_socket.close()\n    print(f\"Echo server stopped at {datetime.now()}\")\n    return f\"Echo server operation completed in {run_duration} seconds.\"", "raw_solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # List for sockets and their corresponding buffers\n    sockets_list = [server_socket]\n    clients_list = {}\n    start_time = datetime.now()\n\n    print(f\"Echo server started at {start_time}\")\n\n    while (datetime.now() - start_time).seconds < run_duration:\n        read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list)\n\n        # Handle received data\n        for notified_socket in read_sockets:\n            if notified_socket == server_socket:\n                client_socket, client_address = server_socket.accept()\n                sockets_list.append(client_socket)\n                clients_list[client_socket] = queue.Queue()\n            else:\n                data = notified_socket.recv(buffer_size)\n                if data:\n                    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    message = f\"{data.decode('utf-8')} - {current_time}\"\n                    notified_socket.send(message.encode('utf-8'))\n                else:\n                    sockets_list.remove(notified_socket)\n                    del clients_list[notified_socket]\n\n        # Handle exceptions\n        for notified_socket in exception_sockets:\n            sockets_list.remove(notified_socket)\n            del clients_list[notified_socket]\n\n    server_socket.close()\n    print(f\"Echo server stopped at {datetime.now()}\")\n    return f\"Echo server operation completed in {run_duration} seconds.\"\n\n# Test the function"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass", "raw_solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef send_email(sender, recipient, password, message):\n    \"\"\"\n    Sends an email from a client socket.\n    \"\"\"\n    msg = EmailMessage()\n    msg['Subject'] = 'Message from Client'\n    msg['From'] = sender\n    msg['To'] = recipient\n    msg.set_content(message)\n\n    server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n    server.starttls()\n    server.login(sender, password)\n    server.send_message(msg)\n    server.quit()\n\n# Test the function\nsender = input(\"Enter your email: \")\nrecipient = input(\"Enter recipient's email: \")\npassword = getpass.getpass(\"Enter your email password: \")\nmessage = \"Hello, this is a test message from a client socket.\"\nsend_email(sender, recipient, password, message)"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File {file_path} not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    # If the CSV file does not have a header, use the first column as the text data\n    if 'Text' in df.columns:\n        text_data = df['Text']\n    else:\n        text_data = df[df.columns[0]]\n\n    # Create a CountVectorizer object\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n\n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n\n    # Get the word counts\n    word_counts = vectorizer.get_feature_names_out()\n    count_values = X.toarray().sum(axis=0)\n\n    # Create a DataFrame of word counts\n    df_word_counts = pd.DataFrame({'word': word_counts, 'count': count_values})\n\n    # Get the top 10 most common words\n    top_10_words = df_word_counts.nlargest(10, 'count')\n\n    # Plot the histogram\n    plt.bar(top_10_words['word'], top_10_words['count'])\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the top 10 most common words')\n    plt.xticks(rotation=90)\n\n    if save_path is not None:\n        # Save the plot to a file\n        plt.savefig(save_path)\n        return None\n    else:\n        return plt.gca()", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File {file_path} not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    # If the CSV file does not have a header, use the first column as the text data\n    if 'Text' in df.columns:\n        text_data = df['Text']\n    else:\n        text_data = df[df.columns[0]]\n\n    # Create a CountVectorizer object\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n\n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n\n    # Get the word counts\n    word_counts = vectorizer.get_feature_names_out()\n    count_values = X.toarray().sum(axis=0)\n\n    # Create a DataFrame of word counts\n    df_word_counts = pd.DataFrame({'word': word_counts, 'count': count_values})\n\n    # Get the top 10 most common words\n    top_10_words = df_word_counts.nlargest(10, 'count')\n\n    # Plot the histogram\n    plt.bar(top_10_words['word'], top_10_words['count'])\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the top 10 most common words')\n    plt.xticks(rotation=90)\n\n    if save_path is not None:\n        # Save the plot to a file\n        plt.savefig(save_path)\n        return None\n    else:\n        return plt.gca()"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    if animals is None:\n        animals = ['Dog', 'Cat', 'Bird']\n    if foods is None:\n        foods = ['Bread', 'Milk', 'Fish']\n\n    if not animals or not foods:\n        return pd.DataFrame()\n\n    combinations = list(itertools.product(animals, foods))\n    np.random.shuffle(combinations)\n\n    df = pd.DataFrame(combinations, columns=foods)\n    df.index = animals\n\n    return df.applymap(lambda x: f\"{x[0]}:{x[1]}\")", "raw_solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    if animals is None:\n        animals = ['Dog', 'Cat', 'Bird']\n    if foods is None:\n        foods = ['Bread', 'Milk', 'Fish']\n\n    if not animals or not foods:\n        return pd.DataFrame()\n\n    combinations = list(itertools.product(animals, foods))\n    np.random.shuffle(combinations)\n\n    df = pd.DataFrame(combinations, columns=foods)\n    df.index = animals\n\n    return df.applymap(lambda x: f\"{x[0]}:{x[1]}\")"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert all timestamps to the specified timezone\n    converted_times = []\n    for time_string in time_strings:\n        time = datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S')\n        converted_time = pytz.utc.localize(time).astimezone(pytz.timezone(timezone))\n        converted_times.append(converted_time)\n\n    # Calculate the time difference in seconds between each consecutive pair of timestamps\n    time_diffs = []\n    for i in range(1, len(converted_times)):\n        diff = (converted_times[i] - converted_times[i-1]).total_seconds()\n        time_diffs.append(diff)\n\n    # Calculate and return the average time difference\n    return np.mean(time_diffs)", "raw_solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert all timestamps to the specified timezone\n    converted_times = []\n    for time_string in time_strings:\n        time = datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S')\n        converted_time = pytz.utc.localize(time).astimezone(pytz.timezone(timezone))\n        converted_times.append(converted_time)\n\n    # Calculate the time difference in seconds between each consecutive pair of timestamps\n    time_diffs = []\n    for i in range(1, len(converted_times)):\n        diff = (converted_times[i] - converted_times[i-1]).total_seconds()\n        time_diffs.append(diff)\n\n    # Calculate and return the average time difference\n    return np.mean(time_diffs)"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Split into words\n    words = text.split()\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Get the 10 most common words\n    most_common_words = word_counts.most_common(10)\n\n    # Create a bar chart\n    words, counts = zip(*most_common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n\n    return most_common_words, ax", "raw_solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Split into words\n    words = text.split()\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Get the 10 most common words\n    most_common_words = word_counts.most_common(10)\n\n    # Create a bar chart\n    words, counts = zip(*most_common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n\n    return most_common_words, ax"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Extract URL from the string\n    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    if not url:\n        return \"No valid URL found in the provided string.\"\n    url = url[0]\n\n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n    except requests.exceptions.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # Parse the content and extract the title\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.find('title')\n    if not title:\n        return \"No title tag found in the webpage.\"\n    return title.text", "raw_solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Extract URL from the string\n    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    if not url:\n        return \"No valid URL found in the provided string.\"\n    url = url[0]\n\n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n    except requests.exceptions.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # Parse the content and extract the title\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.find('title')\n    if not title:\n        return \"No title tag found in the webpage.\"\n    return title.text"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with a specified key, downloads the file from the URL, and saves it with a timestamped filename.\n    \"\"\"\n    # Parse JSON data\n    data = json.loads(json_data)\n\n    # Get URL associated with the specified key\n    url = data[unknown_key]\n\n    # Download file from URL\n    response = requests.get(url)\n\n    # Get current timestamp\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n\n    # Set default save directory if not specified\n    if save_dir is None:\n        save_dir = os.getcwd()\n\n    # Create filename\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n\n    # Save file\n    with open(os.path.join(save_dir, filename), 'wb') as f:\n        f.write(response.content)\n\n    # Return absolute path of the downloaded file\n    return os.path.abspath(os.path.join(save_dir, filename))", "raw_solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with a specified key, downloads the file from the URL, and saves it with a timestamped filename.\n    \"\"\"\n    # Parse JSON data\n    data = json.loads(json_data)\n\n    # Get URL associated with the specified key\n    url = data[unknown_key]\n\n    # Download file from URL\n    response = requests.get(url)\n\n    # Get current timestamp\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n\n    # Set default save directory if not specified\n    if save_dir is None:\n        save_dir = os.getcwd()\n\n    # Create filename\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n\n    # Save file\n    with open(os.path.join(save_dir, filename), 'wb') as f:\n        f.write(response.content)\n\n    # Return absolute path of the downloaded file\n    return os.path.abspath(os.path.join(save_dir, filename))"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n    \"\"\"\n    if \"http\" in url:\n        response = requests.get(url)\n        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    else:\n        with open(url, 'r') as file:\n            soup = bs4.BeautifulSoup(file.read(), 'html.parser')\n\n    phone_numbers = re.findall(r'\\(?\\b[2-9][0-9]{2}\\)?[-. ]?[2-9][0-9]{2}[-. ]?[0-9]{4}\\b', soup.text)\n\n    with open(output_path, 'w') as file:\n        json.dump(phone_numbers, file)\n\n    return phone_numbers", "raw_solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n    \"\"\"\n    if \"http\" in url:\n        response = requests.get(url)\n        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    else:\n        with open(url, 'r') as file:\n            soup = bs4.BeautifulSoup(file.read(), 'html.parser')\n\n    phone_numbers = re.findall(r'\\(?\\b[2-9][0-9]{2}\\)?[-. ]?[2-9][0-9]{2}[-. ]?[0-9]{4}\\b', soup.text)\n\n    with open(output_path, 'w') as file:\n        json.dump(phone_numbers, file)\n\n    return phone_numbers"}
