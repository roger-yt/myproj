{"task_id": "BigCodeBench/0", "solution": "import itertools\nfrom random import shuffle\ndef task_func(numbers=list(range(1, 11))):\n    # Calculate the sum of absolute differences for each pair of consecutive numbers\n    def sum_abs_diff(nums):\n        return sum(abs(nums[i] - nums[i+1]) for i in range(len(nums)-1))\n\n    # Calculate the average of the sums of absolute differences for all permutations\n    def avg_sum_abs_diff(nums):\n        return sum(sum_abs_diff(nums) for _ in range(1000)) / 1000\n\n    # Shuffle the list and calculate the average sum of absolute differences\n    shuffled_nums = numbers[:]\n    total_avg = 0\n    for _ in range(1000):\n        shuffle(shuffled_nums)\n        total_avg += avg_sum_abs_diff(shuffled_nums)\n\n    return total_avg / 1000", "raw_solution": "import itertools\nfrom random import shuffle\n\ndef task_func(numbers=list(range(1, 11))):\n    # Calculate the sum of absolute differences for each pair of consecutive numbers\n    def sum_abs_diff(nums):\n        return sum(abs(nums[i] - nums[i+1]) for i in range(len(nums)-1))\n\n    # Calculate the average of the sums of absolute differences for all permutations\n    def avg_sum_abs_diff(nums):\n        return sum(sum_abs_diff(nums) for _ in range(1000)) / 1000\n\n    # Shuffle the list and calculate the average sum of absolute differences\n    shuffled_nums = numbers[:]\n    total_avg = 0\n    for _ in range(1000):\n        shuffle(shuffled_nums)\n        total_avg += avg_sum_abs_diff(shuffled_nums)\n\n    return total_avg / 1000\n\n# Test the function"}
{"task_id": "BigCodeBench/1", "solution": "import collections\nimport random\nimport string\ndef task_func(length=100):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n    random_string = ''.join(random.choice(string.ascii_letters) for _ in range(length))\n    count_dict = collections.Counter(random_string)\n    return count_dict", "raw_solution": "import collections\nimport random\nimport string\n\ndef task_func(length=100):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n    random_string = ''.join(random.choice(string.ascii_letters) for _ in range(length))\n    count_dict = collections.Counter(random_string)\n    return count_dict"}
{"task_id": "BigCodeBench/2", "solution": "import random\nimport statistics\ndef task_func(LETTERS):\n    # Create a dictionary with random letters as keys and lists of random integers as values\n    random_dict = {chr(65 + i): [random.randint(1, 100) for _ in range(10)] for i in range(LETTERS)}\n\n    # Calculate the mean of the values in the dictionary and sort it in descending order\n    sorted_dict = {k: v for k, v in sorted(random_dict.items(), key=lambda item: statistics.mean(item[1]), reverse=True)}\n\n    return sorted_dict", "raw_solution": "import random\nimport statistics\n\ndef task_func(LETTERS):\n    # Create a dictionary with random letters as keys and lists of random integers as values\n    random_dict = {chr(65 + i): [random.randint(1, 100) for _ in range(10)] for i in range(LETTERS)}\n\n    # Calculate the mean of the values in the dictionary and sort it in descending order\n    sorted_dict = {k: v for k, v in sorted(random_dict.items(), key=lambda item: statistics.mean(item[1]), reverse=True)}\n\n    return sorted_dict\n\n# Test the function"}
{"task_id": "BigCodeBench/3", "solution": "import random\nimport numpy as np\ndef task_func(LETTERS):\n    random.seed(1)\n    result = {}\n    for letter in LETTERS:\n        random_list = [random.randint(0, 100) for _ in range(random.randint(1, 10))]\n        result[letter] = np.mean(random_list)\n    return result\nLETTERS = ['A', 'B', 'C', 'D', 'E']", "raw_solution": "import random\nimport numpy as np\n\ndef task_func(LETTERS):\n    random.seed(1)\n    result = {}\n    for letter in LETTERS:\n        random_list = [random.randint(0, 100) for _ in range(random.randint(1, 10))]\n        result[letter] = np.mean(random_list)\n    return result\n\nLETTERS = ['A', 'B', 'C', 'D', 'E']"}
{"task_id": "BigCodeBench/4", "solution": "from collections import Counter\nimport itertools\ndef task_func(d):\n    # Flatten the list of values in the dictionary\n    flat_list = list(itertools.chain.from_iterable(d.values()))\n\n    # Count the occurrence of each integer in the flattened list\n    count_dict = dict(Counter(flat_list))\n\n    return count_dict", "raw_solution": "from collections import Counter\nimport itertools\n\ndef task_func(d):\n    # Flatten the list of values in the dictionary\n    flat_list = list(itertools.chain.from_iterable(d.values()))\n\n    # Count the occurrence of each integer in the flattened list\n    count_dict = dict(Counter(flat_list))\n\n    return count_dict"}
{"task_id": "BigCodeBench/5", "solution": "import random\nimport math\nimport statistics\ndef task_func(LETTERS=[chr(i) for i in range(97, 123)]):\n    random.seed(1)\n    dict_std_dev = {}\n    for letter in LETTERS:\n        list_size = random.randint(1, 10)\n        random_list = [random.randint(0, 100) for _ in range(list_size)]\n        dict_std_dev[letter] = statistics.pstdev(random_list)\n    return dict_std_dev", "raw_solution": "import random\nimport math\nimport statistics\n\ndef task_func(LETTERS=[chr(i) for i in range(97, 123)]):\n    random.seed(1)\n    dict_std_dev = {}\n    for letter in LETTERS:\n        list_size = random.randint(1, 10)\n        random_list = [random.randint(0, 100) for _ in range(list_size)]\n        dict_std_dev[letter] = statistics.pstdev(random_list)\n    return dict_std_dev\n\n# Test the function"}
{"task_id": "BigCodeBench/6", "solution": "import os\nimport re\nfrom typing import Optional\ndef task_func(pattern: str, log_dir: str = '/var/log/') -> Optional[str]:\n    \"\"\"\n    Finds the latest log file in a specified directory that matches a given regex pattern.\n    :param pattern: The regex pattern to match.\n    :param log_dir: The directory to search for log files. Defaults to '/var/log/'.\n    :return: The path to the most recent log file that matches the pattern, or None if no matching files are found.\n    \"\"\"\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # Initialize variables to store the latest file and its modification time\n    latest_file = None\n    latest_mtime = 0\n\n    # Iterate over all files in the log directory\n    for filename in os.listdir(log_dir):\n        # Check if the file matches the regex pattern\n        if regex.match(filename):\n            # Get the file's modification time\n            mtime = os.path.getmtime(os.path.join(log_dir, filename))\n\n            # If the file's modification time is newer than the current latest file, update the latest file and its modification time\n            if mtime > latest_mtime:\n                latest_file = os.path.join(log_dir, filename)\n                latest_mtime = mtime\n\n    # Return the path to the latest file or None if no files match the pattern\n    return latest_file", "raw_solution": "import os\nimport re\nfrom typing import Optional\n\ndef task_func(pattern: str, log_dir: str = '/var/log/') -> Optional[str]:\n    \"\"\"\n    Finds the latest log file in a specified directory that matches a given regex pattern.\n    :param pattern: The regex pattern to match.\n    :param log_dir: The directory to search for log files. Defaults to '/var/log/'.\n    :return: The path to the most recent log file that matches the pattern, or None if no matching files are found.\n    \"\"\"\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # Initialize variables to store the latest file and its modification time\n    latest_file = None\n    latest_mtime = 0\n\n    # Iterate over all files in the log directory\n    for filename in os.listdir(log_dir):\n        # Check if the file matches the regex pattern\n        if regex.match(filename):\n            # Get the file's modification time\n            mtime = os.path.getmtime(os.path.join(log_dir, filename))\n\n            # If the file's modification time is newer than the current latest file, update the latest file and its modification time\n            if mtime > latest_mtime:\n                latest_file = os.path.join(log_dir, filename)\n                latest_mtime = mtime\n\n    # Return the path to the latest file or None if no files match the pattern\n    return latest_file"}
{"task_id": "BigCodeBench/7", "solution": "import csv\nimport collections\nimport operator\ndef task_func(csv_file_path):\n    \"\"\"\n    Finds the best-selling product from a given CSV file with sales data.\n    The CSV file is assumed to have a header followed by rows containing two columns: 'product' and 'quantity'.\n    It computes the total sales per product and determines the product with the highest cumulative sales.\n    Args:\n        csv_file_path (str): The file path to the CSV file containing sales data.\n    Returns:\n        str: The name of the top-selling product based on the total quantity sold.\n    \"\"\"\n    with open(csv_file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        sales = collections.defaultdict(int)\n        for row in reader:\n            product = row['product']\n            quantity = int(row['quantity'])\n            sales[product] += quantity\n    return max(sales.items(), key=operator.itemgetter(1))[0]", "raw_solution": "import csv\nimport collections\nimport operator\n\ndef task_func(csv_file_path):\n    \"\"\"\n    Finds the best-selling product from a given CSV file with sales data.\n    The CSV file is assumed to have a header followed by rows containing two columns: 'product' and 'quantity'.\n    It computes the total sales per product and determines the product with the highest cumulative sales.\n    Args:\n        csv_file_path (str): The file path to the CSV file containing sales data.\n    Returns:\n        str: The name of the top-selling product based on the total quantity sold.\n    \"\"\"\n    with open(csv_file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        sales = collections.defaultdict(int)\n        for row in reader:\n            product = row['product']\n            quantity = int(row['quantity'])\n            sales[product] += quantity\n    return max(sales.items(), key=operator.itemgetter(1))[0]"}
{"task_id": "BigCodeBench/8", "solution": "from collections import Counter\nimport itertools\nfrom random import randint\ndef task_func(T1, RANGE=100):\n    # Convert elements in 'T1' to integers\n    T1 = list(map(int, T1))\n\n    # Create a list of random integers\n    random_list = [randint(0, RANGE) for _ in range(sum(T1))]\n\n    # Count the occurrences of each number in the generated list\n    counter = Counter(random_list)\n\n    return counter", "raw_solution": "from collections import Counter\nimport itertools\nfrom random import randint\n\ndef task_func(T1, RANGE=100):\n    # Convert elements in 'T1' to integers\n    T1 = list(map(int, T1))\n\n    # Create a list of random integers\n    random_list = [randint(0, RANGE) for _ in range(sum(T1))]\n\n    # Count the occurrences of each number in the generated list\n    counter = Counter(random_list)\n\n    return counter"}
{"task_id": "BigCodeBench/9", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(list_of_pairs):\n    # Create a DataFrame from the list of pairs\n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n\n    # Create a bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.barplot(x='Category', y='Value', data=df, ax=ax)\n    ax.set_title('Category vs Value')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(list_of_pairs):\n    # Create a DataFrame from the list of pairs\n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n\n    # Create a bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.barplot(x='Category', y='Value', data=df, ax=ax)\n    ax.set_title('Category vs Value')\n\n    return df, ax"}
{"task_id": "BigCodeBench/10", "solution": "import numpy as np\nimport itertools\nimport random\nimport statistics\nfrom collections import Counter\ndef task_func(T1, RANGE=100):\n    if not T1:\n        raise statistics.StatisticsError(\"T1 is empty\")\n    T1 = [int(i) for i in T1]\n    size = sum(T1)\n    random_list = [random.randint(0, RANGE) for _ in range(size)]\n    mean = np.mean(random_list)\n    median = np.median(random_list)\n    mode = statistics.mode(random_list)\n    return (mean, median, mode)", "raw_solution": "import numpy as np\nimport itertools\nimport random\nimport statistics\nfrom collections import Counter\n\ndef task_func(T1, RANGE=100):\n    if not T1:\n        raise statistics.StatisticsError(\"T1 is empty\")\n    T1 = [int(i) for i in T1]\n    size = sum(T1)\n    random_list = [random.randint(0, RANGE) for _ in range(size)]\n    mean = np.mean(random_list)\n    median = np.median(random_list)\n    mode = statistics.mode(random_list)\n    return (mean, median, mode)"}
{"task_id": "BigCodeBench/11", "solution": "import numpy as np\nimport itertools\nimport random\ndef task_func(T1, max_value=100):\n    # Convert elements in T1 to integers\n    T1 = [int(i) for i in itertools.chain.from_iterable(T1)]\n\n    # Create a list of random integers with size equal to the sum of T1\n    random_list = [random.randint(0, max_value) for _ in range(sum(T1))]\n\n    # Calculate 25th, 50th, and 75th percentiles of the list\n    p25, p50, p75 = np.percentile(random_list, [25, 50, 75])\n\n    return (p25, p50, p75)", "raw_solution": "import numpy as np\nimport itertools\nimport random\n\ndef task_func(T1, max_value=100):\n    # Convert elements in T1 to integers\n    T1 = [int(i) for i in itertools.chain.from_iterable(T1)]\n\n    # Create a list of random integers with size equal to the sum of T1\n    random_list = [random.randint(0, max_value) for _ in range(sum(T1))]\n\n    # Calculate 25th, 50th, and 75th percentiles of the list\n    p25, p50, p75 = np.percentile(random_list, [25, 50, 75])\n\n    return (p25, p50, p75)"}
{"task_id": "BigCodeBench/12", "solution": "import subprocess\nimport os\nimport json\nfrom datetime import datetime\ndef task_func(script_name='backup.sh', log_file='/home/user/backup_log.json'):\n    if not os.path.exists(script_name):\n        raise FileNotFoundError(f\"Script file {script_name} does not exist.\")\n\n    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    try:\n        subprocess.check_output(['bash', script_name])\n        exit_status = 0\n    except subprocess.CalledProcessError as e:\n        exit_status = e.returncode\n        raise RuntimeError(f\"Error executing script: {e}\")\n\n    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    with open(log_file, 'w') as file:\n        json.dump({\n            'start_time': start_time,\n            'end_time': end_time,\n            'exit_status': exit_status\n        }, file)\n\n    return {\n        'start_time': start_time,\n        'end_time': end_time,\n        'exit_status': exit_status\n    }", "raw_solution": "import subprocess\nimport os\nimport json\nfrom datetime import datetime\n\ndef task_func(script_name='backup.sh', log_file='/home/user/backup_log.json'):\n    if not os.path.exists(script_name):\n        raise FileNotFoundError(f\"Script file {script_name} does not exist.\")\n\n    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    try:\n        subprocess.check_output(['bash', script_name])\n        exit_status = 0\n    except subprocess.CalledProcessError as e:\n        exit_status = e.returncode\n        raise RuntimeError(f\"Error executing script: {e}\")\n\n    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    with open(log_file, 'w') as file:\n        json.dump({\n            'start_time': start_time,\n            'end_time': end_time,\n            'exit_status': exit_status\n        }, file)\n\n    return {\n        'start_time': start_time,\n        'end_time': end_time,\n        'exit_status': exit_status\n    }"}
{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    \"\"\"\n    Downloads all files from a specific directory on an FTP server using wget.\n    Args:\n        ftp_server (str): The FTP server address. Default is 'ftp.dlptest.com'.\n        ftp_user (str): The FTP server username. Default is 'dlpuser'.\n        ftp_password (str): The FTP server password. Default is 'rNrKYTX9g7z3RgJRmxWuGHbeu'.\n        ftp_dir (str): The directory path on the FTP server from which files need to be downloaded. Default is '/ftp/test'.\n    Returns:\n        List[str]: A list of filenames that were attempted to be downloaded from the FTP server.\n    \"\"\"\n    # Connect to the FTP server\n    try:\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n\n    # Login to the FTP server\n    try:\n        ftp.login(ftp_user, ftp_password)\n    except Exception as e:\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n\n    # Change to the specified directory\n    try:\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    # Get a list of files in the directory\n    files = []\n    try:\n        ftp.retrlines('LIST', files.append)\n    except Exception as e:\n        raise Exception(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    # Download each file\n    attempted_files = []\n    for file in files:\n        filename = file.split()[-1]\n        attempted_files.append(filename)\n        try:\n            with open(filename, 'wb') as f:\n                ftp.retrbinary('RETR ' + filename, f.write)\n        except Exception as e:\n            print(f\"Failed to download file {filename} from server {ftp_server}: {str(e)}\")\n\n    # Quit the FTP session\n    ftp.quit()\n\n    return attempted_files", "raw_solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    \"\"\"\n    Downloads all files from a specific directory on an FTP server using wget.\n    Args:\n        ftp_server (str): The FTP server address. Default is 'ftp.dlptest.com'.\n        ftp_user (str): The FTP server username. Default is 'dlpuser'.\n        ftp_password (str): The FTP server password. Default is 'rNrKYTX9g7z3RgJRmxWuGHbeu'.\n        ftp_dir (str): The directory path on the FTP server from which files need to be downloaded. Default is '/ftp/test'.\n    Returns:\n        List[str]: A list of filenames that were attempted to be downloaded from the FTP server.\n    \"\"\"\n    # Connect to the FTP server\n    try:\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n\n    # Login to the FTP server\n    try:\n        ftp.login(ftp_user, ftp_password)\n    except Exception as e:\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n\n    # Change to the specified directory\n    try:\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    # Get a list of files in the directory\n    files = []\n    try:\n        ftp.retrlines('LIST', files.append)\n    except Exception as e:\n        raise Exception(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    # Download each file\n    attempted_files = []\n    for file in files:\n        filename = file.split()[-1]\n        attempted_files.append(filename)\n        try:\n            with open(filename, 'wb') as f:\n                ftp.retrbinary('RETR ' + filename, f.write)\n        except Exception as e:\n            print(f\"Failed to download file {filename} from server {ftp_server}: {str(e)}\")\n\n    # Quit the FTP session\n    ftp.quit()\n\n    return attempted_files"}
{"task_id": "BigCodeBench/14", "solution": "import configparser\nimport os\nimport shutil\nimport zipfile\ndef task_func(config_file_path, archive_dir='/home/user/archive'):\n    \"\"\"\n    Archives a specified project directory into a ZIP file based on the configuration specified in a config file.\n    \"\"\"\n    # Check if the config file exists\n    if not os.path.exists(config_file_path):\n        raise FileNotFoundError(f\"The config file {config_file_path} does not exist.\")\n\n    # Read the config file\n    config = configparser.ConfigParser()\n    config.read(config_file_path)\n\n    # Check if the project directory exists\n    project_dir = config.get('Project', 'directory', fallback=None)\n    if not project_dir or not os.path.exists(project_dir):\n        raise FileNotFoundError(f\"The project directory {project_dir} does not exist.\")\n\n    # Create the archive directory if it doesn't exist\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n\n    # Create the ZIP file\n    zip_file_path = os.path.join(archive_dir, os.path.basename(project_dir) + '.zip')\n    try:\n        with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for root, dirs, files in os.walk(project_dir):\n                for file in files:\n                    zipf.write(os.path.join(root, file),\n                               os.path.relpath(os.path.join(root, file),\n                                               os.path.join(project_dir, '..')))\n    except Exception as e:\n        raise Exception(f\"Failed to create the ZIP archive: {e}\")\n\n    return True", "raw_solution": "import configparser\nimport os\nimport shutil\nimport zipfile\n\ndef task_func(config_file_path, archive_dir='/home/user/archive'):\n    \"\"\"\n    Archives a specified project directory into a ZIP file based on the configuration specified in a config file.\n    \"\"\"\n    # Check if the config file exists\n    if not os.path.exists(config_file_path):\n        raise FileNotFoundError(f\"The config file {config_file_path} does not exist.\")\n\n    # Read the config file\n    config = configparser.ConfigParser()\n    config.read(config_file_path)\n\n    # Check if the project directory exists\n    project_dir = config.get('Project', 'directory', fallback=None)\n    if not project_dir or not os.path.exists(project_dir):\n        raise FileNotFoundError(f\"The project directory {project_dir} does not exist.\")\n\n    # Create the archive directory if it doesn't exist\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n\n    # Create the ZIP file\n    zip_file_path = os.path.join(archive_dir, os.path.basename(project_dir) + '.zip')\n    try:\n        with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for root, dirs, files in os.walk(project_dir):\n                for file in files:\n                    zipf.write(os.path.join(root, file),\n                               os.path.relpath(os.path.join(root, file),\n                                               os.path.join(project_dir, '..')))\n    except Exception as e:\n        raise Exception(f\"Failed to create the ZIP archive: {e}\")\n\n    return True"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"File {commands_file_path} does not exist.\")\n\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_file_paths = []\n\n    with open(commands_file_path, 'r') as file:\n        reader = csv.reader(file)\n        for i, row in enumerate(reader):\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{i}_output.txt')\n            output_file_paths.append(output_file_path)\n\n            try:\n                result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout.decode('utf-8'))\n            except subprocess.CalledProcessError as e:\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(f\"Command failed with exit code {e.returncode}.\\nError message: {e.output.decode('utf-8')}\")\n\n    return output_file_paths", "raw_solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"File {commands_file_path} does not exist.\")\n\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_file_paths = []\n\n    with open(commands_file_path, 'r') as file:\n        reader = csv.reader(file)\n        for i, row in enumerate(reader):\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{i}_output.txt')\n            output_file_paths.append(output_file_path)\n\n            try:\n                result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout.decode('utf-8'))\n            except subprocess.CalledProcessError as e:\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(f\"Command failed with exit code {e.returncode}.\\nError message: {e.output.decode('utf-8')}\")\n\n    return output_file_paths"}
{"task_id": "BigCodeBench/16", "solution": "import os\nimport glob\nimport subprocess\ndef task_func(directory, backup_dir='/path/to/backup'):\n    \"\"\"\n    Backup all '.log' files in a specified directory to a tar.gz file and delete the original files after backup.\n    The backup file is named 'logs_backup.tar.gz' and placed in the specified backup directory.\n    The function raises FileNotFoundError if the specified directory does not exist.\n    The function returns a message 'No logs found to backup' if no '.log' files are found in the directory.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"Directory {directory} does not exist\")\n\n    log_files = glob.glob(os.path.join(directory, '*.log'))\n\n    if not log_files:\n        return 'No logs found to backup'\n\n    backup_file = os.path.join(backup_dir, 'logs_backup.tar.gz')\n    subprocess.run(['tar', '-czvf', backup_file] + log_files, check=True)\n\n    for log_file in log_files:\n        os.remove(log_file)\n\n    return backup_file", "raw_solution": "import os\nimport glob\nimport subprocess\n\ndef task_func(directory, backup_dir='/path/to/backup'):\n    \"\"\"\n    Backup all '.log' files in a specified directory to a tar.gz file and delete the original files after backup.\n    The backup file is named 'logs_backup.tar.gz' and placed in the specified backup directory.\n    The function raises FileNotFoundError if the specified directory does not exist.\n    The function returns a message 'No logs found to backup' if no '.log' files are found in the directory.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"Directory {directory} does not exist\")\n\n    log_files = glob.glob(os.path.join(directory, '*.log'))\n\n    if not log_files:\n        return 'No logs found to backup'\n\n    backup_file = os.path.join(backup_dir, 'logs_backup.tar.gz')\n    subprocess.run(['tar', '-czvf', backup_file] + log_files, check=True)\n\n    for log_file in log_files:\n        os.remove(log_file)\n\n    return backup_file"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # If process is found, kill it\n            subprocess.run(['taskkill', '/F', '/IM', process_name])\n            return f\"Process found. Restarting {process_name}.\"\n\n    # If process is not found, start it\n    subprocess.Popen(process_name)\n    time.sleep(1)  # Wait for the process to start\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            return f\"Process not found. Starting {process_name}.\"", "raw_solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # If process is found, kill it\n            subprocess.run(['taskkill', '/F', '/IM', process_name])\n            return f\"Process found. Restarting {process_name}.\"\n\n    # If process is not found, start it\n    subprocess.Popen(process_name)\n    time.sleep(1)  # Wait for the process to start\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            return f\"Process not found. Starting {process_name}.\"\n\n# Test cases"}
{"task_id": "BigCodeBench/18", "solution": "import subprocess\nimport csv\nimport glob\nimport random\nimport os\ndef task_func(file):\n    \"\"\"\n    Divides a CSV file into several smaller files and shuffles the lines in each file.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(file):\n        return []\n\n    # Check if file is a CSV file\n    if not file.endswith('.csv'):\n        return []\n\n    # Read CSV file\n    try:\n        with open(file, 'r') as f:\n            reader = csv.reader(f)\n            data = list(reader)\n    except Exception as e:\n        print(f\"Error reading CSV file: {e}\")\n        return []\n\n    # Shuffle data\n    random.shuffle(data)\n\n    # Split data into smaller files\n    split_files = []\n    try:\n        with open('split_file.csv', 'w', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerows(data)\n\n        # Use shell 'split' command to split file\n        subprocess.run(['split', '-l', '100', 'split_file.csv', 'split_file'])\n\n        # Get paths to split files\n        split_files = glob.glob('split_file*')\n\n        # Remove temporary file\n        os.remove('split_file.csv')\n    except Exception as e:\n        print(f\"Error splitting and shuffling CSV file: {e}\")\n        return []\n\n    return split_files", "raw_solution": "import subprocess\nimport csv\nimport glob\nimport random\nimport os\n\ndef task_func(file):\n    \"\"\"\n    Divides a CSV file into several smaller files and shuffles the lines in each file.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(file):\n        return []\n\n    # Check if file is a CSV file\n    if not file.endswith('.csv'):\n        return []\n\n    # Read CSV file\n    try:\n        with open(file, 'r') as f:\n            reader = csv.reader(f)\n            data = list(reader)\n    except Exception as e:\n        print(f\"Error reading CSV file: {e}\")\n        return []\n\n    # Shuffle data\n    random.shuffle(data)\n\n    # Split data into smaller files\n    split_files = []\n    try:\n        with open('split_file.csv', 'w', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerows(data)\n\n        # Use shell 'split' command to split file\n        subprocess.run(['split', '-l', '100', 'split_file.csv', 'split_file'])\n\n        # Get paths to split files\n        split_files = glob.glob('split_file*')\n\n        # Remove temporary file\n        os.remove('split_file.csv')\n    except Exception as e:\n        print(f\"Error splitting and shuffling CSV file: {e}\")\n        return []\n\n    return split_files"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    \"\"\"\n    Zips all files (not including subdirectories) located in the specified directory and returns the path to the created zip file.\n    If the specified directory does not exist, it raises a FileNotFoundError.\n    If the directory does not contain any files, it returns None.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    files = glob.glob(os.path.join(directory, \"*\"))\n    files = [f for f in files if os.path.isfile(f)]\n\n    if not files:\n        return None\n\n    zip_path = os.path.join(directory, \"files.zip\")\n    with zipfile.ZipFile(zip_path, \"w\") as zipf:\n        for file in files:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return zip_path", "raw_solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    \"\"\"\n    Zips all files (not including subdirectories) located in the specified directory and returns the path to the created zip file.\n    If the specified directory does not exist, it raises a FileNotFoundError.\n    If the directory does not contain any files, it returns None.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    files = glob.glob(os.path.join(directory, \"*\"))\n    files = [f for f in files if os.path.isfile(f)]\n\n    if not files:\n        return None\n\n    zip_path = os.path.join(directory, \"files.zip\")\n    with zipfile.ZipFile(zip_path, \"w\") as zipf:\n        for file in files:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return zip_path"}
{"task_id": "BigCodeBench/20", "solution": "import ast\nimport pandas as pd\nimport seaborn as sns\ndef task_func(csv_file):\n    # Read the CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert string representations of dictionaries to Python dictionaries\n    df['dict_column'] = df['dict_column'].apply(lambda x: ast.literal_eval(x))\n\n    # Visualize the data with Seaborn's pairplot\n    ax = sns.pairplot(df)\n\n    return (df, ax)", "raw_solution": "import ast\nimport pandas as pd\nimport seaborn as sns\n\ndef task_func(csv_file):\n    # Read the CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert string representations of dictionaries to Python dictionaries\n    df['dict_column'] = df['dict_column'].apply(lambda x: ast.literal_eval(x))\n\n    # Visualize the data with Seaborn's pairplot\n    ax = sns.pairplot(df)\n\n    return (df, ax)"}
{"task_id": "BigCodeBench/21", "solution": "import psutil\nimport platform\ndef task_func():\n    \"\"\"\n    Gathers system details, including operating system, architecture, and memory usage.\n    Returns a dictionary with specific keys for each piece of information.\n    \"\"\"\n    # Get system details\n    os_name = platform.system()\n    architecture = platform.architecture()[0]\n\n    # Get memory details\n    memory = psutil.virtual_memory()\n    total_memory = memory.total / (1024.0 ** 3)  # Convert bytes to GB\n    used_memory = memory.used / (1024.0 ** 3)  # Convert bytes to GB\n\n    # Calculate memory usage percentage\n    memory_usage = (used_memory / total_memory) * 100\n\n    # Return details in a dictionary\n    return {\n        'OS': os_name,\n        'Architecture': architecture,\n        'Memory Usage': f'{memory_usage:.2f}%'\n    }", "raw_solution": "import psutil\nimport platform\n\ndef task_func():\n    \"\"\"\n    Gathers system details, including operating system, architecture, and memory usage.\n    Returns a dictionary with specific keys for each piece of information.\n    \"\"\"\n    # Get system details\n    os_name = platform.system()\n    architecture = platform.architecture()[0]\n\n    # Get memory details\n    memory = psutil.virtual_memory()\n    total_memory = memory.total / (1024.0 ** 3)  # Convert bytes to GB\n    used_memory = memory.used / (1024.0 ** 3)  # Convert bytes to GB\n\n    # Calculate memory usage percentage\n    memory_usage = (used_memory / total_memory) * 100\n\n    # Return details in a dictionary\n    return {\n        'OS': os_name,\n        'Architecture': architecture,\n        'Memory Usage': f'{memory_usage:.2f}%'\n    }\n\n# Test the function"}
{"task_id": "BigCodeBench/22", "solution": "import collections\nfrom itertools import zip_longest\nfrom random import choices\ndef task_func(l1, l2, K=10):\n    # Combine two lists by alternating their elements\n    combined = [item for sublist in zip_longest(l1, l2) for item in sublist if item is not None]\n    \n    # Create a random sample of size K from the combined list\n    sample = choices(combined, k=K)\n    \n    # Calculate the frequency of each element in the sample\n    freq = collections.Counter(sample)\n    \n    return freq", "raw_solution": "import collections\nfrom itertools import zip_longest\nfrom random import choices\n\ndef task_func(l1, l2, K=10):\n    # Combine two lists by alternating their elements\n    combined = [item for sublist in zip_longest(l1, l2) for item in sublist if item is not None]\n    \n    # Create a random sample of size K from the combined list\n    sample = choices(combined, k=K)\n    \n    # Calculate the frequency of each element in the sample\n    freq = collections.Counter(sample)\n    \n    return freq"}
{"task_id": "BigCodeBench/23", "solution": "import numpy as np\nfrom itertools import zip_longest\ndef task_func(l1, l2, THRESHOLD=0.5):\n    # Filter out non-numeric elements\n    l1 = [x for x in l1 if isinstance(x, (int, float))]\n    l2 = [x for x in l2 if isinstance(x, (int, float))]\n\n    # Alternate elements from the two lists\n    l1_l2 = [x for pair in zip_longest(l1, l2) for x in pair if x is not None]\n\n    # Calculate the absolute difference from the threshold\n    diffs = [abs(x - THRESHOLD) for x in l1_l2]\n\n    # Return the element closest to the threshold\n    return l1_l2[np.argmin(diffs)]\nl1 = [0.1, 0.2, 0.3, 0.4, 0.5]\nl2 = [0.6, 0.7, 0.8, 0.9, 1.0]", "raw_solution": "import numpy as np\nfrom itertools import zip_longest\n\ndef task_func(l1, l2, THRESHOLD=0.5):\n    # Filter out non-numeric elements\n    l1 = [x for x in l1 if isinstance(x, (int, float))]\n    l2 = [x for x in l2 if isinstance(x, (int, float))]\n\n    # Alternate elements from the two lists\n    l1_l2 = [x for pair in zip_longest(l1, l2) for x in pair if x is not None]\n\n    # Calculate the absolute difference from the threshold\n    diffs = [abs(x - THRESHOLD) for x in l1_l2]\n\n    # Return the element closest to the threshold\n    return l1_l2[np.argmin(diffs)]\n\n# Test the function\nl1 = [0.1, 0.2, 0.3, 0.4, 0.5]\nl2 = [0.6, 0.7, 0.8, 0.9, 1.0]"}
{"task_id": "BigCodeBench/24", "solution": "import base64\nimport hashlib\nimport os\nfrom typing import Tuple\ndef task_func(password: str, SALT_LENGTH: int = 32) -> Tuple[bytes, bytes]:\n    \"\"\"\n    Hashes a password using the PBKDF2 HMAC algorithm with SHA-256 as the hashing algorithm,\n    combined with a randomly generated salt, and returns both the salt and the hashed password,\n    each base64-encoded.\n    \"\"\"\n    if not password:\n        raise ValueError(\"Password cannot be None or empty\")\n\n    # Generate a random salt\n    salt = os.urandom(SALT_LENGTH)\n\n    # Hash the password with the salt\n    hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n\n    # Return the salt and hashed password, base64-encoded\n    return base64.b64encode(salt), base64.b64encode(hashed_password)", "raw_solution": "import base64\nimport hashlib\nimport os\nfrom typing import Tuple\n\ndef task_func(password: str, SALT_LENGTH: int = 32) -> Tuple[bytes, bytes]:\n    \"\"\"\n    Hashes a password using the PBKDF2 HMAC algorithm with SHA-256 as the hashing algorithm,\n    combined with a randomly generated salt, and returns both the salt and the hashed password,\n    each base64-encoded.\n    \"\"\"\n    if not password:\n        raise ValueError(\"Password cannot be None or empty\")\n\n    # Generate a random salt\n    salt = os.urandom(SALT_LENGTH)\n\n    # Hash the password with the salt\n    hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n\n    # Return the salt and hashed password, base64-encoded\n    return base64.b64encode(salt), base64.b64encode(hashed_password)"}
{"task_id": "BigCodeBench/25", "solution": "import base64\nimport json\nimport zlib\ndef task_func(data_dict):\n    # Convert the dictionary to a JSON string\n    json_str = json.dumps(data_dict)\n\n    # Compress the JSON string using zlib\n    compressed_data = zlib.compress(json_str.encode())\n\n    # Encode the compressed data with base64\n    base64_encoded_data = base64.b64encode(compressed_data)\n\n    # Convert the base64-encoded data to a string and return it\n    return base64_encoded_data.decode()", "raw_solution": "import base64\nimport json\nimport zlib\n\ndef task_func(data_dict):\n    # Convert the dictionary to a JSON string\n    json_str = json.dumps(data_dict)\n\n    # Compress the JSON string using zlib\n    compressed_data = zlib.compress(json_str.encode())\n\n    # Encode the compressed data with base64\n    base64_encoded_data = base64.b64encode(compressed_data)\n\n    # Convert the base64-encoded data to a string and return it\n    return base64_encoded_data.decode()"}
{"task_id": "BigCodeBench/26", "solution": "import base64\nfrom cryptography.fernet import Fernet\ndef task_func(message, encryption_key):\n    \"\"\"\n    Encrypts a message with a symmetric encryption key using Fernet encryption,\n    and then encode the encrypted message using base64.\n    \"\"\"\n    # Create a Fernet object with the encryption key\n    cipher_suite = Fernet(encryption_key)\n\n    # Encrypt the message\n    encrypted_message = cipher_suite.encrypt(message.encode())\n\n    # Encode the encrypted message using base64\n    base64_encoded_message = base64.b64encode(encrypted_message).decode()\n\n    return base64_encoded_message", "raw_solution": "import base64\nfrom cryptography.fernet import Fernet\n\ndef task_func(message, encryption_key):\n    \"\"\"\n    Encrypts a message with a symmetric encryption key using Fernet encryption,\n    and then encode the encrypted message using base64.\n    \"\"\"\n    # Create a Fernet object with the encryption key\n    cipher_suite = Fernet(encryption_key)\n\n    # Encrypt the message\n    encrypted_message = cipher_suite.encrypt(message.encode())\n\n    # Encode the encrypted message using base64\n    base64_encoded_message = base64.b64encode(encrypted_message).decode()\n\n    return base64_encoded_message"}
{"task_id": "BigCodeBench/27", "solution": "import json\nimport base64\nfrom datetime import datetime\ndef task_func(data: dict, DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\") -> str:\n    \"\"\"\n    Takes a Python dictionary, adds a current timestamp to it, serializes the modified dictionary to a JSON-formatted string, and then encodes this string using base64 encoding with ASCII character encoding.\n    \"\"\"\n    # Add current timestamp to the dictionary\n    data['timestamp'] = datetime.now().strftime(DATE_FORMAT)\n\n    # Serialize the dictionary to a JSON-formatted string\n    json_str = json.dumps(data)\n\n    # Encode the JSON string using base64 and ASCII character encoding\n    encoded_str = base64.b64encode(json_str.encode('ascii'))\n\n    return encoded_str.decode('ascii')", "raw_solution": "import json\nimport base64\nfrom datetime import datetime\n\ndef task_func(data: dict, DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\") -> str:\n    \"\"\"\n    Takes a Python dictionary, adds a current timestamp to it, serializes the modified dictionary to a JSON-formatted string, and then encodes this string using base64 encoding with ASCII character encoding.\n    \"\"\"\n    # Add current timestamp to the dictionary\n    data['timestamp'] = datetime.now().strftime(DATE_FORMAT)\n\n    # Serialize the dictionary to a JSON-formatted string\n    json_str = json.dumps(data)\n\n    # Encode the JSON string using base64 and ASCII character encoding\n    encoded_str = base64.b64encode(json_str.encode('ascii'))\n\n    return encoded_str.decode('ascii')"}
{"task_id": "BigCodeBench/28", "solution": "import requests\nimport json\nimport base64\ndef task_func(data, url=\"http://your-api-url.com\"):\n    # Convert the dictionary to a JSON-formatted string\n    json_data = json.dumps(data)\n\n    # Encode the JSON-formatted string in base64 format\n    encoded_data = base64.b64encode(json_data.encode('utf-8')).decode('utf-8')\n\n    # Send a POST request to the API endpoint with the encoded data as the payload\n    response = requests.post(url, data=encoded_data)\n\n    return response", "raw_solution": "import requests\nimport json\nimport base64\n\ndef task_func(data, url=\"http://your-api-url.com\"):\n    # Convert the dictionary to a JSON-formatted string\n    json_data = json.dumps(data)\n\n    # Encode the JSON-formatted string in base64 format\n    encoded_data = base64.b64encode(json_data.encode('utf-8')).decode('utf-8')\n\n    # Send a POST request to the API endpoint with the encoded data as the payload\n    response = requests.post(url, data=encoded_data)\n\n    return response"}
{"task_id": "BigCodeBench/29", "solution": "from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport base64\ndef task_func(data):\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    # Convert the standardized data to a string\n    standardized_str = str(standardized_data.tolist())\n\n    # Encode the string in base64\n    base64_bytes = base64.b64encode(standardized_str.encode('ascii'))\n    base64_str = base64_bytes.decode('ascii')\n\n    return base64_str", "raw_solution": "from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport base64\n\ndef task_func(data):\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data)\n\n    # Convert the standardized data to a string\n    standardized_str = str(standardized_data.tolist())\n\n    # Encode the string in base64\n    base64_bytes = base64.b64encode(standardized_str.encode('ascii'))\n    base64_str = base64_bytes.decode('ascii')\n\n    return base64_str"}
{"task_id": "BigCodeBench/30", "solution": "import json\nimport os\nimport re\ndef task_func(\n    file_path,\n    attribute,\n    INPUT_JSON={\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"age\": {\"type\": \"number\"},\n            \"email\": {\"type\": \"string\"}\n        },\n        \"required\": [\"name\", \"age\", \"email\"]\n    },\n    EMAIL_REGEX=r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"):\n\n    # Check if the file exists\n    if not os.path.isfile(file_path):\n        raise ValueError(f\"File {file_path} does not exist.\")\n\n    # Load the JSON file\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    # Validate the structure and contents of the JSON file\n    if not isinstance(data, dict):\n        raise ValueError(\"The JSON file does not match the schema.\")\n\n    # Check if all required fields exist\n    for field in INPUT_JSON[\"required\"]:\n        if field not in data:\n            raise ValueError(f\"Required attribute {field} is missing.\")\n\n    # Check if the types match\n    for key, value in INPUT_JSON[\"properties\"].items():\n        if key in data and not isinstance(data[key], value[\"type\"]):\n            raise ValueError(f\"The type of attribute {key} does not match.\")\n\n    # Check the validity of the email format\n    if \"email\" in data and not re.match(EMAIL_REGEX, data[\"email\"]):\n        raise ValueError(\"The email format is invalid.\")\n\n    # Return the value of the specified attribute\n    if attribute in data:\n        return data[attribute]\n    else:\n        raise ValueError(f\"The attribute {attribute} does not exist.\")", "raw_solution": "import json\nimport os\nimport re\n\ndef task_func(\n    file_path,\n    attribute,\n    INPUT_JSON={\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"age\": {\"type\": \"number\"},\n            \"email\": {\"type\": \"string\"}\n        },\n        \"required\": [\"name\", \"age\", \"email\"]\n    },\n    EMAIL_REGEX=r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"):\n\n    # Check if the file exists\n    if not os.path.isfile(file_path):\n        raise ValueError(f\"File {file_path} does not exist.\")\n\n    # Load the JSON file\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    # Validate the structure and contents of the JSON file\n    if not isinstance(data, dict):\n        raise ValueError(\"The JSON file does not match the schema.\")\n\n    # Check if all required fields exist\n    for field in INPUT_JSON[\"required\"]:\n        if field not in data:\n            raise ValueError(f\"Required attribute {field} is missing.\")\n\n    # Check if the types match\n    for key, value in INPUT_JSON[\"properties\"].items():\n        if key in data and not isinstance(data[key], value[\"type\"]):\n            raise ValueError(f\"The type of attribute {key} does not match.\")\n\n    # Check the validity of the email format\n    if \"email\" in data and not re.match(EMAIL_REGEX, data[\"email\"]):\n        raise ValueError(\"The email format is invalid.\")\n\n    # Return the value of the specified attribute\n    if attribute in data:\n        return data[attribute]\n    else:\n        raise ValueError(f\"The attribute {attribute} does not exist.\")"}
{"task_id": "BigCodeBench/31", "solution": "import nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nPUNCTUATION = set(punctuation)\ndef task_func(text):\n    \"\"\"\n    Draws a bar chart of the frequency of words in a text beginning with the \"$\" character.\n    Words that start with the '$' character but consist only of punctuation (e.g., '$!$' and '$.$') are not included in the frequency count.\n    If there is no word respecting the above conditions, the plot is None.\n    \"\"\"\n    # Tokenize the text\n    words = nltk.word_tokenize(text)\n\n    # Filter words starting with '$' and not only punctuation\n    words = [word for word in words if word.startswith('$') and not all(char in PUNCTUATION for char in word[1:])]\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    # If there are no words, return None\n    if not word_freq:\n        return None\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    sns.barplot(x=list(word_freq.keys()), y=list(word_freq.values()), ax=ax)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of words beginning with \"$\"')\n\n    return ax", "raw_solution": "import nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Constants\nPUNCTUATION = set(punctuation)\n\ndef task_func(text):\n    \"\"\"\n    Draws a bar chart of the frequency of words in a text beginning with the \"$\" character.\n    Words that start with the '$' character but consist only of punctuation (e.g., '$!$' and '$.$') are not included in the frequency count.\n    If there is no word respecting the above conditions, the plot is None.\n    \"\"\"\n    # Tokenize the text\n    words = nltk.word_tokenize(text)\n\n    # Filter words starting with '$' and not only punctuation\n    words = [word for word in words if word.startswith('$') and not all(char in PUNCTUATION for char in word[1:])]\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    # If there are no words, return None\n    if not word_freq:\n        return None\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    sns.barplot(x=list(word_freq.keys()), y=list(word_freq.values()), ax=ax)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of words beginning with \"$\"')\n\n    return ax"}
{"task_id": "BigCodeBench/32", "solution": "import requests\nfrom bs4 import BeautifulSoup\ndef task_func(url, tag):\n    \"\"\"\n    Scrapes a web page for the first occurrence of a specified HTML tag and returns its text content.\n    If the tag is not found, returns None.\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    first_tag = soup.find(tag)\n    if first_tag:\n        return first_tag.text\n    else:\n        return None", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\n\ndef task_func(url, tag):\n    \"\"\"\n    Scrapes a web page for the first occurrence of a specified HTML tag and returns its text content.\n    If the tag is not found, returns None.\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    first_tag = soup.find(tag)\n    if first_tag:\n        return first_tag.text\n    else:\n        return None"}
{"task_id": "BigCodeBench/33", "solution": "import numpy as np\nfrom functools import reduce\ndef task_func(list_of_pairs):\n    \"\"\"\n    Calculates the product of the second values in each tuple in a list of tuples and returns the product as a single-element numeric array.\n    \"\"\"\n    # Extract the second element of each tuple and multiply them together\n    product = reduce(lambda x, y: x * y, [pair[1] for pair in list_of_pairs])\n\n    # Return the product as a numpy array\n    return np.array([product])", "raw_solution": "import numpy as np\nfrom functools import reduce\n\ndef task_func(list_of_pairs):\n    \"\"\"\n    Calculates the product of the second values in each tuple in a list of tuples and returns the product as a single-element numeric array.\n    \"\"\"\n    # Extract the second element of each tuple and multiply them together\n    product = reduce(lambda x, y: x * y, [pair[1] for pair in list_of_pairs])\n\n    # Return the product as a numpy array\n    return np.array([product])\n\n# Test the function\nassert np.array_equal(task_func([(1, 2), (3, 4), (5, 6)]), np.array([48]))\nassert np.array_equal(task_func([(1, 1), (2, 2), (3, 3)]), np.array([9]))\nassert np.array_equal(task_func([(1, 0), (2, 0), (3, 0)]), np.array([0]))"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Split the text into words\n    words = text.split()\n\n    # If there are no words available, raise an exception\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n\n    # Join the words back into a string\n    text = ' '.join(words)\n\n    # Generate a word cloud\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = None, \n                min_font_size = 10).generate(text)\n\n    # Plot the word cloud\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show()\n\n    return wordcloud", "raw_solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Split the text into words\n    words = text.split()\n\n    # If there are no words available, raise an exception\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n\n    # Join the words back into a string\n    text = ' '.join(words)\n\n    # Generate a word cloud\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = None, \n                min_font_size = 10).generate(text)\n\n    # Plot the word cloud\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show()\n\n    return wordcloud"}
{"task_id": "BigCodeBench/35", "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_values=[1, 3, 4]):\n    # Replace non-target values with zeros\n    df = df.replace(set(df.columns) - set(target_values), 0)\n\n    # Plot distribution of each column\n    for col in df.columns:\n        plt.figure(figsize=(10, 6))\n        sns.histplot(df[col], kde=True, color='blue', bins=30)\n        plt.title(f'Distribution of {col}')\n        plt.show()", "raw_solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, target_values=[1, 3, 4]):\n    # Replace non-target values with zeros\n    df = df.replace(set(df.columns) - set(target_values), 0)\n\n    # Plot distribution of each column\n    for col in df.columns:\n        plt.figure(figsize=(10, 6))\n        sns.histplot(df[col], kde=True, color='blue', bins=30)\n        plt.title(f'Distribution of {col}')\n        plt.show()"}
{"task_id": "BigCodeBench/36", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTARGET_VALUES = np.array([1, 3, 4])\ndef task_func(df):\n    # Replace non-target values with zeros\n    df = df.replace(set(df.values.flatten()) - set(TARGET_VALUES), 0)\n\n    # Perform Box-Cox transformation\n    for col in df.columns:\n        df[col], fitted_lambda = stats.boxcox(df[col] + 1)\n\n    # Create KDE plots\n    fig, ax = plt.subplots(figsize=(10, 10))\n    df.plot.kde(ax=ax)\n\n    return df, fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nTARGET_VALUES = np.array([1, 3, 4])\n\ndef task_func(df):\n    # Replace non-target values with zeros\n    df = df.replace(set(df.values.flatten()) - set(TARGET_VALUES), 0)\n\n    # Perform Box-Cox transformation\n    for col in df.columns:\n        df[col], fitted_lambda = stats.boxcox(df[col] + 1)\n\n    # Create KDE plots\n    fig, ax = plt.subplots(figsize=(10, 10))\n    df.plot.kde(ax=ax)\n\n    return df, fig"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, target_column):\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train a random forest classifier\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X, y)\n\n    # Get feature importances\n    importances = clf.feature_importances_\n    features = X.columns\n\n    # Create a dataframe for feature importances\n    df_importances = pd.DataFrame({'Feature': features, 'Importance': importances})\n\n    # Sort the dataframe by importance\n    df_importances = df_importances.sort_values('Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Feature', data=df_importances)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.tight_layout()\n\n    return clf, plt.gca()", "raw_solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_column):\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train a random forest classifier\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X, y)\n\n    # Get feature importances\n    importances = clf.feature_importances_\n    features = X.columns\n\n    # Create a dataframe for feature importances\n    df_importances = pd.DataFrame({'Feature': features, 'Importance': importances})\n\n    # Sort the dataframe by importance\n    df_importances = df_importances.sort_values('Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Feature', data=df_importances)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.tight_layout()\n\n    return clf, plt.gca()"}
{"task_id": "BigCodeBench/38", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nFEATURE_NAMES = [\"Feature 1\", \"Feature 2\", \"Feature 3\", \"Feature 4\", \"Feature 5\"]\ndef task_func(data_matrix):\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n\n    # Calculate the mean of each row\n    means = standardized_data.mean(axis=1)\n\n    # Create a DataFrame\n    df = pd.DataFrame(standardized_data, columns=FEATURE_NAMES)\n    df['Mean'] = means\n\n    # Plot the distribution of means\n    plt.hist(means, bins=30, edgecolor='black')\n    plt.title('Distribution of Means')\n    plt.xlabel('Mean')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return df, plt.gca()", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURE_NAMES = [\"Feature 1\", \"Feature 2\", \"Feature 3\", \"Feature 4\", \"Feature 5\"]\n\ndef task_func(data_matrix):\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n\n    # Calculate the mean of each row\n    means = standardized_data.mean(axis=1)\n\n    # Create a DataFrame\n    df = pd.DataFrame(standardized_data, columns=FEATURE_NAMES)\n    df['Mean'] = means\n\n    # Plot the distribution of means\n    plt.hist(means, bins=30, edgecolor='black')\n    plt.title('Distribution of Means')\n    plt.xlabel('Mean')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/39", "solution": "import numpy as np\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\nALPHA = 0.05\ndef task_func(data_matrix):\n    # Calculate the mean value of each row\n    row_means = np.mean(data_matrix, axis=1)\n\n    # Calculate the population mean\n    pop_mean = np.mean(data_matrix)\n\n    # Run a t-test from a sample against the population value\n    _, p_values = ttest_1samp(data_matrix, pop_mean)\n\n    # Record the mean values that differ significantly\n    significant_indices = [i for i, p in enumerate(p_values) if p < ALPHA]\n\n    # Create a lineplot with the mean of rows in red\n    plt.plot(row_means, color='red', label='Means')\n\n    # Create a line plot with the significant_indices on the x-axis and the corresponding means on the y-axis\n    plt.plot(significant_indices, [row_means[i] for i in significant_indices], color='blue', label='Significant Means')\n\n    # Create an horizontal line which represent the mean computed on the whole 2D matrix\n    plt.axhline(pop_mean, color='green', label='Population Mean')\n\n    plt.legend()\n\n    return significant_indices, plt.gca()", "raw_solution": "import numpy as np\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHA = 0.05\n\ndef task_func(data_matrix):\n    # Calculate the mean value of each row\n    row_means = np.mean(data_matrix, axis=1)\n\n    # Calculate the population mean\n    pop_mean = np.mean(data_matrix)\n\n    # Run a t-test from a sample against the population value\n    _, p_values = ttest_1samp(data_matrix, pop_mean)\n\n    # Record the mean values that differ significantly\n    significant_indices = [i for i, p in enumerate(p_values) if p < ALPHA]\n\n    # Create a lineplot with the mean of rows in red\n    plt.plot(row_means, color='red', label='Means')\n\n    # Create a line plot with the significant_indices on the x-axis and the corresponding means on the y-axis\n    plt.plot(significant_indices, [row_means[i] for i in significant_indices], color='blue', label='Significant Means')\n\n    # Create an horizontal line which represent the mean computed on the whole 2D matrix\n    plt.axhline(pop_mean, color='green', label='Population Mean')\n\n    plt.legend()\n\n    return significant_indices, plt.gca()"}
{"task_id": "BigCodeBench/40", "solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(data_matrix):\n    # Calculate the Z-values of the data matrix\n    z_scores = pd.DataFrame(zscore(data_matrix, axis=1), columns=data_matrix.columns)\n    \n    # Calculate the mean value of each row\n    z_scores['Mean'] = z_scores.mean(axis=1)\n    \n    # Calculate the correlation matrix of the Z-values\n    correlation_matrix = z_scores.corr()\n    \n    # Visualize the correlation matrix with a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix of Z-Scores')\n    plt.show()\n    \n    return z_scores, correlation_matrix, plt.gca()", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n\ndef task_func(data_matrix):\n    # Calculate the Z-values of the data matrix\n    z_scores = pd.DataFrame(zscore(data_matrix, axis=1), columns=data_matrix.columns)\n    \n    # Calculate the mean value of each row\n    z_scores['Mean'] = z_scores.mean(axis=1)\n    \n    # Calculate the correlation matrix of the Z-values\n    correlation_matrix = z_scores.corr()\n    \n    # Visualize the correlation matrix with a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix of Z-Scores')\n    plt.show()\n    \n    return z_scores, correlation_matrix, plt.gca()"}
{"task_id": "BigCodeBench/41", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\ndef task_func(data_matrix):\n    \"\"\"\n    Calculates the skewness of each row in a 2D data matrix and plots the distribution.\n    \"\"\"\n    # Convert the data matrix to a DataFrame\n    df = pd.DataFrame(data_matrix)\n\n    # Calculate the skewness of each row and store it in a new column\n    df['Skewness'] = df.apply(lambda row: skew(row), axis=1)\n\n    # Plot the distribution of the skewness\n    fig, ax = plt.subplots()\n    df['Skewness'].plot(kind='hist', ax=ax)\n    ax.set_xlabel('Skewness')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Skewness')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\n\ndef task_func(data_matrix):\n    \"\"\"\n    Calculates the skewness of each row in a 2D data matrix and plots the distribution.\n    \"\"\"\n    # Convert the data matrix to a DataFrame\n    df = pd.DataFrame(data_matrix)\n\n    # Calculate the skewness of each row and store it in a new column\n    df['Skewness'] = df.apply(lambda row: skew(row), axis=1)\n\n    # Plot the distribution of the skewness\n    fig, ax = plt.subplots()\n    df['Skewness'].plot(kind='hist', ax=ax)\n    ax.set_xlabel('Skewness')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Skewness')\n\n    return df, ax"}
{"task_id": "BigCodeBench/42", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data_matrix, n_components=2):\n    \"\"\"\n    Apply PCA with n_components components to a 2D data matrix, calculate the mean value of each component,\n    and then return the cumulative explained variance of the components in a plot.\n    \"\"\"\n    # Apply PCA\n    pca = PCA(n_components=n_components)\n    pca_result = pca.fit_transform(data_matrix)\n\n    # Create DataFrame\n    df = pd.DataFrame(pca_result, columns=[f'Component {i+1}' for i in range(n_components)])\n    df['Mean'] = df.mean(axis=1)\n\n    # Calculate cumulative explained variance\n    explained_variance = pca.explained_variance_ratio_\n    cumulative_explained_variance = np.cumsum(explained_variance)\n\n    # Plot cumulative explained variance\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, n_components+1), cumulative_explained_variance, marker='o')\n    plt.xlabel('Number of Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.grid(True)\n\n    return df, plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data_matrix, n_components=2):\n    \"\"\"\n    Apply PCA with n_components components to a 2D data matrix, calculate the mean value of each component,\n    and then return the cumulative explained variance of the components in a plot.\n    \"\"\"\n    # Apply PCA\n    pca = PCA(n_components=n_components)\n    pca_result = pca.fit_transform(data_matrix)\n\n    # Create DataFrame\n    df = pd.DataFrame(pca_result, columns=[f'Component {i+1}' for i in range(n_components)])\n    df['Mean'] = df.mean(axis=1)\n\n    # Calculate cumulative explained variance\n    explained_variance = pca.explained_variance_ratio_\n    cumulative_explained_variance = np.cumsum(explained_variance)\n\n    # Plot cumulative explained variance\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, n_components+1), cumulative_explained_variance, marker='o')\n    plt.xlabel('Number of Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.grid(True)\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/43", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, List\ndef task_func(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[plt.Axes]]:\n    \"\"\"\n    Describes a dataframe and draws a distribution chart for each numeric column after replacing the NaN values with the average of the column.\n    \"\"\"\n    # Calculate the average of each column\n    avg_values = df.mean(numeric_only=True)\n\n    # Replace NaN values with the average of the column\n    df.fillna(avg_values, inplace=True)\n\n    # Describe the dataframe\n    df_stats = df.describe(include=[np.number])\n\n    # Create a list to store the distribution plots\n    axes_list = []\n\n    # Plot the distribution for each numeric column\n    for col in df_stats.columns:\n        ax = sns.histplot(df[col], bins=10)\n        ax.set_title(f'Distribution of {col}')\n        axes_list.append(ax)\n\n    return df_stats, axes_list", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, List\n\ndef task_func(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[plt.Axes]]:\n    \"\"\"\n    Describes a dataframe and draws a distribution chart for each numeric column after replacing the NaN values with the average of the column.\n    \"\"\"\n    # Calculate the average of each column\n    avg_values = df.mean(numeric_only=True)\n\n    # Replace NaN values with the average of the column\n    df.fillna(avg_values, inplace=True)\n\n    # Describe the dataframe\n    df_stats = df.describe(include=[np.number])\n\n    # Create a list to store the distribution plots\n    axes_list = []\n\n    # Plot the distribution for each numeric column\n    for col in df_stats.columns:\n        ax = sns.histplot(df[col], bins=10)\n        ax.set_title(f'Distribution of {col}')\n        axes_list.append(ax)\n\n    return df_stats, axes_list"}
{"task_id": "BigCodeBench/44", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df):\n    # Normalize numeric columns in the DataFrame\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Replace missing values with the column's average\n    df_normalized.fillna(df_normalized.mean(), inplace=True)\n\n    # Create a figure and a set of subplots\n    fig, axes = plt.subplots(nrows=1, ncols=len(df.columns), figsize=(15, 5))\n\n    # Draw a box plot for each column\n    for i, col in enumerate(df.columns):\n        axes[i].boxplot(df_normalized[col])\n        axes[i].set_title(col)\n\n    # Adjust layout and display the plot\n    plt.tight_layout()\n    plt.show()\n\n    return df_normalized, axes", "raw_solution": "from sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    # Normalize numeric columns in the DataFrame\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Replace missing values with the column's average\n    df_normalized.fillna(df_normalized.mean(), inplace=True)\n\n    # Create a figure and a set of subplots\n    fig, axes = plt.subplots(nrows=1, ncols=len(df.columns), figsize=(15, 5))\n\n    # Draw a box plot for each column\n    for i, col in enumerate(df.columns):\n        axes[i].boxplot(df_normalized[col])\n        axes[i].set_title(col)\n\n    # Adjust layout and display the plot\n    plt.tight_layout()\n    plt.show()\n\n    return df_normalized, axes"}
{"task_id": "BigCodeBench/45", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame):\n    \"\"\"\n    Performs PCA on a DataFrame (excluding non-numeric columns) and draws a scatter plot of the first two main components.\n    The principal columns are named 'Component 1' and 'Component 2'. Missing values are replaced by column's average.\n    \"\"\"\n    # Exclude non-numeric columns\n    df_numeric = df.select_dtypes(include=[np.number])\n\n    # Fill missing values with column's average\n    df_numeric.fillna(df_numeric.mean(), inplace=True)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df_numeric)\n\n    # Create a DataFrame for the first two principal components\n    principalDf = pd.DataFrame(data=principalComponents, columns=['Component 1', 'Component 2'])\n\n    # Create a scatter plot of the first two principal components\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.scatterplot(x='Component 1', y='Component 2', data=principalDf, ax=ax)\n    ax.set_xlabel('Component 1')\n    ax.set_ylabel('Component 2')\n\n    return principalDf, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame):\n    \"\"\"\n    Performs PCA on a DataFrame (excluding non-numeric columns) and draws a scatter plot of the first two main components.\n    The principal columns are named 'Component 1' and 'Component 2'. Missing values are replaced by column's average.\n    \"\"\"\n    # Exclude non-numeric columns\n    df_numeric = df.select_dtypes(include=[np.number])\n\n    # Fill missing values with column's average\n    df_numeric.fillna(df_numeric.mean(), inplace=True)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df_numeric)\n\n    # Create a DataFrame for the first two principal components\n    principalDf = pd.DataFrame(data=principalComponents, columns=['Component 1', 'Component 2'])\n\n    # Create a scatter plot of the first two principal components\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.scatterplot(x='Component 1', y='Component 2', data=principalDf, ax=ax)\n    ax.set_xlabel('Component 1')\n    ax.set_ylabel('Component 2')\n\n    return principalDf, ax"}
{"task_id": "BigCodeBench/46", "solution": "from scipy.stats import zscore\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df):\n    # Calculate Z-scores for numeric columns in a DataFrame\n    df_z_scores = df.select_dtypes(include=[np.number]).apply(zscore)\n\n    # Replace missing values with the column's average\n    df_z_scores.fillna(df_z_scores.mean(), inplace=True)\n\n    # Draw a histogram for each column\n    figs, axes = plt.subplots(len(df_z_scores.columns), 1)\n    hist_axes = []\n\n    for i, col in enumerate(df_z_scores.columns):\n        axes[i].hist(df_z_scores[col], bins=10)\n        axes[i].set_title(f'Histogram of {col}')\n        hist_axes.append(axes[i])\n\n    plt.tight_layout()\n\n    return df_z_scores, hist_axes", "raw_solution": "from scipy.stats import zscore\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    # Calculate Z-scores for numeric columns in a DataFrame\n    df_z_scores = df.select_dtypes(include=[np.number]).apply(zscore)\n\n    # Replace missing values with the column's average\n    df_z_scores.fillna(df_z_scores.mean(), inplace=True)\n\n    # Draw a histogram for each column\n    figs, axes = plt.subplots(len(df_z_scores.columns), 1)\n    hist_axes = []\n\n    for i, col in enumerate(df_z_scores.columns):\n        axes[i].hist(df_z_scores[col], bins=10)\n        axes[i].set_title(f'Histogram of {col}')\n        hist_axes.append(axes[i])\n\n    plt.tight_layout()\n\n    return df_z_scores, hist_axes"}
{"task_id": "BigCodeBench/47", "solution": "from sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df):\n    # Standardize numeric columns in the DataFrame\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    df[numeric_cols] = StandardScaler().fit_transform(df[numeric_cols])\n\n    # Replace missing values with the column's average\n    df.fillna(df.mean(), inplace=True)\n\n    # Calculate the correlation matrix\n    corr = df.corr()\n\n    # Plot the heatmap of the correlation matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, fmt=\".2f\")\n    plt.show()\n\n    return df, plt.gca()", "raw_solution": "from sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    # Standardize numeric columns in the DataFrame\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    df[numeric_cols] = StandardScaler().fit_transform(df[numeric_cols])\n\n    # Replace missing values with the column's average\n    df.fillna(df.mean(), inplace=True)\n\n    # Calculate the correlation matrix\n    corr = df.corr()\n\n    # Plot the heatmap of the correlation matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, fmt=\".2f\")\n    plt.show()\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/48", "solution": "import time\nfrom datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\ndef task_func(n, output_path=None):\n    \"\"\"\n    Generates n random Unix timestamps and converts them to strings formatted as UTC DATE_FORMAT.\n    Plots a histogram of the distribution of the generated timestamps. If an output path is provided,\n    saves the histogram to the specified path. Otherwise, displays the plot.\n    \"\"\"\n    # Generate n random Unix timestamps\n    timestamps = [random.randint(0, int(time.time())) for _ in range(n)]\n\n    # Convert timestamps to strings\n    formatted_timestamps = [datetime.utcfromtimestamp(ts).strftime(DATE_FORMAT) for ts in timestamps]\n\n    # Plot histogram\n    plt.hist(formatted_timestamps, bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Timestamp')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Generated Timestamps')\n\n    # Save or display the plot\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n\n    return formatted_timestamps", "raw_solution": "import time\nfrom datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef task_func(n, output_path=None):\n    \"\"\"\n    Generates n random Unix timestamps and converts them to strings formatted as UTC DATE_FORMAT.\n    Plots a histogram of the distribution of the generated timestamps. If an output path is provided,\n    saves the histogram to the specified path. Otherwise, displays the plot.\n    \"\"\"\n    # Generate n random Unix timestamps\n    timestamps = [random.randint(0, int(time.time())) for _ in range(n)]\n\n    # Convert timestamps to strings\n    formatted_timestamps = [datetime.utcfromtimestamp(ts).strftime(DATE_FORMAT) for ts in timestamps]\n\n    # Plot histogram\n    plt.hist(formatted_timestamps, bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Timestamp')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Generated Timestamps')\n\n    # Save or display the plot\n    if output_path:\n        plt.savefig(output_path)\n    else:\n        plt.show()\n\n    return formatted_timestamps"}
{"task_id": "BigCodeBench/49", "solution": "from datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\ndef task_func(timestamps):\n    if not timestamps:\n        raise ValueError(\"Input list of timestamps is empty.\")\n\n    # Convert Unix timestamps to datetime objects\n    datetime_objects = [datetime.fromtimestamp(ts) for ts in timestamps]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Timestamp': timestamps,\n        'Datetime': datetime_objects\n    })\n\n    # Convert 'Datetime' column to string format\n    df['Datetime'] = df['Datetime'].dt.strftime(DATE_FORMAT)\n\n    # Draw a histogram\n    plt.hist(df['Datetime'], bins=10)\n    plt.xlabel('Datetime')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Datetime')\n\n    return df, plt.gca()", "raw_solution": "from datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef task_func(timestamps):\n    if not timestamps:\n        raise ValueError(\"Input list of timestamps is empty.\")\n\n    # Convert Unix timestamps to datetime objects\n    datetime_objects = [datetime.fromtimestamp(ts) for ts in timestamps]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Timestamp': timestamps,\n        'Datetime': datetime_objects\n    })\n\n    # Convert 'Datetime' column to string format\n    df['Datetime'] = df['Datetime'].dt.strftime(DATE_FORMAT)\n\n    # Draw a histogram\n    plt.hist(df['Datetime'], bins=10)\n    plt.xlabel('Datetime')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Datetime')\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/50", "solution": "from datetime import datetime\nimport pandas as pd\nimport pytz\nimport matplotlib.pyplot as plt\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\nTIMEZONES = [\n    \"America/New_York\",\n    \"Europe/London\",\n    \"Asia/Shanghai\",\n    \"Asia/Tokyo\",\n    \"Australia/Sydney\",\n]\ndef task_func(timestamp):\n    \"\"\"\n    Converts a Unix timestamp to date objects in different time zones, creates a Pandas DataFrame, and draws a bar chart.\n    \"\"\"\n    # Create a list to store the datetime objects in different time zones\n    datetime_list = []\n\n    # Convert the Unix timestamp to date objects in different time zones\n    for timezone in TIMEZONES:\n        dt = datetime.fromtimestamp(timestamp)\n        tz = pytz.timezone(timezone)\n        dt = dt.replace(tzinfo=tz)\n        datetime_list.append(dt.strftime(DATE_FORMAT))\n\n    # Create a DataFrame\n    df = pd.DataFrame(list(zip(TIMEZONES, datetime_list)), columns=['Timezone', 'Datetime'])\n\n    # Draw a bar chart\n    ax = df.plot(x='Timezone', y='Datetime', kind='bar', legend=False)\n    ax.set_xlabel('Timezone')\n    ax.set_ylabel('Datetime')\n    ax.set_title('Datetime = f(Timezone)')\n\n    return df, ax", "raw_solution": "from datetime import datetime\nimport pandas as pd\nimport pytz\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\nTIMEZONES = [\n    \"America/New_York\",\n    \"Europe/London\",\n    \"Asia/Shanghai\",\n    \"Asia/Tokyo\",\n    \"Australia/Sydney\",\n]\n\ndef task_func(timestamp):\n    \"\"\"\n    Converts a Unix timestamp to date objects in different time zones, creates a Pandas DataFrame, and draws a bar chart.\n    \"\"\"\n    # Create a list to store the datetime objects in different time zones\n    datetime_list = []\n\n    # Convert the Unix timestamp to date objects in different time zones\n    for timezone in TIMEZONES:\n        dt = datetime.fromtimestamp(timestamp)\n        tz = pytz.timezone(timezone)\n        dt = dt.replace(tzinfo=tz)\n        datetime_list.append(dt.strftime(DATE_FORMAT))\n\n    # Create a DataFrame\n    df = pd.DataFrame(list(zip(TIMEZONES, datetime_list)), columns=['Timezone', 'Datetime'])\n\n    # Draw a bar chart\n    ax = df.plot(x='Timezone', y='Datetime', kind='bar', legend=False)\n    ax.set_xlabel('Timezone')\n    ax.set_ylabel('Datetime')\n    ax.set_title('Datetime = f(Timezone)')\n\n    return df, ax"}
{"task_id": "BigCodeBench/51", "solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df, age: int, height: int):\n    # Filter the dataframe based on the specified 'Age' and 'Height' conditions\n    filtered_df = df[(df['Age'] >= age) & (df['Height'] >= height)]\n\n    # If the filtered dataframe has less than 3 columns, add a column 'Cluster' with 0 for each row\n    if len(filtered_df.columns) < 3:\n        filtered_df['Cluster'] = 0\n    else:\n        # Otherwise, do a KMeans clustering (by Age and Height) with 3 clusters and add a column 'Cluster' to the dataframe\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(filtered_df[['Age', 'Height']])\n        filtered_df['Cluster'] = kmeans.labels_\n\n    # Plot a scatter plot of the 'Age' and 'Height' and colored by the cluster indices\n    if 'Cluster' in filtered_df.columns:\n        plt.scatter(filtered_df['Age'], filtered_df['Height'], c=filtered_df['Cluster'])\n        plt.xlabel('Age')\n        plt.ylabel('Height')\n        plt.title('KMeans Clustering based on Age and Height')\n        plt.show()\n\n    return filtered_df", "raw_solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df, age: int, height: int):\n    # Filter the dataframe based on the specified 'Age' and 'Height' conditions\n    filtered_df = df[(df['Age'] >= age) & (df['Height'] >= height)]\n\n    # If the filtered dataframe has less than 3 columns, add a column 'Cluster' with 0 for each row\n    if len(filtered_df.columns) < 3:\n        filtered_df['Cluster'] = 0\n    else:\n        # Otherwise, do a KMeans clustering (by Age and Height) with 3 clusters and add a column 'Cluster' to the dataframe\n        kmeans = KMeans(n_clusters=3)\n        kmeans.fit(filtered_df[['Age', 'Height']])\n        filtered_df['Cluster'] = kmeans.labels_\n\n    # Plot a scatter plot of the 'Age' and 'Height' and colored by the cluster indices\n    if 'Cluster' in filtered_df.columns:\n        plt.scatter(filtered_df['Age'], filtered_df['Height'], c=filtered_df['Cluster'])\n        plt.xlabel('Age')\n        plt.ylabel('Height')\n        plt.title('KMeans Clustering based on Age and Height')\n        plt.show()\n\n    return filtered_df"}
{"task_id": "BigCodeBench/52", "solution": "import pandas as pd\nfrom collections import Counter\nSTOPWORDS = [\"a\", \"an\", \"the\", \"in\", \"is\", \"are\"]\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    text = ''.join(c for c in text if c.isalnum() or c.isspace()).lower()\n\n    # Split the text into words\n    words = text.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Convert the Counter to a pandas Series\n    series = pd.Series(word_counts)\n\n    return series", "raw_solution": "import pandas as pd\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = [\"a\", \"an\", \"the\", \"in\", \"is\", \"are\"]\n\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    text = ''.join(c for c in text if c.isalnum() or c.isspace()).lower()\n\n    # Split the text into words\n    words = text.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Convert the Counter to a pandas Series\n    series = pd.Series(word_counts)\n\n    return series"}
{"task_id": "BigCodeBench/53", "solution": "import pandas as pd\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMN_NAMES = [\"Name\", \"Email\", \"Age\", \"Country\"]\ndef task_func(text):\n    # Extract data using regular expression\n    data = re.findall(r\"Name: (.*?), Email: (.*?), Age: (.*?), Country: (.*?)($|\\n)\", text)\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Convert Age to integer\n    df['Age'] = df['Age'].astype(int)\n\n    # Plot age distribution\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df, x='Age', bins=30, kde=False)\n    plt.title('Age Distribution')\n    plt.show()\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nCOLUMN_NAMES = [\"Name\", \"Email\", \"Age\", \"Country\"]\n\ndef task_func(text):\n    # Extract data using regular expression\n    data = re.findall(r\"Name: (.*?), Email: (.*?), Age: (.*?), Country: (.*?)($|\\n)\", text)\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Convert Age to integer\n    df['Age'] = df['Age'].astype(int)\n\n    # Plot age distribution\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df, x='Age', bins=30, kde=False)\n    plt.title('Age Distribution')\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/54", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef task_func(text):\n    # Split the text into sentences\n    sentences = [sentence for sentence in re.split(r'(?<=[.!?])\\s', text) if sentence]\n\n    # Create a CountVectorizer object\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    dtm = vectorizer.fit_transform(sentences)\n\n    # Create a DataFrame from the document-term matrix\n    df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef task_func(text):\n    # Split the text into sentences\n    sentences = [sentence for sentence in re.split(r'(?<=[.!?])\\s', text) if sentence]\n\n    # Create a CountVectorizer object\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    dtm = vectorizer.fit_transform(sentences)\n\n    # Create a DataFrame from the document-term matrix\n    df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df"}
{"task_id": "BigCodeBench/55", "solution": "import re\nimport pandas as pd\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(text):\n    # Split the text into sentences\n    sentences = re.split('[.!?]', text)\n    sentences = [s.strip() for s in sentences if s.strip()]\n\n    # Build a dictionary where each key is a sentence and the value is the number of words in the sentence\n    sentence_word_counts = {}\n    for i, sentence in enumerate(sentences, 1):\n        words = re.findall(r'\\b\\w+\\b', sentence)\n        words = [word for word in words if word.lower() not in STOPWORDS]\n        sentence_word_counts[f\"Sentence {i}\"] = len(words)\n\n    # Build a pandas Series from the dictionary\n    series = pd.Series(sentence_word_counts)\n\n    return series", "raw_solution": "import re\nimport pandas as pd\nfrom nltk.corpus import stopwords\n\nSTOPWORDS = set(stopwords.words('english'))\nSTOPWORDS.update([\"Those\", \"are\", \"the\", \"words\", \"to\", \"ignore\"])\n\ndef task_func(text):\n    # Split the text into sentences\n    sentences = re.split('[.!?]', text)\n    sentences = [s.strip() for s in sentences if s.strip()]\n\n    # Build a dictionary where each key is a sentence and the value is the number of words in the sentence\n    sentence_word_counts = {}\n    for i, sentence in enumerate(sentences, 1):\n        words = re.findall(r'\\b\\w+\\b', sentence)\n        words = [word for word in words if word.lower() not in STOPWORDS]\n        sentence_word_counts[f\"Sentence {i}\"] = len(words)\n\n    # Build a pandas Series from the dictionary\n    series = pd.Series(sentence_word_counts)\n\n    return series"}
{"task_id": "BigCodeBench/56", "solution": "import pandas as pd\nimport regex as re\ndef task_func(text):\n    # Split the text into lines\n    lines = text.split('\\n')\n\n    # Initialize lists to store the scores and categories\n    scores = []\n    categories = []\n\n    # Iterate over each line\n    for line in lines:\n        # Use regex to extract the score and category\n        match = re.search(r'Score: (\\d+), Category: (\\w+)', line)\n        if match:\n            scores.append(int(match.group(1)))\n            categories.append(match.group(2))\n\n    # Create a pandas DataFrame from the scores and categories\n    df = pd.DataFrame({\n        'Score': scores,\n        'Category': categories\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport regex as re\n\ndef task_func(text):\n    # Split the text into lines\n    lines = text.split('\\n')\n\n    # Initialize lists to store the scores and categories\n    scores = []\n    categories = []\n\n    # Iterate over each line\n    for line in lines:\n        # Use regex to extract the score and category\n        match = re.search(r'Score: (\\d+), Category: (\\w+)', line)\n        if match:\n            scores.append(int(match.group(1)))\n            categories.append(match.group(2))\n\n    # Create a pandas DataFrame from the scores and categories\n    df = pd.DataFrame({\n        'Score': scores,\n        'Category': categories\n    })\n\n    return df"}
{"task_id": "BigCodeBench/57", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(csv_file_path: str, title: str):\n    # Read the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Calculate the correlation matrix\n    corr = df.corr().round(2)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.title(title)\n    plt.show()\n\n    return corr, plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(csv_file_path: str, title: str):\n    # Read the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Calculate the correlation matrix\n    corr = df.corr().round(2)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.title(title)\n    plt.show()\n\n    return corr, plt.gca()"}
{"task_id": "BigCodeBench/58", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mu, sigma, num_samples):\n    # Generate a normal distribution\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Normal Distribution')\n\n    # Generate random samples from the normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Plot a histogram of the samples\n    plt.hist(samples, bins=30, density=True, alpha=0.5, label='Histogram')\n\n    # Set plot title and labels\n    plt.title('Normal Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.legend()\n\n    # Save the figure\n    fig = plt.gcf()\n\n    # Clear the current figure\n    plt.clf()\n\n    return fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(mu, sigma, num_samples):\n    # Generate a normal distribution\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Normal Distribution')\n\n    # Generate random samples from the normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Plot a histogram of the samples\n    plt.hist(samples, bins=30, density=True, alpha=0.5, label='Histogram')\n\n    # Set plot title and labels\n    plt.title('Normal Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.legend()\n\n    # Save the figure\n    fig = plt.gcf()\n\n    # Clear the current figure\n    plt.clf()\n\n    return fig"}
{"task_id": "BigCodeBench/59", "solution": "import wikipedia\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(page_title):\n    try:\n        # Search for the page\n        page = wikipedia.page(page_title)\n\n        # Generate a word cloud\n        wordcloud = WordCloud().generate(page.content)\n\n        # Display the word cloud\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis('off')\n        plt.show()\n\n        return plt.gca()\n    except wikipedia.exceptions.PageError:\n        # If the page does not exist, return None\n        return None", "raw_solution": "import wikipedia\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(page_title):\n    try:\n        # Search for the page\n        page = wikipedia.page(page_title)\n\n        # Generate a word cloud\n        wordcloud = WordCloud().generate(page.content)\n\n        # Display the word cloud\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis('off')\n        plt.show()\n\n        return plt.gca()\n    except wikipedia.exceptions.PageError:\n        # If the page does not exist, return None\n        return None"}
{"task_id": "BigCodeBench/60", "solution": "import json\nimport pandas as pd\ndef task_func(result, csv_file_path=\"test.csv\", json_file_path=\"test.json\"):\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(result)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file_path, index=False)\n\n    # Save the DataFrame to a JSON file\n    with open(json_file_path, 'w') as f:\n        json.dump(result, f)\n\n    return None", "raw_solution": "import json\nimport pandas as pd\n\ndef task_func(result, csv_file_path=\"test.csv\", json_file_path=\"test.json\"):\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(result)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file_path, index=False)\n\n    # Save the DataFrame to a JSON file\n    with open(json_file_path, 'w') as f:\n        json.dump(result, f)\n\n    return None"}
{"task_id": "BigCodeBench/61", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nPLOT_TITLE = 'Square root plot'\nX_LABEL = 'x'\nY_LABEL = 'sqrt(x)'\nTIME_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(input_list, plot_title=PLOT_TITLE, x_label=X_LABEL, y_label=Y_LABEL):\n    \"\"\"\n    Plots the square root function for values associated with the key 'from_user' from the input list of dictionaries.\n    Annotates the graph with the current date and time.\n    \"\"\"\n    # Extract 'from_user' values from the input list of dictionaries\n    x_values = [d['from_user'] for d in input_list]\n\n    # Calculate square root values\n    y_values = np.sqrt(x_values)\n\n    # Create the plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(x_values, y_values, marker='o')\n    plt.title(plot_title)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n\n    # Annotate the graph with the current date and time\n    current_time = datetime.now().strftime(TIME_FORMAT)\n    plt.annotate(current_time, xy=(1, 1), xycoords='axes fraction', xytext=(5, -5), textcoords='offset points', ha='left', va='top')\n\n    plt.grid(True)\n    plt.show()\n\n    return y_values", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Constants\nPLOT_TITLE = 'Square root plot'\nX_LABEL = 'x'\nY_LABEL = 'sqrt(x)'\nTIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(input_list, plot_title=PLOT_TITLE, x_label=X_LABEL, y_label=Y_LABEL):\n    \"\"\"\n    Plots the square root function for values associated with the key 'from_user' from the input list of dictionaries.\n    Annotates the graph with the current date and time.\n    \"\"\"\n    # Extract 'from_user' values from the input list of dictionaries\n    x_values = [d['from_user'] for d in input_list]\n\n    # Calculate square root values\n    y_values = np.sqrt(x_values)\n\n    # Create the plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(x_values, y_values, marker='o')\n    plt.title(plot_title)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n\n    # Annotate the graph with the current date and time\n    current_time = datetime.now().strftime(TIME_FORMAT)\n    plt.annotate(current_time, xy=(1, 1), xycoords='axes fraction', xytext=(5, -5), textcoords='offset points', ha='left', va='top')\n\n    plt.grid(True)\n    plt.show()\n\n    return y_values"}
{"task_id": "BigCodeBench/62", "solution": "import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(result, colors=['b', 'g', 'r', 'c', 'm', 'y', 'k']):\n    \"\"\"\n    Draws a histogram of the \"from_user\" values in the provided result. The color of the histogram bars is selected at random from the provided colors list.\n    \"\"\"\n    # Extract the \"from_user\" values\n    values = [row['from_user'] for row in result]\n\n    # Select a random color for the histogram bars\n    color = random.choice(colors)\n\n    # Create the histogram\n    plt.hist(values, color=color)\n    plt.xlabel('from_user')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of from_user values')\n    plt.show()\n\n    return None", "raw_solution": "import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(result, colors=['b', 'g', 'r', 'c', 'm', 'y', 'k']):\n    \"\"\"\n    Draws a histogram of the \"from_user\" values in the provided result. The color of the histogram bars is selected at random from the provided colors list.\n    \"\"\"\n    # Extract the \"from_user\" values\n    values = [row['from_user'] for row in result]\n\n    # Select a random color for the histogram bars\n    color = random.choice(colors)\n\n    # Create the histogram\n    plt.hist(values, color=color)\n    plt.xlabel('from_user')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of from_user values')\n    plt.show()\n\n    return None"}
{"task_id": "BigCodeBench/63", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(car_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(car_dict.items()), columns=['Car', 'Color'])\n\n    # Count the number of cars for each color\n    color_counts = df['Color'].value_counts()\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    color_counts.plot(kind='bar', ax=ax)\n    ax.set_title('Distribution of Vehicle Colors')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(car_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(car_dict.items()), columns=['Car', 'Color'])\n\n    # Count the number of cars for each color\n    color_counts = df['Color'].value_counts()\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    color_counts.plot(kind='bar', ax=ax)\n    ax.set_title('Distribution of Vehicle Colors')\n\n    return df, ax"}
{"task_id": "BigCodeBench/64", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    \"\"\"\n    Analyzes the distribution of different values in a column \"col3\" of a pandas DataFrame df,\n    grouped by \"col1\" and \"col2,\" using a heatmap.\n\n    Args:\n    data (list): A list of elements, each element is a list with the same length as COLUMNS,\n                 representing one row a dataframe df to create.\n\n    Returns:\n    tuple:\n        pandas.DataFrame: The DataFrame of the analyzed data.\n        plt.Axes: The heatmap visualization.\n    \"\"\"\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Group by 'col1' and 'col2' and calculate the distribution of 'col3'\n    grouped = df.groupby(['col1', 'col2'])['col3'].value_counts().unstack(fill_value=0)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(grouped, cmap='viridis', annot=True, fmt='d')\n\n    return df, heatmap.axes", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    \"\"\"\n    Analyzes the distribution of different values in a column \"col3\" of a pandas DataFrame df,\n    grouped by \"col1\" and \"col2,\" using a heatmap.\n\n    Args:\n    data (list): A list of elements, each element is a list with the same length as COLUMNS,\n                 representing one row a dataframe df to create.\n\n    Returns:\n    tuple:\n        pandas.DataFrame: The DataFrame of the analyzed data.\n        plt.Axes: The heatmap visualization.\n    \"\"\"\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Group by 'col1' and 'col2' and calculate the distribution of 'col3'\n    grouped = df.groupby(['col1', 'col2'])['col3'].value_counts().unstack(fill_value=0)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(grouped, cmap='viridis', annot=True, fmt='d')\n\n    return df, heatmap.axes"}
{"task_id": "BigCodeBench/65", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    df = pd.DataFrame(data, columns=COLUMNS)\n    grouped = df.groupby(COLUMNS[:-1].copy())[COLUMNS[-1]].unique()\n    x_label = \"-\".join(COLUMNS[:-1])\n    y_label = COLUMNS[-1]\n    fig, ax = plt.subplots()\n    grouped.plot(kind='line', ax=ax)\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)\n    plt.tight_layout()\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    df = pd.DataFrame(data, columns=COLUMNS)\n    grouped = df.groupby(COLUMNS[:-1].copy())[COLUMNS[-1]].unique()\n    x_label = \"-\".join(COLUMNS[:-1])\n    y_label = COLUMNS[-1]\n    fig, ax = plt.subplots()\n    grouped.plot(kind='line', ax=ax)\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)\n    plt.tight_layout()\n    return df, ax"}
{"task_id": "BigCodeBench/66", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    # Create a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Group by 'col1' and 'col2' and create a new dataframe\n    grouped_df = df.groupby(['col1', 'col2'])['col3'].apply(list).reset_index()\n\n    # Create a distribution plot\n    plt.figure(figsize=(10, 6))\n    sns.distplot(grouped_df['col3'], kde=False, bins=30)\n    plt.xlabel('col3')\n    plt.title('Distribution of col3 grouped by col1 and col2')\n\n    return grouped_df, plt.gca()", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    # Create a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Group by 'col1' and 'col2' and create a new dataframe\n    grouped_df = df.groupby(['col1', 'col2'])['col3'].apply(list).reset_index()\n\n    # Create a distribution plot\n    plt.figure(figsize=(10, 6))\n    sns.distplot(grouped_df['col3'], kde=False, bins=30)\n    plt.xlabel('col3')\n    plt.title('Distribution of col3 grouped by col1 and col2')\n\n    return grouped_df, plt.gca()"}
{"task_id": "BigCodeBench/67", "solution": "import pandas as pd\nimport re\nimport os\ndef task_func(dir_path: str, pattern: str = '^EMP'):\n    \"\"\"\n    Finds all ascendingly sorted files in a directory that start with a given pattern,\n    and returns the number of files against their size.\n    \"\"\"\n    # Initialize an empty list to store file names and sizes\n    files_sizes = []\n\n    # Iterate over all files in the directory\n    for file in os.listdir(dir_path):\n        # Check if the file starts with the given pattern and is ascendingly sorted\n        if re.match(pattern, file) and file.endswith('.txt'):\n            # Get the file size\n            file_size = os.path.getsize(os.path.join(dir_path, file))\n            # Append the file name and size to the list\n            files_sizes.append((file, file_size))\n\n    # Convert the list to a pandas DataFrame\n    df = pd.DataFrame(files_sizes, columns=['File', 'Size'])\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nimport os\n\ndef task_func(dir_path: str, pattern: str = '^EMP'):\n    \"\"\"\n    Finds all ascendingly sorted files in a directory that start with a given pattern,\n    and returns the number of files against their size.\n    \"\"\"\n    # Initialize an empty list to store file names and sizes\n    files_sizes = []\n\n    # Iterate over all files in the directory\n    for file in os.listdir(dir_path):\n        # Check if the file starts with the given pattern and is ascendingly sorted\n        if re.match(pattern, file) and file.endswith('.txt'):\n            # Get the file size\n            file_size = os.path.getsize(os.path.join(dir_path, file))\n            # Append the file name and size to the list\n            files_sizes.append((file, file_size))\n\n    # Convert the list to a pandas DataFrame\n    df = pd.DataFrame(files_sizes, columns=['File', 'Size'])\n\n    return df"}
{"task_id": "BigCodeBench/68", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data='/path/to/data.csv', emp_prefix='EMP'):\n    # Load CSV file into a DataFrame\n    df = pd.read_csv(data)\n\n    # Filter lines where the employee ID begins with the given prefix\n    filtered_df = df[df['Employee ID'].str.startswith(emp_prefix)]\n\n    # Draw a histogram of the age\n    plt.figure(figsize=(10, 6))\n    sns.histplot(filtered_df['Age'], bins=30, kde=False)\n    plt.title('Histogram of Age')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return filtered_df, plt.gca()", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data='/path/to/data.csv', emp_prefix='EMP'):\n    # Load CSV file into a DataFrame\n    df = pd.read_csv(data)\n\n    # Filter lines where the employee ID begins with the given prefix\n    filtered_df = df[df['Employee ID'].str.startswith(emp_prefix)]\n\n    # Draw a histogram of the age\n    plt.figure(figsize=(10, 6))\n    sns.histplot(filtered_df['Age'], bins=30, kde=False)\n    plt.title('Histogram of Age')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return filtered_df, plt.gca()"}
{"task_id": "BigCodeBench/69", "solution": "import random\nimport matplotlib.pyplot as plt\nSALARY_RANGE = (20000, 100000)\ndef task_func(dict1):\n    \"\"\"\n    Generates random salaries for each employee in the department with code 'EMPXX',\n    creates a histogram, and returns the Axes object representing the histogram.\n    \"\"\"\n    # Generate random salaries\n    salaries = [random.randint(*SALARY_RANGE) for _ in range(dict1['EMPXX'])]\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(salaries, bins='auto', color='#007BFF', edgecolor='black')\n\n    # Set title and labels\n    ax.set_title('Salary Distribution in EMPXX Department')\n    ax.set_xlabel('Salary')\n    ax.set_ylabel('Number of Employees')\n\n    return ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\n# Constants\nSALARY_RANGE = (20000, 100000)\n\ndef task_func(dict1):\n    \"\"\"\n    Generates random salaries for each employee in the department with code 'EMPXX',\n    creates a histogram, and returns the Axes object representing the histogram.\n    \"\"\"\n    # Generate random salaries\n    salaries = [random.randint(*SALARY_RANGE) for _ in range(dict1['EMPXX'])]\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(salaries, bins='auto', color='#007BFF', edgecolor='black')\n\n    # Set title and labels\n    ax.set_title('Salary Distribution in EMPXX Department')\n    ax.set_xlabel('Salary')\n    ax.set_ylabel('Number of Employees')\n\n    return ax"}
{"task_id": "BigCodeBench/70", "solution": "import pandas as pd\nimport json\nimport matplotlib.pyplot as plt\nCOLUMNS = ['email', 'list', 'sum', 'mean']\ndef task_func(json_file):\n    \"\"\"\n    Loads e-mail data from a JSON file, calculates the sum and mean of the list associated with each e-mail,\n    and records those values. Also, it plots the sum and mean values for each email.\n    \"\"\"\n    # Load data from JSON file\n    try:\n        with open(json_file, 'r') as file:\n            data = json.load(file)\n    except FileNotFoundError:\n        return pd.DataFrame(columns=COLUMNS), None\n\n    # Convert data into a DataFrame\n    df = pd.DataFrame(data)\n\n    # Calculate sum and mean for each email\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(lambda x: sum(x)/len(x))\n\n    # Plot sum and mean values for each email\n    fig, ax = plt.subplots()\n    df.plot(x='email', y=['sum', 'mean'], ax=ax)\n\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport json\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['email', 'list', 'sum', 'mean']\n\ndef task_func(json_file):\n    \"\"\"\n    Loads e-mail data from a JSON file, calculates the sum and mean of the list associated with each e-mail,\n    and records those values. Also, it plots the sum and mean values for each email.\n    \"\"\"\n    # Load data from JSON file\n    try:\n        with open(json_file, 'r') as file:\n            data = json.load(file)\n    except FileNotFoundError:\n        return pd.DataFrame(columns=COLUMNS), None\n\n    # Convert data into a DataFrame\n    df = pd.DataFrame(data)\n\n    # Calculate sum and mean for each email\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(lambda x: sum(x)/len(x))\n\n    # Plot sum and mean values for each email\n    fig, ax = plt.subplots()\n    df.plot(x='email', y=['sum', 'mean'], ax=ax)\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/71", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport ast\ndef task_func(csv_file):\n    # Load e-mail data from a CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert the 'list' column from string to list\n    df['list'] = df['list'].apply(lambda x: ast.literal_eval(x))\n\n    # Calculate the sum, mean, and standard deviation of the list associated with each e-mail\n    df['sum'] = df['list'].apply(lambda x: sum(x))\n    df['mean'] = df['list'].apply(lambda x: np.mean(x))\n    df['std'] = df['list'].apply(lambda x: np.std(x))\n\n    # Draw a histogram of the mean values\n    plot = sns.histplot(df['mean'], kde=False)\n\n    return (df, plot)", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport ast\n\ndef task_func(csv_file):\n    # Load e-mail data from a CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert the 'list' column from string to list\n    df['list'] = df['list'].apply(lambda x: ast.literal_eval(x))\n\n    # Calculate the sum, mean, and standard deviation of the list associated with each e-mail\n    df['sum'] = df['list'].apply(lambda x: sum(x))\n    df['mean'] = df['list'].apply(lambda x: np.mean(x))\n    df['std'] = df['list'].apply(lambda x: np.std(x))\n\n    # Draw a histogram of the mean values\n    plot = sns.histplot(df['mean'], kde=False)\n\n    return (df, plot)"}
{"task_id": "BigCodeBench/72", "solution": "import pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\ndef task_func(directory):\n    # Initialize variables\n    max_len = 0\n    max_file = None\n    df_list = []\n\n    # Traverse directory for CSV files\n    for file in os.listdir(directory):\n        if file.endswith('.csv'):\n            if len(file) > max_len:\n                max_len = len(file)\n                max_file = file\n\n    # If no CSV file found, return empty dataframe\n    if max_file is None:\n        return pd.DataFrame(columns=['email', 'list', 'sum', 'mean', 'median']), None\n\n    # Load CSV file into DataFrame\n    df = pd.read_csv(os.path.join(directory, max_file))\n\n    # Convert 'list' column from string to list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and median\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(lambda x: np.mean(x))\n    df['median'] = df['list'].apply(lambda x: np.median(x))\n\n    # Draw histogram of median\n    plt.figure(figsize=(10, 6))\n    plt.hist(df['median'], bins=10, color='blue', edgecolor='black')\n    plt.xlabel('Median')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Median')\n    ax = plt.gca()\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\n\ndef task_func(directory):\n    # Initialize variables\n    max_len = 0\n    max_file = None\n    df_list = []\n\n    # Traverse directory for CSV files\n    for file in os.listdir(directory):\n        if file.endswith('.csv'):\n            if len(file) > max_len:\n                max_len = len(file)\n                max_file = file\n\n    # If no CSV file found, return empty dataframe\n    if max_file is None:\n        return pd.DataFrame(columns=['email', 'list', 'sum', 'mean', 'median']), None\n\n    # Load CSV file into DataFrame\n    df = pd.read_csv(os.path.join(directory, max_file))\n\n    # Convert 'list' column from string to list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and median\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(lambda x: np.mean(x))\n    df['median'] = df['list'].apply(lambda x: np.median(x))\n\n    # Draw histogram of median\n    plt.figure(figsize=(10, 6))\n    plt.hist(df['median'], bins=10, color='blue', edgecolor='black')\n    plt.xlabel('Median')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Median')\n    ax = plt.gca()\n\n    return df, ax"}
{"task_id": "BigCodeBench/73", "solution": "import pandas as pd\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\ndef task_func(db_file):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # Load data from the 'EmailData' table\n    df = pd.read_sql_query(\"SELECT * from EmailData\", conn)\n\n    # Convert the 'list' column from string to list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate the sum, mean, and variance of the list for each email\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['var'] = df['list'].apply(np.var)\n\n    # Plot the sum, mean, and variance\n    fig, ax = plt.subplots()\n    df[['sum', 'mean', 'var']].plot(kind='bar', ax=ax)\n    ax.set_xlabel('Email')\n    ax.set_ylabel('Value')\n    ax.legend(loc='upper left')\n\n    # Close the database connection\n    conn.close()\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\n\ndef task_func(db_file):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # Load data from the 'EmailData' table\n    df = pd.read_sql_query(\"SELECT * from EmailData\", conn)\n\n    # Convert the 'list' column from string to list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate the sum, mean, and variance of the list for each email\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['var'] = df['list'].apply(np.var)\n\n    # Plot the sum, mean, and variance\n    fig, ax = plt.subplots()\n    df[['sum', 'mean', 'var']].plot(kind='bar', ax=ax)\n    ax.set_xlabel('Email')\n    ax.set_ylabel('Value')\n    ax.legend(loc='upper left')\n\n    # Close the database connection\n    conn.close()\n\n    return df, ax"}
{"task_id": "BigCodeBench/74", "solution": "import socket\nimport requests\nfrom requests.exceptions import ConnectionError\ndef task_func(host):\n    if host is None or host == '':\n        raise ValueError(\"'host' cannot be None or an empty string\")\n\n    try:\n        ip_address = socket.gethostbyname(host)\n    except socket.gaierror:\n        raise ConnectionError(f\"Failed to resolve host: {host}\")\n\n    try:\n        response = requests.get(f\"https://ipinfo.io/{ip_address}\")\n        response.raise_for_status()\n    except (requests.exceptions.RequestException, requests.exceptions.HTTPError):\n        raise ConnectionError(\"Failed to connect to the geolocation service\")\n\n    geolocation_info = response.json()\n    return {'IP Address': ip_address, 'Geolocation Information': geolocation_info}", "raw_solution": "import socket\nimport requests\nfrom requests.exceptions import ConnectionError\n\ndef task_func(host):\n    if host is None or host == '':\n        raise ValueError(\"'host' cannot be None or an empty string\")\n\n    try:\n        ip_address = socket.gethostbyname(host)\n    except socket.gaierror:\n        raise ConnectionError(f\"Failed to resolve host: {host}\")\n\n    try:\n        response = requests.get(f\"https://ipinfo.io/{ip_address}\")\n        response.raise_for_status()\n    except (requests.exceptions.RequestException, requests.exceptions.HTTPError):\n        raise ConnectionError(\"Failed to connect to the geolocation service\")\n\n    geolocation_info = response.json()\n    return {'IP Address': ip_address, 'Geolocation Information': geolocation_info}"}
{"task_id": "BigCodeBench/75", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\nfrom datetime import datetime, timedelta\nimport seaborn as sns\ndef task_func(df, fruits=None, days=None, seed=None, sales_lower_bound=1, sales_upper_bound=50):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"'df' should be a pandas DataFrame\")\n    if not df.empty:\n        raise ValueError(\"'df' should be empty\")\n    if not fruits:\n        fruits = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n    if not days:\n        days = 30\n    if not seed:\n        seed = np.random.randint(0, 100)\n    if not isinstance(sales_lower_bound, int) or not isinstance(sales_upper_bound, int):\n        raise TypeError(\"'sales_lower_bound' and 'sales_upper_bound' should be integers\")\n    if sales_lower_bound >= sales_upper_bound:\n        raise ValueError(\"'sales_lower_bound' should be less than 'sales_upper_bound'\")\n\n    np.random.seed(seed)\n    dates = pd.date_range(datetime.now(), periods=days)\n    data = {\n        'Date': dates,\n        'Fruit': np.random.choice(fruits, size=days),\n        'Sales': np.random.randint(sales_lower_bound, sales_upper_bound, size=days)\n    }\n    df = pd.DataFrame(data)\n    df.set_index('Date', inplace=True)\n\n    g = sns.boxplot(x=\"Fruit\", y=\"Sales\", data=df)\n\n    return df, g", "raw_solution": "import pandas as pd\nimport numpy as np\nimport itertools\nfrom datetime import datetime, timedelta\nimport seaborn as sns\n\ndef task_func(df, fruits=None, days=None, seed=None, sales_lower_bound=1, sales_upper_bound=50):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"'df' should be a pandas DataFrame\")\n    if not df.empty:\n        raise ValueError(\"'df' should be empty\")\n    if not fruits:\n        fruits = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n    if not days:\n        days = 30\n    if not seed:\n        seed = np.random.randint(0, 100)\n    if not isinstance(sales_lower_bound, int) or not isinstance(sales_upper_bound, int):\n        raise TypeError(\"'sales_lower_bound' and 'sales_upper_bound' should be integers\")\n    if sales_lower_bound >= sales_upper_bound:\n        raise ValueError(\"'sales_lower_bound' should be less than 'sales_upper_bound'\")\n\n    np.random.seed(seed)\n    dates = pd.date_range(datetime.now(), periods=days)\n    data = {\n        'Date': dates,\n        'Fruit': np.random.choice(fruits, size=days),\n        'Sales': np.random.randint(sales_lower_bound, sales_upper_bound, size=days)\n    }\n    df = pd.DataFrame(data)\n    df.set_index('Date', inplace=True)\n\n    g = sns.boxplot(x=\"Fruit\", y=\"Sales\", data=df)\n\n    return df, g"}
{"task_id": "BigCodeBench/76", "solution": "import random\nimport string\nfrom django.http import HttpResponse\ndef task_func(request, session_expire_time):\n    \"\"\"\n    This function creates a random session key comprising letters and digits with a specific length of 20,\n    then sets this key in a cookie on an HttpResponse object with the specified expiration time.\n    \"\"\"\n    # Generate a random session key\n    session_key = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n\n    # Check if the session key contains both letters and digits\n    if not any(c.isdigit() for c in session_key) or not any(c.isalpha() for c in session_key):\n        raise ValueError(\"Session key must contain both letters and digits\")\n\n    # Set the session key in a cookie on the HttpResponse object\n    response = HttpResponse(\"Session key generated successfully.\")\n    response.set_cookie('session_key', session_key, max_age=session_expire_time)\n\n    return response", "raw_solution": "import random\nimport string\nfrom django.http import HttpResponse\n\ndef task_func(request, session_expire_time):\n    \"\"\"\n    This function creates a random session key comprising letters and digits with a specific length of 20,\n    then sets this key in a cookie on an HttpResponse object with the specified expiration time.\n    \"\"\"\n    # Generate a random session key\n    session_key = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n\n    # Check if the session key contains both letters and digits\n    if not any(c.isdigit() for c in session_key) or not any(c.isalpha() for c in session_key):\n        raise ValueError(\"Session key must contain both letters and digits\")\n\n    # Set the session key in a cookie on the HttpResponse object\n    response = HttpResponse(\"Session key generated successfully.\")\n    response.set_cookie('session_key', session_key, max_age=session_expire_time)\n\n    return response"}
{"task_id": "BigCodeBench/77", "solution": "import hashlib\nimport base64\nimport binascii\nfrom django.http import HttpResponseBadRequest, HttpResponse\ndef task_func(data):\n    try:\n        username = data['username']\n        password = data['password']\n    except KeyError:\n        return HttpResponseBadRequest('Bad Request.')\n\n    try:\n        decoded_password = base64.b64decode(password)\n    except (UnicodeDecodeError, binascii.Error, ValueError):\n        return HttpResponseBadRequest('Bad Request.')\n\n    hashed_password = hashlib.sha256(decoded_password).digest()\n    expected_password = base64.b64encode(hashlib.sha256('password'.encode()).digest()).decode()\n\n    if username == 'admin' and hashed_password == base64.b64decode(expected_password):\n        return HttpResponse('Login successful.', status=400)\n    else:\n        return HttpResponse('Login failed.', status=401)", "raw_solution": "import hashlib\nimport base64\nimport binascii\nfrom django.http import HttpResponseBadRequest, HttpResponse\n\ndef task_func(data):\n    try:\n        username = data['username']\n        password = data['password']\n    except KeyError:\n        return HttpResponseBadRequest('Bad Request.')\n\n    try:\n        decoded_password = base64.b64decode(password)\n    except (UnicodeDecodeError, binascii.Error, ValueError):\n        return HttpResponseBadRequest('Bad Request.')\n\n    hashed_password = hashlib.sha256(decoded_password).digest()\n    expected_password = base64.b64encode(hashlib.sha256('password'.encode()).digest()).decode()\n\n    if username == 'admin' and hashed_password == base64.b64decode(expected_password):\n        return HttpResponse('Login successful.', status=400)\n    else:\n        return HttpResponse('Login failed.', status=401)"}
{"task_id": "BigCodeBench/78", "solution": "import csv\nimport io\nfrom django.http import HttpRequest, FileResponse", "raw_solution": "import csv\nimport io\nfrom django.http import HttpRequest, FileResponse\n\ndef generate_csv_response(request: HttpRequest, header: list, csv_data: list) -> FileResponse:\n    \"\"\"\n    Generates a CSV file response from a Django HttpRequest.\n    \"\"\"\n    # Create a StringIO object\n    output = io.StringIO()\n\n    # Create a CSV writer\n    writer = csv.writer(output)\n\n    # Write the header\n    writer.writerow(header)\n\n    # Write the CSV data\n    for row in csv_data:\n        writer.writerow(row)\n\n    # Reset the file pointer to the start\n    output.seek(0)\n\n    # Create a FileResponse object\n    response = FileResponse(output, as_attachment=True, filename='output.csv')\n\n    return response"}
{"task_id": "BigCodeBench/79", "solution": "import zipfile\nimport io\nfrom django.http import FileResponse, HttpRequest\nfrom django.conf import settings\ndef task_func(request: HttpRequest, file_paths: list) -> FileResponse:\n    \"\"\"\n    Generates a ZIP file response for a Django HttpRequest, zipping the specified files.\n    This function is useful for scenarios where multiple file downloads are required in response to a web request.\n    The actual HttpRequest is not utilized within the function but is required for compatibility with Django view structures.\n    The function should output with:\n        FileResponse: A Django FileResponse object containing the ZIP file as an attachment.\n    \"\"\"\n    # Create a buffer to hold the ZIP file\n    zip_buffer = io.BytesIO()\n\n    # Create a ZIP file object\n    with zipfile.ZipFile(zip_buffer, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n        # Add each file to the ZIP file\n        for file_path in file_paths:\n            # Get the absolute path to the file\n            abs_path = settings.BASE_DIR / file_path\n\n            # Add the file to the ZIP file\n            zip_file.write(abs_path, arcname=file_path)\n\n    # Reset the buffer position to the start\n    zip_buffer.seek(0)\n\n    # Create a FileResponse object with the ZIP file as the content\n    response = FileResponse(zip_buffer, as_attachment=True, filename='files.zip')\n\n    return response", "raw_solution": "import zipfile\nimport io\nfrom django.http import FileResponse, HttpRequest\nfrom django.conf import settings\n\ndef task_func(request: HttpRequest, file_paths: list) -> FileResponse:\n    \"\"\"\n    Generates a ZIP file response for a Django HttpRequest, zipping the specified files.\n    This function is useful for scenarios where multiple file downloads are required in response to a web request.\n    The actual HttpRequest is not utilized within the function but is required for compatibility with Django view structures.\n    The function should output with:\n        FileResponse: A Django FileResponse object containing the ZIP file as an attachment.\n    \"\"\"\n    # Create a buffer to hold the ZIP file\n    zip_buffer = io.BytesIO()\n\n    # Create a ZIP file object\n    with zipfile.ZipFile(zip_buffer, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n        # Add each file to the ZIP file\n        for file_path in file_paths:\n            # Get the absolute path to the file\n            abs_path = settings.BASE_DIR / file_path\n\n            # Add the file to the ZIP file\n            zip_file.write(abs_path, arcname=file_path)\n\n    # Reset the buffer position to the start\n    zip_buffer.seek(0)\n\n    # Create a FileResponse object with the ZIP file as the content\n    response = FileResponse(zip_buffer, as_attachment=True, filename='files.zip')\n\n    return response"}
{"task_id": "BigCodeBench/80", "solution": "from flask import Flask, render_template, request\nimport json\nimport logging\ndef task_func(template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n\n    @app.route('/', methods=['POST'])\n    def root():\n        data = request.get_json()\n        logging.info(json.dumps(data))\n        return render_template('index.html', data=data)\n\n    return app", "raw_solution": "from flask import Flask, render_template, request\nimport json\nimport logging\nlogging.basicConfig(filename=\"out.log\", level=logging.INFO)\n\ndef task_func(template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n\n    @app.route('/', methods=['POST'])\n    def root():\n        data = request.get_json()\n        logging.info(json.dumps(data))\n        return render_template('index.html', data=data)\n\n    return app"}
{"task_id": "BigCodeBench/81", "solution": "from flask import Flask, jsonify\nfrom flask_restful import Resource, Api\nimport requests\ndef task_func(api_url, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    api = Api(app)\n\n    class ExternalApi(Resource):\n        def get(self):\n            response = requests.get(api_url)\n            return jsonify(response.json())\n\n    api.add_resource(ExternalApi, '/api')\n\n    return app", "raw_solution": "from flask import Flask, jsonify\nfrom flask_restful import Resource, Api\nimport requests\n\ndef task_func(api_url, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    api = Api(app)\n\n    class ExternalApi(Resource):\n        def get(self):\n            response = requests.get(api_url)\n            return jsonify(response.json())\n\n    api.add_resource(ExternalApi, '/api')\n\n    return app"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash", "raw_solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass User(UserMixin):\n    def __init__(self, username, password):\n        self.username = username\n        self.password = generate_password_hash(password)\n\n    def check_password(self, password):\n        return check_password_hash(self.password, password)\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\n@login_manager.user_loader\ndef load_user(user_id):\n    for user in users:\n        if user.username == user_id:\n            return user\n    return None\n\n@login_manager.unauthorized_handler\ndef unauthorized_handler():\n    return 'Unauthorized', 401\n\ndef create_app(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n\n    users = [User('admin', 'password')]\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            for user in users:\n                if user.username == form.username.data and user.check_password(form.password.data):\n                    login_user(user)\n                    return redirect(url_for('protected'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return 'Logged in as: ' + current_user.username\n\n    return app"}
{"task_id": "BigCodeBench/83", "solution": "from flask import Flask\nfrom flask_mail import Mail, Message\ndef task_func(smtp_server, smtp_port, smtp_user, smtp_password, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['MAIL_SERVER'] = smtp_server\n    app.config['MAIL_PORT'] = smtp_port\n    app.config['MAIL_USERNAME'] = smtp_user\n    app.config['MAIL_PASSWORD'] = smtp_password\n    app.config['MAIL_USE_TLS'] = True\n    mail = Mail(app)\n\n    @app.route('/send_email')\n    def send_email():\n        msg = Message('Hello', sender=smtp_user, recipients=['recipient@example.com'])\n        msg.body = 'This is a test email'\n        mail.send(msg)\n        return 'Email sent'\n\n    return app", "raw_solution": "from flask import Flask\nfrom flask_mail import Mail, Message\n\ndef task_func(smtp_server, smtp_port, smtp_user, smtp_password, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['MAIL_SERVER'] = smtp_server\n    app.config['MAIL_PORT'] = smtp_port\n    app.config['MAIL_USERNAME'] = smtp_user\n    app.config['MAIL_PASSWORD'] = smtp_password\n    app.config['MAIL_USE_TLS'] = True\n    mail = Mail(app)\n\n    @app.route('/send_email')\n    def send_email():\n        msg = Message('Hello', sender=smtp_user, recipients=['recipient@example.com'])\n        msg.body = 'This is a test email'\n        mail.send(msg)\n        return 'Email sent'\n\n    return app"}
{"task_id": "BigCodeBench/84", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(products, n_samples=100, sales_lower=50, sales_upper=200, profit_margin_min=0.1, profit_margin_max=0.5, random_seed=42):\n    # Validate input types\n    if not isinstance(products, list) or not all(isinstance(product, str) for product in products):\n        raise TypeError(\"products must be a list of strings\")\n    if not isinstance(n_samples, int) or n_samples <= 0:\n        raise ValueError(\"n_samples must be a positive integer\")\n    if not isinstance(sales_lower, (int, float)) or not isinstance(sales_upper, (int, float)):\n        raise TypeError(\"sales_lower and sales_upper must be numeric\")\n    if sales_lower > sales_upper:\n        raise ValueError(\"sales_lower must be less than or equal to sales_upper\")\n    if not isinstance(profit_margin_min, (int, float)) or not isinstance(profit_margin_max, (int, float)):\n        raise TypeError(\"profit_margin_min and profit_margin_max must be numeric\")\n    if profit_margin_min > profit_margin_max:\n        raise ValueError(\"profit_margin_min must be less than or equal to profit_margin_max\")\n\n    # Set random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Generate sales and profit data\n    sales = np.random.randint(sales_lower, sales_upper, n_samples)\n    profit_margins = np.random.uniform(profit_margin_min, profit_margin_max, n_samples)\n    profits = sales * profit_margins\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Product\": np.random.choice(products, n_samples),\n        \"Sales\": sales,\n        \"Profit Margin\": profit_margins,\n        \"Profit\": profits\n    })\n\n    # Aggregate by product and sort by total profit\n    df_agg = df.groupby(\"Product\").agg({\"Sales\": \"sum\", \"Profit\": \"sum\"}).sort_values(\"Profit\", ascending=False)\n\n    return df_agg", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(products, n_samples=100, sales_lower=50, sales_upper=200, profit_margin_min=0.1, profit_margin_max=0.5, random_seed=42):\n    # Validate input types\n    if not isinstance(products, list) or not all(isinstance(product, str) for product in products):\n        raise TypeError(\"products must be a list of strings\")\n    if not isinstance(n_samples, int) or n_samples <= 0:\n        raise ValueError(\"n_samples must be a positive integer\")\n    if not isinstance(sales_lower, (int, float)) or not isinstance(sales_upper, (int, float)):\n        raise TypeError(\"sales_lower and sales_upper must be numeric\")\n    if sales_lower > sales_upper:\n        raise ValueError(\"sales_lower must be less than or equal to sales_upper\")\n    if not isinstance(profit_margin_min, (int, float)) or not isinstance(profit_margin_max, (int, float)):\n        raise TypeError(\"profit_margin_min and profit_margin_max must be numeric\")\n    if profit_margin_min > profit_margin_max:\n        raise ValueError(\"profit_margin_min must be less than or equal to profit_margin_max\")\n\n    # Set random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Generate sales and profit data\n    sales = np.random.randint(sales_lower, sales_upper, n_samples)\n    profit_margins = np.random.uniform(profit_margin_min, profit_margin_max, n_samples)\n    profits = sales * profit_margins\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Product\": np.random.choice(products, n_samples),\n        \"Sales\": sales,\n        \"Profit Margin\": profit_margins,\n        \"Profit\": profits\n    })\n\n    # Aggregate by product and sort by total profit\n    df_agg = df.groupby(\"Product\").agg({\"Sales\": \"sum\", \"Profit\": \"sum\"}).sort_values(\"Profit\", ascending=False)\n\n    return df_agg"}
{"task_id": "BigCodeBench/85", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\ndef generate_weather_data(start_date, end_date, random_seed=42):\n    \"\"\"\n    Generates a DataFrame containing simulated daily weather data within the specified date range.\n    The function generates random values for temperature, humidity, and wind speed for each day.\n    The function also plots these parameters over the date range and returns both the DataFrame and the plot object.\n    \"\"\"\n    if end_date < start_date:\n        raise ValueError(\"'end_date' cannot be before 'start_date'\")\n\n    np.random.seed(random_seed)\n\n    # Generate dates\n    dates = pd.date_range(start=start_date, end=end_date)\n\n    # Generate random data\n    temperatures = np.random.uniform(-10, 40, size=len(dates))\n    humidities = np.random.uniform(20, 100, size=len(dates))\n    wind_speeds = np.random.uniform(0, 20, size=len(dates))\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'Date': dates,\n        'Temperature': temperatures,\n        'Humidity': humidities,\n        'Wind Speed': wind_speeds\n    })\n\n    # Plot data\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Date'], df['Temperature'], label='Temperature')\n    ax.plot(df['Date'], df['Humidity'], label='Humidity')\n    ax.plot(df['Date'], df['Wind Speed'], label='Wind Speed')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/86", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(students=[\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"], seed=42):\n    np.random.seed(seed)\n    scores = np.random.randint(0, 100, len(students))\n    df = pd.DataFrame({'Student': students, 'Score': scores})\n    df = df.sort_values('Score')\n    fig, ax = plt.subplots()\n    ax.bar(df['Student'], df['Score'])\n    ax.set_xlabel('Student')\n    ax.set_ylabel('Score')\n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(students=[\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"], seed=42):\n    np.random.seed(seed)\n    scores = np.random.randint(0, 100, len(students))\n    df = pd.DataFrame({'Student': students, 'Score': scores})\n    df = df.sort_values('Score')\n    fig, ax = plt.subplots()\n    ax.bar(df['Student'], df['Score'])\n    ax.set_xlabel('Student')\n    ax.set_ylabel('Score')\n    return df, ax"}
{"task_id": "BigCodeBench/87", "solution": "import pandas as pd\nfrom random import choices, seed\ndef task_func(products, ratings, weights, random_seed=42):\n    seed(random_seed)\n    product_ratings = choices(products, weights, k=len(products))\n    ratings_df = pd.DataFrame(list(zip(products, product_ratings)), columns=['Product', 'Rating'])\n    ratings_df = ratings_df.sort_values(by='Rating', ascending=False)\n    return ratings_df\nproducts = ['Product1', 'Product2', 'Product3', 'Product4', 'Product5']\nratings = [1, 2, 3, 4, 5]\nweights = [0.1, 0.2, 0.3, 0.2, 0.2]", "raw_solution": "import pandas as pd\nfrom random import choices, seed\n\ndef task_func(products, ratings, weights, random_seed=42):\n    seed(random_seed)\n    product_ratings = choices(products, weights, k=len(products))\n    ratings_df = pd.DataFrame(list(zip(products, product_ratings)), columns=['Product', 'Rating'])\n    ratings_df = ratings_df.sort_values(by='Rating', ascending=False)\n    return ratings_df\n\n# Test the function\nproducts = ['Product1', 'Product2', 'Product3', 'Product4', 'Product5']\nratings = [1, 2, 3, 4, 5]\nweights = [0.1, 0.2, 0.3, 0.2, 0.2]"}
{"task_id": "BigCodeBench/88", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\ndef generate_sales_data(start_date, end_date, seed=42):\n    np.random.seed(seed)\n    dates = pd.date_range(start=start_date, end=end_date)\n    sales = np.random.randint(0, 500, len(dates))\n    df = pd.DataFrame({'Date': dates, 'Sales': sales})\n    return df, df.plot(x='Date', y='Sales', kind='line')\n\n# Test the function\nstart_date = '2022-01-01'\nend_date = '2022-12-31'\ndf, ax = generate_sales_data(start_date, end_date)"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    It standardizes the column, calculates Z-scores, and removes data points where the Z-score exceeds a threshold.\n    The function also visualizes the data before and after outlier removal.\n\n    Parameters:\n    data (pandas.DataFrame): The dataset to be processed.\n    column (str): The column name in the dataset to be processed.\n    outlier_z_score (float): The Z-score threshold for identifying outliers.\n\n    Returns:\n    tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Standardize the column\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[column].values.reshape(-1, 1))\n\n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data[column]))\n\n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove outliers\n    data_without_outliers = data.drop(outliers)\n\n    # Visualize data before and after outlier removal\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.scatter(range(len(data[column])), data[column])\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(range(len(data_without_outliers[column])), data_without_outliers[column])\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.tight_layout()\n    plt.show()\n\n    return data, data_without_outliers, outliers", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    It standardizes the column, calculates Z-scores, and removes data points where the Z-score exceeds a threshold.\n    The function also visualizes the data before and after outlier removal.\n\n    Parameters:\n    data (pandas.DataFrame): The dataset to be processed.\n    column (str): The column name in the dataset to be processed.\n    outlier_z_score (float): The Z-score threshold for identifying outliers.\n\n    Returns:\n    tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Standardize the column\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[column].values.reshape(-1, 1))\n\n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data[column]))\n\n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove outliers\n    data_without_outliers = data.drop(outliers)\n\n    # Visualize data before and after outlier removal\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.scatter(range(len(data[column])), data[column])\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(range(len(data_without_outliers[column])), data_without_outliers[column])\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.tight_layout()\n    plt.show()\n\n    return data, data_without_outliers, outliers"}
{"task_id": "BigCodeBench/90", "solution": "import numpy as np\nimport math\ndef task_func(data, target, k):\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"'k' should be a non-negative integer\")\n\n    def calculate_distance(point1, point2):\n        lat1, lon1 = math.radians(point1[0]), math.radians(point1[1])\n        lat2, lon2 = math.radians(point2[0]), math.radians(point2[1])\n\n        dlat = lat2 - lat1\n        dlon = lon2 - lon1\n\n        a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n\n        distance = 6371 * c\n        return distance\n\n    distances = [calculate_distance(target, point) for point in data]\n    nearest_neighbors = np.argsort(distances)[:k]\n\n    return [data[i] for i in nearest_neighbors]", "raw_solution": "import numpy as np\nimport math\n\ndef task_func(data, target, k):\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"'k' should be a non-negative integer\")\n\n    def calculate_distance(point1, point2):\n        lat1, lon1 = math.radians(point1[0]), math.radians(point1[1])\n        lat2, lon2 = math.radians(point2[0]), math.radians(point2[1])\n\n        dlat = lat2 - lat1\n        dlon = lon2 - lon1\n\n        a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n\n        distance = 6371 * c\n        return distance\n\n    distances = [calculate_distance(target, point) for point in data]\n    nearest_neighbors = np.argsort(distances)[:k]\n\n    return [data[i] for i in nearest_neighbors]"}
{"task_id": "BigCodeBench/91", "solution": "from scipy.stats import linregress\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data, column1, column2):\n    if column1 not in data.columns or column2 not in data.columns:\n        raise ValueError(\"Specified columns do not exist in the DataFrame\")\n\n    slope, intercept, r_value, p_value, std_err = linregress(data[column1], data[column2])\n\n    fig, ax = plt.subplots()\n    ax.scatter(data[column1], data[column2])\n    ax.plot(data[column1], slope * data[column1] + intercept, 'r', label='Fitted line')\n    ax.set_xlabel(column1)\n    ax.set_ylabel(column2)\n    ax.legend()\n\n    return (slope, intercept, r_value, p_value, std_err), ax", "raw_solution": "from scipy.stats import linregress\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data, column1, column2):\n    if column1 not in data.columns or column2 not in data.columns:\n        raise ValueError(\"Specified columns do not exist in the DataFrame\")\n\n    slope, intercept, r_value, p_value, std_err = linregress(data[column1], data[column2])\n\n    fig, ax = plt.subplots()\n    ax.scatter(data[column1], data[column2])\n    ax.plot(data[column1], slope * data[column1] + intercept, 'r', label='Fitted line')\n    ax.set_xlabel(column1)\n    ax.set_ylabel(column2)\n    ax.legend()\n\n    return (slope, intercept, r_value, p_value, std_err), ax"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' should be an integer greater than 1\")\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels)\n\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=169, linewidths=3, color='black', zorder=10)\n\n    plt.title('K-Means Clustering')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n\n    return labels, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' should be an integer greater than 1\")\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels)\n\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=169, linewidths=3, color='black', zorder=10)\n\n    plt.title('K-Means Clustering')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n\n    return labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    transformed_df = pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('2D PCA Scatter Plot')\n\n    return transformed_df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    transformed_df = pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('2D PCA Scatter Plot')\n\n    return transformed_df, ax"}
{"task_id": "BigCodeBench/94", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(mean, std_dev, num_samples):\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, num_samples)\n\n    # Create a histogram of the samples\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n\n    # Generate x values for the PDF\n    x = np.linspace(samples.min(), samples.max(), 100)\n\n    # Plot the PDF\n    plt.plot(x, norm.pdf(x, mean, std_dev), color='r', label='PDF')\n\n    # Set plot title\n    plt.title(f\"Fit results: mean = {mean:.2f}, std = {std_dev:.2f}\")\n\n    # Add legend\n    plt.legend()\n\n    # Return the plot and the samples\n    return (plt.gcf(), samples)", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, num_samples):\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, num_samples)\n\n    # Create a histogram of the samples\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n\n    # Generate x values for the PDF\n    x = np.linspace(samples.min(), samples.max(), 100)\n\n    # Plot the PDF\n    plt.plot(x, norm.pdf(x, mean, std_dev), color='r', label='PDF')\n\n    # Set plot title\n    plt.title(f\"Fit results: mean = {mean:.2f}, std = {std_dev:.2f}\")\n\n    # Add legend\n    plt.legend()\n\n    # Return the plot and the samples\n    return (plt.gcf(), samples)"}
{"task_id": "BigCodeBench/95", "solution": "import pandas as pd\nfrom random import randint, uniform, seed\ndef task_func(categories=None, months=None, random_seed=42):\n    if not isinstance(categories, list) or not isinstance(months, list):\n        raise ValueError(\"Both 'categories' and 'months' should be provided as lists.\")\n    if not categories or not months:\n        raise ValueError(\"Both 'categories' and 'months' should not be empty lists.\")\n\n    seed(random_seed)\n\n    data = []\n    for category in categories:\n        for month in months:\n            sales = randint(100, 500) + uniform(0, 1)\n            data.append([month, category, sales])\n\n    df = pd.DataFrame(data, columns=['Month', 'Category', 'Sales'])\n    return df", "raw_solution": "import pandas as pd\nfrom random import randint, uniform, seed\n\ndef task_func(categories=None, months=None, random_seed=42):\n    if not isinstance(categories, list) or not isinstance(months, list):\n        raise ValueError(\"Both 'categories' and 'months' should be provided as lists.\")\n    if not categories or not months:\n        raise ValueError(\"Both 'categories' and 'months' should not be empty lists.\")\n\n    seed(random_seed)\n\n    data = []\n    for category in categories:\n        for month in months:\n            sales = randint(100, 500) + uniform(0, 1)\n            data.append([month, category, sales])\n\n    df = pd.DataFrame(data, columns=['Month', 'Category', 'Sales'])\n    return df"}
{"task_id": "BigCodeBench/96", "solution": "import csv\nfrom collections import Counter\nimport operator\ndef task_func(csv_file, csv_delimiter):\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file, delimiter=csv_delimiter)\n        data = [word for row in reader for word in row]\n\n    word_counts = Counter(data)\n    sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n\n    return sorted_word_counts", "raw_solution": "import csv\nfrom collections import Counter\nimport operator\n\ndef task_func(csv_file, csv_delimiter):\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file, delimiter=csv_delimiter)\n        data = [word for row in reader for word in row]\n\n    word_counts = Counter(data)\n    sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n\n    return sorted_word_counts"}
{"task_id": "BigCodeBench/97", "solution": "import math\nimport itertools\nfrom functools import reduce\nfrom operator import mul\ndef task_func(numbers):\n    \"\"\"\n    Generates all possible combinations of the provided numbers for each possible length.\n    For each combination, it computes the product of the numbers in the combination.\n    It then computes the logarithm of each product and sums these logarithms to produce the final result.\n    \"\"\"\n    # Initialize the sum of logarithms\n    sum_logs = 0\n\n    # Generate all possible combinations of the numbers\n    for r in range(1, len(numbers) + 1):\n        # Generate all combinations of length r\n        combinations = list(itertools.combinations(numbers, r))\n\n        # For each combination, compute the product of the numbers\n        for comb in combinations:\n            # Compute the product of the numbers in the combination\n            product = reduce(mul, comb)\n\n            # Compute the logarithm of the product and add it to the sum\n            sum_logs += math.log(product)\n\n    return sum_logs", "raw_solution": "import math\nimport itertools\nfrom functools import reduce\nfrom operator import mul\n\ndef task_func(numbers):\n    \"\"\"\n    Generates all possible combinations of the provided numbers for each possible length.\n    For each combination, it computes the product of the numbers in the combination.\n    It then computes the logarithm of each product and sums these logarithms to produce the final result.\n    \"\"\"\n    # Initialize the sum of logarithms\n    sum_logs = 0\n\n    # Generate all possible combinations of the numbers\n    for r in range(1, len(numbers) + 1):\n        # Generate all combinations of length r\n        combinations = list(itertools.combinations(numbers, r))\n\n        # For each combination, compute the product of the numbers\n        for comb in combinations:\n            # Compute the product of the numbers in the combination\n            product = reduce(mul, comb)\n\n            # Compute the logarithm of the product and add it to the sum\n            sum_logs += math.log(product)\n\n    return sum_logs"}
{"task_id": "BigCodeBench/98", "solution": "import random\nimport string\nfrom collections import Counter\ndef task_func(num_strings, string_length):\n    # Generate a list of random strings\n    strings = [''.join(random.choices(string.ascii_lowercase, k=string_length)) for _ in range(num_strings)]\n\n    # Flatten the list of strings and count the frequency of each character\n    char_counts = Counter(''.join(strings))\n\n    # Sort the characters by frequency in descending order and return the result\n    return sorted(char_counts.items(), key=lambda x: x[1], reverse=True)", "raw_solution": "import random\nimport string\nfrom collections import Counter\n\ndef task_func(num_strings, string_length):\n    # Generate a list of random strings\n    strings = [''.join(random.choices(string.ascii_lowercase, k=string_length)) for _ in range(num_strings)]\n\n    # Flatten the list of strings and count the frequency of each character\n    char_counts = Counter(''.join(strings))\n\n    # Sort the characters by frequency in descending order and return the result\n    return sorted(char_counts.items(), key=lambda x: x[1], reverse=True)\n\n# Test the function"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Generate the pair plot\n    g = sns.pairplot(iris_df, hue='species', palette='Dark2', diag_kind='kde', height=2.5)\n    g.fig.suptitle(\"Iris Dataset Pair Plot\", y=1.08)\n\n    return g.figure", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Generate the pair plot\n    g = sns.pairplot(iris_df, hue='species', palette='Dark2', diag_kind='kde', height=2.5)\n    g.fig.suptitle(\"Iris Dataset Pair Plot\", y=1.08)\n\n    return g.figure"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(seed=42):\n    try:\n        random.seed(seed)\n        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]\n        values = [random.randint(1, 100) for _ in range(30)]\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        df['Date'] = pd.to_datetime(df['Date'])\n        df.set_index('Date', inplace=True)\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(df.index, df['Value'])\n        plt.xlabel('Date')\n        plt.ylabel('Value')\n        plt.title('Random Time Series Data')\n        plt.xticks(rotation=45)\n        plt.grid(True)\n        plt.rcParams[\"font.family\"] = \"Arial\"\n\n        return plt.gca()\n    except Exception as e:\n        raise ValueError(f'Error generating data or plot: {e}')", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(seed=42):\n    try:\n        random.seed(seed)\n        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]\n        values = [random.randint(1, 100) for _ in range(30)]\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        df['Date'] = pd.to_datetime(df['Date'])\n        df.set_index('Date', inplace=True)\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(df.index, df['Value'])\n        plt.xlabel('Date')\n        plt.ylabel('Value')\n        plt.title('Random Time Series Data')\n        plt.xticks(rotation=45)\n        plt.grid(True)\n        plt.rcParams[\"font.family\"] = \"Arial\"\n\n        return plt.gca()\n    except Exception as e:\n        raise ValueError(f'Error generating data or plot: {e}')"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_to=None):\n    \"\"\"\n    Draws the correlation heatmap of the Boston Housing dataset using Seaborn.\n    If `save_to` is provided, saves the plot to a file.\n    Raises ValueError if an error occurs in generating or saving the plot.\n    Returns the Axes object containing the heatmap plot.\n    \"\"\"\n    np.random.seed(seed)\n\n    # Load the dataset\n    try:\n        data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    except Exception as e:\n        raise ValueError(f\"Error loading dataset: {e}\")\n\n    # Define column names\n    column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n    data.columns = column_names\n\n    # Calculate correlation matrix\n    correlation_matrix = data.corr().round(2)\n\n    # Generate heatmap\n    try:\n        fig, ax = plt.subplots(figsize=(10, 10))\n        sns.heatmap(data=correlation_matrix, annot=True, ax=ax)\n    except Exception as e:\n        raise ValueError(f\"Error generating heatmap: {e}\")\n\n    # Save plot to file if specified\n    if save_to is not None:\n        try:\n            plt.savefig(save_to)\n        except Exception as e:\n            raise ValueError(f\"Error saving plot: {e}\")\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_to=None):\n    \"\"\"\n    Draws the correlation heatmap of the Boston Housing dataset using Seaborn.\n    If `save_to` is provided, saves the plot to a file.\n    Raises ValueError if an error occurs in generating or saving the plot.\n    Returns the Axes object containing the heatmap plot.\n    \"\"\"\n    np.random.seed(seed)\n\n    # Load the dataset\n    try:\n        data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    except Exception as e:\n        raise ValueError(f\"Error loading dataset: {e}\")\n\n    # Define column names\n    column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n    data.columns = column_names\n\n    # Calculate correlation matrix\n    correlation_matrix = data.corr().round(2)\n\n    # Generate heatmap\n    try:\n        fig, ax = plt.subplots(figsize=(10, 10))\n        sns.heatmap(data=correlation_matrix, annot=True, ax=ax)\n    except Exception as e:\n        raise ValueError(f\"Error generating heatmap: {e}\")\n\n    # Save plot to file if specified\n    if save_to is not None:\n        try:\n            plt.savefig(save_to)\n        except Exception as e:\n            raise ValueError(f\"Error saving plot: {e}\")\n\n    return ax"}
{"task_id": "BigCodeBench/102", "solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\ndef task_func():\n    # Load the diabetes dataset\n    diabetes = load_diabetes()\n\n    # Create a DataFrame from the dataset\n    df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n    df['target'] = diabetes.target\n\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pairplot\n    pairplot = sns.pairplot(df)\n\n    return pairplot, df", "raw_solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\n\ndef task_func():\n    # Load the diabetes dataset\n    diabetes = load_diabetes()\n\n    # Create a DataFrame from the dataset\n    df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n    df['target'] = diabetes.target\n\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pairplot\n    pairplot = sns.pairplot(df)\n\n    return pairplot, df"}
{"task_id": "BigCodeBench/103", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(temperatures):\n    \"\"\"\n    Calculates and plots the daytime temperatures for New York over a given period.\n    The plot uses Arial font for display.\n    The function raises ValueError if the input DataFrame is not in the expected format or empty.\n    The function returns a matplotlib.axes.Axes object containing the temperature plot.\n    \"\"\"\n    if not isinstance(temperatures, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame\")\n    if temperatures.empty:\n        raise ValueError(\"Input DataFrame cannot be empty\")\n    if 'Date' not in temperatures.columns or 'Temperature' not in temperatures.columns:\n        raise ValueError(\"Input DataFrame must contain 'Date' and 'Temperature' columns\")\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(temperatures['Date'], temperatures['Temperature'])\n    plt.xlabel('Date', fontsize=14, fontname='Arial')\n    plt.ylabel('Temperature (\u00b0C)', fontsize=14, fontname='Arial')\n    plt.title('Daily Temperatures in New York', fontsize=16, fontname='Arial')\n    plt.xticks(fontsize=12, fontname='Arial')\n    plt.yticks(fontsize=12, fontname='Arial')\n\n    return plt.gca()", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(temperatures):\n    \"\"\"\n    Calculates and plots the daytime temperatures for New York over a given period.\n    The plot uses Arial font for display.\n    The function raises ValueError if the input DataFrame is not in the expected format or empty.\n    The function returns a matplotlib.axes.Axes object containing the temperature plot.\n    \"\"\"\n    if not isinstance(temperatures, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame\")\n    if temperatures.empty:\n        raise ValueError(\"Input DataFrame cannot be empty\")\n    if 'Date' not in temperatures.columns or 'Temperature' not in temperatures.columns:\n        raise ValueError(\"Input DataFrame must contain 'Date' and 'Temperature' columns\")\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(temperatures['Date'], temperatures['Temperature'])\n    plt.xlabel('Date', fontsize=14, fontname='Arial')\n    plt.ylabel('Temperature (\u00b0C)', fontsize=14, fontname='Arial')\n    plt.title('Daily Temperatures in New York', fontsize=16, fontname='Arial')\n    plt.xticks(fontsize=12, fontname='Arial')\n    plt.yticks(fontsize=12, fontname='Arial')\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/104", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\ndef task_func(df, groups=['A', 'B', 'C', 'D', 'E']):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if 'Date' not in df.columns or 'Value' not in df.columns or 'Group' not in df.columns:\n        raise ValueError(\"'df' lacks required columns\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Date (ordinal)'] = df['Date'].apply(lambda x: x.toordinal())\n\n    colors = cycle('rgbcmy')\n    fig, ax = plt.subplots()\n\n    for group, color in zip(groups, colors):\n        df_group = df[df['Group'] == group]\n        ax.scatter(df_group['Date (ordinal)'], df_group['Value'], color=color, label=group)\n\n    ax.set_title('Scatterplot of Values for Each Group Over Time')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\ndef task_func(df, groups=['A', 'B', 'C', 'D', 'E']):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if 'Date' not in df.columns or 'Value' not in df.columns or 'Group' not in df.columns:\n        raise ValueError(\"'df' lacks required columns\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Date (ordinal)'] = df['Date'].apply(lambda x: x.toordinal())\n\n    colors = cycle('rgbcmy')\n    fig, ax = plt.subplots()\n\n    for group, color in zip(groups, colors):\n        df_group = df[df['Group'] == group]\n        ax.scatter(df_group['Date (ordinal)'], df_group['Value'], color=color, label=group)\n\n    ax.set_title('Scatterplot of Values for Each Group Over Time')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/105", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"Dataframe is empty\")\n    if 'date' not in df.columns:\n        raise ValueError(\"'date' column is missing\")\n    try:\n        df['date'] = pd.to_datetime(df['date'])\n    except ValueError:\n        raise ValueError(\"'date' column is not in datetime format\")\n\n    df['date'] = df['date'].apply(lambda date: date.toordinal())\n\n    correlation_matrix = df.corr()\n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", linewidths=.5, cmap='coolwarm', cbar_kws={\"shrink\": .5}, ax=ax)\n    plt.title('Correlation Matrix')\n\n    pair_grid = sns.pairplot(df)\n\n    return fig, pair_grid", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"Dataframe is empty\")\n    if 'date' not in df.columns:\n        raise ValueError(\"'date' column is missing\")\n    try:\n        df['date'] = pd.to_datetime(df['date'])\n    except ValueError:\n        raise ValueError(\"'date' column is not in datetime format\")\n\n    df['date'] = df['date'].apply(lambda date: date.toordinal())\n\n    correlation_matrix = df.corr()\n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", linewidths=.5, cmap='coolwarm', cbar_kws={\"shrink\": .5}, ax=ax)\n    plt.title('Correlation Matrix')\n\n    pair_grid = sns.pairplot(df)\n\n    return fig, pair_grid"}
{"task_id": "BigCodeBench/106", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a valid DataFrame\")\n    if not {'date', 'value'}.issubset(df.columns):\n        raise ValueError(\"'df' lacks the required columns\")\n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"'date' column is not in datetime format\")\n\n    df['date'] = df['date'].map(dt.datetime.toordinal)\n    X = df['date'].values.reshape(-1, 1)\n    y = df['value'].values.reshape(-1, 1)\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    predictions = model.predict(X)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X, y, color='blue')\n    ax.plot(X, predictions, color='red')\n    ax.set_title('Value vs Date (Linear Regression Prediction)')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n\n    return model, predictions, ax", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a valid DataFrame\")\n    if not {'date', 'value'}.issubset(df.columns):\n        raise ValueError(\"'df' lacks the required columns\")\n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"'date' column is not in datetime format\")\n\n    df['date'] = df['date'].map(dt.datetime.toordinal)\n    X = df['date'].values.reshape(-1, 1)\n    y = df['value'].values.reshape(-1, 1)\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    predictions = model.predict(X)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X, y, color='blue')\n    ax.plot(X, predictions, color='red')\n    ax.set_title('Value vs Date (Linear Regression Prediction)')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n\n    return model, predictions, ax"}
{"task_id": "BigCodeBench/107", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(df, n_clusters=3, random_state=0):\n    if df.empty or 'date' not in df.columns or 'value' not in df.columns:\n        raise ValueError(\"DataFrame is empty or lacks required columns\")\n\n    df['date'] = df['date'].apply(lambda x: x.toordinal())\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    df['cluster'] = kmeans.fit_predict(df[['date', 'value']])\n\n    fig, ax = plt.subplots()\n    for cluster in np.unique(df['cluster']):\n        ax.scatter(df[df['cluster'] == cluster]['date'], df[df['cluster'] == cluster]['value'], label=f'Cluster {cluster}')\n\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('KMeans Clustering of Value vs Date')\n    ax.legend()\n\n    return ax", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(df, n_clusters=3, random_state=0):\n    if df.empty or 'date' not in df.columns or 'value' not in df.columns:\n        raise ValueError(\"DataFrame is empty or lacks required columns\")\n\n    df['date'] = df['date'].apply(lambda x: x.toordinal())\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    df['cluster'] = kmeans.fit_predict(df[['date', 'value']])\n\n    fig, ax = plt.subplots()\n    for cluster in np.unique(df['cluster']):\n        ax.scatter(df[df['cluster'] == cluster]['date'], df[df['cluster'] == cluster]['value'], label=f'Cluster {cluster}')\n\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('KMeans Clustering of Value vs Date')\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.seasonal import DecomposeResult\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n\n    # Check if df has required columns\n    if 'value' not in df.columns:\n        raise ValueError(\"'df' should have a 'value' column\")\n\n    # Check if df has valid data types\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column should have numeric data types\")\n\n    # Check if freq is a valid frequency string\n    if freq not in ['D', 'M', 'Q', 'Y']:\n        raise ValueError(\"'freq' should be a valid frequency string\")\n\n    # Check if decomposition_model is valid\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' should be 'additive' or 'multiplicative'\")\n\n    # Decompose the time series\n    result = seasonal_decompose(df['value'], model=decomposition_model, freq=freq)\n\n    # Plot the decomposition\n    fig, ax = plt.subplots(4, 1, figsize=(10, 10))\n    result.plot(ax=ax)\n    plt.tight_layout()\n\n    return (result, ax)", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.seasonal import DecomposeResult\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n\n    # Check if df has required columns\n    if 'value' not in df.columns:\n        raise ValueError(\"'df' should have a 'value' column\")\n\n    # Check if df has valid data types\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column should have numeric data types\")\n\n    # Check if freq is a valid frequency string\n    if freq not in ['D', 'M', 'Q', 'Y']:\n        raise ValueError(\"'freq' should be a valid frequency string\")\n\n    # Check if decomposition_model is valid\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' should be 'additive' or 'multiplicative'\")\n\n    # Decompose the time series\n    result = seasonal_decompose(df['value'], model=decomposition_model, freq=freq)\n\n    # Plot the decomposition\n    fig, ax = plt.subplots(4, 1, figsize=(10, 10))\n    result.plot(ax=ax)\n    plt.tight_layout()\n\n    return (result, ax)"}
{"task_id": "BigCodeBench/109", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, items=None, locations=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if 'Item' not in df.columns or 'Location' not in df.columns:\n        raise ValueError(\"'Item' or 'Location' columns are missing\")\n\n    if items is not None:\n        df = df[df['Item'].isin(items)]\n    if locations is not None:\n        df = df[df['Location'].isin(locations)]\n\n    item_counts = df['Item'].value_counts()\n    location_counts = df['Location'].value_counts()\n\n    fig, ax1 = plt.subplots()\n\n    ax1.bar(item_counts.index, item_counts.values, color='b', alpha=0.5)\n    ax1.set_xlabel('Items')\n    ax1.set_ylabel('Count', color='b')\n    for tick in ax1.get_yticklabels():\n        tick.set_color('b')\n\n    ax2 = ax1.twinx()\n    ax2.bar(location_counts.index, location_counts.values, color='r', alpha=0.5)\n    ax2.set_ylabel('Count', color='r')\n    for tick in ax2.get_yticklabels():\n        tick.set_color('r')\n\n    plt.show()\n\n    return ax1", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, items=None, locations=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if 'Item' not in df.columns or 'Location' not in df.columns:\n        raise ValueError(\"'Item' or 'Location' columns are missing\")\n\n    if items is not None:\n        df = df[df['Item'].isin(items)]\n    if locations is not None:\n        df = df[df['Location'].isin(locations)]\n\n    item_counts = df['Item'].value_counts()\n    location_counts = df['Location'].value_counts()\n\n    fig, ax1 = plt.subplots()\n\n    ax1.bar(item_counts.index, item_counts.values, color='b', alpha=0.5)\n    ax1.set_xlabel('Items')\n    ax1.set_ylabel('Count', color='b')\n    for tick in ax1.get_yticklabels():\n        tick.set_color('b')\n\n    ax2 = ax1.twinx()\n    ax2.bar(location_counts.index, location_counts.values, color='r', alpha=0.5)\n    ax2.set_ylabel('Count', color='r')\n    for tick in ax2.get_yticklabels():\n        tick.set_color('r')\n\n    plt.show()\n\n    return ax1"}
{"task_id": "BigCodeBench/110", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if 'Date' not in df.columns or 'Sales' not in df.columns:\n        raise ValueError(\"'df' lacks 'Date' or 'Sales' columns\")\n    if df.empty:\n        raise ValueError(\"'df' has no data to plot\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    df['Sales'].plot(ax=ax, kind='line')\n    ax.set_title('Daily Turnover Line Chart')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if 'Date' not in df.columns or 'Sales' not in df.columns:\n        raise ValueError(\"'df' lacks 'Date' or 'Sales' columns\")\n    if df.empty:\n        raise ValueError(\"'df' has no data to plot\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    df['Sales'].plot(ax=ax, kind='line')\n    ax.set_title('Daily Turnover Line Chart')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n\n    return ax"}
{"task_id": "BigCodeBench/111", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if not all(col in df.columns for col in ['Date', 'Time', 'Temperature']):\n        raise ValueError(\"'df' lacks 'Date', 'Time', or 'Temperature' columns\")\n\n    df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n    df.set_index('DateTime', inplace=True)\n    df.drop(['Date', 'Time'], axis=1, inplace=True)\n\n    pivot_df = df.pivot_table(values='Temperature', index=df.index.date, columns=df.index.time, aggfunc='mean')\n\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(pivot_df, cmap='coolwarm')\n\n    return heatmap", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if not all(col in df.columns for col in ['Date', 'Time', 'Temperature']):\n        raise ValueError(\"'df' lacks 'Date', 'Time', or 'Temperature' columns\")\n\n    df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n    df.set_index('DateTime', inplace=True)\n    df.drop(['Date', 'Time'], axis=1, inplace=True)\n\n    pivot_df = df.pivot_table(values='Temperature', index=df.index.date, columns=df.index.time, aggfunc='mean')\n\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(pivot_df, cmap='coolwarm')\n\n    return heatmap"}
{"task_id": "BigCodeBench/112", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a pandas DataFrame\")\n    if 'Status' not in df.columns:\n        raise ValueError(\"DataFrame does not contain the 'Status' column\")\n\n    status_counts = df['Status'].value_counts()\n    fig, ax = plt.subplots()\n    ax.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%', startangle=90)\n    ax.axis('equal')\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a pandas DataFrame\")\n    if 'Status' not in df.columns:\n        raise ValueError(\"DataFrame does not contain the 'Status' column\")\n\n    status_counts = df['Status'].value_counts()\n    fig, ax = plt.subplots()\n    ax.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%', startangle=90)\n    ax.axis('equal')\n    return ax"}
{"task_id": "BigCodeBench/113", "solution": "import json\nfrom collections import Counter\nimport random\nimport os\ndef task_func(my_dict, keys):\n    if len(set(keys)) != 10:\n        raise ValueError(\"'keys' must contain exactly 10 unique elements\")\n\n    for key in keys:\n        my_dict[key] = random.randint(1, 100)\n\n    json_file = 'updated_dictionary.json'\n    txt_file = 'key_frequencies.txt'\n\n    with open(json_file, 'w') as f:\n        json.dump(my_dict, f)\n\n    with open(txt_file, 'w') as f:\n        for key, value in Counter(my_dict).items():\n            f.write(f'{key}: {value}\\n')\n\n    return (my_dict, json_file, txt_file)", "raw_solution": "import json\nfrom collections import Counter\nimport random\nimport os\n\ndef task_func(my_dict, keys):\n    if len(set(keys)) != 10:\n        raise ValueError(\"'keys' must contain exactly 10 unique elements\")\n\n    for key in keys:\n        my_dict[key] = random.randint(1, 100)\n\n    json_file = 'updated_dictionary.json'\n    txt_file = 'key_frequencies.txt'\n\n    with open(json_file, 'w') as f:\n        json.dump(my_dict, f)\n\n    with open(txt_file, 'w') as f:\n        for key, value in Counter(my_dict).items():\n            f.write(f'{key}: {value}\\n')\n\n    return (my_dict, json_file, txt_file)"}
{"task_id": "BigCodeBench/114", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(my_dict):\n    if not isinstance(my_dict, dict):\n        raise TypeError(\"Input must be a dictionary\")\n    if 'array' not in my_dict:\n        raise KeyError(\"'array' key not found in the dictionary\")\n    if not isinstance(my_dict['array'], np.ndarray):\n        raise TypeError(\"The value of the 'array' key must be a numpy array\")\n\n    array = my_dict['array']\n    scaler = MinMaxScaler()\n    normalized_array = scaler.fit_transform(array.reshape(-1, 1))\n    my_dict['normalized_array'] = normalized_array\n\n    return my_dict", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(my_dict):\n    if not isinstance(my_dict, dict):\n        raise TypeError(\"Input must be a dictionary\")\n    if 'array' not in my_dict:\n        raise KeyError(\"'array' key not found in the dictionary\")\n    if not isinstance(my_dict['array'], np.ndarray):\n        raise TypeError(\"The value of the 'array' key must be a numpy array\")\n\n    array = my_dict['array']\n    scaler = MinMaxScaler()\n    normalized_array = scaler.fit_transform(array.reshape(-1, 1))\n    my_dict['normalized_array'] = normalized_array\n\n    return my_dict"}
{"task_id": "BigCodeBench/115", "solution": "import numpy as np\nfrom scipy.stats import mode\nfrom scipy.stats import entropy\ndef task_func(numbers):\n    if not numbers:\n        raise ValueError(\"Input list 'numbers' cannot be empty\")\n\n    arr = np.array(numbers)\n    mode_val = mode(arr)[0][0]\n    entropy_val = entropy(arr, base=2)\n\n    return {'mode': mode_val, 'entropy': entropy_val}", "raw_solution": "import numpy as np\nfrom scipy.stats import mode\nfrom scipy.stats import entropy\n\ndef task_func(numbers):\n    if not numbers:\n        raise ValueError(\"Input list 'numbers' cannot be empty\")\n\n    arr = np.array(numbers)\n    mode_val = mode(arr)[0][0]\n    entropy_val = entropy(arr, base=2)\n\n    return {'mode': mode_val, 'entropy': entropy_val}"}
{"task_id": "BigCodeBench/116", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(mu, sigma, sample_size):\n    \"\"\"\n    Generates a numpy array of random samples drawn from a normal distribution and plots the histogram of these samples.\n    \"\"\"\n    # Generate samples\n    samples = np.random.normal(mu, sigma, sample_size)\n\n    # Plot histogram\n    plt.hist(samples, bins=30, alpha=0.75, color='blue')\n    plt.xlabel(\"Sample values\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Histogram of Generated Samples\")\n    plt.show()\n\n    return samples", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(mu, sigma, sample_size):\n    \"\"\"\n    Generates a numpy array of random samples drawn from a normal distribution and plots the histogram of these samples.\n    \"\"\"\n    # Generate samples\n    samples = np.random.normal(mu, sigma, sample_size)\n\n    # Plot histogram\n    plt.hist(samples, bins=30, alpha=0.75, color='blue')\n    plt.xlabel(\"Sample values\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Histogram of Generated Samples\")\n    plt.show()\n\n    return samples"}
{"task_id": "BigCodeBench/117", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, seed as set_seed\ndef task_func(num_of_students, seed=42, name_list=None, gender_list=None, age_range=(15, 20), score_range=(50, 100)):\n    if num_of_students <= 0:\n        raise ValueError(\"num_of_students must be a positive integer\")\n\n    if name_list is None:\n        name_list = [\"John\", \"Jane\", \"Mary\", \"James\", \"Jennifer\", \"Maria\", \"Michael\", \"Jessica\", \"Sarah\", \"Jacob\"]\n\n    if gender_list is None:\n        gender_list = [\"Male\", \"Female\"]\n\n    set_seed(seed)\n\n    data = {\n        \"Name\": [choice(name_list) for _ in range(num_of_students)],\n        \"Age\": np.random.randint(*age_range, num_of_students),\n        \"Gender\": [choice(gender_list) for _ in range(num_of_students)],\n        \"Score\": np.random.randint(*score_range, num_of_students)\n    }\n\n    return pd.DataFrame(data)", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, seed as set_seed\n\ndef task_func(num_of_students, seed=42, name_list=None, gender_list=None, age_range=(15, 20), score_range=(50, 100)):\n    if num_of_students <= 0:\n        raise ValueError(\"num_of_students must be a positive integer\")\n\n    if name_list is None:\n        name_list = [\"John\", \"Jane\", \"Mary\", \"James\", \"Jennifer\", \"Maria\", \"Michael\", \"Jessica\", \"Sarah\", \"Jacob\"]\n\n    if gender_list is None:\n        gender_list = [\"Male\", \"Female\"]\n\n    set_seed(seed)\n\n    data = {\n        \"Name\": [choice(name_list) for _ in range(num_of_students)],\n        \"Age\": np.random.randint(*age_range, num_of_students),\n        \"Gender\": [choice(gender_list) for _ in range(num_of_students)],\n        \"Score\": np.random.randint(*score_range, num_of_students)\n    }\n\n    return pd.DataFrame(data)"}
{"task_id": "BigCodeBench/118", "solution": "import os\nimport shutil\ndef task_func(directory, backup_directory):\n    # Check if backup directory exists, if not, create it\n    if not os.path.exists(backup_directory):\n        os.makedirs(backup_directory)\n\n    # Get a list of all files in the directory\n    files = os.listdir(directory)\n\n    # Filter out non-JSON files\n    json_files = [file for file in files if file.endswith('.json')]\n\n    # Copy JSON files to backup directory\n    copied_files = []\n    for file in json_files:\n        src_path = os.path.join(directory, file)\n        dst_path = os.path.join(backup_directory, file)\n        shutil.copy2(src_path, dst_path)\n        copied_files.append(dst_path)\n\n    return copied_files", "raw_solution": "import os\nimport shutil\n\ndef task_func(directory, backup_directory):\n    # Check if backup directory exists, if not, create it\n    if not os.path.exists(backup_directory):\n        os.makedirs(backup_directory)\n\n    # Get a list of all files in the directory\n    files = os.listdir(directory)\n\n    # Filter out non-JSON files\n    json_files = [file for file in files if file.endswith('.json')]\n\n    # Copy JSON files to backup directory\n    copied_files = []\n    for file in json_files:\n        src_path = os.path.join(directory, file)\n        dst_path = os.path.join(backup_directory, file)\n        shutil.copy2(src_path, dst_path)\n        copied_files.append(dst_path)\n\n    return copied_files"}
{"task_id": "BigCodeBench/119", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    x = np.linspace(-10, 10, 400)\n    y = x**2\n\n    plt.plot(x, y)\n    plt.title('y = x^2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.show()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    x = np.linspace(-10, 10, 400)\n    y = x**2\n\n    plt.plot(x, y)\n    plt.title('y = x^2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.show()\n\n# Test the function\ntask_func()"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"'start_date' and 'end_date' must be datetime.datetime instances\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' must be earlier than 'end_date'\")\n\n    random_seed(seed)\n    time_between_dates = end_date - start_date\n    days_between_dates = time_between_dates.days\n\n    random_day_offsets = [randint(0, days_between_dates) for _ in range(days_between_dates)]\n    random_dates = [start_date + timedelta(days=day) for day in random_day_offsets]\n\n    return pd.Series(random_dates)", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"'start_date' and 'end_date' must be datetime.datetime instances\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' must be earlier than 'end_date'\")\n\n    random_seed(seed)\n    time_between_dates = end_date - start_date\n    days_between_dates = time_between_dates.days\n\n    random_day_offsets = [randint(0, days_between_dates) for _ in range(days_between_dates)]\n    random_dates = [start_date + timedelta(days=day) for day in random_day_offsets]\n\n    return pd.Series(random_dates)"}
{"task_id": "BigCodeBench/121", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(my_list, seed=42):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n\n    np.random.seed(seed)\n\n    categories = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n    sales_data = pd.DataFrame(np.random.randint(100, 500, size=(12, len(categories))), columns=categories)\n\n    my_list.append(\"12\")\n\n    fig, ax = plt.subplots()\n    sales_data.plot(kind='bar', ax=ax)\n    ax.set_title('Sales Data')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n\n    return (sales_data, ax)", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, seed=42):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n\n    np.random.seed(seed)\n\n    categories = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n    sales_data = pd.DataFrame(np.random.randint(100, 500, size=(12, len(categories))), columns=categories)\n\n    my_list.append(\"12\")\n\n    fig, ax = plt.subplots()\n    sales_data.plot(kind='bar', ax=ax)\n    ax.set_title('Sales Data')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n\n    return (sales_data, ax)"}
{"task_id": "BigCodeBench/122", "solution": "import numpy as np\nimport random\ndef task_func(my_list):\n    # Append a random integer between 0 and 100 to the list\n    my_list.append(random.randint(0, 100))\n\n    # Generate a numpy array of random floating-point numbers\n    return np.random.rand(sum(my_list))\nmy_list = [1, 2, 3, 4, 5]", "raw_solution": "import numpy as np\nimport random\n\ndef task_func(my_list):\n    # Append a random integer between 0 and 100 to the list\n    my_list.append(random.randint(0, 100))\n\n    # Generate a numpy array of random floating-point numbers\n    return np.random.rand(sum(my_list))\n\n# Test the function\nmy_list = [1, 2, 3, 4, 5]"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n    if not os.path.exists(file_dir):\n        raise FileNotFoundError(f\"No files found in the directory: {file_dir}\")\n\n    my_list.append('12')\n    file_list = glob.glob(os.path.join(file_dir, f\"*{file_ext}\"))\n    if not file_list:\n        raise FileNotFoundError(f\"No files found with extension: {file_ext} in the directory: {file_dir}\")\n\n    df = pd.DataFrame()\n    for file in file_list[:sum(my_list)]:\n        df = pd.concat([df, pd.read_csv(file)])\n\n    return df", "raw_solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n    if not os.path.exists(file_dir):\n        raise FileNotFoundError(f\"No files found in the directory: {file_dir}\")\n\n    my_list.append('12')\n    file_list = glob.glob(os.path.join(file_dir, f\"*{file_ext}\"))\n    if not file_list:\n        raise FileNotFoundError(f\"No files found with extension: {file_ext} in the directory: {file_dir}\")\n\n    df = pd.DataFrame()\n    for file in file_list[:sum(my_list)]:\n        df = pd.concat([df, pd.read_csv(file)])\n\n    return df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' must be a list\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"'my_list' must contain only numeric elements\")\n\n    random_seed(seed)\n    start_time = time.time()\n\n    my_list.append(12)\n    sum_list = sum(my_list)\n    random_list = [randint(1, 100) for _ in range(min(sum_list, size))]\n\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 101), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return (time_taken, ax)", "raw_solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' must be a list\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"'my_list' must contain only numeric elements\")\n\n    random_seed(seed)\n    start_time = time.time()\n\n    my_list.append(12)\n    sum_list = sum(my_list)\n    random_list = [randint(1, 100) for _ in range(min(sum_list, size))]\n\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 101), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return (time_taken, ax)"}
{"task_id": "BigCodeBench/125", "solution": "from collections import defaultdict\nimport itertools\nimport json\nimport random\nimport string\ndef task_func(LETTERS, n):\n    # Generate all possible combinations of given set of letters of length 'n'\n    combinations = [''.join(p) for p in itertools.product(LETTERS, repeat=n)]\n\n    # Count the occurrences of each letter in these combinations\n    letter_counts = defaultdict(int)\n    for combination in combinations:\n        for letter in combination:\n            letter_counts[letter] += 1\n\n    # Save the results in a JSON file\n    random_number = random.randint(0, 100)\n    filename = f\"prefix_{random_number}.json\"\n    with open(filename, 'w') as f:\n        json.dump(letter_counts, f)\n\n    return filename", "raw_solution": "from collections import defaultdict\nimport itertools\nimport json\nimport random\nimport string\n\ndef task_func(LETTERS, n):\n    # Generate all possible combinations of given set of letters of length 'n'\n    combinations = [''.join(p) for p in itertools.product(LETTERS, repeat=n)]\n\n    # Count the occurrences of each letter in these combinations\n    letter_counts = defaultdict(int)\n    for combination in combinations:\n        for letter in combination:\n            letter_counts[letter] += 1\n\n    # Save the results in a JSON file\n    random_number = random.randint(0, 100)\n    filename = f\"prefix_{random_number}.json\"\n    with open(filename, 'w') as f:\n        json.dump(letter_counts, f)\n\n    return filename"}
{"task_id": "BigCodeBench/126", "solution": "import pandas as pd\nfrom random import randint, seed as random_seed\nimport statistics\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(animals=None, seed=42):\n    if animals is None:\n        animals = ['Lion', 'Tiger', 'Bear', 'Monkey', 'Elephant', 'Hippo', 'Gorilla', 'Rhino', 'Leopard', 'Cheetah']\n\n    random_seed(seed)\n\n    data = {}\n    for animal in animals:\n        counts = [randint(1, 100) for _ in range(10)]\n        data[animal] = counts\n\n    df = pd.DataFrame(data)\n\n    stats = {\n        'Animal': [],\n        'Mean': [],\n        'Median': [],\n        'Standard Deviation': []\n    }\n\n    for animal in animals:\n        stats['Animal'].append(animal)\n        stats['Mean'].append(np.mean(data[animal]))\n        stats['Median'].append(np.median(data[animal]))\n        stats['Standard Deviation'].append(np.std(data[animal]))\n\n    stats_df = pd.DataFrame(stats)\n\n    # Plotting the counts\n    df.plot(kind='bar', figsize=(10, 5))\n    plt.ylabel('Count')\n    plt.title('Counts of Animals')\n    plt.show()\n\n    return stats_df", "raw_solution": "import pandas as pd\nfrom random import randint, seed as random_seed\nimport statistics\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(animals=None, seed=42):\n    if animals is None:\n        animals = ['Lion', 'Tiger', 'Bear', 'Monkey', 'Elephant', 'Hippo', 'Gorilla', 'Rhino', 'Leopard', 'Cheetah']\n\n    random_seed(seed)\n\n    data = {}\n    for animal in animals:\n        counts = [randint(1, 100) for _ in range(10)]\n        data[animal] = counts\n\n    df = pd.DataFrame(data)\n\n    stats = {\n        'Animal': [],\n        'Mean': [],\n        'Median': [],\n        'Standard Deviation': []\n    }\n\n    for animal in animals:\n        stats['Animal'].append(animal)\n        stats['Mean'].append(np.mean(data[animal]))\n        stats['Median'].append(np.median(data[animal]))\n        stats['Standard Deviation'].append(np.std(data[animal]))\n\n    stats_df = pd.DataFrame(stats)\n\n    # Plotting the counts\n    df.plot(kind='bar', figsize=(10, 5))\n    plt.ylabel('Count')\n    plt.title('Counts of Animals')\n    plt.show()\n\n    return stats_df"}
{"task_id": "BigCodeBench/127", "solution": "import os\nimport shutil\nimport glob\nimport hashlib\ndef task_func(ROOT_DIR, DEST_DIR, SPECIFIC_HASH):\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    count = 0\n    for filename in glob.iglob(ROOT_DIR + '**/*', recursive=True):\n        if os.path.isfile(filename):\n            with open(filename, 'rb') as f:\n                file_hash = hashlib.md5()\n                while chunk := f.read(8192):\n                    file_hash.update(chunk)\n            if file_hash.hexdigest() == SPECIFIC_HASH:\n                shutil.move(filename, DEST_DIR)\n                count += 1\n    return count", "raw_solution": "import os\nimport shutil\nimport glob\nimport hashlib\n\ndef task_func(ROOT_DIR, DEST_DIR, SPECIFIC_HASH):\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    count = 0\n    for filename in glob.iglob(ROOT_DIR + '**/*', recursive=True):\n        if os.path.isfile(filename):\n            with open(filename, 'rb') as f:\n                file_hash = hashlib.md5()\n                while chunk := f.read(8192):\n                    file_hash.update(chunk)\n            if file_hash.hexdigest() == SPECIFIC_HASH:\n                shutil.move(filename, DEST_DIR)\n                count += 1\n    return count"}
{"task_id": "BigCodeBench/128", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef random_walk(POINTS=100):\n    \"\"\"\n    Simulates a random walk in a two-dimensional space.\n    The walk is determined by randomly choosing directions at each step.\n    The function generates two numpy arrays representing the x and y coordinates of each step and plots these points to visualize the path of the walk.\n    \"\"\"\n    # Initialize arrays to store x and y coordinates\n    x = np.zeros(POINTS)\n    y = np.zeros(POINTS)\n\n    # Define the directions\n    directions = ['up', 'down', 'left', 'right']\n\n    # Simulate the random walk\n    for i in range(1, POINTS):\n        direction = directions[randint(0, 3)]\n        if direction == 'up':\n            y[i] = y[i - 1] + 1\n            x[i] = x[i - 1]\n        elif direction == 'down':\n            y[i] = y[i - 1] - 1\n            x[i] = x[i - 1]\n        elif direction == 'left':\n            x[i] = x[i - 1] - 1\n            y[i] = y[i - 1]\n        elif direction == 'right':\n            x[i] = x[i - 1] + 1\n            y[i] = y[i - 1]\n\n    # Plot the path of the walk\n    plt.figure()\n    plt.plot(x, y)\n    plt.title('Random Walk')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.grid(True)\n\n    return plt.gcf()"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom requests.exceptions import ConnectionError, HTTPError\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except (ConnectionError, HTTPError) as err:\n        raise SystemExit(err)\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find_all('table')\n\n    if not table:\n        raise ValueError(\"No table data found on the page\")\n\n    table = table[0]\n    rows = table.find_all('tr')\n    header = [th.text for th in rows[0].find_all('th')]\n    data = []\n\n    for row in rows[1:]:\n        cols = row.find_all('td')\n        cols = [col.text.strip() for col in cols]\n        data.append(cols)\n\n    df = pd.DataFrame(data, columns=header)\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom requests.exceptions import ConnectionError, HTTPError\n\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except (ConnectionError, HTTPError) as err:\n        raise SystemExit(err)\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find_all('table')\n\n    if not table:\n        raise ValueError(\"No table data found on the page\")\n\n    table = table[0]\n    rows = table.find_all('tr')\n    header = [th.text for th in rows[0].find_all('th')]\n    data = []\n\n    for row in rows[1:]:\n        cols = row.find_all('td')\n        cols = [col.text.strip() for col in cols]\n        data.append(cols)\n\n    df = pd.DataFrame(data, columns=header)\n    return df"}
{"task_id": "BigCodeBench/130", "solution": "import base64\nimport binascii\nimport os\nimport hashlib\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    hex_bytes = binascii.unhexlify(hex_str)\n\n    # Generate a random salt of the specified size\n    salt = os.urandom(salt_size)\n\n    # Append the salt to the byte representation of the hex string\n    salted_data = hex_bytes + salt\n\n    # Compute the SHA256 hash of the salted data\n    hash_obj = hashlib.sha256()\n    hash_obj.update(salted_data)\n    hash_value = hash_obj.digest()\n\n    # Return the base64-encoded salt and the SHA256 hash\n    return (base64.b64encode(salt).decode(), base64.b64encode(hash_value).decode())", "raw_solution": "import base64\nimport binascii\nimport os\nimport hashlib\n\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    hex_bytes = binascii.unhexlify(hex_str)\n\n    # Generate a random salt of the specified size\n    salt = os.urandom(salt_size)\n\n    # Append the salt to the byte representation of the hex string\n    salted_data = hex_bytes + salt\n\n    # Compute the SHA256 hash of the salted data\n    hash_obj = hashlib.sha256()\n    hash_obj.update(salted_data)\n    hash_value = hash_obj.digest()\n\n    # Return the base64-encoded salt and the SHA256 hash\n    return (base64.b64encode(salt).decode(), base64.b64encode(hash_value).decode())"}
{"task_id": "BigCodeBench/131", "solution": "import base64\nimport binascii\nimport os\nimport hashlib\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    hex_bytes = binascii.unhexlify(hex_str)\n\n    # Generate a random salt of the given size\n    salt = os.urandom(salt_size)\n\n    # Append the salt to the byte representation of the hex string\n    salted_data = hex_bytes + salt\n\n    # Compute the SHA256 hash of the salted data\n    hash_obj = hashlib.sha256()\n    hash_obj.update(salted_data)\n    hash_value = hash_obj.digest()\n\n    # Return the base64-encoded salt and the SHA256 hash\n    return (base64.b64encode(salt).decode(), base64.b64encode(hash_value).decode())", "raw_solution": "import base64\nimport binascii\nimport os\nimport hashlib\n\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    hex_bytes = binascii.unhexlify(hex_str)\n\n    # Generate a random salt of the given size\n    salt = os.urandom(salt_size)\n\n    # Append the salt to the byte representation of the hex string\n    salted_data = hex_bytes + salt\n\n    # Compute the SHA256 hash of the salted data\n    hash_obj = hashlib.sha256()\n    hash_obj.update(salted_data)\n    hash_value = hash_obj.digest()\n\n    # Return the base64-encoded salt and the SHA256 hash\n    return (base64.b64encode(salt).decode(), base64.b64encode(hash_value).decode())"}
{"task_id": "BigCodeBench/132", "solution": "import binascii\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(hex_str):\n    if not isinstance(hex_str, str):\n        raise ValueError(\"'hex_str' must be a string\")\n\n    if hex_str.startswith('\\\\x'):\n        hex_str = hex_str[2:]\n\n    try:\n        bytes_obj = binascii.unhexlify(hex_str)\n    except binascii.Error:\n        raise ValueError(\"'hex_str' is not a valid hex string\")\n\n    byte_freq = np.bincount(bytes_obj)\n    df = pd.DataFrame({'Byte Value': range(256), 'Frequency': byte_freq})\n    df = df[df['Frequency'] > 0]\n\n    fig, ax = plt.subplots()\n    ax.bar(df['Byte Value'], df['Frequency'])\n    ax.set_xlabel('Byte Value')\n    ax.set_ylabel('Frequency')\n\n    return (df, ax)", "raw_solution": "import binascii\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(hex_str):\n    if not isinstance(hex_str, str):\n        raise ValueError(\"'hex_str' must be a string\")\n\n    if hex_str.startswith('\\\\x'):\n        hex_str = hex_str[2:]\n\n    try:\n        bytes_obj = binascii.unhexlify(hex_str)\n    except binascii.Error:\n        raise ValueError(\"'hex_str' is not a valid hex string\")\n\n    byte_freq = np.bincount(bytes_obj)\n    df = pd.DataFrame({'Byte Value': range(256), 'Frequency': byte_freq})\n    df = df[df['Frequency'] > 0]\n\n    fig, ax = plt.subplots()\n    ax.bar(df['Byte Value'], df['Frequency'])\n    ax.set_xlabel('Byte Value')\n    ax.set_ylabel('Frequency')\n\n    return (df, ax)"}
{"task_id": "BigCodeBench/133", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Normalize the last column\n    scaler = MinMaxScaler()\n    df.iloc[:, -1] = scaler.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))\n\n    # Plot the normalized data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.iloc[:, -1])\n    plt.title(f'Normalized Data of {df.columns[-1]}')\n    plt.xlabel('Index')\n    plt.ylabel('Normalized Value')\n    plt.tight_layout()\n\n    return df, plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Normalize the last column\n    scaler = MinMaxScaler()\n    df.iloc[:, -1] = scaler.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))\n\n    # Plot the normalized data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.iloc[:, -1])\n    plt.title(f'Normalized Data of {df.columns[-1]}')\n    plt.xlabel('Index')\n    plt.ylabel('Normalized Value')\n    plt.tight_layout()\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/134", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=20):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    last_column = df.columns[-1]\n    plt.hist(df[last_column], bins=bins)\n    plt.title(f'Histogram of {last_column}')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    return plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=20):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    last_column = df.columns[-1]\n    plt.hist(df[last_column], bins=bins)\n    plt.title(f'Histogram of {last_column}')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    return plt.gca()"}
{"task_id": "BigCodeBench/135", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n    if df.shape[1] == 0:\n        raise ValueError(\"DataFrame has no columns\")\n\n    # Impute missing values in the last column using mean imputation\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n    df.iloc[:, -1] = imp.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))\n\n    # Create a box plot to visualize the distribution of data in the last column\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=df.iloc[:, -1])\n    plt.title('Box plot of the last column')\n    plt.xlabel('Last Column')\n    plt.ylabel('Value')\n\n    return df, plt.gca()", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n    if df.shape[1] == 0:\n        raise ValueError(\"DataFrame has no columns\")\n\n    # Impute missing values in the last column using mean imputation\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n    df.iloc[:, -1] = imp.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))\n\n    # Create a box plot to visualize the distribution of data in the last column\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=df.iloc[:, -1])\n    plt.title('Box plot of the last column')\n    plt.xlabel('Last Column')\n    plt.ylabel('Value')\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/136", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df)\n\n    principalDf = pd.DataFrame(data = principalComponents, columns = ['Principal Component 1', 'Principal Component 2'])\n\n    fig, ax = plt.subplots()\n    ax.scatter(principalDf['Principal Component 1'], principalDf['Principal Component 2'])\n    ax.set_title('2 Component PCA')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return principalDf, ax", "raw_solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(df)\n\n    principalDf = pd.DataFrame(data = principalComponents, columns = ['Principal Component 1', 'Principal Component 2'])\n\n    fig, ax = plt.subplots()\n    ax.scatter(principalDf['Principal Component 1'], principalDf['Principal Component 2'])\n    ax.set_title('2 Component PCA')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return principalDf, ax"}
{"task_id": "BigCodeBench/137", "solution": "import pandas as pd\nfrom scipy.stats import skew\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.shape[1] == 0:\n        raise ValueError(\"DataFrame has no columns\")\n    return skew(df.iloc[:, -1])\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})", "raw_solution": "import pandas as pd\nfrom scipy.stats import skew\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.shape[1] == 0:\n        raise ValueError(\"DataFrame has no columns\")\n    return skew(df.iloc[:, -1])\n\n# Testing the function\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})\n\nassert task_func(df) == 0.0, \"Test Case 1 Failed\"\n\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500],\n    'D': [1000, 2000, 3000, 4000, 5000]\n})\n\nassert task_func(df) == 0.0, \"Test Case 2 Failed\"\n\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500],\n    'D': [1000, 2000, 3000, 4000, 5000],\n    'E': [10000, 20000, 30000, 40000, 50000]\n})\n\nassert task_func(df) == 0.0, \"Test Case 3 Failed\"\n"}
{"task_id": "BigCodeBench/138", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, letters=list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if 'Letters' not in df.columns:\n        raise ValueError(\"'df' lacks the 'Letters' column\")\n\n    letter_freq = df['Letters'].value_counts()\n    letter_freq = letter_freq[letters]\n\n    fig, ax = plt.subplots()\n    ax.bar(letter_freq.index, letter_freq.values)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, letters=list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if 'Letters' not in df.columns:\n        raise ValueError(\"'df' lacks the 'Letters' column\")\n\n    letter_freq = df['Letters'].value_counts()\n    letter_freq = letter_freq[letters]\n\n    fig, ax = plt.subplots()\n    ax.bar(letter_freq.index, letter_freq.values)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n\n    return ax"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input should be a non-empty DataFrame\")\n\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if not numeric_cols.any():\n        raise ValueError(\"There are no numeric columns in the DataFrame\")\n\n    figs, axes = plt.subplots(len(numeric_cols), figsize=(10, 10))\n    if len(numeric_cols) == 1:\n        axes = [axes]\n\n    for ax, col in zip(axes, numeric_cols):\n        df[col].plot(kind='hist', ax=ax, title=col, xlabel='Value', ylabel='Frequency')\n\n    plt.tight_layout()\n    return axes", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input should be a non-empty DataFrame\")\n\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if not numeric_cols.any():\n        raise ValueError(\"There are no numeric columns in the DataFrame\")\n\n    figs, axes = plt.subplots(len(numeric_cols), figsize=(10, 10))\n    if len(numeric_cols) == 1:\n        axes = [axes]\n\n    for ax, col in zip(axes, numeric_cols):\n        df[col].plot(kind='hist', ax=ax, title=col, xlabel='Value', ylabel='Frequency')\n\n    plt.tight_layout()\n    return axes"}
{"task_id": "BigCodeBench/140", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, cols):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if not isinstance(cols, list):\n        raise ValueError(\"'cols' is not a list\")\n    for col in cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' does not exist in 'df'\")\n\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, cols):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if not isinstance(cols, list):\n        raise ValueError(\"'cols' is not a list\")\n    for col in cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' does not exist in 'df'\")\n\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n    return df"}
{"task_id": "BigCodeBench/141", "solution": "import numpy as np\nimport pandas as pd\nimport statistics\ndef task_func(rows, columns=['A', 'B', 'C', 'D', 'E', 'F'], seed=42):\n    if not isinstance(rows, int) or rows <= 0:\n        raise ValueError(\"'rows' must be a positive integer greater than 0.\")\n\n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n\n    stats_dict = {}\n    for col in columns:\n        mean_val = df[col].mean()\n        median_val = df[col].median()\n        stats_dict[col] = {'mean': mean_val, 'median': median_val}\n\n    return df, stats_dict", "raw_solution": "import numpy as np\nimport pandas as pd\nimport statistics\n\ndef task_func(rows, columns=['A', 'B', 'C', 'D', 'E', 'F'], seed=42):\n    if not isinstance(rows, int) or rows <= 0:\n        raise ValueError(\"'rows' must be a positive integer greater than 0.\")\n\n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n\n    stats_dict = {}\n    for col in columns:\n        mean_val = df[col].mean()\n        median_val = df[col].median()\n        stats_dict[col] = {'mean': mean_val, 'median': median_val}\n\n    return df, stats_dict"}
{"task_id": "BigCodeBench/142", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    x = np.linspace(0, 2*np.pi, 1000)\n    y_sin = np.sin(x)\n    y_cos = np.cos(x)\n\n    fig, axs = plt.subplots(2)\n    fig.set_size_inches(10, 10)\n\n    axs[0].plot(x, y_sin, label='Sine function')\n    axs[0].set_xlabel('x')\n    axs[0].set_ylabel('sin(x)')\n    axs[0].set_title('Sine function')\n    axs[0].legend()\n\n    axs[1].plot(x, y_cos, label='Cosine function')\n    axs[1].set_xlabel('x')\n    axs[1].set_ylabel('cos(x)')\n    axs[1].set_title('Cosine function')\n    axs[1].legend()\n\n    plt.tight_layout()\n\n    return fig, axs", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    x = np.linspace(0, 2*np.pi, 1000)\n    y_sin = np.sin(x)\n    y_cos = np.cos(x)\n\n    fig, axs = plt.subplots(2)\n    fig.set_size_inches(10, 10)\n\n    axs[0].plot(x, y_sin, label='Sine function')\n    axs[0].set_xlabel('x')\n    axs[0].set_ylabel('sin(x)')\n    axs[0].set_title('Sine function')\n    axs[0].legend()\n\n    axs[1].plot(x, y_cos, label='Cosine function')\n    axs[1].set_xlabel('x')\n    axs[1].set_ylabel('cos(x)')\n    axs[1].set_title('Cosine function')\n    axs[1].legend()\n\n    plt.tight_layout()\n\n    return fig, axs"}
{"task_id": "BigCodeBench/143", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    # Define the equation y = 2x + 1\n    def equation(x):\n        return 2 * x + 1\n\n    # Define the x values\n    x_values = np.linspace(-10, 10, 400)\n\n    # Calculate the corresponding y values\n    y_values = equation(x_values)\n\n    # Create the plot\n    plt.figure(figsize=(8, 6))\n    plt.plot(x_values, y_values, label='y=2x+1', color='red')\n\n    # Mark the solution point\n    x_solution = 2\n    y_solution = equation(x_solution)\n    plt.plot(x_solution, y_solution, 'go', label=f'Solution at x={x_solution}, y={y_solution}')\n\n    # Set the title and labels\n    plt.title('Solution of the equation y=2x+1 at x=2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n\n    # Set the y-axis range automatically\n    plt.ylim(np.min(y_values), np.max(y_values))\n\n    # Add a legend\n    plt.legend()\n\n    # Return the plot object\n    return plt.gca()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    # Define the equation y = 2x + 1\n    def equation(x):\n        return 2 * x + 1\n\n    # Define the x values\n    x_values = np.linspace(-10, 10, 400)\n\n    # Calculate the corresponding y values\n    y_values = equation(x_values)\n\n    # Create the plot\n    plt.figure(figsize=(8, 6))\n    plt.plot(x_values, y_values, label='y=2x+1', color='red')\n\n    # Mark the solution point\n    x_solution = 2\n    y_solution = equation(x_solution)\n    plt.plot(x_solution, y_solution, 'go', label=f'Solution at x={x_solution}, y={y_solution}')\n\n    # Set the title and labels\n    plt.title('Solution of the equation y=2x+1 at x=2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n\n    # Set the y-axis range automatically\n    plt.ylim(np.min(y_values), np.max(y_values))\n\n    # Add a legend\n    plt.legend()\n\n    # Return the plot object\n    return plt.gca()"}
{"task_id": "BigCodeBench/144", "solution": "import ipaddress\nimport requests\ndef task_func(ip_range, timeout):\n    try:\n        network = ipaddress.IPv4Network(ip_range)\n    except ValueError:\n        raise ValueError(\"Invalid IP range provided\")\n\n    active_ips = []\n\n    for host in network.hosts():\n        url = f\"http://{str(host)}\"\n        try:\n            response = requests.get(url, timeout=timeout)\n            if response.status_code == 200:\n                active_ips.append(str(host))\n        except requests.exceptions.RequestException:\n            continue\n\n    return active_ips", "raw_solution": "import ipaddress\nimport requests\n\ndef task_func(ip_range, timeout):\n    try:\n        network = ipaddress.IPv4Network(ip_range)\n    except ValueError:\n        raise ValueError(\"Invalid IP range provided\")\n\n    active_ips = []\n\n    for host in network.hosts():\n        url = f\"http://{str(host)}\"\n        try:\n            response = requests.get(url, timeout=timeout)\n            if response.status_code == 200:\n                active_ips.append(str(host))\n        except requests.exceptions.RequestException:\n            continue\n\n    return active_ips"}
{"task_id": "BigCodeBench/145", "solution": "import csv\nfrom ipaddress import IPv4Network\ndef task_func(ip_range, csv_path):\n    \"\"\"\n    Generates a CSV file listing all IP addresses in the specified IP range.\n    Each IP address is written as a row in the CSV file.\n    \"\"\"\n    network = IPv4Network(ip_range)\n\n    with open(csv_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        for ip in network.hosts():\n            writer.writerow([str(ip)])\n\n    return csv_path", "raw_solution": "import csv\nfrom ipaddress import IPv4Network\n\ndef task_func(ip_range, csv_path):\n    \"\"\"\n    Generates a CSV file listing all IP addresses in the specified IP range.\n    Each IP address is written as a row in the CSV file.\n    \"\"\"\n    network = IPv4Network(ip_range)\n\n    with open(csv_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        for ip in network.hosts():\n            writer.writerow([str(ip)])\n\n    return csv_path"}
{"task_id": "BigCodeBench/146", "solution": "import subprocess\nfrom ipaddress import IPv4Network\ndef task_func(ip_range):\n    \"\"\"\n    Scans the specified IP address range and pings each IP to check if it is active.\n    The function returns a dictionary with IP addresses as keys and a boolean value indicating their active status (True if the ping is successful, False otherwise).\n    The function raises the exception for: subprocess.CalledProcessError: If a ping command fails due to a subprocess error.\n    \"\"\"\n    # Create an empty dictionary to store the IP addresses and their active status\n    ip_status = {}\n\n    # Iterate over each IP address in the specified IP address range\n    for ip in IPv4Network(ip_range):\n        # Ping the IP address and capture the output\n        try:\n            output = subprocess.check_output([\"ping\", \"-c\", \"1\", str(ip)], stderr=subprocess.STDOUT)\n        except subprocess.CalledProcessError as e:\n            # If the ping command fails, set the IP address's active status to False\n            ip_status[str(ip)] = False\n            continue\n\n        # If the ping command is successful, set the IP address's active status to True\n        ip_status[str(ip)] = True\n\n    # Return the dictionary of IP addresses and their active status\n    return ip_status", "raw_solution": "import subprocess\nfrom ipaddress import IPv4Network\n\ndef task_func(ip_range):\n    \"\"\"\n    Scans the specified IP address range and pings each IP to check if it is active.\n    The function returns a dictionary with IP addresses as keys and a boolean value indicating their active status (True if the ping is successful, False otherwise).\n    The function raises the exception for: subprocess.CalledProcessError: If a ping command fails due to a subprocess error.\n    \"\"\"\n    # Create an empty dictionary to store the IP addresses and their active status\n    ip_status = {}\n\n    # Iterate over each IP address in the specified IP address range\n    for ip in IPv4Network(ip_range):\n        # Ping the IP address and capture the output\n        try:\n            output = subprocess.check_output([\"ping\", \"-c\", \"1\", str(ip)], stderr=subprocess.STDOUT)\n        except subprocess.CalledProcessError as e:\n            # If the ping command fails, set the IP address's active status to False\n            ip_status[str(ip)] = False\n            continue\n\n        # If the ping command is successful, set the IP address's active status to True\n        ip_status[str(ip)] = True\n\n    # Return the dictionary of IP addresses and their active status\n    return ip_status"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    def check_port(ip, port):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((str(ip), port))\n        sock.close()\n        return result == 0\n\n    ip_network = IPv4Network(ip_range)\n    results = {}\n\n    for ip in ip_network.hosts():\n        Thread(target=lambda ip=ip: results.update({ip: check_port(ip, port)})).start()\n\n    while len(results) < len(ip_network.hosts()):\n        pass\n\n    return results", "raw_solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    def check_port(ip, port):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((str(ip), port))\n        sock.close()\n        return result == 0\n\n    ip_network = IPv4Network(ip_range)\n    results = {}\n\n    for ip in ip_network.hosts():\n        Thread(target=lambda ip=ip: results.update({ip: check_port(ip, port)})).start()\n\n    while len(results) < len(ip_network.hosts()):\n        pass\n\n    return results"}
{"task_id": "BigCodeBench/148", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Encrypt the categorical data in a specific column of a DataFrame using LabelEncoder.\n    \"\"\"\n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    return df\ndf = pd.DataFrame({\n    'category': ['cat', 'dog', 'cat', 'bird', 'dog', 'cat', 'bird']\n})", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Encrypt the categorical data in a specific column of a DataFrame using LabelEncoder.\n    \"\"\"\n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    return df\n\n# Test the function\ndf = pd.DataFrame({\n    'category': ['cat', 'dog', 'cat', 'bird', 'dog', 'cat', 'bird']\n})\n\ndf = task_func(df, 'category')"}
{"task_id": "BigCodeBench/149", "solution": "import pandas as pd\nimport numpy as np\nDEFAULT_COLUMNS = ['Element', 'Count']\ndef task_func(elements, include_index=False):\n    \"\"\"\n    Constructs a DataFrame that enumerates the character counts of each string in a provided list of elements.\n    Optionally, includes an index column for each row in the DataFrame.\n    \"\"\"\n    if include_index:\n        DEFAULT_COLUMNS.insert(0, 'Index')\n\n    data = [(i, element, len(element)) for i, element in enumerate(elements)]\n    df = pd.DataFrame(data, columns=DEFAULT_COLUMNS)\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\nDEFAULT_COLUMNS = ['Element', 'Count']\n\ndef task_func(elements, include_index=False):\n    \"\"\"\n    Constructs a DataFrame that enumerates the character counts of each string in a provided list of elements.\n    Optionally, includes an index column for each row in the DataFrame.\n    \"\"\"\n    if include_index:\n        DEFAULT_COLUMNS.insert(0, 'Index')\n\n    data = [(i, element, len(element)) for i, element in enumerate(elements)]\n    df = pd.DataFrame(data, columns=DEFAULT_COLUMNS)\n\n    return df"}
{"task_id": "BigCodeBench/150", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(product_dict, product_keys):\n    if not product_dict or not product_keys:\n        return None, None\n\n    data = []\n    total_price = 0\n    total_profit = 0\n\n    for key in product_keys:\n        product = product_dict[key]\n        data.append([key, product['quantity'], product['price'], product['profit']])\n        total_price += product['price'] * product['quantity']\n        total_profit += product['profit'] * product['quantity']\n\n    average_price = total_price / sum(product['quantity'] for product in product_dict.values())\n    average_profit = total_profit / sum(product['quantity'] for product in product_dict.values())\n\n    df = pd.DataFrame(data, columns=['Product', 'Quantity', 'Price', 'Profit'])\n    df['Average Price'] = average_price\n    df['Average Profit'] = average_profit\n\n    fig, ax = plt.subplots()\n    df.plot(x='Product', y='Profit', kind='bar', ax=ax)\n    ax.set_ylabel('Profit')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(product_dict, product_keys):\n    if not product_dict or not product_keys:\n        return None, None\n\n    data = []\n    total_price = 0\n    total_profit = 0\n\n    for key in product_keys:\n        product = product_dict[key]\n        data.append([key, product['quantity'], product['price'], product['profit']])\n        total_price += product['price'] * product['quantity']\n        total_profit += product['profit'] * product['quantity']\n\n    average_price = total_price / sum(product['quantity'] for product in product_dict.values())\n    average_profit = total_profit / sum(product['quantity'] for product in product_dict.values())\n\n    df = pd.DataFrame(data, columns=['Product', 'Quantity', 'Price', 'Profit'])\n    df['Average Price'] = average_price\n    df['Average Profit'] = average_profit\n\n    fig, ax = plt.subplots()\n    df.plot(x='Product', y='Profit', kind='bar', ax=ax)\n    ax.set_ylabel('Profit')\n\n    return df, ax"}
{"task_id": "BigCodeBench/151", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data_dict, data_keys):\n    \"\"\"\n    Normalizes data specified by keys in a dictionary using MinMax scaling and plot the results.\n    \"\"\"\n    # Check if any key in data_keys is in data_dict\n    if not any(key in data_dict for key in data_keys):\n        raise ValueError(\"No keys in `data_keys` are found in `data_dict`.\")\n\n    # Prepare data for MinMaxScaler\n    data = pd.DataFrame([data_dict[key] for key in data_keys], index=data_keys).T\n\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    normalized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n\n    # Plot the normalized data\n    fig, ax = plt.subplots()\n    normalized_data.plot(ax=ax)\n\n    return normalized_data, ax.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data_dict, data_keys):\n    \"\"\"\n    Normalizes data specified by keys in a dictionary using MinMax scaling and plot the results.\n    \"\"\"\n    # Check if any key in data_keys is in data_dict\n    if not any(key in data_dict for key in data_keys):\n        raise ValueError(\"No keys in `data_keys` are found in `data_dict`.\")\n\n    # Prepare data for MinMaxScaler\n    data = pd.DataFrame([data_dict[key] for key in data_keys], index=data_keys).T\n\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    normalized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n\n    # Plot the normalized data\n    fig, ax = plt.subplots()\n    normalized_data.plot(ax=ax)\n\n    return normalized_data, ax.figure"}
{"task_id": "BigCodeBench/152", "solution": "import pandas as pd\nimport numpy as np\nimport random", "raw_solution": "import pandas as pd\nimport numpy as np\nimport random\n\n# Constants\nSTUDENTS = ['Joe', 'Amy', 'Mark', 'Sara', 'John', 'Emily', 'Zoe', 'Matt']\nCOURSES = ['Math', 'Physics', 'Chemistry', 'Biology', 'English', 'History', 'Geography', 'Computer Science']\n\ndef generate_grades():\n    # Create a DataFrame with students and courses\n    df = pd.DataFrame(STUDENTS, columns=['Student'])\n    df['Courses'] = COURSES\n\n    # Generate random grades for each student in each course\n    for course in COURSES:\n        df[course] = [random.uniform(0, 100) for _ in range(len(STUDENTS))]\n\n    # Calculate the average grade for each student\n    df['Average'] = df[COURSES].mean(axis=1)\n\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/153", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(data):\n    \"\"\"\n    Transforms categorical data into a numerical format suitable for machine learning algorithms using sklearn's LabelEncoder.\n    This function generates a DataFrame that pairs original categorical values with their numerical encodings.\n    The function should output with:\n        DataFrame: A DataFrame with columns 'Category' and 'Encoded', where 'Category' is the original data and 'Encoded'\n        is the numerical representation.\n    \"\"\"\n    le = LabelEncoder()\n    data_encoded = le.fit_transform(data)\n    df = pd.DataFrame({'Category': data, 'Encoded': data_encoded})\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(data):\n    \"\"\"\n    Transforms categorical data into a numerical format suitable for machine learning algorithms using sklearn's LabelEncoder.\n    This function generates a DataFrame that pairs original categorical values with their numerical encodings.\n    The function should output with:\n        DataFrame: A DataFrame with columns 'Category' and 'Encoded', where 'Category' is the original data and 'Encoded'\n        is the numerical representation.\n    \"\"\"\n    le = LabelEncoder()\n    data_encoded = le.fit_transform(data)\n    df = pd.DataFrame({'Category': data, 'Encoded': data_encoded})\n    return df"}
{"task_id": "BigCodeBench/154", "solution": "import re\nimport os\nimport glob\nimport mimetypes\ndef task_func(directory, file_pattern, suffix):\n    \"\"\"\n    Scans a specified directory for files matching a given pattern and with a certain suffix,\n    then determines their file types. The function returns a dictionary with file names as keys\n    and their corresponding MIME types as values.\n    \"\"\"\n    # Compile the file pattern into a regular expression\n    pattern = re.compile(file_pattern)\n\n    # Initialize an empty dictionary to store the file names and their MIME types\n    file_dict = {}\n\n    # Iterate over all files in the directory\n    for file in glob.glob(os.path.join(directory, '*')):\n        # Check if the file name matches the pattern and has the specified suffix\n        if pattern.match(os.path.basename(file)) and file.endswith(suffix):\n            # Determine the MIME type of the file\n            mime_type = mimetypes.guess_type(file)[0]\n\n            # Add the file name and its MIME type to the dictionary\n            file_dict[os.path.basename(file)] = mime_type\n\n    # Return the dictionary\n    return file_dict", "raw_solution": "import re\nimport os\nimport glob\nimport mimetypes\n\ndef task_func(directory, file_pattern, suffix):\n    \"\"\"\n    Scans a specified directory for files matching a given pattern and with a certain suffix,\n    then determines their file types. The function returns a dictionary with file names as keys\n    and their corresponding MIME types as values.\n    \"\"\"\n    # Compile the file pattern into a regular expression\n    pattern = re.compile(file_pattern)\n\n    # Initialize an empty dictionary to store the file names and their MIME types\n    file_dict = {}\n\n    # Iterate over all files in the directory\n    for file in glob.glob(os.path.join(directory, '*')):\n        # Check if the file name matches the pattern and has the specified suffix\n        if pattern.match(os.path.basename(file)) and file.endswith(suffix):\n            # Determine the MIME type of the file\n            mime_type = mimetypes.guess_type(file)[0]\n\n            # Add the file name and its MIME type to the dictionary\n            file_dict[os.path.basename(file)] = mime_type\n\n    # Return the dictionary\n    return file_dict"}
{"task_id": "BigCodeBench/155", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\ndef task_func(data):\n    \"\"\"\n    Computes the average of each row in a provided 2D array and appends these averages as a new column.\n    Additionally, it plots the averages against their respective row indices.\n    \"\"\"\n    # Convert the 2D array to a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Compute the average of each row and append it as a new column\n    df['Average'] = df.mean(axis=1)\n\n    # Plot the averages against their respective row indices\n    fig, ax = plt.subplots()\n    ax.plot(df['Average'])\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Average')\n    ax.set_title('Row Averages')\n\n    return (df, ax)", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef task_func(data):\n    \"\"\"\n    Computes the average of each row in a provided 2D array and appends these averages as a new column.\n    Additionally, it plots the averages against their respective row indices.\n    \"\"\"\n    # Convert the 2D array to a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Compute the average of each row and append it as a new column\n    df['Average'] = df.mean(axis=1)\n\n    # Plot the averages against their respective row indices\n    fig, ax = plt.subplots()\n    ax.plot(df['Average'])\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Average')\n    ax.set_title('Row Averages')\n\n    return (df, ax)"}
{"task_id": "BigCodeBench/156", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Normalize the data using MinMaxScaler\n    scaler = MinMaxScaler()\n    data_normalized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Calculate the average of each row and add it as a new column 'Average'\n    data_normalized['Average'] = data_normalized.mean(axis=1)\n\n    # Plot the average values\n    plt.figure(figsize=(10, 6))\n    plt.bar(range(len(data_normalized)), data_normalized['Average'])\n    plt.xlabel('Row')\n    plt.ylabel('Average')\n    plt.title('Average values across the dataset')\n    plt.tight_layout()\n\n    return data_normalized, plt.gca()", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Normalize the data using MinMaxScaler\n    scaler = MinMaxScaler()\n    data_normalized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Calculate the average of each row and add it as a new column 'Average'\n    data_normalized['Average'] = data_normalized.mean(axis=1)\n\n    # Plot the average values\n    plt.figure(figsize=(10, 6))\n    plt.bar(range(len(data_normalized)), data_normalized['Average'])\n    plt.xlabel('Row')\n    plt.ylabel('Average')\n    plt.title('Average values across the dataset')\n    plt.tight_layout()\n\n    return data_normalized, plt.gca()"}
{"task_id": "BigCodeBench/157", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n    if data.shape[1] < 2:\n        raise ValueError(\"Input data should be a 2D array\")\n    if not np.issubdtype(data.dtypes[0], np.number):\n        raise ValueError(\"Input data should contain only numeric data\")\n\n    df = data.copy()\n    df['Average'] = df.mean(axis=1)\n\n    corr = df.corr()\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(corr, annot=True, fmt=\".2f\", linewidths=.5, cmap='coolwarm', cbar=True, ax=ax)\n    plt.title('Correlation Matrix Heatmap')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n    if data.shape[1] < 2:\n        raise ValueError(\"Input data should be a 2D array\")\n    if not np.issubdtype(data.dtypes[0], np.number):\n        raise ValueError(\"Input data should contain only numeric data\")\n\n    df = data.copy()\n    df['Average'] = df.mean(axis=1)\n\n    corr = df.corr()\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(corr, annot=True, fmt=\".2f\", linewidths=.5, cmap='coolwarm', cbar=True, ax=ax)\n    plt.title('Correlation Matrix Heatmap')\n\n    return df, ax"}
{"task_id": "BigCodeBench/158", "solution": "import json\nimport urllib.request\nimport urllib.parse\nimport gzip\nimport os\ndef task_func(url_str, file_path):\n    \"\"\"\n    Fetches JSON data from a given URL, decodes the json-formatted data, and compresses it into a gzip file.\n    \"\"\"\n    # Fetch JSON data from URL\n    with urllib.request.urlopen(url_str) as url:\n        data = json.loads(url.read().decode())\n\n    # Compress JSON data into gzip file\n    with gzip.open(file_path, 'wb') as f:\n        f.write(json.dumps(data).encode())\n\n    # Return the path to the compressed gzip file\n    return file_path", "raw_solution": "import json\nimport urllib.request\nimport urllib.parse\nimport gzip\nimport os\n\ndef task_func(url_str, file_path):\n    \"\"\"\n    Fetches JSON data from a given URL, decodes the json-formatted data, and compresses it into a gzip file.\n    \"\"\"\n    # Fetch JSON data from URL\n    with urllib.request.urlopen(url_str) as url:\n        data = json.loads(url.read().decode())\n\n    # Compress JSON data into gzip file\n    with gzip.open(file_path, 'wb') as f:\n        f.write(json.dumps(data).encode())\n\n    # Return the path to the compressed gzip file\n    return file_path"}
{"task_id": "BigCodeBench/159", "solution": "import struct\nimport io\nimport gzip\nimport numpy as np", "raw_solution": "import struct\nimport io\nimport gzip\nimport numpy as np\n\ndef compress_numpy_array(array):\n    \"\"\"\n    Compresses a given NumPy array using gzip compression and returns the compressed data.\n    \"\"\"\n    # Convert the NumPy array to bytes\n    bytes_data = array.tobytes()\n\n    # Compress the bytes data using gzip\n    compressed_data = gzip.compress(bytes_data)\n\n    return compressed_data\n\n# Test the function\narray = np.array([1, 2, 3, 4, 5], dtype=np.uint8)\ncompressed_data = compress_numpy_array(array)"}
{"task_id": "BigCodeBench/160", "solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\ndef task_func(data):\n    \"\"\"\n    Processes a given dataset to compute the average of each row, plots the distribution of these averages, and evaluates their normality.\n    \"\"\"\n    if data.shape[1] != len(COLUMN_NAMES):\n        raise ValueError(\"The input data does not have exactly eight columns.\")\n\n    data['Average'] = data[COLUMN_NAMES].mean(axis=1)\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.distplot(data['Average'], ax=ax)\n    ax.set_title('Distribution of Averages')\n\n    p_value = None\n    if data.shape[0] >= 20:\n        _, p_value = stats.normaltest(data['Average'])\n\n    return data, ax, p_value", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef task_func(data):\n    \"\"\"\n    Processes a given dataset to compute the average of each row, plots the distribution of these averages, and evaluates their normality.\n    \"\"\"\n    if data.shape[1] != len(COLUMN_NAMES):\n        raise ValueError(\"The input data does not have exactly eight columns.\")\n\n    data['Average'] = data[COLUMN_NAMES].mean(axis=1)\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.distplot(data['Average'], ax=ax)\n    ax.set_title('Distribution of Averages')\n\n    p_value = None\n    if data.shape[0] >= 20:\n        _, p_value = stats.normaltest(data['Average'])\n\n    return data, ax, p_value"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information such as message type, timestamp, and the message itself from a log file and stores the data in a CSV format.\n    \"\"\"\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(.*?)\\] - (.*)'\n\n    # Initialize an empty list to store the log entries\n    log_entries = []\n\n    # Open the log file and read each line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Use the regular expression to match the log entry pattern\n            match = re.match(pattern, line)\n\n            # If a match is found, extract the message type, timestamp, and message\n            if match:\n                message_type, timestamp, message = match.groups()\n\n                # Validate the timestamp\n                try:\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                except ValueError:\n                    raise ValueError(f'Invalid timestamp in log entry: {line}')\n\n                # Append the log entry to the list\n                log_entries.append({\n                    'message_type': message_type,\n                    'timestamp': timestamp,\n                    'message': message\n                })\n\n    # If no valid log entries are found, raise an exception\n    if not log_entries:\n        raise ValueError('No valid log entries found')\n\n    # Convert the log entries to a pandas DataFrame\n    df = pd.DataFrame(log_entries)\n\n    # Generate a file path for the CSV file\n    csv_file = f'{log_file}.csv'\n\n    # Write the DataFrame to the CSV file\n    df.to_csv(csv_file, index=False)\n\n    # Return the file path to the CSV file\n    return csv_file", "raw_solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information such as message type, timestamp, and the message itself from a log file and stores the data in a CSV format.\n    \"\"\"\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(.*?)\\] - (.*)'\n\n    # Initialize an empty list to store the log entries\n    log_entries = []\n\n    # Open the log file and read each line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Use the regular expression to match the log entry pattern\n            match = re.match(pattern, line)\n\n            # If a match is found, extract the message type, timestamp, and message\n            if match:\n                message_type, timestamp, message = match.groups()\n\n                # Validate the timestamp\n                try:\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                except ValueError:\n                    raise ValueError(f'Invalid timestamp in log entry: {line}')\n\n                # Append the log entry to the list\n                log_entries.append({\n                    'message_type': message_type,\n                    'timestamp': timestamp,\n                    'message': message\n                })\n\n    # If no valid log entries are found, raise an exception\n    if not log_entries:\n        raise ValueError('No valid log entries found')\n\n    # Convert the log entries to a pandas DataFrame\n    df = pd.DataFrame(log_entries)\n\n    # Generate a file path for the CSV file\n    csv_file = f'{log_file}.csv'\n\n    # Write the DataFrame to the CSV file\n    df.to_csv(csv_file, index=False)\n\n    # Return the file path to the CSV file\n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a text.\n    The function generates a histogram subplot, which facilitates the understanding of how word lengths vary within the provided text.\n    If there are no words in the input text, or all words are filtered out, the histogram will be empty as no bins will be created.\n    \"\"\"\n    # Filter out non-word characters and split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # If there are no words in the input text, return an empty histogram\n    if not words:\n        return plt.hist([], bins=0)\n\n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n\n    # Generate a histogram of word lengths\n    bins = np.arange(min(word_lengths), max(word_lengths) + 2, 1) - 0.5\n    ax = plt.hist(word_lengths, bins=bins, rwidth=rwidth)\n\n    # Set the x-axis label and title\n    plt.xlabel('Word Length')\n    plt.title('Distribution of Word Lengths')\n\n    return ax", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a text.\n    The function generates a histogram subplot, which facilitates the understanding of how word lengths vary within the provided text.\n    If there are no words in the input text, or all words are filtered out, the histogram will be empty as no bins will be created.\n    \"\"\"\n    # Filter out non-word characters and split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # If there are no words in the input text, return an empty histogram\n    if not words:\n        return plt.hist([], bins=0)\n\n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n\n    # Generate a histogram of word lengths\n    bins = np.arange(min(word_lengths), max(word_lengths) + 2, 1) - 0.5\n    ax = plt.hist(word_lengths, bins=bins, rwidth=rwidth)\n\n    # Set the x-axis label and title\n    plt.xlabel('Word Length')\n    plt.title('Distribution of Word Lengths')\n\n    return ax"}
{"task_id": "BigCodeBench/163", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(rows=5, cols=5):\n    if cols > 26:\n        raise ValueError(\"The number of columns exceeds the number of available categories\")\n\n    # Generate random numerical data\n    data = np.random.randint(1, 100, size=(rows, cols))\n    df = pd.DataFrame(data, columns=[chr(65 + i) for i in range(cols)])\n\n    # Plot stacked bar chart\n    ax = df.plot(kind='bar', stacked=True, figsize=(10, 6))\n    plt.title('Stacked Bar Chart')\n    plt.xlabel('Categories')\n    plt.ylabel('Values')\n    plt.legend(loc='upper left')\n\n    return ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(rows=5, cols=5):\n    if cols > 26:\n        raise ValueError(\"The number of columns exceeds the number of available categories\")\n\n    # Generate random numerical data\n    data = np.random.randint(1, 100, size=(rows, cols))\n    df = pd.DataFrame(data, columns=[chr(65 + i) for i in range(cols)])\n\n    # Plot stacked bar chart\n    ax = df.plot(kind='bar', stacked=True, figsize=(10, 6))\n    plt.title('Stacked Bar Chart')\n    plt.xlabel('Categories')\n    plt.ylabel('Values')\n    plt.legend(loc='upper left')\n\n    return ax"}
{"task_id": "BigCodeBench/164", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(num_labels=5, data_range=(0, 1)):\n    # Generate random numeric data\n    data = np.random.rand(num_labels, num_labels)\n    data = data / data.sum(axis=0)\n    data = data * (data_range[1] - data_range[0]) + data_range[0]\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=[f'Label {i+1}' for i in range(num_labels)], index=[f'Data {i+1}' for i in range(num_labels)])\n\n    # Create a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n\n    # Set the title and labels\n    ax.set_title('Stacked Bar Chart of Generated Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Labels')\n\n    return fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(num_labels=5, data_range=(0, 1)):\n    # Generate random numeric data\n    data = np.random.rand(num_labels, num_labels)\n    data = data / data.sum(axis=0)\n    data = data * (data_range[1] - data_range[0]) + data_range[0]\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=[f'Label {i+1}' for i in range(num_labels)], index=[f'Data {i+1}' for i in range(num_labels)])\n\n    # Create a stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n\n    # Set the title and labels\n    ax.set_title('Stacked Bar Chart of Generated Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Labels')\n\n    return fig"}
{"task_id": "BigCodeBench/165", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(num_rows=5, rand_range=(0, 100)):\n    \"\"\"\n    Generates a DataFrame with random integer values within a specified range for categories 'A' through 'E'.\n    Visualizes this data with a stacked bar chart.\n    \"\"\"\n    # Generate DataFrame with random integer values\n    data = {chr(65 + i): [randint(*rand_range) for _ in range(num_rows)] for i in range(5)}\n    df = pd.DataFrame(data)\n\n    # Plot stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n\n    ax.set_xlabel('Categories')\n    ax.set_ylabel('Values')\n    ax.set_title('Stacked Bar Chart')\n\n    return fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef task_func(num_rows=5, rand_range=(0, 100)):\n    \"\"\"\n    Generates a DataFrame with random integer values within a specified range for categories 'A' through 'E'.\n    Visualizes this data with a stacked bar chart.\n    \"\"\"\n    # Generate DataFrame with random integer values\n    data = {chr(65 + i): [randint(*rand_range) for _ in range(num_rows)] for i in range(5)}\n    df = pd.DataFrame(data)\n\n    # Plot stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n\n    ax.set_xlabel('Categories')\n    ax.set_ylabel('Values')\n    ax.set_title('Stacked Bar Chart')\n\n    return fig"}
{"task_id": "BigCodeBench/166", "solution": "import pandas as pd\nfrom datetime import datetime\nimport holidays\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    if not isinstance(start_date, datetime) or start_date > end_date:\n        raise ValueError(\"start_date must be a datetime object and must be before end_date\")\n    if not isinstance(end_date, datetime) or end_date < start_date:\n        raise ValueError(\"end_date must be a datetime object and must be after start_date\")\n\n    country_holidays = holidays.CountryHoliday(country)\n    business_days = pd.bdate_range(start_date, end_date)\n    business_days = business_days[~business_days.isin(country_holidays)]\n\n    return business_days.tolist()", "raw_solution": "import pandas as pd\nfrom datetime import datetime\nimport holidays\n\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    if not isinstance(start_date, datetime) or start_date > end_date:\n        raise ValueError(\"start_date must be a datetime object and must be before end_date\")\n    if not isinstance(end_date, datetime) or end_date < start_date:\n        raise ValueError(\"end_date must be a datetime object and must be after start_date\")\n\n    country_holidays = holidays.CountryHoliday(country)\n    business_days = pd.bdate_range(start_date, end_date)\n    business_days = business_days[~business_days.isin(country_holidays)]\n\n    return business_days.tolist()"}
{"task_id": "BigCodeBench/167", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(num_types=5, integer_range=(0, 100)):\n    # Generate a DataFrame with random integer values\n    data = {f'Category_{i}': [randint(*integer_range) for _ in range(num_types)] for i in range(1, num_types+1)}\n    df = pd.DataFrame(data)\n\n    # Create a horizontal stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Categories')\n    ax.set_title('Horizontal Stacked Bar Chart')\n\n    return (fig, ax)", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef task_func(num_types=5, integer_range=(0, 100)):\n    # Generate a DataFrame with random integer values\n    data = {f'Category_{i}': [randint(*integer_range) for _ in range(num_types)] for i in range(1, num_types+1)}\n    df = pd.DataFrame(data)\n\n    # Create a horizontal stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Categories')\n    ax.set_title('Horizontal Stacked Bar Chart')\n\n    return (fig, ax)"}
{"task_id": "BigCodeBench/168", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(num_groups=5, data_size=5, labels=None):\n    if labels is None:\n        labels = ['Group {}'.format(i) for i in range(1, num_groups + 1)]\n\n    # Generate random data\n    np.random.seed(0)\n    data = np.random.randint(0, 100, (data_size, num_groups))\n    df = pd.DataFrame(data, columns=labels)\n\n    # Create stacked bar chart\n    fig, ax = plt.subplots()\n    ax.bar(labels, df.loc[0], label='Group 1')\n    for i in range(1, num_groups):\n        ax.bar(labels, df.loc[i], bottom=df.loc[:i-1].sum(axis=1), label='Group {}'.format(i + 1))\n\n    ax.legend()\n    ax.set_ylabel('Values')\n    ax.set_title('Stacked Bar Chart')\n\n    # Save the plot to a file\n    filename = 'test_plot.png'\n    plt.savefig(filename)\n\n    return (fig, df, filename)", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(num_groups=5, data_size=5, labels=None):\n    if labels is None:\n        labels = ['Group {}'.format(i) for i in range(1, num_groups + 1)]\n\n    # Generate random data\n    np.random.seed(0)\n    data = np.random.randint(0, 100, (data_size, num_groups))\n    df = pd.DataFrame(data, columns=labels)\n\n    # Create stacked bar chart\n    fig, ax = plt.subplots()\n    ax.bar(labels, df.loc[0], label='Group 1')\n    for i in range(1, num_groups):\n        ax.bar(labels, df.loc[i], bottom=df.loc[:i-1].sum(axis=1), label='Group {}'.format(i + 1))\n\n    ax.legend()\n    ax.set_ylabel('Values')\n    ax.set_title('Stacked Bar Chart')\n\n    # Save the plot to a file\n    filename = 'test_plot.png'\n    plt.savefig(filename)\n\n    return (fig, df, filename)"}
{"task_id": "BigCodeBench/169", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\ndef task_func(image, sigma=2):\n    if not isinstance(image, np.ndarray):\n        raise TypeError(\"Input image must be a numpy array.\")\n    if sigma <= 0:\n        raise ValueError(\"Sigma must be a positive number.\")\n\n    filtered_image = gaussian_filter(image, sigma=sigma)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].imshow(image, cmap='gray')\n    ax[0].set_title('Original')\n    ax[1].imshow(filtered_image, cmap='gray')\n    ax[1].set_title('Filtered')\n    plt.tight_layout()\n\n    return ax, filtered_image", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\n\ndef task_func(image, sigma=2):\n    if not isinstance(image, np.ndarray):\n        raise TypeError(\"Input image must be a numpy array.\")\n    if sigma <= 0:\n        raise ValueError(\"Sigma must be a positive number.\")\n\n    filtered_image = gaussian_filter(image, sigma=sigma)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].imshow(image, cmap='gray')\n    ax[0].set_title('Original')\n    ax[1].imshow(filtered_image, cmap='gray')\n    ax[1].set_title('Filtered')\n    plt.tight_layout()\n\n    return ax, filtered_image"}
{"task_id": "BigCodeBench/170", "solution": "import pandas as pd\nimport requests\nfrom io import StringIO\ndef task_func(csv_url, sort_by_column=\"title\"):\n    \"\"\"\n    Fetches data from a given CSV URL and returns a pandas DataFrame sorted based on the specified column.\n    \"\"\"\n    response = requests.get(csv_url)\n\n    if response.status_code != 200:\n        raise Exception(f\"Response status code is not 200. Code: {response.status_code}\")\n\n    data = response.text\n    df = pd.read_csv(StringIO(data))\n    df = df.sort_values(by=[sort_by_column])\n\n    return df", "raw_solution": "import pandas as pd\nimport requests\nfrom io import StringIO\n\ndef task_func(csv_url, sort_by_column=\"title\"):\n    \"\"\"\n    Fetches data from a given CSV URL and returns a pandas DataFrame sorted based on the specified column.\n    \"\"\"\n    response = requests.get(csv_url)\n\n    if response.status_code != 200:\n        raise Exception(f\"Response status code is not 200. Code: {response.status_code}\")\n\n    data = response.text\n    df = pd.read_csv(StringIO(data))\n    df = df.sort_values(by=[sort_by_column])\n\n    return df"}
{"task_id": "BigCodeBench/171", "solution": "import random\nimport pandas as pd\nfrom collections import Counter\ndef task_func(vegetable_dict, seed=0):\n    random.seed(seed)\n    # Reverse the dictionary to map vegetables to people\n    reversed_dict = {v: k for k, vs in vegetable_dict.items() for v in vs}\n    # Assign random counts to these vegetables\n    for vegetable in reversed_dict:\n        reversed_dict[vegetable] = random.randint(1, 10)\n    # Calculate the total counts\n    total_counts = sum(reversed_dict.values())\n    # Calculate the occurrences of each vegetable as a percentage of the total counts\n    percentage_dict = {vegetable: (count / total_counts) * 100 for vegetable, count in reversed_dict.items()}\n    # Create a DataFrame\n    df = pd.DataFrame(list(percentage_dict.items()), columns=['Vegetable', 'Percentage'])\n    return df\nvegetable_dict = {\n    'Alice': ['Carrot', 'Potato'],\n    'Bob': ['Tomato', 'Cabbage'],\n    'Charlie': ['Spinach', 'Potato']\n}", "raw_solution": "import random\nimport pandas as pd\nfrom collections import Counter\n\n# Constants\nVEGETABLES = ['Carrot', 'Potato', 'Tomato', 'Cabbage', 'Spinach']\n\ndef task_func(vegetable_dict, seed=0):\n    random.seed(seed)\n    # Reverse the dictionary to map vegetables to people\n    reversed_dict = {v: k for k, vs in vegetable_dict.items() for v in vs}\n    # Assign random counts to these vegetables\n    for vegetable in reversed_dict:\n        reversed_dict[vegetable] = random.randint(1, 10)\n    # Calculate the total counts\n    total_counts = sum(reversed_dict.values())\n    # Calculate the occurrences of each vegetable as a percentage of the total counts\n    percentage_dict = {vegetable: (count / total_counts) * 100 for vegetable, count in reversed_dict.items()}\n    # Create a DataFrame\n    df = pd.DataFrame(list(percentage_dict.items()), columns=['Vegetable', 'Percentage'])\n    return df\n\n# Test the function\nvegetable_dict = {\n    'Alice': ['Carrot', 'Potato'],\n    'Bob': ['Tomato', 'Cabbage'],\n    'Charlie': ['Spinach', 'Potato']\n}"}
{"task_id": "BigCodeBench/172", "solution": "import json\nfrom datetime import datetime\ndef task_func(json_data):\n    \"\"\"\n    Determines if the given datetime is a weekend.\n    \"\"\"\n    # Parse the JSON data\n    data = json.loads(json_data)\n\n    # Extract the datetime from the JSON data\n    utc_datetime_str = data['utc_datetime']\n    utc_datetime = datetime.strptime(utc_datetime_str, '%Y-%m-%d %H:%M:%S')\n\n    # Determine if the date is a weekend\n    return utc_datetime.weekday() > 4\njson_data = '{\"utc_datetime\": \"2022-01-01 12:00:00\"}'", "raw_solution": "import json\nfrom datetime import datetime\n\ndef task_func(json_data):\n    \"\"\"\n    Determines if the given datetime is a weekend.\n    \"\"\"\n    # Parse the JSON data\n    data = json.loads(json_data)\n\n    # Extract the datetime from the JSON data\n    utc_datetime_str = data['utc_datetime']\n    utc_datetime = datetime.strptime(utc_datetime_str, '%Y-%m-%d %H:%M:%S')\n\n    # Determine if the date is a weekend\n    return utc_datetime.weekday() > 4\n\n# Test the function\njson_data = '{\"utc_datetime\": \"2022-01-01 12:00:00\"}'"}
{"task_id": "BigCodeBench/173", "solution": "import numpy as np\nimport pandas as pd", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef generate_gdp_data(country_dict):\n    \"\"\"\n    Generates a DataFrame representing the GDP for a predefined set of countries based on their presence in the provided dictionary.\n    The GDP values are simulated with random integers to model economic data.\n\n    Args:\n    country_dict (dict): A dictionary where the keys are country names and the values are their GDP values.\n\n    Returns:\n    pandas.DataFrame: A DataFrame with each country's name from the input as the index and a randomly generated GDP\n    value as the column. GDP values range between 1,000,000,000 and 100,000,000,000.\n    \"\"\"\n    gdp_values = np.random.randint(1000000000, 100000000000, len(country_dict))\n    df = pd.DataFrame(list(country_dict.keys()), columns=['Country'])\n    df['GDP'] = gdp_values\n    df.set_index('Country', inplace=True)\n    return df\n\n# Test the function\ncountry_dict = {'USA': 1, 'China': 2, 'Japan': 3, 'Germany': 4, 'UK': 5}"}
{"task_id": "BigCodeBench/174", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data, key, min_value, max_value):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n\n    data[key] = np.random.randint(min_value, max_value, data.shape[0])\n    return data\ndata = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\nkey = 'C'\nmin_value = 10\nmax_value = 20", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data, key, min_value, max_value):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n\n    data[key] = np.random.randint(min_value, max_value, data.shape[0])\n    return data\n\n# Test the function\ndata = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\nkey = 'C'\nmin_value = 10\nmax_value = 20\n\ntry:\n    updated_data = task_func(data, key, min_value, max_value)\n    print(updated_data)\nexcept ValueError as e:\n    print(e)"}
{"task_id": "BigCodeBench/175", "solution": "import re\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df):\n    if 'Title' not in df.columns or 'Views' not in df.columns or 'Likes' not in df.columns:\n        return plt.subplot(111)\n\n    df = df[df['Title'].str.contains(r'(how|what)', flags=re.IGNORECASE)]\n\n    if df.empty:\n        return plt.subplot(111)\n\n    df['Like Ratio'] = df['Likes'] / df['Views']\n\n    ax = df.plot(x='Title', y='Like Ratio', kind='bar', legend=False)\n    ax.set_xlabel('Video Title')\n    ax.set_ylabel('Like Ratio')\n    ax.set_title('Like Ratio of Videos with \"How\" or \"What\" in Title')\n\n    return ax", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df):\n    if 'Title' not in df.columns or 'Views' not in df.columns or 'Likes' not in df.columns:\n        return plt.subplot(111)\n\n    df = df[df['Title'].str.contains(r'(how|what)', flags=re.IGNORECASE)]\n\n    if df.empty:\n        return plt.subplot(111)\n\n    df['Like Ratio'] = df['Likes'] / df['Views']\n\n    ax = df.plot(x='Title', y='Like Ratio', kind='bar', legend=False)\n    ax.set_xlabel('Video Title')\n    ax.set_ylabel('Like Ratio')\n    ax.set_title('Like Ratio of Videos with \"How\" or \"What\" in Title')\n\n    return ax"}
{"task_id": "BigCodeBench/176", "solution": "import re\nimport socket\ndef task_func(ip_addresses: list) -> dict:\n    \"\"\"\n    Given a list of IP addresses, this function returns a dictionary mapping each valid IP address to its respective hostname. If the hostname cannot be determined, the value will be None.\n    \"\"\"\n    result = {}\n    for ip in ip_addresses:\n        try:\n            hostname = socket.gethostbyaddr(ip)[0]\n            result[ip] = hostname\n        except socket.herror:\n            result[ip] = None\n    return result\nip_addresses = ['127.0.0.1', '8.8.8.8', 'invalid_ip']", "raw_solution": "import re\nimport socket\n\ndef task_func(ip_addresses: list) -> dict:\n    \"\"\"\n    Given a list of IP addresses, this function returns a dictionary mapping each valid IP address to its respective hostname. If the hostname cannot be determined, the value will be None.\n    \"\"\"\n    result = {}\n    for ip in ip_addresses:\n        try:\n            hostname = socket.gethostbyaddr(ip)[0]\n            result[ip] = hostname\n        except socket.herror:\n            result[ip] = None\n    return result\n\n# Test the function\nip_addresses = ['127.0.0.1', '8.8.8.8', 'invalid_ip']"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\ndef task_func(df):\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"The DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    df = df[df['Title'].str.contains('like|what', case=False)]\n\n    content = df['Content'].str.replace('[{}]'.format(punctuation), '').str.lower()\n    words = nltk.word_tokenize(content)\n\n    word_freq = Counter(words)\n\n    return word_freq", "raw_solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\ndef task_func(df):\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"The DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    df = df[df['Title'].str.contains('like|what', case=False)]\n\n    content = df['Content'].str.replace('[{}]'.format(punctuation), '').str.lower()\n    words = nltk.word_tokenize(content)\n\n    word_freq = Counter(words)\n\n    return word_freq"}
{"task_id": "BigCodeBench/178", "solution": "import re\nimport json\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\ndef task_func(ip_address):\n    \"\"\"\n    Checks if the provided IP address is valid. If the IP address is not valid, the function will return 'Invalid IP address received'.\n    \"\"\"\n    if re.match(IP_REGEX, ip_address):\n        return ip_address\n    else:\n        return 'Invalid IP address received'", "raw_solution": "import re\nimport json\n\n# Constants\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef task_func(ip_address):\n    \"\"\"\n    Checks if the provided IP address is valid. If the IP address is not valid, the function will return 'Invalid IP address received'.\n    \"\"\"\n    if re.match(IP_REGEX, ip_address):\n        return ip_address\n    else:\n        return 'Invalid IP address received'\n\n# Test cases\nassert task_func('192.168.1.1') == '192.168.1.1'\nassert task_func('255.255.255.255') == '255.255.255.255'\nassert task_func('Not an IP address') == 'Invalid IP address received'\nassert task_func('192.168.1.256') == 'Invalid IP address received'\nassert task_func('192.168.1.255.1') == 'Invalid IP address received'"}
{"task_id": "BigCodeBench/179", "solution": "import re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\ndef task_func(df):\n    if 'Title' not in df.columns or 'Content' not in df.columns:\n        return plt.figure()\n\n    df = df[df['Title'].str.contains('how|what', case=False)]\n\n    if df.empty:\n        return plt.figure()\n\n    vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(df['Content'])\n\n    feature_names = vectorizer.get_feature_names_out()\n    tfidf_scores = tfidf_matrix.toarray().sum(axis=0)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(feature_names, tfidf_scores)\n    ax.set_ylabel('TF-IDF Score')\n    ax.set_xticklabels(feature_names, rotation=90)\n\n    return fig", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\ndef task_func(df):\n    if 'Title' not in df.columns or 'Content' not in df.columns:\n        return plt.figure()\n\n    df = df[df['Title'].str.contains('how|what', case=False)]\n\n    if df.empty:\n        return plt.figure()\n\n    vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(df['Content'])\n\n    feature_names = vectorizer.get_feature_names_out()\n    tfidf_scores = tfidf_matrix.toarray().sum(axis=0)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(feature_names, tfidf_scores)\n    ax.set_ylabel('TF-IDF Score')\n    ax.set_xticklabels(feature_names, rotation=90)\n\n    return fig"}
{"task_id": "BigCodeBench/180", "solution": "from PIL import Image\nimport numpy as np\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(img_path, scale_factors=[0.5, 0.75, 1.5, 2.0]):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"Image file {img_path} not found\")\n\n    img = Image.open(img_path)\n    img_array = np.array(img)\n\n    result = []\n    for scale in scale_factors:\n        scaled_img_array = resize(img_array, (int(img_array.shape[0] * scale), int(img_array.shape[1] * scale)))\n        fig, ax = plt.subplots(1, 1)\n        ax.imshow(scaled_img_array)\n        ax.set_title(f\"Scaled Image by {scale}\")\n        plt.show()\n        result.append((ax, scaled_img_array))\n\n    return result", "raw_solution": "from PIL import Image\nimport numpy as np\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(img_path, scale_factors=[0.5, 0.75, 1.5, 2.0]):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"Image file {img_path} not found\")\n\n    img = Image.open(img_path)\n    img_array = np.array(img)\n\n    result = []\n    for scale in scale_factors:\n        scaled_img_array = resize(img_array, (int(img_array.shape[0] * scale), int(img_array.shape[1] * scale)))\n        fig, ax = plt.subplots(1, 1)\n        ax.imshow(scaled_img_array)\n        ax.set_title(f\"Scaled Image by {scale}\")\n        plt.show()\n        result.append((ax, scaled_img_array))\n\n    return result"}
{"task_id": "BigCodeBench/181", "solution": "from django.http import HttpResponse\nfrom django.conf import settings\nimport random\nimport time\nimport json\ndef task_func(data, min_delay, max_delay):\n    delay = random.randint(min_delay, max_delay)\n    time.sleep(delay)\n    json_data = json.dumps(data)\n    return HttpResponse(json_data, content_type='application/json')", "raw_solution": "from django.http import HttpResponse\nfrom django.conf import settings\nimport random\nimport time\nimport json\n\ndef task_func(data, min_delay, max_delay):\n    delay = random.randint(min_delay, max_delay)\n    time.sleep(delay)\n    json_data = json.dumps(data)\n    return HttpResponse(json_data, content_type='application/json')"}
{"task_id": "BigCodeBench/182", "solution": "import re\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef task_func(df):\n    \"\"\"\n    Analyzes articles by their titles for specific case-insensitive keywords (\"how\" or \"what\"),\n    vectorizes the content using CountVectorizer, and groups them into clusters using KMeans clustering.\n    This function is intended for basic content analysis and clustering to understand common themes or topics\n    among articles asking questions starting with \"how\" or \"what\".\n    The function should output with:\n        list: List of cluster labels for the filtered articles, indicating the cluster to which each article belongs.\n    \"\"\"\n    # Filter articles by their titles for specific case-insensitive keywords (\"how\" or \"what\")\n    df = df[df['title'].str.lower().str.contains('how') | df['title'].str.lower().str.contains('what')]\n\n    # Vectorize the content using CountVectorizer\n    vectorizer = CountVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(df['content'])\n\n    # Group them into clusters using KMeans clustering\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(X)\n\n    # Return the cluster labels for the filtered articles\n    return kmeans.labels_.tolist()", "raw_solution": "import re\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef task_func(df):\n    \"\"\"\n    Analyzes articles by their titles for specific case-insensitive keywords (\"how\" or \"what\"),\n    vectorizes the content using CountVectorizer, and groups them into clusters using KMeans clustering.\n    This function is intended for basic content analysis and clustering to understand common themes or topics\n    among articles asking questions starting with \"how\" or \"what\".\n    The function should output with:\n        list: List of cluster labels for the filtered articles, indicating the cluster to which each article belongs.\n    \"\"\"\n    # Filter articles by their titles for specific case-insensitive keywords (\"how\" or \"what\")\n    df = df[df['title'].str.lower().str.contains('how') | df['title'].str.lower().str.contains('what')]\n\n    # Vectorize the content using CountVectorizer\n    vectorizer = CountVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(df['content'])\n\n    # Group them into clusters using KMeans clustering\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(X)\n\n    # Return the cluster labels for the filtered articles\n    return kmeans.labels_.tolist()"}
{"task_id": "BigCodeBench/183", "solution": "from django.http import HttpResponse\nimport uuid\nimport json\ndef task_func(data):\n    \"\"\"\n    Creates a Django HttpResponse with JSON data and a UUID in the HTTP headers.\n    \"\"\"\n    # Generate a UUID\n    request_id = uuid.uuid4()\n\n    # Convert the data to JSON\n    json_data = json.dumps(data)\n\n    # Create the HttpResponse with JSON data\n    response = HttpResponse(json_data, content_type='application/json')\n\n    # Add the UUID to the HTTP headers\n    response['X-Request-ID'] = str(request_id)\n\n    return response", "raw_solution": "from django.http import HttpResponse\nimport uuid\nimport json\n\ndef task_func(data):\n    \"\"\"\n    Creates a Django HttpResponse with JSON data and a UUID in the HTTP headers.\n    \"\"\"\n    # Generate a UUID\n    request_id = uuid.uuid4()\n\n    # Convert the data to JSON\n    json_data = json.dumps(data)\n\n    # Create the HttpResponse with JSON data\n    response = HttpResponse(json_data, content_type='application/json')\n\n    # Add the UUID to the HTTP headers\n    response['X-Request-ID'] = str(request_id)\n\n    return response"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    # Remove stopwords, numbers, and punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join(\n        [word for word in x.split() if word not in STOPWORDS and not word.isdigit() and word.isalpha()]))\n\n    # Apply vectorization\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert to DataFrame\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    # Remove stopwords, numbers, and punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join(\n        [word for word in x.split() if word not in STOPWORDS and not word.isdigit() and word.isalpha()]))\n\n    # Apply vectorization\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert to DataFrame\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n\n    return df"}
{"task_id": "BigCodeBench/185", "solution": "import pandas as pd\nimport numpy as np\nimport folium\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not isinstance(dic, dict):\n        raise TypeError(\"Input should be a dictionary\")\n    if not isinstance(cities, list):\n        raise TypeError(\"Cities should be a list\")\n    if not all(isinstance(city, str) for city in cities):\n        raise TypeError(\"All cities should be strings\")\n    if not all(key in dic for key in ['Lon', 'Lat']):\n        raise ValueError(\"'Lon' and 'Lat' keys are missing in the dictionary\")\n    if not all(isinstance(dic[key], tuple) and len(dic[key]) == 2 for key in ['Lon', 'Lat']):\n        raise ValueError(\"Values of 'Lon' and 'Lat' should be tuples with 2 elements\")\n\n    lon_range, lat_range = dic['Lon'], dic['Lat']\n    df = pd.DataFrame(cities, columns=['City'])\n    df['Longitude'] = np.random.uniform(lon_range[0], lon_range[1], len(cities))\n    df['Latitude'] = np.random.uniform(lat_range[0], lat_range[1], len(cities))\n\n    m = folium.Map(location=[(df['Latitude'][0] + df['Latitude'][1]) / 2, (df['Longitude'][0] + df['Longitude'][1]) / 2], zoom_start=2)\n    for i in range(0, len(df)):\n        folium.Marker([df.iloc[i]['Latitude'], df.iloc[i]['Longitude']], popup=df.iloc[i]['City']).add_to(m)\n\n    return (m, df)", "raw_solution": "import pandas as pd\nimport numpy as np\nimport folium\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not isinstance(dic, dict):\n        raise TypeError(\"Input should be a dictionary\")\n    if not isinstance(cities, list):\n        raise TypeError(\"Cities should be a list\")\n    if not all(isinstance(city, str) for city in cities):\n        raise TypeError(\"All cities should be strings\")\n    if not all(key in dic for key in ['Lon', 'Lat']):\n        raise ValueError(\"'Lon' and 'Lat' keys are missing in the dictionary\")\n    if not all(isinstance(dic[key], tuple) and len(dic[key]) == 2 for key in ['Lon', 'Lat']):\n        raise ValueError(\"Values of 'Lon' and 'Lat' should be tuples with 2 elements\")\n\n    lon_range, lat_range = dic['Lon'], dic['Lat']\n    df = pd.DataFrame(cities, columns=['City'])\n    df['Longitude'] = np.random.uniform(lon_range[0], lon_range[1], len(cities))\n    df['Latitude'] = np.random.uniform(lat_range[0], lat_range[1], len(cities))\n\n    m = folium.Map(location=[(df['Latitude'][0] + df['Latitude'][1]) / 2, (df['Longitude'][0] + df['Longitude'][1]) / 2], zoom_start=2)\n    for i in range(0, len(df)):\n        folium.Marker([df.iloc[i]['Latitude'], df.iloc[i]['Longitude']], popup=df.iloc[i]['City']).add_to(m)\n\n    return (m, df)"}
{"task_id": "BigCodeBench/186", "solution": "from geopy.distance import geodesic\nimport folium\ndef task_func(dic):\n    if not dic:\n        raise ValueError(\"Input dictionary is empty\")\n\n    locations = list(dic.keys())\n    distances = {}\n\n    # Create a Folium map\n    m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)\n\n    # Add markers to the map\n    for location in locations:\n        folium.Marker(location=location, popup=location).add_to(m)\n\n    # Calculate the geodesic distances between each pair of locations\n    for i in range(len(locations)):\n        for j in range(i + 1, len(locations)):\n            dist = geodesic(locations[i], locations[j]).km\n            distances[f\"{locations[i]}-{locations[j]}\"] = dist\n\n    return (m, distances)", "raw_solution": "from geopy.distance import geodesic\nimport folium\n\ndef task_func(dic):\n    if not dic:\n        raise ValueError(\"Input dictionary is empty\")\n\n    locations = list(dic.keys())\n    distances = {}\n\n    # Create a Folium map\n    m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)\n\n    # Add markers to the map\n    for location in locations:\n        folium.Marker(location=location, popup=location).add_to(m)\n\n    # Calculate the geodesic distances between each pair of locations\n    for i in range(len(locations)):\n        for j in range(i + 1, len(locations)):\n            dist = geodesic(locations[i], locations[j]).km\n            distances[f\"{locations[i]}-{locations[j]}\"] = dist\n\n    return (m, distances)"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not all(key in dic for key in ('Lon', 'Lat')):\n        raise ValueError(\"'Lon' and 'Lat' keys are required in the dictionary\")\n    if not all(isinstance(dic[key], tuple) and len(dic[key]) == 2 for key in ('Lon', 'Lat')):\n        raise ValueError(\"'Lon' and 'Lat' values should be tuples with 2 elements\")\n\n    lon_range, lat_range = dic['Lon'], dic['Lat']\n    coordinates = [(np.random.uniform(lon_range[0], lon_range[1]), np.random.uniform(lat_range[0], lat_range[1])) for _ in cities]\n    points = [Point(x, y) for x, y in coordinates]\n    df = gpd.GeoDataFrame(cities, geometry=points, crs='epsg:4326')\n    df.rename(columns={0: 'City', 1: 'Coordinates'}, inplace=True)\n    return df", "raw_solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not all(key in dic for key in ('Lon', 'Lat')):\n        raise ValueError(\"'Lon' and 'Lat' keys are required in the dictionary\")\n    if not all(isinstance(dic[key], tuple) and len(dic[key]) == 2 for key in ('Lon', 'Lat')):\n        raise ValueError(\"'Lon' and 'Lat' values should be tuples with 2 elements\")\n\n    lon_range, lat_range = dic['Lon'], dic['Lat']\n    coordinates = [(np.random.uniform(lon_range[0], lon_range[1]), np.random.uniform(lat_range[0], lat_range[1])) for _ in cities]\n    points = [Point(x, y) for x, y in coordinates]\n    df = gpd.GeoDataFrame(cities, geometry=points, crs='epsg:4326')\n    df.rename(columns={0: 'City', 1: 'Coordinates'}, inplace=True)\n    return df"}
{"task_id": "BigCodeBench/188", "solution": "import pandas as pd\nimport folium\nfrom geopy.geocoders import Photon\ndef task_func(dic):\n    \"\"\"\n    Generates a Folium map with markers for specified locations. It preprocesses the input to handle both direct geographical coordinates and address strings. For address strings, it dynamically resolves their latitude and longitude using the Photon geolocation service. This flexible input handling allows for easy mapping of various location types.\n    \"\"\"\n    geolocator = Photon(user_agent=\"geoapiExercises\")\n\n    # Create a DataFrame from the input dictionary\n    df = pd.DataFrame(dic)\n\n    # Initialize an empty list to store the coordinates\n    coordinates = []\n\n    # Iterate over each row in the DataFrame\n    for index, row in df.iterrows():\n        location = row['location']\n\n        # If the location is a string, resolve its latitude and longitude using Photon\n        if isinstance(location, str):\n            location = geolocator.geocode(location)\n            coordinates.append((location.latitude, location.longitude))\n        else:\n            coordinates.append(location)\n\n    # Create a Folium map\n    m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)\n\n    # Add markers to the map for each location\n    for coord in coordinates:\n        folium.Marker(location=coord).add_to(m)\n\n    return m", "raw_solution": "import pandas as pd\nimport folium\nfrom geopy.geocoders import Photon\n\ndef task_func(dic):\n    \"\"\"\n    Generates a Folium map with markers for specified locations. It preprocesses the input to handle both direct geographical coordinates and address strings. For address strings, it dynamically resolves their latitude and longitude using the Photon geolocation service. This flexible input handling allows for easy mapping of various location types.\n    \"\"\"\n    geolocator = Photon(user_agent=\"geoapiExercises\")\n\n    # Create a DataFrame from the input dictionary\n    df = pd.DataFrame(dic)\n\n    # Initialize an empty list to store the coordinates\n    coordinates = []\n\n    # Iterate over each row in the DataFrame\n    for index, row in df.iterrows():\n        location = row['location']\n\n        # If the location is a string, resolve its latitude and longitude using Photon\n        if isinstance(location, str):\n            location = geolocator.geocode(location)\n            coordinates.append((location.latitude, location.longitude))\n        else:\n            coordinates.append(location)\n\n    # Create a Folium map\n    m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)\n\n    # Add markers to the map for each location\n    for coord in coordinates:\n        folium.Marker(location=coord).add_to(m)\n\n    return m"}
{"task_id": "BigCodeBench/189", "solution": "import re\nimport json\nimport requests\ndef task_func(data_url: str) -> list:\n    \"\"\"\n    Fetches data from a specific URL and extracts all names from the JSON-formatted data that are not enclosed by square brackets.\n    If any exception is raised during the request, the function returns \"Invalid url input\".\n    \"\"\"\n    try:\n        response = requests.get(data_url)\n        data = json.loads(response.text)\n        names = re.findall(r'\"name\":\\s*\"([^\"]+)\"', json.dumps(data))\n        return names\n    except Exception:\n        return \"Invalid url input\"", "raw_solution": "import re\nimport json\nimport requests\n\ndef task_func(data_url: str) -> list:\n    \"\"\"\n    Fetches data from a specific URL and extracts all names from the JSON-formatted data that are not enclosed by square brackets.\n    If any exception is raised during the request, the function returns \"Invalid url input\".\n    \"\"\"\n    try:\n        response = requests.get(data_url)\n        data = json.loads(response.text)\n        names = re.findall(r'\"name\":\\s*\"([^\"]+)\"', json.dumps(data))\n        return names\n    except Exception:\n        return \"Invalid url input\""}
{"task_id": "BigCodeBench/190", "solution": "import sqlite3\nimport pandas as pd\nimport csv\nfrom io import StringIO\nDATABASE_NAME = 'test.db'\nTABLE_NAME = 'test_table'\ndef task_func(csv_input):\n    # Connect to SQLite database\n    conn = sqlite3.connect(DATABASE_NAME)\n    cursor = conn.cursor()\n\n    # Create table if it doesn't exist\n    cursor.execute(f'''\n        CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n            id INTEGER PRIMARY KEY,\n            name TEXT,\n            age INTEGER,\n            occupation TEXT\n        )\n    ''')\n\n    # If input is a file path, read the CSV file\n    if isinstance(csv_input, str):\n        with open(csv_input, 'r') as file:\n            csv_data = csv.reader(file)\n            next(csv_data)  # Skip header\n            cursor.executemany(f'INSERT INTO {TABLE_NAME} (name, age, occupation) VALUES (?, ?, ?)', csv_data)\n    # If input is a string, read the CSV string\n    elif isinstance(csv_input, str):\n        csv_data = csv.reader(StringIO(csv_input))\n        next(csv_data)  # Skip header\n        cursor.executemany(f'INSERT INTO {TABLE_NAME} (name, age, occupation) VALUES (?, ?, ?)', csv_data)\n\n    # Commit changes and close connection\n    conn.commit()\n    conn.close()\n\n    # Query the database and return the data as a pandas DataFrame\n    conn = sqlite3.connect(DATABASE_NAME)\n    df = pd.read_sql_query(f'SELECT * FROM {TABLE_NAME}', conn)\n    conn.close()\n\n    return df", "raw_solution": "import sqlite3\nimport pandas as pd\nimport csv\nfrom io import StringIO\n\n# Constants\nDATABASE_NAME = 'test.db'\nTABLE_NAME = 'test_table'\n\ndef task_func(csv_input):\n    # Connect to SQLite database\n    conn = sqlite3.connect(DATABASE_NAME)\n    cursor = conn.cursor()\n\n    # Create table if it doesn't exist\n    cursor.execute(f'''\n        CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n            id INTEGER PRIMARY KEY,\n            name TEXT,\n            age INTEGER,\n            occupation TEXT\n        )\n    ''')\n\n    # If input is a file path, read the CSV file\n    if isinstance(csv_input, str):\n        with open(csv_input, 'r') as file:\n            csv_data = csv.reader(file)\n            next(csv_data)  # Skip header\n            cursor.executemany(f'INSERT INTO {TABLE_NAME} (name, age, occupation) VALUES (?, ?, ?)', csv_data)\n    # If input is a string, read the CSV string\n    elif isinstance(csv_input, str):\n        csv_data = csv.reader(StringIO(csv_input))\n        next(csv_data)  # Skip header\n        cursor.executemany(f'INSERT INTO {TABLE_NAME} (name, age, occupation) VALUES (?, ?, ?)', csv_data)\n\n    # Commit changes and close connection\n    conn.commit()\n    conn.close()\n\n    # Query the database and return the data as a pandas DataFrame\n    conn = sqlite3.connect(DATABASE_NAME)\n    df = pd.read_sql_query(f'SELECT * FROM {TABLE_NAME}', conn)\n    conn.close()\n\n    return df"}
{"task_id": "BigCodeBench/191", "solution": "import random\nfrom scipy import stats\ndef task_func(animals, mean):\n    # Generate a Poisson-distributed number of customers\n    num_customers = int(stats.poisson.rvs(mean))\n\n    # Initialize a dictionary to store the sales\n    sales = {animal: 0 for animal in animals}\n\n    # Simulate each customer's purchase\n    for _ in range(num_customers):\n        # Randomly select an animal for the customer to buy\n        animal = random.choice(animals)\n\n        # Increment the count for the selected animal in the sales dictionary\n        sales[animal] += 1\n\n    return sales", "raw_solution": "import random\nfrom scipy import stats\n\ndef task_func(animals, mean):\n    # Generate a Poisson-distributed number of customers\n    num_customers = int(stats.poisson.rvs(mean))\n\n    # Initialize a dictionary to store the sales\n    sales = {animal: 0 for animal in animals}\n\n    # Simulate each customer's purchase\n    for _ in range(num_customers):\n        # Randomly select an animal for the customer to buy\n        animal = random.choice(animals)\n\n        # Increment the count for the selected animal in the sales dictionary\n        sales[animal] += 1\n\n    return sales"}
{"task_id": "BigCodeBench/192", "solution": "import re\nimport smtplib\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\nRECEPIENT_ADDRESS = \"names@gmail.com\"\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\ndef task_func(text=TEXT, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, recepient_address=RECEPIENT_ADDRESS, smtp=None):\n    # Extract names\n    names = re.findall(r'\\[.*?\\]|(\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b)', text)\n    names = [name for name in names if name]  # Remove empty strings\n\n    # Send email\n    if smtp is None:\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        smtp.starttls()\n        smtp.login(email_address, email_password)\n\n    subject = \"Extracted Names\"\n    body = \"\\n\".join(names)\n    message = f\"Subject: {subject}\\n\\n{body}\"\n\n    smtp.sendmail(email_address, recepient_address, message)\n    smtp.quit()\n\n    return names", "raw_solution": "import re\nimport smtplib\n\n# Constants\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\nRECEPIENT_ADDRESS = \"names@gmail.com\"\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\n\ndef task_func(text=TEXT, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, recepient_address=RECEPIENT_ADDRESS, smtp=None):\n    # Extract names\n    names = re.findall(r'\\[.*?\\]|(\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b)', text)\n    names = [name for name in names if name]  # Remove empty strings\n\n    # Send email\n    if smtp is None:\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        smtp.starttls()\n        smtp.login(email_address, email_password)\n\n    subject = \"Extracted Names\"\n    body = \"\\n\".join(names)\n    message = f\"Subject: {subject}\\n\\n{body}\"\n\n    smtp.sendmail(email_address, recepient_address, message)\n    smtp.quit()\n\n    return names"}
{"task_id": "BigCodeBench/193", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, randint, random\nDATA_TYPES = [str, int, float, list, tuple, dict, set]\ndef generate_random_data(data_type):\n    if data_type == str:\n        return ''.join(choice('abcdefghijklmnopqrstuvwxyz') for _ in range(5))\n    elif data_type == int:\n        return randint(0, 9)\n    elif data_type == float:\n        return float(randint(0, 9))\n    elif data_type == list:\n        return [randint(0, 9) for _ in range(randint(1, 5))]\n    elif data_type == tuple:\n        return tuple(randint(0, 9) for _ in range(randint(1, 5)))\n    elif data_type == dict:\n        return {str(i): randint(0, 9) for i in range(randint(1, 5))}\n    elif data_type == set:\n        return set(randint(0, 9) for _ in range(randint(1, 5)))\ndef task_func(rows, columns):\n    data = {f'col{i}': [generate_random_data(choice(DATA_TYPES)) for _ in range(rows)] for i in range(columns)}\n    return pd.DataFrame(data)", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, randint, random\n\n# Constants\nDATA_TYPES = [str, int, float, list, tuple, dict, set]\n\ndef generate_random_data(data_type):\n    if data_type == str:\n        return ''.join(choice('abcdefghijklmnopqrstuvwxyz') for _ in range(5))\n    elif data_type == int:\n        return randint(0, 9)\n    elif data_type == float:\n        return float(randint(0, 9))\n    elif data_type == list:\n        return [randint(0, 9) for _ in range(randint(1, 5))]\n    elif data_type == tuple:\n        return tuple(randint(0, 9) for _ in range(randint(1, 5)))\n    elif data_type == dict:\n        return {str(i): randint(0, 9) for i in range(randint(1, 5))}\n    elif data_type == set:\n        return set(randint(0, 9) for _ in range(randint(1, 5)))\n\ndef task_func(rows, columns):\n    data = {f'col{i}': [generate_random_data(choice(DATA_TYPES)) for _ in range(rows)] for i in range(columns)}\n    return pd.DataFrame(data)"}
{"task_id": "BigCodeBench/194", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nBAR_COLOR = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\ndef task_func(data_size):\n    \"\"\"\n    Generates random numeric data and creates a histogram of the data. The color of the histogram bars is randomly selected from a predefined list.\n    The function returns a tuple with:\n        ndarray: The array of randomly generated data.\n        str: The color used for the histogram bars.\n    \"\"\"\n    # Generate random data\n    data = np.random.rand(data_size)\n\n    # Select a random color from the list\n    color = np.random.choice(BAR_COLOR)\n\n    # Create a histogram\n    plt.hist(data, color=color)\n    plt.show()\n\n    return data, color", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nBAR_COLOR = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\n\ndef task_func(data_size):\n    \"\"\"\n    Generates random numeric data and creates a histogram of the data. The color of the histogram bars is randomly selected from a predefined list.\n    The function returns a tuple with:\n        ndarray: The array of randomly generated data.\n        str: The color used for the histogram bars.\n    \"\"\"\n    # Generate random data\n    data = np.random.rand(data_size)\n\n    # Select a random color from the list\n    color = np.random.choice(BAR_COLOR)\n\n    # Create a histogram\n    plt.hist(data, color=color)\n    plt.show()\n\n    return data, color"}
{"task_id": "BigCodeBench/195", "solution": "import subprocess\nimport platform\nimport time\ndef task_func(url):\n    \"\"\"\n    Opens a web page in the default web browser in a background process.\n    The function returns the return code of the subprocess.\n    \"\"\"\n    if platform.system() == \"Windows\":\n        # Windows\n        return subprocess.Popen([\"start\", url], shell=True).wait()\n    elif platform.system() == \"Darwin\":\n        # macOS\n        return subprocess.Popen([\"open\", url], shell=True).wait()\n    else:\n        # Linux\n        return subprocess.Popen([\"xdg-open\", url], shell=True).wait()\nurl = \"https://www.google.com\"", "raw_solution": "import subprocess\nimport platform\nimport time\n\ndef task_func(url):\n    \"\"\"\n    Opens a web page in the default web browser in a background process.\n    The function returns the return code of the subprocess.\n    \"\"\"\n    if platform.system() == \"Windows\":\n        # Windows\n        return subprocess.Popen([\"start\", url], shell=True).wait()\n    elif platform.system() == \"Darwin\":\n        # macOS\n        return subprocess.Popen([\"open\", url], shell=True).wait()\n    else:\n        # Linux\n        return subprocess.Popen([\"xdg-open\", url], shell=True).wait()\n\n# Test the function\nurl = \"https://www.google.com\"\nreturn_code = task_func(url)"}
{"task_id": "BigCodeBench/196", "solution": "import random\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom typing import Tuple, List\ndef task_func(length: int, range_limit: int = 100, seed: int = 0) -> Tuple[List[int], plt.Axes]:\n    \"\"\"\n    Generates a list of random numbers, sorts them, and plots the distribution of the numbers in a histogram.\n    Returns the axes object and the list of random numbers.\n    \"\"\"\n    if range_limit <= 1:\n        raise ValueError(\"range_limit must be greater than 1\")\n\n    random.seed(seed)\n    numbers = [random.randint(0, range_limit) for _ in range(length)]\n    numbers.sort()\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(numbers, bins=range_limit, kde=False)\n    ax.set_title('Histogram of Random Numbers')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return numbers, ax", "raw_solution": "import random\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom typing import Tuple, List\n\ndef task_func(length: int, range_limit: int = 100, seed: int = 0) -> Tuple[List[int], plt.Axes]:\n    \"\"\"\n    Generates a list of random numbers, sorts them, and plots the distribution of the numbers in a histogram.\n    Returns the axes object and the list of random numbers.\n    \"\"\"\n    if range_limit <= 1:\n        raise ValueError(\"range_limit must be greater than 1\")\n\n    random.seed(seed)\n    numbers = [random.randint(0, range_limit) for _ in range(length)]\n    numbers.sort()\n\n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(numbers, bins=range_limit, kde=False)\n    ax.set_title('Histogram of Random Numbers')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return numbers, ax"}
{"task_id": "BigCodeBench/197", "solution": "import heapq\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(l1, l2, N=10):\n    # Calculate the differences and square the differences\n    diffs = [(i, j, (abs(i-j))**2) for i, j in zip(l1, l2)]\n\n    # Use a heap to get the N biggest differences\n    biggest_diffs = heapq.nlargest(N, diffs, key=lambda x: x[2])\n\n    # Extract the square root of the differences\n    sqrt_diffs = [math.sqrt(diff[2]) for diff in biggest_diffs]\n\n    # Plot the differences\n    fig, ax = plt.subplots()\n    ax.plot(range(1, N+1), sqrt_diffs)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Square Root of Difference')\n    ax.set_title('Square Root of N Largest Differences')\n\n    return ax", "raw_solution": "import heapq\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(l1, l2, N=10):\n    # Calculate the differences and square the differences\n    diffs = [(i, j, (abs(i-j))**2) for i, j in zip(l1, l2)]\n\n    # Use a heap to get the N biggest differences\n    biggest_diffs = heapq.nlargest(N, diffs, key=lambda x: x[2])\n\n    # Extract the square root of the differences\n    sqrt_diffs = [math.sqrt(diff[2]) for diff in biggest_diffs]\n\n    # Plot the differences\n    fig, ax = plt.subplots()\n    ax.plot(range(1, N+1), sqrt_diffs)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Square Root of Difference')\n    ax.set_title('Square Root of N Largest Differences')\n\n    return ax"}
{"task_id": "BigCodeBench/198", "solution": "import numpy as np\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(data, value):\n    if not data:\n        return np.array([]), 0\n\n    avg = statistics.mean(data)\n    data_above_avg = [x for x in data if x > avg]\n    count = bisect.bisect_right(data, value)\n\n    plt.hist(data, bins='auto')\n    plt.title(\"Histogram of the sorted numbers\")\n    plt.show()\n\n    return np.array(data_above_avg), count", "raw_solution": "import numpy as np\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef task_func(data, value):\n    if not data:\n        return np.array([]), 0\n\n    avg = statistics.mean(data)\n    data_above_avg = [x for x in data if x > avg]\n    count = bisect.bisect_right(data, value)\n\n    plt.hist(data, bins='auto')\n    plt.title(\"Histogram of the sorted numbers\")\n    plt.show()\n\n    return np.array(data_above_avg), count"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n\n    set_seed(seed)\n\n    data = []\n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"No timezone found for city {city}\")\n\n        local_tz = pytz.timezone(timezones[city])\n        local_datetime = utc_datetime.astimezone(local_tz)\n\n        data.append({\n            'City': city,\n            'Local Time': local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather_conditions[randint(0, len(weather_conditions) - 1)]\n        })\n\n    return pd.DataFrame(data)", "raw_solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n\n    set_seed(seed)\n\n    data = []\n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"No timezone found for city {city}\")\n\n        local_tz = pytz.timezone(timezones[city])\n        local_datetime = utc_datetime.astimezone(local_tz)\n\n        data.append({\n            'City': city,\n            'Local Time': local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather_conditions[randint(0, len(weather_conditions) - 1)]\n        })\n\n    return pd.DataFrame(data)"}
{"task_id": "BigCodeBench/200", "solution": "import random\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(n, value):\n    # Generate 'n' random numbers between 0 and 1\n    numbers = [random.random() for _ in range(n)]\n\n    # Calculate the average of the generated numbers\n    avg = statistics.mean(numbers)\n\n    # Find those numbers greater than the average\n    numbers_greater_than_avg = [num for num in numbers if num > avg]\n\n    # Count how many are greater than or equal to the specified value\n    count = bisect.bisect_right(numbers_greater_than_avg, value)\n\n    # Sort the numbers\n    numbers_greater_than_avg.sort()\n\n    # Plot the sorted numbers\n    plt.plot(numbers_greater_than_avg)\n    plt.show()\n\n    return numbers_greater_than_avg, count", "raw_solution": "import random\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef task_func(n, value):\n    # Generate 'n' random numbers between 0 and 1\n    numbers = [random.random() for _ in range(n)]\n\n    # Calculate the average of the generated numbers\n    avg = statistics.mean(numbers)\n\n    # Find those numbers greater than the average\n    numbers_greater_than_avg = [num for num in numbers if num > avg]\n\n    # Count how many are greater than or equal to the specified value\n    count = bisect.bisect_right(numbers_greater_than_avg, value)\n\n    # Sort the numbers\n    numbers_greater_than_avg.sort()\n\n    # Plot the sorted numbers\n    plt.plot(numbers_greater_than_avg)\n    plt.show()\n\n    return numbers_greater_than_avg, count"}
{"task_id": "BigCodeBench/201", "solution": "import bisect\nimport statistics\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df, column, value):\n    if column not in df.columns:\n        raise ValueError(\"Column does not exist in the DataFrame\")\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value is not a number\")\n\n    column_data = df[column]\n    avg = statistics.mean(column_data)\n    values_greater_avg = column_data[column_data > avg]\n    count_greater_value = bisect.bisect_right(values_greater_avg, value)\n\n    fig, ax = plt.subplots()\n    ax.hist(values_greater_avg, bins='auto', color='blue')\n    ax.set_title('Histogram of values greater than average')\n    plt.show()\n\n    return values_greater_avg.values, count_greater_value, ax", "raw_solution": "import bisect\nimport statistics\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df, column, value):\n    if column not in df.columns:\n        raise ValueError(\"Column does not exist in the DataFrame\")\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value is not a number\")\n\n    column_data = df[column]\n    avg = statistics.mean(column_data)\n    values_greater_avg = column_data[column_data > avg]\n    count_greater_value = bisect.bisect_right(values_greater_avg, value)\n\n    fig, ax = plt.subplots()\n    ax.hist(values_greater_avg, bins='auto', color='blue')\n    ax.set_title('Histogram of values greater than average')\n    plt.show()\n\n    return values_greater_avg.values, count_greater_value, ax"}
{"task_id": "BigCodeBench/202", "solution": "import re\nimport json\nfrom collections import Counter\ndef task_func(json_str, top_n=10):\n    # Extract URLs from JSON string\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', json_str)\n\n    # Count the occurrences of each URL\n    url_counts = Counter(urls)\n\n    # Return the top n URLs\n    return dict(url_counts.most_common(top_n))\njson_str = '{\"urls\": [\"http://example.com\", \"https://example.com\", \"http://example.com\", \"https://example.com\", \"https://example2.com\"]}'", "raw_solution": "import re\nimport json\nfrom collections import Counter\n\ndef task_func(json_str, top_n=10):\n    # Extract URLs from JSON string\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', json_str)\n\n    # Count the occurrences of each URL\n    url_counts = Counter(urls)\n\n    # Return the top n URLs\n    return dict(url_counts.most_common(top_n))\n\n# Test the function\njson_str = '{\"urls\": [\"http://example.com\", \"https://example.com\", \"http://example.com\", \"https://example.com\", \"https://example2.com\"]}'"}
{"task_id": "BigCodeBench/203", "solution": "import json\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\ndef task_func(input_data=None, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, smtp=None):\n    if not smtp:\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        smtp.starttls()\n        smtp.login(email_address, email_password)\n\n    data = json.loads(input_data)\n    names = [item['name'] for item in data['items']]\n\n    msg = MIMEMultipart()\n    msg['From'] = email_address\n    msg['To'] = ', '.join([item['email'] for item in data['items']])\n    msg['Subject'] = 'Extracted Names'\n    msg.attach(MIMEText('\\n'.join(names)))\n\n    smtp.send_message(msg)\n    smtp.quit()\n\n    return names", "raw_solution": "import json\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\n\n# Constants\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\n\ndef task_func(input_data=None, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, smtp=None):\n    if not smtp:\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        smtp.starttls()\n        smtp.login(email_address, email_password)\n\n    data = json.loads(input_data)\n    names = [item['name'] for item in data['items']]\n\n    msg = MIMEMultipart()\n    msg['From'] = email_address\n    msg['To'] = ', '.join([item['email'] for item in data['items']])\n    msg['Subject'] = 'Extracted Names'\n    msg.attach(MIMEText('\\n'.join(names)))\n\n    smtp.send_message(msg)\n    smtp.quit()\n\n    return names"}
{"task_id": "BigCodeBench/204", "solution": "import numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    mean = np.mean(L)\n    median = np.median(L)\n    mode = Counter(L).most_common(1)[0][0]\n    std_dev = np.std(L)\n\n    fig, ax = plt.subplots()\n    ax.hist(L, bins='auto')\n    ax.set_title('Histogram of L')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'std_dev': std_dev,\n        'plot': ax\n    }", "raw_solution": "import numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    mean = np.mean(L)\n    median = np.median(L)\n    mode = Counter(L).most_common(1)[0][0]\n    std_dev = np.std(L)\n\n    fig, ax = plt.subplots()\n    ax.hist(L, bins='auto')\n    ax.set_title('Histogram of L')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'std_dev': std_dev,\n        'plot': ax\n    }"}
{"task_id": "BigCodeBench/205", "solution": "import subprocess\nfrom multiprocessing import Pool\ndef execute_command(command):\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    output, _ = process.communicate()\n    return output\ndef task_func(commands):\n    if not commands:\n        return []\n    with Pool(len(commands)) as p:\n        return p.map(execute_command, commands)", "raw_solution": "import subprocess\nfrom multiprocessing import Pool\n\ndef execute_command(command):\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    output, _ = process.communicate()\n    return output\n\ndef task_func(commands):\n    if not commands:\n        return []\n    with Pool(len(commands)) as p:\n        return p.map(execute_command, commands)"}
{"task_id": "BigCodeBench/206", "solution": "import csv\nimport json\nimport os\ndef task_func(file_name):\n    try:\n        with open(file_name, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            data = [row for row in csv_reader]\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_name} does not exist.\")\n\n    json_file_name = os.path.splitext(file_name)[0] + '.json'\n    with open(json_file_name, 'w') as json_file:\n        json.dump(data, json_file)\n\n    return json_file_name", "raw_solution": "import csv\nimport json\nimport os\n\ndef task_func(file_name):\n    try:\n        with open(file_name, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            data = [row for row in csv_reader]\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_name} does not exist.\")\n\n    json_file_name = os.path.splitext(file_name)[0] + '.json'\n    with open(json_file_name, 'w') as json_file:\n        json.dump(data, json_file)\n\n    return json_file_name"}
{"task_id": "BigCodeBench/207", "solution": "import re\nimport requests\ndef task_func(input_string):\n    # Extract API endpoint from the input string\n    api_endpoint = re.findall(r'https?://[^\\s]+', input_string)[0]\n\n    # Send a GET request to the endpoint\n    response = requests.get(api_endpoint)\n\n    # Return the response data in JSON format\n    return response.json()", "raw_solution": "import re\nimport requests\n\ndef task_func(input_string):\n    # Extract API endpoint from the input string\n    api_endpoint = re.findall(r'https?://[^\\s]+', input_string)[0]\n\n    # Send a GET request to the endpoint\n    response = requests.get(api_endpoint)\n\n    # Return the response data in JSON format\n    return response.json()"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    np.random.seed(seed)\n    random_walk = np.random.choice([-1, 1], size=elements)\n    random_walk_cumulative = np.cumsum(random_walk)\n\n    stats = {\n        'count': elements,\n        'mean': np.mean(random_walk_cumulative),\n        'std': np.std(random_walk_cumulative),\n        'min': np.min(random_walk_cumulative),\n        '5th_percentile': np.percentile(random_walk_cumulative, 5),\n        '25th_percentile': np.percentile(random_walk_cumulative, 25),\n        'median': np.median(random_walk_cumulative),\n        '75th_percentile': np.percentile(random_walk_cumulative, 75),\n        '95th_percentile': np.percentile(random_walk_cumulative, 95),\n        'max': np.max(random_walk_cumulative),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(random_walk_cumulative)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Cumulative Sum')\n\n    return stats, ax.figure", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    np.random.seed(seed)\n    random_walk = np.random.choice([-1, 1], size=elements)\n    random_walk_cumulative = np.cumsum(random_walk)\n\n    stats = {\n        'count': elements,\n        'mean': np.mean(random_walk_cumulative),\n        'std': np.std(random_walk_cumulative),\n        'min': np.min(random_walk_cumulative),\n        '5th_percentile': np.percentile(random_walk_cumulative, 5),\n        '25th_percentile': np.percentile(random_walk_cumulative, 25),\n        'median': np.median(random_walk_cumulative),\n        '75th_percentile': np.percentile(random_walk_cumulative, 75),\n        '95th_percentile': np.percentile(random_walk_cumulative, 95),\n        'max': np.max(random_walk_cumulative),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(random_walk_cumulative)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Cumulative Sum')\n\n    return stats, ax.figure"}
{"task_id": "BigCodeBench/209", "solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Find the tuple with the maximum value at index 1\n    max_tuple = max(data, key=itemgetter(1))\n\n    # Separate the x and y coordinates\n    x, y = zip(*data)\n\n    # Create the scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n\n    # Highlight the maximum tuple\n    ax.scatter(max_tuple[0], max_tuple[1], color='r', s=100, label='Max Tuple')\n\n    # Set the title and labels\n    ax.set_title('Max Tuple Highlighted')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    # Add a legend\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Find the tuple with the maximum value at index 1\n    max_tuple = max(data, key=itemgetter(1))\n\n    # Separate the x and y coordinates\n    x, y = zip(*data)\n\n    # Create the scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n\n    # Highlight the maximum tuple\n    ax.scatter(max_tuple[0], max_tuple[1], color='r', s=100, label='Max Tuple')\n\n    # Set the title and labels\n    ax.set_title('Max Tuple Highlighted')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    # Add a legend\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/210", "solution": "import collections\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Count the frequency of each letter in the data\n    letter_counts = collections.Counter(data)\n\n    # Find the letter with the maximum count\n    max_count_letter = max(letter_counts.items(), key=itemgetter(1))[0]\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(letter_counts.keys(), letter_counts.values())\n\n    # Highlight the letter with the maximum count\n    rects = ax.patches\n    for rect in rects:\n        if rect.get_x() <= max_count_letter <= rect.get_x() + rect.get_width():\n            rect.set_color('r')\n\n    # Set labels and title\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Count')\n    ax.set_title('Letter Counts with Max Value Letter Highlighted')\n    ax.legend(['Letter Counts', 'Max Value Letter'])\n\n    return ax", "raw_solution": "import collections\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Count the frequency of each letter in the data\n    letter_counts = collections.Counter(data)\n\n    # Find the letter with the maximum count\n    max_count_letter = max(letter_counts.items(), key=itemgetter(1))[0]\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(letter_counts.keys(), letter_counts.values())\n\n    # Highlight the letter with the maximum count\n    rects = ax.patches\n    for rect in rects:\n        if rect.get_x() <= max_count_letter <= rect.get_x() + rect.get_width():\n            rect.set_color('r')\n\n    # Set labels and title\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Count')\n    ax.set_title('Letter Counts with Max Value Letter Highlighted')\n    ax.legend(['Letter Counts', 'Max Value Letter'])\n\n    return ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\nfrom io import BytesIO\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Downloads a zip file from a URL, extracts its contents to a specified directory,\n    and returns a list of the extracted files.\n\n    Args:\n        url (str): The URL of the zip file to download.\n        destination_directory (str): The directory to extract the zip file contents to.\n        headers (dict, optional): Headers to be sent with the HTTP request. Defaults to None.\n\n    Returns:\n        list: A list of filenames of the extracted files.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n\n    # Raise an exception if the request was unsuccessful\n    response.raise_for_status()\n\n    # Create a zipfile object from the response content\n    zip_file = zipfile.ZipFile(BytesIO(response.content))\n\n    # Extract all files in the zip file to the destination directory\n    zip_file.extractall(destination_directory)\n\n    # Return a list of filenames of the extracted files\n    return zip_file.namelist()", "raw_solution": "import requests\nimport os\nimport zipfile\nfrom io import BytesIO\n\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Downloads a zip file from a URL, extracts its contents to a specified directory,\n    and returns a list of the extracted files.\n\n    Args:\n        url (str): The URL of the zip file to download.\n        destination_directory (str): The directory to extract the zip file contents to.\n        headers (dict, optional): Headers to be sent with the HTTP request. Defaults to None.\n\n    Returns:\n        list: A list of filenames of the extracted files.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n\n    # Raise an exception if the request was unsuccessful\n    response.raise_for_status()\n\n    # Create a zipfile object from the response content\n    zip_file = zipfile.ZipFile(BytesIO(response.content))\n\n    # Extract all files in the zip file to the destination directory\n    zip_file.extractall(destination_directory)\n\n    # Return a list of filenames of the extracted files\n    return zip_file.namelist()"}
{"task_id": "BigCodeBench/212", "solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Draws a scatter plot of dots and marks the point with the maximum y-value.\n    Returns the axes object as well as the maximum y-value point.\n    \"\"\"\n    # Extract x and y values from the data\n    x, y = zip(*data)\n\n    # Find the point with the maximum y-value\n    max_y_point = max(data, key=itemgetter(1))\n\n    # Create the scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n\n    # Mark the point with the maximum y-value\n    ax.scatter(*max_y_point, color='r')\n\n    # Label the axes and set the title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Points with Max Y Point Highlighted')\n\n    return ax, max_y_point", "raw_solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    Draws a scatter plot of dots and marks the point with the maximum y-value.\n    Returns the axes object as well as the maximum y-value point.\n    \"\"\"\n    # Extract x and y values from the data\n    x, y = zip(*data)\n\n    # Find the point with the maximum y-value\n    max_y_point = max(data, key=itemgetter(1))\n\n    # Create the scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n\n    # Mark the point with the maximum y-value\n    ax.scatter(*max_y_point, color='r')\n\n    # Label the axes and set the title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Points with Max Y Point Highlighted')\n\n    return ax, max_y_point"}
{"task_id": "BigCodeBench/213", "solution": "import time\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kurtosis\ndef task_func(intervals=100, seed=0):\n    random.seed(seed)\n    numbers = []\n    for i in range(intervals):\n        numbers.append(random.random())\n        time.sleep(1)\n\n    elapsed_time = [i for i in range(intervals)]\n\n    fig, ax = plt.subplots()\n    ax.plot(elapsed_time, numbers)\n    ax.set_xlabel('Elapsed Time (intervals)')\n    ax.set_ylabel('Random Number')\n    ax.set_title('Random Number Generation Over Time')\n\n    kurt = kurtosis(numbers)\n\n    return ax, kurt", "raw_solution": "import time\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kurtosis\n\ndef task_func(intervals=100, seed=0):\n    random.seed(seed)\n    numbers = []\n    for i in range(intervals):\n        numbers.append(random.random())\n        time.sleep(1)\n\n    elapsed_time = [i for i in range(intervals)]\n\n    fig, ax = plt.subplots()\n    ax.plot(elapsed_time, numbers)\n    ax.set_xlabel('Elapsed Time (intervals)')\n    ax.set_ylabel('Random Number')\n    ax.set_title('Random Number Generation Over Time')\n\n    kurt = kurtosis(numbers)\n\n    return ax, kurt"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size)\n\n    fig, ax = plt.subplots(1, 1)\n    ax.imshow(image)\n    ax.axis('off')\n    plt.show()\n\n    return ax, image", "raw_solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size)\n\n    fig, ax = plt.subplots(1, 1)\n    ax.imshow(image)\n    ax.axis('off')\n    plt.show()\n\n    return ax, image"}
{"task_id": "BigCodeBench/215", "solution": "import requests\nimport json\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nHEADERS = {\n    'accept': 'application/json'\n}\ndef task_func(url, parameters):\n    \"\"\"\n    Retrieves data from a specific API endpoint with the provided parameters,\n    converts the data into a pandas dataframe, and draws a heatmap to show the\n    correlation between numerical characteristics.\n    \"\"\"\n    try:\n        response = requests.get(url, headers=HEADERS, params=parameters)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"Invalid URL or URL cannot be accessed: {e}\")\n\n    data = response.json()\n\n    if not data:\n        raise Exception(\"Invalid data: Empty data\")\n\n    df = pd.DataFrame(data)\n\n    correlation_matrix = df.corr()\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n    plt.show()\n\n    return (df, ax)", "raw_solution": "import requests\nimport json\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Constants\nHEADERS = {\n    'accept': 'application/json'\n}\n\ndef task_func(url, parameters):\n    \"\"\"\n    Retrieves data from a specific API endpoint with the provided parameters,\n    converts the data into a pandas dataframe, and draws a heatmap to show the\n    correlation between numerical characteristics.\n    \"\"\"\n    try:\n        response = requests.get(url, headers=HEADERS, params=parameters)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"Invalid URL or URL cannot be accessed: {e}\")\n\n    data = response.json()\n\n    if not data:\n        raise Exception(\"Invalid data: Empty data\")\n\n    df = pd.DataFrame(data)\n\n    correlation_matrix = df.corr()\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n    plt.show()\n\n    return (df, ax)"}
{"task_id": "BigCodeBench/216", "solution": "import pandas as pd\nimport os\nimport json\nfrom collections import Counter\nimport re\ndef task_func(json_dir_path, word_count):\n    \"\"\"\n    Analyzes text content in JSON files from a given directory and finds the most common words.\n    \"\"\"\n    # Initialize an empty list to store the text content\n    text_content = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(json_dir_path):\n        # Check if the file is a JSON file\n        if filename.endswith('.json'):\n            # Open the file and load its content\n            with open(os.path.join(json_dir_path, filename), 'r') as file:\n                data = json.load(file)\n\n            # Extract the text content from the file\n            # This is a placeholder, you should replace it with the actual code to extract the text content from the JSON file\n            text_content.append(data['text'])\n\n    # Join all the text content into a single string\n    text_content = ' '.join(text_content)\n\n    # Remove punctuation and convert the text to lowercase\n    text_content = re.sub(r'[^\\w\\s]', '', text_content).lower()\n\n    # Split the text into words\n    words = text_content.split()\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Return the most common words and their counts\n    return word_counts.most_common(word_count)", "raw_solution": "import pandas as pd\nimport os\nimport json\nfrom collections import Counter\nimport re\n\ndef task_func(json_dir_path, word_count):\n    \"\"\"\n    Analyzes text content in JSON files from a given directory and finds the most common words.\n    \"\"\"\n    # Initialize an empty list to store the text content\n    text_content = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(json_dir_path):\n        # Check if the file is a JSON file\n        if filename.endswith('.json'):\n            # Open the file and load its content\n            with open(os.path.join(json_dir_path, filename), 'r') as file:\n                data = json.load(file)\n\n            # Extract the text content from the file\n            # This is a placeholder, you should replace it with the actual code to extract the text content from the JSON file\n            text_content.append(data['text'])\n\n    # Join all the text content into a single string\n    text_content = ' '.join(text_content)\n\n    # Remove punctuation and convert the text to lowercase\n    text_content = re.sub(r'[^\\w\\s]', '', text_content).lower()\n\n    # Split the text into words\n    words = text_content.split()\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Return the most common words and their counts\n    return word_counts.most_common(word_count)"}
{"task_id": "BigCodeBench/217", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mu=0, sigma=1, sample_size=1000, seed=0):\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n\n    # Calculate empirical mean and standard deviation\n    empirical_mean = np.mean(sample)\n    empirical_std_dev = np.std(sample)\n\n    # Generate x values for the normal distribution\n    x = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\n\n    # Calculate the PDF of the normal distribution\n    pdf = stats.norm.pdf(x, mu, sigma)\n\n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.hist(sample, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n    ax.plot(x, pdf, 'k-', label='Normal PDF')\n    ax.set_title(f'Normal Distribution with $\\\\mu = {empirical_mean:.2f}, \\\\sigma = {empirical_std_dev:.2f}$')\n    ax.legend()\n\n    return ax, empirical_mean, empirical_std_dev", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(mu=0, sigma=1, sample_size=1000, seed=0):\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n\n    # Calculate empirical mean and standard deviation\n    empirical_mean = np.mean(sample)\n    empirical_std_dev = np.std(sample)\n\n    # Generate x values for the normal distribution\n    x = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\n\n    # Calculate the PDF of the normal distribution\n    pdf = stats.norm.pdf(x, mu, sigma)\n\n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.hist(sample, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n    ax.plot(x, pdf, 'k-', label='Normal PDF')\n    ax.set_title(f'Normal Distribution with $\\\\mu = {empirical_mean:.2f}, \\\\sigma = {empirical_std_dev:.2f}$')\n    ax.legend()\n\n    return ax, empirical_mean, empirical_std_dev"}
{"task_id": "BigCodeBench/218", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\nTARGET = 'target'\ndef task_func(df, dict_mapping, plot_histogram=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n    if TARGET not in df.columns or any(col not in df.columns for col in FEATURES):\n        raise ValueError(\"FEATURES and TARGET columns not in the input DataFrame\")\n\n    # Replace values according to dict_mapping\n    df.replace(dict_mapping, inplace=True)\n\n    # Standardize specified features\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n\n    if plot_histogram:\n        # Draw a histogram of the target variable\n        plt.hist(df[TARGET], bins=30)\n        plt.title('Histogram of the target variable')\n        plt.xlabel(TARGET)\n        plt.ylabel('Frequency')\n        return df, plt.gca()\n    else:\n        return df, None", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\nTARGET = 'target'\n\ndef task_func(df, dict_mapping, plot_histogram=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n    if TARGET not in df.columns or any(col not in df.columns for col in FEATURES):\n        raise ValueError(\"FEATURES and TARGET columns not in the input DataFrame\")\n\n    # Replace values according to dict_mapping\n    df.replace(dict_mapping, inplace=True)\n\n    # Standardize specified features\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n\n    if plot_histogram:\n        # Draw a histogram of the target variable\n        plt.hist(df[TARGET], bins=30)\n        plt.title('Histogram of the target variable')\n        plt.xlabel(TARGET)\n        plt.ylabel('Frequency')\n        return df, plt.gca()\n    else:\n        return df, None"}
{"task_id": "BigCodeBench/219", "solution": "import math\nimport statistics\nimport numpy as np\nfrom scipy.signal import periodogram\ndef task_func(input_list):\n    # Sort the input list in ascending order based on the degree value of its elements\n    sorted_list = sorted(input_list, key=lambda x: x[1])\n\n    # Calculate the mean, median, and mode of both the sorted list and the same for the magnitude of the fast fourier transform of the degree values\n    degree_values = [x[1] for x in sorted_list]\n    mean_sorted_list = round(statistics.mean(degree_values), 2)\n    median_sorted_list = round(statistics.median(degree_values), 2)\n    mode_sorted_list = round(statistics.mode(degree_values), 2)\n\n    # Calculate the magnitude of the fast fourier transform of the degree values\n    fft_values = np.fft.fft(degree_values).real\n    fft_magnitude = [round(abs(x), 2) for x in fft_values]\n    mean_fft_magnitude = round(statistics.mean(fft_magnitude), 2)\n    median_fft_magnitude = round(statistics.median(fft_magnitude), 2)\n    mode_fft_magnitude = round(statistics.mode(fft_magnitude), 2)\n\n    return (mean_sorted_list, median_sorted_list, mode_sorted_list, mean_fft_magnitude, median_fft_magnitude, mode_fft_magnitude)", "raw_solution": "import math\nimport statistics\nimport numpy as np\nfrom scipy.signal import periodogram\n\ndef task_func(input_list):\n    # Sort the input list in ascending order based on the degree value of its elements\n    sorted_list = sorted(input_list, key=lambda x: x[1])\n\n    # Calculate the mean, median, and mode of both the sorted list and the same for the magnitude of the fast fourier transform of the degree values\n    degree_values = [x[1] for x in sorted_list]\n    mean_sorted_list = round(statistics.mean(degree_values), 2)\n    median_sorted_list = round(statistics.median(degree_values), 2)\n    mode_sorted_list = round(statistics.mode(degree_values), 2)\n\n    # Calculate the magnitude of the fast fourier transform of the degree values\n    fft_values = np.fft.fft(degree_values).real\n    fft_magnitude = [round(abs(x), 2) for x in fft_values]\n    mean_fft_magnitude = round(statistics.mean(fft_magnitude), 2)\n    median_fft_magnitude = round(statistics.median(fft_magnitude), 2)\n    mode_fft_magnitude = round(statistics.mode(fft_magnitude), 2)\n\n    return (mean_sorted_list, median_sorted_list, mode_sorted_list, mean_fft_magnitude, median_fft_magnitude, mode_fft_magnitude)"}
{"task_id": "BigCodeBench/220", "solution": "from random import choice\nimport turtle\nimport time\ndef draw_square(turtle, color):\n    turtle.color(color)\n    turtle.begin_fill()\n    for _ in range(4):\n        turtle.forward(100)\n        turtle.right(90)\n    turtle.end_fill()\ndef task_func(colors):\n    window = turtle.Screen()\n    window.bgcolor(\"white\")\n    t = turtle.Turtle()\n    t.speed(1)\n    for _ in range(5):\n        color = choice(colors)\n        draw_square(t, color)\n        t.penup()\n        t.right(90)\n        t.forward(100)\n        t.left(90)\n        t.pendown()\n        time.sleep(1)\n    window.mainloop()\ncolors = [\"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\"]", "raw_solution": "from random import choice\nimport turtle\nimport time\n\ndef draw_square(turtle, color):\n    turtle.color(color)\n    turtle.begin_fill()\n    for _ in range(4):\n        turtle.forward(100)\n        turtle.right(90)\n    turtle.end_fill()\n\ndef task_func(colors):\n    window = turtle.Screen()\n    window.bgcolor(\"white\")\n    t = turtle.Turtle()\n    t.speed(1)\n    for _ in range(5):\n        color = choice(colors)\n        draw_square(t, color)\n        t.penup()\n        t.right(90)\n        t.forward(100)\n        t.left(90)\n        t.pendown()\n        time.sleep(1)\n    window.mainloop()\n\ncolors = [\"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\"]\ntask_func(colors)"}
{"task_id": "BigCodeBench/221", "solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\ndef task_func(df, dct):\n    \"\"\"\n    Calculates and returns the mean, median, mode, and variance for specified features in a DataFrame.\n    Replaces certain values in the DataFrame based on a provided dictionary mapping before performing the calculations.\n    \"\"\"\n    # Check if the DataFrame contains the required features\n    for feature in FEATURES:\n        if feature not in df.columns:\n            return \"Invalid input: DataFrame does not contain the '{}' feature\".format(feature)\n\n    # Replace values in the DataFrame based on the provided dictionary\n    for key, value in dct.items():\n        df[key] = df[key].replace(key, value)\n\n    # Calculate and return the statistics for each feature\n    try:\n        stats_dict = {}\n        for feature in FEATURES:\n            feature_data = df[feature].dropna()\n            mean = feature_data.mean()\n            median = feature_data.median()\n            mode = feature_data.mode()[0] if len(feature_data.mode()) > 0 else None\n            variance = feature_data.var()\n            stats_dict[feature] = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance}\n        return stats_dict\n    except Exception as e:\n        return \"Invalid input: Error in calculation - {}\".format(str(e))", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n\ndef task_func(df, dct):\n    \"\"\"\n    Calculates and returns the mean, median, mode, and variance for specified features in a DataFrame.\n    Replaces certain values in the DataFrame based on a provided dictionary mapping before performing the calculations.\n    \"\"\"\n    # Check if the DataFrame contains the required features\n    for feature in FEATURES:\n        if feature not in df.columns:\n            return \"Invalid input: DataFrame does not contain the '{}' feature\".format(feature)\n\n    # Replace values in the DataFrame based on the provided dictionary\n    for key, value in dct.items():\n        df[key] = df[key].replace(key, value)\n\n    # Calculate and return the statistics for each feature\n    try:\n        stats_dict = {}\n        for feature in FEATURES:\n            feature_data = df[feature].dropna()\n            mean = feature_data.mean()\n            median = feature_data.median()\n            mode = feature_data.mode()[0] if len(feature_data.mode()) > 0 else None\n            variance = feature_data.var()\n            stats_dict[feature] = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance}\n        return stats_dict\n    except Exception as e:\n        return \"Invalid input: Error in calculation - {}\".format(str(e))"}
{"task_id": "BigCodeBench/222", "solution": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(list_input):\n    # Sort the list based on the degree value of its elements\n    sorted_list = sorted(list_input, key=lambda x: abs(x))\n\n    # Calculate the cumulative sum of the sorted list\n    cumulative_sum = np.cumsum(sorted_list)\n\n    # Plot the cumulative sum\n    fig, ax = plt.subplots()\n    ax.plot(cumulative_sum)\n\n    return cumulative_sum, ax", "raw_solution": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(list_input):\n    # Sort the list based on the degree value of its elements\n    sorted_list = sorted(list_input, key=lambda x: abs(x))\n\n    # Calculate the cumulative sum of the sorted list\n    cumulative_sum = np.cumsum(sorted_list)\n\n    # Plot the cumulative sum\n    fig, ax = plt.subplots()\n    ax.plot(cumulative_sum)\n\n    return cumulative_sum, ax"}
{"task_id": "BigCodeBench/223", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport numpy as np\ndef task_func(df, dct, columns=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    if columns is not None:\n        df = df[columns]\n\n    for col in df.columns:\n        if df[col].dtype == np.object:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col])\n        elif df[col].dtype in [np.float64, np.int64]:\n            scaler = StandardScaler()\n            df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport numpy as np\n\ndef task_func(df, dct, columns=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    if columns is not None:\n        df = df[columns]\n\n    for col in df.columns:\n        if df[col].dtype == np.object:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col])\n        elif df[col].dtype in [np.float64, np.int64]:\n            scaler = StandardScaler()\n            df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n\n    return df"}
{"task_id": "BigCodeBench/224", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    def generator():\n        for x in np.arange(range_start, range_end, step):\n            yield (x, np.sin(x), np.cos(x), np.abs(np.sin(x) - np.cos(x)))\n\n    x, sin_x, cos_x, abs_diff = zip(*generator())\n\n    fig, ax = plt.subplots()\n    ax.plot(x, sin_x, label='sin(x)')\n    ax.plot(x, cos_x, label='cos(x)')\n    ax.plot(x, abs_diff, label='abs(sin(x) - cos(x))')\n    ax.legend()\n\n    fft_abs_diff = np.abs(fft(abs_diff))\n    mean_fft_abs_diff = np.mean(fft_abs_diff)\n    median_fft_abs_diff = np.median(fft_abs_diff)\n\n    return generator, ax, mean_fft_abs_diff, median_fft_abs_diff", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    def generator():\n        for x in np.arange(range_start, range_end, step):\n            yield (x, np.sin(x), np.cos(x), np.abs(np.sin(x) - np.cos(x)))\n\n    x, sin_x, cos_x, abs_diff = zip(*generator())\n\n    fig, ax = plt.subplots()\n    ax.plot(x, sin_x, label='sin(x)')\n    ax.plot(x, cos_x, label='cos(x)')\n    ax.plot(x, abs_diff, label='abs(sin(x) - cos(x))')\n    ax.legend()\n\n    fft_abs_diff = np.abs(fft(abs_diff))\n    mean_fft_abs_diff = np.mean(fft_abs_diff)\n    median_fft_abs_diff = np.median(fft_abs_diff)\n\n    return generator, ax, mean_fft_abs_diff, median_fft_abs_diff"}
{"task_id": "BigCodeBench/225", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, dct, columns=None, plot_histograms=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    df = df.copy()\n\n    if columns is None:\n        columns = df.columns\n    else:\n        columns = [col for col in columns if col in df.columns]\n\n    for col in columns:\n        df[col] = df[col].replace(dct)\n\n    if plot_histograms:\n        for col in columns:\n            plt.figure(figsize=(10, 6))\n            df[col].hist(bins=30)\n            plt.title(f'Histogram of {col}')\n            plt.show()\n\n    return df", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, dct, columns=None, plot_histograms=False):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    df = df.copy()\n\n    if columns is None:\n        columns = df.columns\n    else:\n        columns = [col for col in columns if col in df.columns]\n\n    for col in columns:\n        df[col] = df[col].replace(dct)\n\n    if plot_histograms:\n        for col in columns:\n            plt.figure(figsize=(10, 6))\n            df[col].hist(bins=30)\n            plt.title(f'Histogram of {col}')\n            plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/226", "solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(range_start=0, range_end=10, step=0.1):\n    def generator_func():\n        for x in np.arange(range_start, range_end, step):\n            yield (x, math.exp(x))\n    return generator_func()", "raw_solution": "import numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(range_start=0, range_end=10, step=0.1):\n    def generator_func():\n        for x in np.arange(range_start, range_end, step):\n            yield (x, math.exp(x))\n    return generator_func()\n\ndef plot_func(generator):\n    x_values = []\n    y_values = []\n    for x, y in generator:\n        x_values.append(x)\n        y_values.append(y)\n    plt.plot(x_values, y_values)\n    plt.xlabel('x')\n    plt.ylabel('e^x')\n    plt.title('Exponential Function')\n    return plt.gca()\n\n# Test the function\ngenerator = task_func()\naxes = plot_func(generator)\nplt.show()"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from a list L, normalizes it based on the sound pressure level (SPL) of a specified audio file, and generates a spectrogram from the matrix.\n    \"\"\"\n    # Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"The specified audio file {audio_file} does not exist.\")\n\n    # Read the audio file\n    data, sample_rate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    SPL = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10 ** (SPL / 20))\n\n    # Generate the spectrogram\n    D = librosa.stft(normalized_matrix.flatten())\n    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n\n    # Plot the spectrogram\n    plt.figure(figsize=(14, 5))\n    librosa.display.specshow(S_db, sr=sample_rate, x_axis='time', y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Spectrogram')\n\n    return normalized_matrix, plt.gcf()", "raw_solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from a list L, normalizes it based on the sound pressure level (SPL) of a specified audio file, and generates a spectrogram from the matrix.\n    \"\"\"\n    # Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"The specified audio file {audio_file} does not exist.\")\n\n    # Read the audio file\n    data, sample_rate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    SPL = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10 ** (SPL / 20))\n\n    # Generate the spectrogram\n    D = librosa.stft(normalized_matrix.flatten())\n    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n\n    # Plot the spectrogram\n    plt.figure(figsize=(14, 5))\n    librosa.display.specshow(S_db, sr=sample_rate, x_axis='time', y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Spectrogram')\n\n    return normalized_matrix, plt.gcf()"}
{"task_id": "BigCodeBench/228", "solution": "import pandas as pd\nimport numpy as np\nfrom typing import Dict\ndef task_func(df: pd.DataFrame, dct: Dict[str, str]) -> pd.DataFrame:\n    \"\"\"\n    Replaces certain values in a DataFrame with a dictionary mapping and calculates the Pearson correlation coefficient between each pair of columns.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    # Replace values in DataFrame with dictionary mapping\n    df.replace(dct, inplace=True)\n\n    # Calculate Pearson correlation coefficient\n    corr_matrix = df.corr(method='pearson')\n\n    return corr_matrix", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom typing import Dict\n\ndef task_func(df: pd.DataFrame, dct: Dict[str, str]) -> pd.DataFrame:\n    \"\"\"\n    Replaces certain values in a DataFrame with a dictionary mapping and calculates the Pearson correlation coefficient between each pair of columns.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    # Replace values in DataFrame with dictionary mapping\n    df.replace(dct, inplace=True)\n\n    # Calculate Pearson correlation coefficient\n    corr_matrix = df.corr(method='pearson')\n\n    return corr_matrix"}
{"task_id": "BigCodeBench/229", "solution": "import json\nimport random\nfrom datetime import datetime, timedelta\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\nACTIONS = ['login', 'logout', 'upload', 'download', 'delete']\ndef task_func(file_path, num_entries, seed=None):\n    \"\"\"\n    Generates a JSON file with user activity data.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    data = []\n    for _ in range(num_entries):\n        user = random.choice(USERS)\n        action = random.choice(ACTIONS)\n        timestamp = (datetime.now() - timedelta(minutes=random.randint(1, 1440))).isoformat()\n        data.append({'user': user, 'action': action, 'timestamp': timestamp})\n\n    with open(file_path, 'w') as f:\n        json.dump(data, f)\n\n    return file_path", "raw_solution": "import json\nimport random\nfrom datetime import datetime, timedelta\n\n# Constants\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\nACTIONS = ['login', 'logout', 'upload', 'download', 'delete']\n\ndef task_func(file_path, num_entries, seed=None):\n    \"\"\"\n    Generates a JSON file with user activity data.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    data = []\n    for _ in range(num_entries):\n        user = random.choice(USERS)\n        action = random.choice(ACTIONS)\n        timestamp = (datetime.now() - timedelta(minutes=random.randint(1, 1440))).isoformat()\n        data.append({'user': user, 'action': action, 'timestamp': timestamp})\n\n    with open(file_path, 'w') as f:\n        json.dump(data, f)\n\n    return file_path"}
{"task_id": "BigCodeBench/230", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nCOLUMNS = ['Name', 'Age', 'Country', 'Score']\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        return \"Invalid input\"\n    if not all(col in df.columns for col in COLUMNS):\n        return \"Invalid input\"\n\n    # Remove duplicates\n    df = df.drop_duplicates(subset=['Name'])\n\n    # Histogram of Scores\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df, x='Score', bins=20, kde=False, color='skyblue', alpha=0.7)\n    plt.title('Histogram of Scores')\n    plt.xlabel('Score')\n    plt.ylabel('Frequency')\n\n    # Boxplot of Scores by Country\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(df, x='Country', y='Score', color='skyblue')\n    plt.title('Boxplot of Scores by Country')\n    plt.xlabel('Country')\n    plt.ylabel('Score')\n\n    return plt.gcf()", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Country', 'Score']\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        return \"Invalid input\"\n    if not all(col in df.columns for col in COLUMNS):\n        return \"Invalid input\"\n\n    # Remove duplicates\n    df = df.drop_duplicates(subset=['Name'])\n\n    # Histogram of Scores\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df, x='Score', bins=20, kde=False, color='skyblue', alpha=0.7)\n    plt.title('Histogram of Scores')\n    plt.xlabel('Score')\n    plt.ylabel('Frequency')\n\n    # Boxplot of Scores by Country\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(df, x='Country', y='Score', color='skyblue')\n    plt.title('Boxplot of Scores by Country')\n    plt.xlabel('Country')\n    plt.ylabel('Score')\n\n    return plt.gcf()"}
{"task_id": "BigCodeBench/231", "solution": "import numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.axes import Axes\ndef task_func(obj_list) -> Axes:\n    values = [obj.value for obj in obj_list]\n    mean = np.mean(values)\n    std_dev = np.std(values)\n\n    x = np.linspace(mean - 3*std_dev, mean + 3*std_dev, 100)\n    plt.plot(x, stats.norm.pdf(x, mean, std_dev), label='Custom Normal Distribution')\n\n    plt.hist(values, bins='auto', alpha=0.7, rwidth=0.85, color='#003f5c', label='Histogram')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram and Custom Normal Distribution')\n    plt.legend()\n\n    return plt.gca()", "raw_solution": "import numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.axes import Axes\n\nclass ValueObject:\n    value = 0\n\n    def __init__(self, mu=0, std=1, seed=77):\n        random.seed(seed)\n        self.value = random.gauss(mu, std)\n\ndef task_func(obj_list) -> Axes:\n    values = [obj.value for obj in obj_list]\n    mean = np.mean(values)\n    std_dev = np.std(values)\n\n    x = np.linspace(mean - 3*std_dev, mean + 3*std_dev, 100)\n    plt.plot(x, stats.norm.pdf(x, mean, std_dev), label='Custom Normal Distribution')\n\n    plt.hist(values, bins='auto', alpha=0.7, rwidth=0.85, color='#003f5c', label='Histogram')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram and Custom Normal Distribution')\n    plt.legend()\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/232", "solution": "import pandas as pd\nfrom collections import Counter", "raw_solution": "import pandas as pd\nfrom collections import Counter\n\ndef sales_report(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Exclude duplicate customer names\n    df = df.drop_duplicates(subset='Customer Name')\n\n    # Calculate total sales\n    total_sales = df['Sales'].sum()\n\n    # Find the most popular sales category\n    category_counts = Counter(df['Sales Category'])\n    most_popular_category = category_counts.most_common(1)[0][0]\n\n    return {'Total Sales': total_sales, 'Most Popular Category': most_popular_category}"}
{"task_id": "BigCodeBench/233", "solution": "import random\nimport matplotlib.pyplot as plt\ndef task_func(obj_list, attr, num_bins=30):\n    values = [getattr(obj, attr) for obj in obj_list]\n    plt.hist(values, bins=num_bins)\n    plt.title('Histogram of attribute values')\n    plt.xlabel('Attribute Value')\n    plt.ylabel('Count')\n    return plt.gca()", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\nclass Object:\n    def __init__(self, value=None):\n        if value is None:\n            self.value = random.gauss(0, 1)\n        else:\n            self.value = value\n\ndef task_func(obj_list, attr, num_bins=30):\n    values = [getattr(obj, attr) for obj in obj_list]\n    plt.hist(values, bins=num_bins)\n    plt.title('Histogram of attribute values')\n    plt.xlabel('Attribute Value')\n    plt.ylabel('Count')\n    return plt.gca()\n\n# Testing\nrandom.seed(0)\nobjects = [Object() for _ in range(1000)]\nhist_ax = task_func(objects, 'value', num_bins=30)\nplt.show()"}
{"task_id": "BigCodeBench/234", "solution": "import pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    df = df.drop_duplicates(subset='name')\n\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df['age'], df['score'])\n\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df['age'], df['score'], color='blue')\n    plt.plot(df['age'], intercept + slope*df['age'], 'r', label='y={:.2f}x+{:.2f}'.format(slope,intercept))\n    plt.title('Linear Regression')\n    plt.xlabel('Age')\n    plt.ylabel('Score')\n    plt.legend()\n\n    return (plt, plt.gca())", "raw_solution": "import pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    df = df.drop_duplicates(subset='name')\n\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df['age'], df['score'])\n\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df['age'], df['score'], color='blue')\n    plt.plot(df['age'], intercept + slope*df['age'], 'r', label='y={:.2f}x+{:.2f}'.format(slope,intercept))\n    plt.title('Linear Regression')\n    plt.xlabel('Age')\n    plt.ylabel('Score')\n    plt.legend()\n\n    return (plt, plt.gca())"}
{"task_id": "BigCodeBench/235", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom statsmodels.formula.api import ols\ndef task_func(mu, sigma, seed=0, num_samples=1000, num_bins=30):\n    np.random.seed(seed)\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(samples, bins=num_bins, density=True, alpha=0.5, label='Histogram')\n\n    # Calculate the PDF values\n    x = np.linspace(min(samples), max(samples), 100)\n    pdf = norm.pdf(x, mu, sigma)\n    ax.plot(x, pdf, 'r', label='PDF')\n\n    # Fit a second order polynomial to the histogram using OLS\n    x_binned = np.histogram(samples, bins=num_bins, density=True)[0]\n    bin_centers = np.histogram(samples, bins=num_bins, density=True)[1][:-1] + 0.5 * (np.histogram(samples, bins=num_bins, density=True)[1][1]-np.histogram(samples, bins=num_bins, density=True)[1][0])\n    model = ols('y ~ x + I(x**2)', data={'x': bin_centers, 'y': x_binned}).fit()\n    y_ols = model.predict(exog={'x': x})\n    ax.plot(x, y_ols, 'g', label='OLS')\n\n    ax.legend()\n    plt.show()\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom statsmodels.formula.api import ols\n\ndef task_func(mu, sigma, seed=0, num_samples=1000, num_bins=30):\n    np.random.seed(seed)\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(samples, bins=num_bins, density=True, alpha=0.5, label='Histogram')\n\n    # Calculate the PDF values\n    x = np.linspace(min(samples), max(samples), 100)\n    pdf = norm.pdf(x, mu, sigma)\n    ax.plot(x, pdf, 'r', label='PDF')\n\n    # Fit a second order polynomial to the histogram using OLS\n    x_binned = np.histogram(samples, bins=num_bins, density=True)[0]\n    bin_centers = np.histogram(samples, bins=num_bins, density=True)[1][:-1] + 0.5 * (np.histogram(samples, bins=num_bins, density=True)[1][1]-np.histogram(samples, bins=num_bins, density=True)[1][0])\n    model = ols('y ~ x + I(x**2)', data={'x': bin_centers, 'y': x_binned}).fit()\n    y_ols = model.predict(exog={'x': x})\n    ax.plot(x, y_ols, 'g', label='OLS')\n\n    ax.legend()\n    plt.show()\n    return ax"}
{"task_id": "BigCodeBench/236", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\ndef task_func(df, test_size=0.2, random_state=42):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    df = df.drop_duplicates(subset='Name')\n\n    X = df[['Age', 'Score']]\n    y = df['Category']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    clf = RandomForestClassifier(random_state=random_state)\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef task_func(df, test_size=0.2, random_state=42):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    df = df.drop_duplicates(subset='Name')\n\n    X = df[['Age', 'Score']]\n    y = df['Category']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    clf = RandomForestClassifier(random_state=random_state)\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy"}
{"task_id": "BigCodeBench/237", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(data, save_plot=False, plot_path=None):\n    \"\"\"\n    Unzips a list of objects and their 3D coordinates, runs PCA to reduce the dimensionality to 2D,\n    and depending on the value of save_plot parameter, either saves the plot to the provided path and\n    returns the 2D coordinates or returns the 2D coordinates and the plot's Axes.\n    \"\"\"\n    if save_plot and plot_path is None:\n        raise ValueError(\"If save_plot is True, plot_path must be provided.\")\n\n    # Unzip the data\n    objects, coordinates_3d = zip(*data)\n    coordinates_3d = np.array(coordinates_3d)\n\n    # Run PCA\n    pca = PCA(n_components=2)\n    coordinates_2d = pca.fit_transform(coordinates_3d)\n\n    if save_plot:\n        # Plot the 2D coordinates\n        fig, ax = plt.subplots()\n        ax.scatter(coordinates_2d[:, 0], coordinates_2d[:, 1])\n\n        # Save the plot\n        plt.savefig(plot_path)\n\n        return coordinates_2d, ax\n    else:\n        return coordinates_2d", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(data, save_plot=False, plot_path=None):\n    \"\"\"\n    Unzips a list of objects and their 3D coordinates, runs PCA to reduce the dimensionality to 2D,\n    and depending on the value of save_plot parameter, either saves the plot to the provided path and\n    returns the 2D coordinates or returns the 2D coordinates and the plot's Axes.\n    \"\"\"\n    if save_plot and plot_path is None:\n        raise ValueError(\"If save_plot is True, plot_path must be provided.\")\n\n    # Unzip the data\n    objects, coordinates_3d = zip(*data)\n    coordinates_3d = np.array(coordinates_3d)\n\n    # Run PCA\n    pca = PCA(n_components=2)\n    coordinates_2d = pca.fit_transform(coordinates_3d)\n\n    if save_plot:\n        # Plot the 2D coordinates\n        fig, ax = plt.subplots()\n        ax.scatter(coordinates_2d[:, 0], coordinates_2d[:, 1])\n\n        # Save the plot\n        plt.savefig(plot_path)\n\n        return coordinates_2d, ax\n    else:\n        return coordinates_2d"}
{"task_id": "BigCodeBench/238", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    # Standardize 'Age' and 'Score' columns\n    scaler = StandardScaler()\n    df[['Age', 'Score']] = scaler.fit_transform(df[['Age', 'Score']])\n\n    # Remove duplicate entries based on 'Name'\n    df = df.drop_duplicates(subset='Name')\n\n    # Plot a scatter plot of standardized 'Age' and 'Score'\n    plt.figure(figsize=(8, 6))\n    plt.scatter(df['Age'], df['Score'])\n    plt.title('Scatter Plot of Standardized Age and Score')\n    plt.xlabel('Age (standardized)')\n    plt.ylabel('Score (standardized)')\n    plt.show()\n\n    return df, plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    # Standardize 'Age' and 'Score' columns\n    scaler = StandardScaler()\n    df[['Age', 'Score']] = scaler.fit_transform(df[['Age', 'Score']])\n\n    # Remove duplicate entries based on 'Name'\n    df = df.drop_duplicates(subset='Name')\n\n    # Plot a scatter plot of standardized 'Age' and 'Score'\n    plt.figure(figsize=(8, 6))\n    plt.scatter(df['Age'], df['Score'])\n    plt.title('Scatter Plot of Standardized Age and Score')\n    plt.xlabel('Age (standardized)')\n    plt.ylabel('Score (standardized)')\n    plt.show()\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values\n    numeric_values = [val for sublist in original for val in sublist if isinstance(val, (int, float))]\n    numeric_values = np.array(numeric_values)\n\n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n\n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n\n    # Generate PDF\n    x = np.linspace(stats_dict['min'], stats_dict['max'], 1000)\n    pdf = stats.norm.pdf(x, stats_dict['mean'], stats_dict['std_dev'])\n\n    # Plot PDF\n    ax.plot(x, pdf, label='PDF')\n    ax.legend()\n\n    return numeric_values, stats_dict, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extract numeric values\n    numeric_values = [val for sublist in original for val in sublist if isinstance(val, (int, float))]\n    numeric_values = np.array(numeric_values)\n\n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n\n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n\n    # Generate PDF\n    x = np.linspace(stats_dict['min'], stats_dict['max'], 1000)\n    pdf = stats.norm.pdf(x, stats_dict['mean'], stats_dict['std_dev'])\n\n    # Plot PDF\n    ax.plot(x, pdf, label='PDF')\n    ax.legend()\n\n    return numeric_values, stats_dict, ax"}
{"task_id": "BigCodeBench/240", "solution": "import pandas as pd\nfrom random import uniform\ndef task_func(n_data_points=1000, min_value=0.0, max_value=10.0, column_name='Value'):\n    data = [round(uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=[column_name])\n    return df", "raw_solution": "import pandas as pd\nfrom random import uniform\n\ndef task_func(n_data_points=1000, min_value=0.0, max_value=10.0, column_name='Value'):\n    data = [round(uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=[column_name])\n    return df\n\n# Test the function\ndf = task_func(n_data_points=5, min_value=1.0, max_value=5.0, column_name='Random_Values')"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array], norm='l2')[0]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the original and normalized arrays\n    ax.plot(original_array, label='Original Data')\n    ax.plot(normalized_array, label='Normalized Data')\n\n    # Set the title and legend\n    ax.set_title('Original vs. Normalized Data')\n    ax.legend()\n\n    # Return the numpy arrays and the axes object\n    return original_array, normalized_array, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array], norm='l2')[0]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the original and normalized arrays\n    ax.plot(original_array, label='Original Data')\n    ax.plot(normalized_array, label='Normalized Data')\n\n    # Set the title and legend\n    ax.set_title('Original vs. Normalized Data')\n    ax.legend()\n\n    # Return the numpy arrays and the axes object\n    return original_array, normalized_array, ax"}
{"task_id": "BigCodeBench/242", "solution": "import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(image_path, kernel_size):\n    if not isinstance(kernel_size, int) or kernel_size <= 0:\n        raise ValueError(\"Kernel size must be a positive integer.\")\n\n    try:\n        image = cv2.imread(image_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The specified image file {image_path} does not exist.\")\n\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    axs[0].set_title('Original Image')\n    axs[0].axis('off')\n    axs[1].imshow(cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB))\n    axs[1].set_title('Blurred Image')\n    axs[1].axis('off')\n    plt.show()\n\n    return blurred_image, axs", "raw_solution": "import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(image_path, kernel_size):\n    if not isinstance(kernel_size, int) or kernel_size <= 0:\n        raise ValueError(\"Kernel size must be a positive integer.\")\n\n    try:\n        image = cv2.imread(image_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The specified image file {image_path} does not exist.\")\n\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    axs[0].set_title('Original Image')\n    axs[0].axis('off')\n    axs[1].imshow(cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB))\n    axs[1].set_title('Blurred Image')\n    axs[1].axis('off')\n    plt.show()\n\n    return blurred_image, axs"}
{"task_id": "BigCodeBench/243", "solution": "import pandas as pd\nimport random\ndef task_func(n_data_points=10000):\n    if n_data_points == 0:\n        return pd.DataFrame()\n    data = [round(random.uniform(0.0, 10.0), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=['Value'])\n    return df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(n_data_points=10000):\n    if n_data_points == 0:\n        return pd.DataFrame()\n    data = [round(random.uniform(0.0, 10.0), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=['Value'])\n    return df"}
{"task_id": "BigCodeBench/244", "solution": "import numpy as np\nfrom scipy.fft import fft\nfrom matplotlib import pyplot as plt\ndef task_func(original):\n    if not original:\n        return np.array([]), np.array([]), None\n\n    original_array = np.array(original)\n    fft_data = fft(original_array)\n\n    fig, ax = plt.subplots()\n    ax.hist(np.abs(fft_data), bins=30, color='blue', edgecolor='black')\n    ax.set_title('Histogram of FFT Data')\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Frequency')\n\n    return original_array, fft_data, ax.figure", "raw_solution": "import numpy as np\nfrom scipy.fft import fft\nfrom matplotlib import pyplot as plt\n\ndef task_func(original):\n    if not original:\n        return np.array([]), np.array([]), None\n\n    original_array = np.array(original)\n    fft_data = fft(original_array)\n\n    fig, ax = plt.subplots()\n    ax.hist(np.abs(fft_data), bins=30, color='blue', edgecolor='black')\n    ax.set_title('Histogram of FFT Data')\n    ax.set_xlabel('Magnitude')\n    ax.set_ylabel('Frequency')\n\n    return original_array, fft_data, ax.figure"}
{"task_id": "BigCodeBench/245", "solution": "import pandas as pd\nimport random\nfrom scipy import stats\nfrom collections import Counter\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    # Generate a random dataset of floating-point numbers within a specified range\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Calculate statistical measures (mean, median, mode) of the data\n    mean_val = round(sum(data) / n_data_points, 3)\n    median_val = round(stats.median(data), 3)\n    mode_val = round(stats.mode(data)[0][0], 3)\n\n    # Return the results in a dictionary\n    return {'mean': mean_val, 'median': median_val, 'mode': mode_val}", "raw_solution": "import pandas as pd\nimport random\nfrom scipy import stats\nfrom collections import Counter\n\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    # Generate a random dataset of floating-point numbers within a specified range\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Calculate statistical measures (mean, median, mode) of the data\n    mean_val = round(sum(data) / n_data_points, 3)\n    median_val = round(stats.median(data), 3)\n    mode_val = round(stats.mode(data)[0][0], 3)\n\n    # Return the results in a dictionary\n    return {'mean': mean_val, 'median': median_val, 'mode': mode_val}\n\n# Test the function"}
{"task_id": "BigCodeBench/246", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\nANGLES = np.arange(0, 2*np.pi, 0.01)\ndef task_func(n_waves, seed=0):\n    if n_waves < 1:\n        return [], [], None\n\n    np.random.seed(seed)\n    frequencies = np.random.uniform(0, 10, n_waves)\n    amplitudes = np.ones(n_waves)\n\n    signal = np.zeros(len(ANGLES))\n    for freq, amp in zip(frequencies, amplitudes):\n        signal += amp * np.sin(freq * ANGLES)\n\n    fft_data = fft(signal)\n    fft_magnitude = np.abs(fft_data)\n\n    fig, ax = plt.subplots()\n    ax.hist(fft_magnitude, bins=30, color='blue', edgecolor='black')\n    ax.set_title('FFT Magnitude Histogram')\n    ax.set_xlabel('Frequency')\n    ax.set_ylabel('Magnitude')\n\n    return [signal], fft_data, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\nANGLES = np.arange(0, 2*np.pi, 0.01)\n\ndef task_func(n_waves, seed=0):\n    if n_waves < 1:\n        return [], [], None\n\n    np.random.seed(seed)\n    frequencies = np.random.uniform(0, 10, n_waves)\n    amplitudes = np.ones(n_waves)\n\n    signal = np.zeros(len(ANGLES))\n    for freq, amp in zip(frequencies, amplitudes):\n        signal += amp * np.sin(freq * ANGLES)\n\n    fft_data = fft(signal)\n    fft_magnitude = np.abs(fft_data)\n\n    fig, ax = plt.subplots()\n    ax.hist(fft_magnitude, bins=30, color='blue', edgecolor='black')\n    ax.set_title('FFT Magnitude Histogram')\n    ax.set_xlabel('Frequency')\n    ax.set_ylabel('Magnitude')\n\n    return [signal], fft_data, ax"}
{"task_id": "BigCodeBench/247", "solution": "import pandas as pd\nimport random\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    if max_value < min_value:\n        raise ValueError(\"max_value should be greater than min_value\")\n\n    # Generate a random dataset of floating point numbers\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Normalize the data using standard scaling\n    scaler = StandardScaler()\n    data = np.array(data).reshape(-1, 1)\n    normalized_data = scaler.fit_transform(data)\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(normalized_data, columns=[\"Normalized Value\"])\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Constants\nN_DATA_POINTS = 5000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\n\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    if max_value < min_value:\n        raise ValueError(\"max_value should be greater than min_value\")\n\n    # Generate a random dataset of floating point numbers\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Normalize the data using standard scaling\n    scaler = StandardScaler()\n    data = np.array(data).reshape(-1, 1)\n    normalized_data = scaler.fit_transform(data)\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(normalized_data, columns=[\"Normalized Value\"])\n\n    return df"}
{"task_id": "BigCodeBench/248", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"The data_list is empty.\")\n\n    unzipped_data = list(itertools.chain(*data_list))\n\n    plt.plot(unzipped_data)\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.title('Unzipped Data Plot')\n    plt.show()\ndata_list = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"The data_list is empty.\")\n\n    unzipped_data = list(itertools.chain(*data_list))\n\n    plt.plot(unzipped_data)\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.title('Unzipped Data Plot')\n    plt.show()\n\n# Test the function\ndata_list = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\ntask_func(data_list)"}
{"task_id": "BigCodeBench/249", "solution": "import pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\ndef task_func(n_data_points=10000, min_value=0.0, max_value=10.0, test_size=0.2):\n    # Generate a random set of floating-point numbers within a specified range\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Divide the data into train and test sets based on a given test size\n    train, test = train_test_split(data, test_size=test_size, random_state=42)\n\n    # Create DataFrames\n    train_df = pd.DataFrame(train, columns=[\"Value\"])\n    test_df = pd.DataFrame(test, columns=[\"Value\"])\n\n    return (train_df, test_df)", "raw_solution": "import pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(n_data_points=10000, min_value=0.0, max_value=10.0, test_size=0.2):\n    # Generate a random set of floating-point numbers within a specified range\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Divide the data into train and test sets based on a given test size\n    train, test = train_test_split(data, test_size=test_size, random_state=42)\n\n    # Create DataFrames\n    train_df = pd.DataFrame(train, columns=[\"Value\"])\n    test_df = pd.DataFrame(test, columns=[\"Value\"])\n\n    return (train_df, test_df)"}
{"task_id": "BigCodeBench/250", "solution": "import numpy as np\nimport itertools\nimport json\ndef task_func(data_list, json_file_name=\"mean_values.json\"):\n    # Calculate the mean of the numeric values for each position in the provided data list\n    mean_values = {f'Position {i}': np.mean(list(itertools.chain(*[x[i] for x in data_list]))) for i in range(len(data_list[0]))}\n\n    # Optionally, the results can be exported to a specified JSON file\n    with open(json_file_name, 'w') as f:\n        json.dump(mean_values, f)\n\n    return mean_values", "raw_solution": "import numpy as np\nimport itertools\nimport json\n\ndef task_func(data_list, json_file_name=\"mean_values.json\"):\n    # Calculate the mean of the numeric values for each position in the provided data list\n    mean_values = {f'Position {i}': np.mean(list(itertools.chain(*[x[i] for x in data_list]))) for i in range(len(data_list[0]))}\n\n    # Optionally, the results can be exported to a specified JSON file\n    with open(json_file_name, 'w') as f:\n        json.dump(mean_values, f)\n\n    return mean_values"}
{"task_id": "BigCodeBench/251", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n\n    # Assuming the data has two columns: 'Job' and 'Count'\n    if 'Job' not in data.columns or 'Count' not in data.columns:\n        raise ValueError(\"Data should have 'Job' and 'Count' columns\")\n\n    # Create a pie chart\n    fig, ax = plt.subplots()\n    ax.pie(data['Count'], labels=data['Job'], autopct='%1.1f%%', startangle=140)\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    return fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n\n    # Assuming the data has two columns: 'Job' and 'Count'\n    if 'Job' not in data.columns or 'Count' not in data.columns:\n        raise ValueError(\"Data should have 'Job' and 'Count' columns\")\n\n    # Create a pie chart\n    fig, ax = plt.subplots()\n    ax.pie(data['Count'], labels=data['Job'], autopct='%1.1f%%', startangle=140)\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    return fig"}
{"task_id": "BigCodeBench/252", "solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\ndef task_func(data, labels):\n    \"\"\"\n    Plots a list of data with different colors. If there are more data series than the predefined colors,\n    the function cycles through the colors. In case of even more series than colors + labels, 'black' is used.\n    \"\"\"\n    # Create a cycle object from the colors list\n    color_cycle = cycle(COLORS)\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot each data series with a different color\n    for i, (d, l) in enumerate(zip(data, labels)):\n        # If there are more data series than colors, use the next color from the cycle\n        if i >= len(COLORS):\n            color = next(color_cycle)\n        else:\n            color = COLORS[i]\n\n        # Plot the data series with the corresponding color and label\n        ax.plot(d, label=l, color=color)\n\n    # If there are more data series than colors + labels, use 'black' for the remaining series\n    if len(data) > len(COLORS) + len(labels):\n        for i in range(len(COLORS) + len(labels), len(data)):\n            ax.plot(data[i], label=f'Extra series {i+1}', color='black')\n\n    # Add a legend to the plot\n    ax.legend()\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\n\n# Constants\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\n\ndef task_func(data, labels):\n    \"\"\"\n    Plots a list of data with different colors. If there are more data series than the predefined colors,\n    the function cycles through the colors. In case of even more series than colors + labels, 'black' is used.\n    \"\"\"\n    # Create a cycle object from the colors list\n    color_cycle = cycle(COLORS)\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot each data series with a different color\n    for i, (d, l) in enumerate(zip(data, labels)):\n        # If there are more data series than colors, use the next color from the cycle\n        if i >= len(COLORS):\n            color = next(color_cycle)\n        else:\n            color = COLORS[i]\n\n        # Plot the data series with the corresponding color and label\n        ax.plot(d, label=l, color=color)\n\n    # If there are more data series than colors + labels, use 'black' for the remaining series\n    if len(data) > len(COLORS) + len(labels):\n        for i in range(len(COLORS) + len(labels), len(data)):\n            ax.plot(data[i], label=f'Extra series {i+1}', color='black')\n\n    # Add a legend to the plot\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/253", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\ndef task_func(ax):\n    # Generate a random sine wave\n    x = np.linspace(0, 2 * np.pi, 100)\n    y = np.sin(x)\n\n    # Randomly select a color\n    color = random.choice(COLORS)\n\n    # Plot the sine wave\n    ax.plot(x, y, color=color)\n\n    # Randomly select a position for radial labels\n    pos = random.randint(0, 100)\n\n    # Set radial labels\n    ax.set_rlabel_position(pos)\n\n    return color", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef task_func(ax):\n    # Generate a random sine wave\n    x = np.linspace(0, 2 * np.pi, 100)\n    y = np.sin(x)\n\n    # Randomly select a color\n    color = random.choice(COLORS)\n\n    # Plot the sine wave\n    ax.plot(x, y, color=color)\n\n    # Randomly select a position for radial labels\n    pos = random.randint(0, 100)\n\n    # Set radial labels\n    ax.set_rlabel_position(pos)\n\n    return color\n\n# Test the function\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\ncolor = task_func(ax)\nplt.show()"}
{"task_id": "BigCodeBench/254", "solution": "import json\nimport math\ndef task_func(decimal_value, precision=2):\n    \"\"\"\n    Calculates the square root of the given decimal value to a certain precision and then encodes the result as a JSON string.\n    \"\"\"\n    # Calculate the square root\n    sqrt_value = round(math.sqrt(decimal_value), precision)\n\n    # Encode the result as a JSON string\n    json_string = json.dumps({\"square_root\": sqrt_value})\n\n    return json_string", "raw_solution": "import json\nimport math\n\ndef task_func(decimal_value, precision=2):\n    \"\"\"\n    Calculates the square root of the given decimal value to a certain precision and then encodes the result as a JSON string.\n    \"\"\"\n    # Calculate the square root\n    sqrt_value = round(math.sqrt(decimal_value), precision)\n\n    # Encode the result as a JSON string\n    json_string = json.dumps({\"square_root\": sqrt_value})\n\n    return json_string"}
{"task_id": "BigCodeBench/255", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nFUNCTIONS = [np.sin, np.cos, np.tan]\ndef task_func(ax, func_index):\n    \"\"\"\n    Draws a mathematical function (sine, cosine, or tangent) on a polar diagram 'ax'.\n    The radial ticks are placed at a position corresponding to the index of the function multiplied by 45 degrees.\n    The function raises a ValueError if the input ax is not a matplotlib.axes._axes.Axes.\n    \"\"\"\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax must be a matplotlib.axes._axes.Axes\")\n\n    if func_index not in range(len(FUNCTIONS)):\n        raise ValueError(\"Invalid function index. Choose from 0 (sin), 1 (cos), or 2 (tan)\")\n\n    theta = np.linspace(0, 2 * np.pi, 1000)\n    r = FUNCTIONS[func_index](theta)\n\n    ax.plot(theta, r)\n    ax.set_theta_zero_location(\"N\")\n    ax.set_theta_direction(-1)\n\n    # Set radial ticks at multiples of 45 degrees\n    ax.set_rticks([0.5, 1, 1.5, 2])\n    ax.set_rlabel_position(45)\n\n    plt.show()", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nFUNCTIONS = [np.sin, np.cos, np.tan]\n\ndef task_func(ax, func_index):\n    \"\"\"\n    Draws a mathematical function (sine, cosine, or tangent) on a polar diagram 'ax'.\n    The radial ticks are placed at a position corresponding to the index of the function multiplied by 45 degrees.\n    The function raises a ValueError if the input ax is not a matplotlib.axes._axes.Axes.\n    \"\"\"\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax must be a matplotlib.axes._axes.Axes\")\n\n    if func_index not in range(len(FUNCTIONS)):\n        raise ValueError(\"Invalid function index. Choose from 0 (sin), 1 (cos), or 2 (tan)\")\n\n    theta = np.linspace(0, 2 * np.pi, 1000)\n    r = FUNCTIONS[func_index](theta)\n\n    ax.plot(theta, r)\n    ax.set_theta_zero_location(\"N\")\n    ax.set_theta_direction(-1)\n\n    # Set radial ticks at multiples of 45 degrees\n    ax.set_rticks([0.5, 1, 1.5, 2])\n    ax.set_rlabel_position(45)\n\n    plt.show()\n\n# Test the function\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\ntask_func(ax, 0)"}
{"task_id": "BigCodeBench/256", "solution": "import json\nimport random\nimport hashlib\nimport string\nfrom datetime import datetime\ndef task_func(utc_datetime, salt='salt', password_length=10, seed=0):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not isinstance(salt, str):\n        raise ValueError(\"salt must be a string\")\n\n    random.seed(seed)\n    chars = string.ascii_lowercase + string.digits\n    password = ''.join(random.choice(chars) for _ in range(password_length))\n\n    hashed_password = hashlib.sha256()\n    hashed_password.update(password.encode())\n    hashed_password.update(salt.encode())\n    hashed_password.update(str(utc_datetime).encode())\n\n    hashed_password_str = hashed_password.hexdigest()\n\n    return json.dumps({\"hashed_password\": hashed_password_str})", "raw_solution": "import json\nimport random\nimport hashlib\nimport string\nfrom datetime import datetime\n\ndef task_func(utc_datetime, salt='salt', password_length=10, seed=0):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not isinstance(salt, str):\n        raise ValueError(\"salt must be a string\")\n\n    random.seed(seed)\n    chars = string.ascii_lowercase + string.digits\n    password = ''.join(random.choice(chars) for _ in range(password_length))\n\n    hashed_password = hashlib.sha256()\n    hashed_password.update(password.encode())\n    hashed_password.update(salt.encode())\n    hashed_password.update(str(utc_datetime).encode())\n\n    hashed_password_str = hashed_password.hexdigest()\n\n    return json.dumps({\"hashed_password\": hashed_password_str})"}
{"task_id": "BigCodeBench/257", "solution": "import numpy as np\nimport matplotlib.pyplot as plt", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef draw_spiral(ax, num_turns):\n    theta = np.linspace(0, 2*np.pi*num_turns, 1000)\n    r = theta\n    x = r * np.cos(theta)\n    y = r * np.sin(theta)\n    ax.plot(x, y)\n    ax.set_aspect('equal')\n    ax.set_xlim(-1.1, 1.1)\n    ax.set_ylim(-1.1, 1.1)\n    ax.spines['left'].set_position('center')\n    ax.spines['right'].set_color('none')\n    ax.spines['bottom'].set_position('center')\n    ax.spines['top'].set_color('none')\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    return ax\n\nfig, ax = plt.subplots()\ndraw_spiral(ax, 1)\nplt.show()"}
{"task_id": "BigCodeBench/258", "solution": "import json\nimport random\nfrom datetime import datetime\nDATA = [\n    {'name': 'John', 'age': 30, 'city': 'New York'},\n    {'name': 'Peter', 'age': 35, 'city': 'London'},\n    {'name': 'Susan', 'age': 25, 'city': 'Sydney'},\n    {'name': 'Alice', 'age': 28, 'city': 'Paris'},\n    {'name': 'Bob', 'age': 40, 'city': 'Tokyo'},\n    {'name': 'Charlie', 'age': 22, 'city': 'Beijing'},\n    {'name': 'David', 'age': 33, 'city': 'Mumbai'},\n    {'name': 'Eve', 'age': 27, 'city': 'Berlin'},\n    {'name': 'Frank', 'age': 32, 'city': 'Moscow'},\n    {'name': 'Grace', 'age': 29, 'city': 'Rome'}\n]\ndef task_func(utc_datetime, seed=0):\n    random.seed(seed)\n    person = random.choice(DATA)\n    person['timestamp'] = utc_datetime.strftime('%Y-%m-%dT%H:%M:%SZ')\n    return json.dumps(person)\nutc_datetime = datetime.utcnow()", "raw_solution": "import json\nimport random\nfrom datetime import datetime\n\n# Constants\nDATA = [\n    {'name': 'John', 'age': 30, 'city': 'New York'},\n    {'name': 'Peter', 'age': 35, 'city': 'London'},\n    {'name': 'Susan', 'age': 25, 'city': 'Sydney'},\n    {'name': 'Alice', 'age': 28, 'city': 'Paris'},\n    {'name': 'Bob', 'age': 40, 'city': 'Tokyo'},\n    {'name': 'Charlie', 'age': 22, 'city': 'Beijing'},\n    {'name': 'David', 'age': 33, 'city': 'Mumbai'},\n    {'name': 'Eve', 'age': 27, 'city': 'Berlin'},\n    {'name': 'Frank', 'age': 32, 'city': 'Moscow'},\n    {'name': 'Grace', 'age': 29, 'city': 'Rome'}\n]\n\ndef task_func(utc_datetime, seed=0):\n    random.seed(seed)\n    person = random.choice(DATA)\n    person['timestamp'] = utc_datetime.strftime('%Y-%m-%dT%H:%M:%SZ')\n    return json.dumps(person)\n\n# Testing\nutc_datetime = datetime.utcnow()"}
{"task_id": "BigCodeBench/259", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(ax, num_points):\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax is not a matplotlib Axes object.\")\n    if num_points < 0:\n        raise ValueError(\"Number of points cannot be negative.\")\n\n    # Generate random points\n    theta = np.random.uniform(0, 2*np.pi, num_points)\n    r = np.random.uniform(0, 1, num_points)\n\n    # Plot the points\n    ax.plot(theta, r, 'o')\n\n    # Set radial ticks\n    radial_ticks = np.arange(0, 1.1, 0.1)\n    ax.set_rticks(radial_ticks)\n    ax.set_rlabel_position(radial_ticks * 180 / np.pi - 90)\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(ax, num_points):\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax is not a matplotlib Axes object.\")\n    if num_points < 0:\n        raise ValueError(\"Number of points cannot be negative.\")\n\n    # Generate random points\n    theta = np.random.uniform(0, 2*np.pi, num_points)\n    r = np.random.uniform(0, 1, num_points)\n\n    # Plot the points\n    ax.plot(theta, r, 'o')\n\n    # Set radial ticks\n    radial_ticks = np.arange(0, 1.1, 0.1)\n    ax.set_rticks(radial_ticks)\n    ax.set_rlabel_position(radial_ticks * 180 / np.pi - 90)\n\n    return ax\n\n# Test the function\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\ntask_func(ax, 100)\nplt.show()"}
{"task_id": "BigCodeBench/260", "solution": "import json\nimport os\nimport glob\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\ndef task_func(directory):\n    \"\"\"\n    Adds a new key-value pair to all JSON files in a specific directory and saves the updated JSON files.\n    \"\"\"\n    # Get all JSON files in the directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n\n    # Initialize counter for updated JSON files\n    updated_count = 0\n\n    # Iterate over each JSON file\n    for json_file in json_files:\n        # Open the JSON file\n        with open(json_file, 'r+') as file:\n            # Load the JSON data\n            data = json.load(file)\n\n            # Check if the key already exists\n            if KEY not in data:\n                # Add the new key-value pair\n                data[KEY] = VALUE\n\n                # Move the read cursor to the start of the file\n                file.seek(0)\n\n                # Dump the updated data to the file\n                json.dump(data, file)\n\n                # Truncate the remaining data\n                file.truncate()\n\n                # Increment the updated count\n                updated_count += 1\n\n    # Return the number of updated JSON files\n    return updated_count", "raw_solution": "import json\nimport os\nimport glob\n\n# Constants\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\n\ndef task_func(directory):\n    \"\"\"\n    Adds a new key-value pair to all JSON files in a specific directory and saves the updated JSON files.\n    \"\"\"\n    # Get all JSON files in the directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n\n    # Initialize counter for updated JSON files\n    updated_count = 0\n\n    # Iterate over each JSON file\n    for json_file in json_files:\n        # Open the JSON file\n        with open(json_file, 'r+') as file:\n            # Load the JSON data\n            data = json.load(file)\n\n            # Check if the key already exists\n            if KEY not in data:\n                # Add the new key-value pair\n                data[KEY] = VALUE\n\n                # Move the read cursor to the start of the file\n                file.seek(0)\n\n                # Dump the updated data to the file\n                json.dump(data, file)\n\n                # Truncate the remaining data\n                file.truncate()\n\n                # Increment the updated count\n                updated_count += 1\n\n    # Return the number of updated JSON files\n    return updated_count"}
{"task_id": "BigCodeBench/261", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(ax, radius):\n    if not isinstance(ax, plt.Axes):\n        raise TypeError(\"'ax' must be a matplotlib.axes._axes.Axes object\")\n    if radius < 0:\n        raise ValueError(\"Radius cannot be negative\")\n\n    # Generate data for the circle\n    theta = np.linspace(0, 2*np.pi, 1000)\n    x = radius * np.cos(theta)\n    y = radius * np.sin(theta)\n\n    # Plot the circle\n    ax.plot(x, y)\n\n    # Set radial ticks\n    ax.set_rticks([0.5, 1, 1.5, 2])\n    ax.set_rlabel_position(-22.5)  # Move radial labels away from plotted line\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(ax, radius):\n    if not isinstance(ax, plt.Axes):\n        raise TypeError(\"'ax' must be a matplotlib.axes._axes.Axes object\")\n    if radius < 0:\n        raise ValueError(\"Radius cannot be negative\")\n\n    # Generate data for the circle\n    theta = np.linspace(0, 2*np.pi, 1000)\n    x = radius * np.cos(theta)\n    y = radius * np.sin(theta)\n\n    # Plot the circle\n    ax.plot(x, y)\n\n    # Set radial ticks\n    ax.set_rticks([0.5, 1, 1.5, 2])\n    ax.set_rlabel_position(-22.5)  # Move radial labels away from plotted line\n\n    return ax\n\n# Test the function\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\ntask_func(ax, 1)\nplt.show()"}
{"task_id": "BigCodeBench/262", "solution": "import collections\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(dictionary, new_key, new_value):\n    \"\"\"\n    Adds a new key-value pair to the dictionary and plots the distribution of its values.\n    \"\"\"\n    # Add new key-value pair to the dictionary\n    dictionary[new_key] = new_value\n\n    # Create a counter from the dictionary values\n    counter = collections.Counter(dictionary.values())\n\n    # Plot the distribution of values\n    fig, ax = plt.subplots()\n    sns.barplot(list(counter.keys()), list(counter.values()), ax=ax)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Values in Dictionary')\n\n    return dictionary, ax", "raw_solution": "import collections\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(dictionary, new_key, new_value):\n    \"\"\"\n    Adds a new key-value pair to the dictionary and plots the distribution of its values.\n    \"\"\"\n    # Add new key-value pair to the dictionary\n    dictionary[new_key] = new_value\n\n    # Create a counter from the dictionary values\n    counter = collections.Counter(dictionary.values())\n\n    # Plot the distribution of values\n    fig, ax = plt.subplots()\n    sns.barplot(list(counter.keys()), list(counter.values()), ax=ax)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Values in Dictionary')\n\n    return dictionary, ax"}
{"task_id": "BigCodeBench/263", "solution": "import os\nimport glob\nimport shutil\nimport time\nFILE_EXTENSIONS = ['.txt', '.csv', '.xlsx', '.docx', '.pdf']\ndef task_func(my_path: str, days_old: int) -> str:\n    \"\"\"\n    Archives files that were changed older than a specified number of days in a given directory.\n    \"\"\"\n    # Create the archive directory if it doesn't exist\n    archive_path = os.path.join(my_path, 'archive')\n    if not os.path.exists(archive_path):\n        os.makedirs(archive_path)\n\n    # Get the current time\n    current_time = time.time()\n\n    # Iterate over all files in the directory\n    for file_path in glob.glob(os.path.join(my_path, '*')):\n        # Check if the file is a file and has a supported extension\n        if os.path.isfile(file_path) and os.path.splitext(file_path)[1] in FILE_EXTENSIONS:\n            # Get the file's modification time\n            file_time = os.path.getmtime(file_path)\n\n            # Check if the file is older than the specified number of days\n            if (current_time - file_time) / (24 * 60 * 60) > days_old:\n                # Move the file to the archive directory\n                shutil.move(file_path, os.path.join(archive_path, os.path.basename(file_path)))\n\n    return archive_path", "raw_solution": "import os\nimport glob\nimport shutil\nimport time\n\n# Constants\nFILE_EXTENSIONS = ['.txt', '.csv', '.xlsx', '.docx', '.pdf']\n\ndef task_func(my_path: str, days_old: int) -> str:\n    \"\"\"\n    Archives files that were changed older than a specified number of days in a given directory.\n    \"\"\"\n    # Create the archive directory if it doesn't exist\n    archive_path = os.path.join(my_path, 'archive')\n    if not os.path.exists(archive_path):\n        os.makedirs(archive_path)\n\n    # Get the current time\n    current_time = time.time()\n\n    # Iterate over all files in the directory\n    for file_path in glob.glob(os.path.join(my_path, '*')):\n        # Check if the file is a file and has a supported extension\n        if os.path.isfile(file_path) and os.path.splitext(file_path)[1] in FILE_EXTENSIONS:\n            # Get the file's modification time\n            file_time = os.path.getmtime(file_path)\n\n            # Check if the file is older than the specified number of days\n            if (current_time - file_time) / (24 * 60 * 60) > days_old:\n                # Move the file to the archive directory\n                shutil.move(file_path, os.path.join(archive_path, os.path.basename(file_path)))\n\n    return archive_path"}
{"task_id": "BigCodeBench/264", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(dictionary, key, value, n=100, bins=30, seed=0):\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"The provided value is not a number.\")\n\n    dictionary[key] = value\n\n    np.random.seed(seed)\n    data = np.random.normal(value, 1, n)\n    series = pd.Series(data)\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(series, bins=bins, color='skyblue', ec='black')\n    plt.title(f'Histogram of Normal Distribution with Mean={value} and Std=1')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return dictionary, series, plt", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(dictionary, key, value, n=100, bins=30, seed=0):\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"The provided value is not a number.\")\n\n    dictionary[key] = value\n\n    np.random.seed(seed)\n    data = np.random.normal(value, 1, n)\n    series = pd.Series(data)\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(series, bins=bins, color='skyblue', ec='black')\n    plt.title(f'Histogram of Normal Distribution with Mean={value} and Std=1')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return dictionary, series, plt"}
{"task_id": "BigCodeBench/265", "solution": "import collections\nimport json\nimport os\ndef task_func(data, json_file_name='data.json'):\n    # Add a new key \"a\" with the value 1 to the input dictionary\n    data['a'] = 1\n\n    # Calculate the frequency of its values\n    freq = collections.Counter(data.values())\n\n    # Save the updated dictionary and its frequency distribution to a JSON file\n    data_with_freq = {'data': data, 'freq': dict(freq)}\n\n    with open(json_file_name, 'w') as f:\n        json.dump(data_with_freq, f)\n\n    # Return the path of the JSON file\n    return os.path.abspath(json_file_name)", "raw_solution": "import collections\nimport json\nimport os\n\ndef task_func(data, json_file_name='data.json'):\n    # Add a new key \"a\" with the value 1 to the input dictionary\n    data['a'] = 1\n\n    # Calculate the frequency of its values\n    freq = collections.Counter(data.values())\n\n    # Save the updated dictionary and its frequency distribution to a JSON file\n    data_with_freq = {'data': data, 'freq': dict(freq)}\n\n    with open(json_file_name, 'w') as f:\n        json.dump(data_with_freq, f)\n\n    # Return the path of the JSON file\n    return os.path.abspath(json_file_name)"}
{"task_id": "BigCodeBench/266", "solution": "import os\nimport csv\nimport collections\nFILE_NAME = 'file_sizes.csv'\ndef task_func(my_path):\n    \"\"\"\n    This function generates a report on the file size in a directory and writes it to a CSV file.\n    The function takes a path as input and returns the path of the CSV file.\n    \"\"\"\n    # Initialize an empty dictionary to store file sizes\n    file_sizes = collections.defaultdict(int)\n\n    # Walk through the directory and its subdirectories\n    for dirpath, dirnames, filenames in os.walk(my_path):\n        for file in filenames:\n            # Get the full path of the file\n            file_path = os.path.join(dirpath, file)\n            # Get the size of the file\n            file_size = os.path.getsize(file_path)\n            # Add the file size to the dictionary\n            file_sizes[file_path] = file_size\n\n    # Write the file sizes to a CSV file\n    with open(FILE_NAME, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['File Path', 'File Size'])\n        for file_path, file_size in file_sizes.items():\n            writer.writerow([file_path, file_size])\n\n    return FILE_NAME", "raw_solution": "import os\nimport csv\nimport collections\n\n# Constants\nFILE_NAME = 'file_sizes.csv'\n\ndef task_func(my_path):\n    \"\"\"\n    This function generates a report on the file size in a directory and writes it to a CSV file.\n    The function takes a path as input and returns the path of the CSV file.\n    \"\"\"\n    # Initialize an empty dictionary to store file sizes\n    file_sizes = collections.defaultdict(int)\n\n    # Walk through the directory and its subdirectories\n    for dirpath, dirnames, filenames in os.walk(my_path):\n        for file in filenames:\n            # Get the full path of the file\n            file_path = os.path.join(dirpath, file)\n            # Get the size of the file\n            file_size = os.path.getsize(file_path)\n            # Add the file size to the dictionary\n            file_sizes[file_path] = file_size\n\n    # Write the file sizes to a CSV file\n    with open(FILE_NAME, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['File Path', 'File Size'])\n        for file_path, file_size in file_sizes.items():\n            writer.writerow([file_path, file_size])\n\n    return FILE_NAME"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # Generate a signal based on the values in \"data\"\n    signal = list(data.values())\n\n    # Run a Fast Fourier Transform (FFT) on the signal\n    fft_signal = fftpack.fft(signal)\n\n    # Generate the frequency axis\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n\n    # Plot and return the FFT of the signal with a title of 'FFT of the signal'\n    plt.figure(figsize=(10, 5))\n    plt.plot(freqs, np.abs(fft_signal))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n\n    return (fft_signal, plt.gca())", "raw_solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # Generate a signal based on the values in \"data\"\n    signal = list(data.values())\n\n    # Run a Fast Fourier Transform (FFT) on the signal\n    fft_signal = fftpack.fft(signal)\n\n    # Generate the frequency axis\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n\n    # Plot and return the FFT of the signal with a title of 'FFT of the signal'\n    plt.figure(figsize=(10, 5))\n    plt.plot(freqs, np.abs(fft_signal))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n\n    return (fft_signal, plt.gca())"}
{"task_id": "BigCodeBench/268", "solution": "import collections\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\ndef task_func(n_keys, n_values):\n    \"\"\"\n    Creates a Python dictionary with a specified number of keys and values.\n    Keys are randomly selected from a predefined list of letters, and values are consecutive integers starting from 1.\n    \"\"\"\n    keys = random.sample(LETTERS, n_keys)\n    values = list(range(1, n_values + 1))\n    return dict(zip(keys, values))", "raw_solution": "import collections\nimport random\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndef task_func(n_keys, n_values):\n    \"\"\"\n    Creates a Python dictionary with a specified number of keys and values.\n    Keys are randomly selected from a predefined list of letters, and values are consecutive integers starting from 1.\n    \"\"\"\n    keys = random.sample(LETTERS, n_keys)\n    values = list(range(1, n_values + 1))\n    return dict(zip(keys, values))\n\n# Test the function"}
{"task_id": "BigCodeBench/269", "solution": "import numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    # Add key \"a\" with a value of 1\n    data_dict[\"a\"] = 1\n\n    # Conduct statistical analysis on its values\n    values = np.array(list(data_dict.values()))\n    mean = round(np.mean(values), 2)\n    median = round(np.median(values), 2)\n    mode = round(stats.mode(values)[0][0], 2)\n    stats_dict = {\"mean\": mean, \"median\": median, \"mode\": mode}\n\n    # Normalize the values using MinMaxScaler\n    scaler = MinMaxScaler()\n    values_scaled = scaler.fit_transform(values.reshape(-1, 1))\n\n    # Plot a histogram of the normalized values\n    plt.hist(values_scaled, bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Normalized Values')\n    hist_plot = plt.gca()\n\n    return data_dict, stats_dict, hist_plot", "raw_solution": "import numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict):\n    # Add key \"a\" with a value of 1\n    data_dict[\"a\"] = 1\n\n    # Conduct statistical analysis on its values\n    values = np.array(list(data_dict.values()))\n    mean = round(np.mean(values), 2)\n    median = round(np.median(values), 2)\n    mode = round(stats.mode(values)[0][0], 2)\n    stats_dict = {\"mean\": mean, \"median\": median, \"mode\": mode}\n\n    # Normalize the values using MinMaxScaler\n    scaler = MinMaxScaler()\n    values_scaled = scaler.fit_transform(values.reshape(-1, 1))\n\n    # Plot a histogram of the normalized values\n    plt.hist(values_scaled, bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Normalized Values')\n    hist_plot = plt.gca()\n\n    return data_dict, stats_dict, hist_plot"}
{"task_id": "BigCodeBench/270", "solution": "import re\nfrom collections import Counter\ndef task_func(sentence):\n    words = re.findall(r'\\b\\w+\\b', sentence)\n    return dict(Counter(words))", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(sentence):\n    words = re.findall(r'\\b\\w+\\b', sentence)\n    return dict(Counter(words))\n\n# Test the function"}
{"task_id": "BigCodeBench/271", "solution": "import random\nimport string\nimport hashlib\nimport time\ndef task_func(data_dict: dict, seed=0) -> dict:\n    random.seed(seed)\n    salt = ''.join(random.choices(string.ascii_lowercase, k=5))\n    data_dict['a'] = 1\n    for key, value in data_dict.items():\n        hashed_value = hashlib.sha256((salt + str(value)).encode()).hexdigest()\n        data_dict[key] = hashed_value\n    data_dict['timestamp'] = int(time.time())\n    return data_dict", "raw_solution": "import random\nimport string\nimport hashlib\nimport time\n\ndef task_func(data_dict: dict, seed=0) -> dict:\n    random.seed(seed)\n    salt = ''.join(random.choices(string.ascii_lowercase, k=5))\n    data_dict['a'] = 1\n    for key, value in data_dict.items():\n        hashed_value = hashlib.sha256((salt + str(value)).encode()).hexdigest()\n        data_dict[key] = hashed_value\n    data_dict['timestamp'] = int(time.time())\n    return data_dict"}
{"task_id": "BigCodeBench/272", "solution": "import cgi\nimport http.server\nimport json", "raw_solution": "import cgi\nimport http.server\nimport json\n\nclass TaskHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n\n        if self.headers['Content-Type'] != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            response = {\"status\": \"error\", \"message\": \"Content-Type header is not application/json\"}\n            self.wfile.write(json.dumps(response).encode())\n            return\n\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            response = {\"status\": \"error\", \"message\": \"Invalid JSON\"}\n            self.wfile.write(json.dumps(response).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.end_headers()\n            response = {\"status\": \"error\", \"message\": \"No data received\"}\n            self.wfile.write(json.dumps(response).encode())\n            return\n\n        self.send_response(200)\n        self.end_headers()\n        response = {\"status\": \"success\", \"message\": \"Data received successfully.\"}\n        self.wfile.write(json.dumps(response).encode())\n"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nclass JsonPostHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n\n        if self.headers['Content-Type'] != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Content-Type header is not application/json'\n            }).encode())\n            return\n\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Invalid JSON'\n            }).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'No data key in request'\n            }).encode())\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\ndef task_func():\n    \"\"\"\n    This function returns a class that handles HTTP POST requests and validates incoming data.\n    \"\"\"\n    return JsonPostHandler", "raw_solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\nclass JsonPostHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n\n        if self.headers['Content-Type'] != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Content-Type header is not application/json'\n            }).encode())\n            return\n\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Invalid JSON'\n            }).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'No data key in request'\n            }).encode())\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\n\ndef task_func():\n    \"\"\"\n    This function returns a class that handles HTTP POST requests and validates incoming data.\n    \"\"\"\n    return JsonPostHandler"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\nfrom json import JSONDecodeError\nclass EmailHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        try:\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            email_data = json.loads(post_data)\n\n            if 'subject' not in email_data or 'message' not in email_data or 'to' not in email_data:\n                raise ValueError(\"Missing 'subject', 'message', or 'to' keys in the email data\")\n\n            msg = MIMEText(email_data['message'])\n            msg['Subject'] = email_data['subject']\n            msg['From'] = smtp_username\n            msg['To'] = email_data['to']\n\n            smtp = smtplib.SMTP(smtp_server, smtp_port)\n            smtp.login(smtp_username, smtp_password)\n            smtp.send_message(msg)\n            smtp.quit()\n\n            self.send_response(200)\n            self.end_headers()\n            self.wfile.write(b'Email sent successfully')\n\n        except JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Invalid JSON')\n\n        except ValueError as e:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(str(e).encode())\n\n        except smtplib.SMTPAuthenticationError:\n            self.send_response(535)\n            self.end_headers()\n            self.wfile.write(b'Authentication Failed')\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    \"\"\"\n    function: A class that handles HTTP POST requests and sends emails based on\n    the provided data.\n    \"\"\"\n    server_address = ('', 8000)\n    httpd = http.server.HTTPServer(server_address, EmailHandler)\n    httpd.serve_forever()", "raw_solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\nfrom json import JSONDecodeError\n\nclass EmailHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        try:\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            email_data = json.loads(post_data)\n\n            if 'subject' not in email_data or 'message' not in email_data or 'to' not in email_data:\n                raise ValueError(\"Missing 'subject', 'message', or 'to' keys in the email data\")\n\n            msg = MIMEText(email_data['message'])\n            msg['Subject'] = email_data['subject']\n            msg['From'] = smtp_username\n            msg['To'] = email_data['to']\n\n            smtp = smtplib.SMTP(smtp_server, smtp_port)\n            smtp.login(smtp_username, smtp_password)\n            smtp.send_message(msg)\n            smtp.quit()\n\n            self.send_response(200)\n            self.end_headers()\n            self.wfile.write(b'Email sent successfully')\n\n        except JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(b'Invalid JSON')\n\n        except ValueError as e:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(str(e).encode())\n\n        except smtplib.SMTPAuthenticationError:\n            self.send_response(535)\n            self.end_headers()\n            self.wfile.write(b'Authentication Failed')\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    \"\"\"\n    function: A class that handles HTTP POST requests and sends emails based on\n    the provided data.\n    \"\"\"\n    server_address = ('', 8000)\n    httpd = http.server.HTTPServer(server_address, EmailHandler)\n    httpd.serve_forever()"}
{"task_id": "BigCodeBench/275", "solution": "import numpy as np\nfrom itertools import combinations\ndef task_func(n):\n    if n < 1:\n        raise ValueError(\"Input n should be greater than 0\")\n    return list(combinations(range(1, n+1), 2))", "raw_solution": "import numpy as np\nfrom itertools import combinations\n\ndef task_func(n):\n    if n < 1:\n        raise ValueError(\"Input n should be greater than 0\")\n    return list(combinations(range(1, n+1), 2))\n\n# Testing the function"}
{"task_id": "BigCodeBench/276", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(matrix):\n    # Calculate the maximum values of each row\n    max_values = np.max(matrix, axis=1)\n\n    # Calculate the histogram and the bin edges\n    hist, bin_edges = np.histogram(max_values, bins='auto', density=True)\n\n    # Calculate the bin centers\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    # Calculate the skewness and kurtosis\n    skewness = stats.skew(max_values)\n    kurtosis = stats.kurtosis(max_values)\n\n    # Create a histogram plot\n    fig, ax = plt.subplots()\n    ax.bar(bin_centers, hist, width=np.diff(bin_edges)[0], align='center')\n    ax.set_xlabel('Maximum Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Maximum Values')\n\n    return skewness, kurtosis, ax", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(matrix):\n    # Calculate the maximum values of each row\n    max_values = np.max(matrix, axis=1)\n\n    # Calculate the histogram and the bin edges\n    hist, bin_edges = np.histogram(max_values, bins='auto', density=True)\n\n    # Calculate the bin centers\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    # Calculate the skewness and kurtosis\n    skewness = stats.skew(max_values)\n    kurtosis = stats.kurtosis(max_values)\n\n    # Create a histogram plot\n    fig, ax = plt.subplots()\n    ax.bar(bin_centers, hist, width=np.diff(bin_edges)[0], align='center')\n    ax.set_xlabel('Maximum Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Maximum Values')\n\n    return skewness, kurtosis, ax"}
{"task_id": "BigCodeBench/277", "solution": "import random\nfrom itertools import combinations\nimport math\ndef task_func(n):\n    if n < 2:\n        return None\n\n    # Generate n random dots within a unit square\n    dots = [(random.random(), random.random()) for _ in range(n)]\n\n    # Initialize variables to keep track of the closest pair and its distance\n    closest_pair = None\n    min_distance = float('inf')\n\n    # Iterate over all pairs of dots\n    for pair in combinations(dots, 2):\n        # Calculate the Euclidean distance between the pair\n        distance = math.sqrt((pair[0][0] - pair[1][0])**2 + (pair[0][1] - pair[1][1])**2)\n\n        # If this distance is less than the current minimum, update the closest pair and its distance\n        if distance < min_distance:\n            closest_pair = pair\n            min_distance = distance\n\n    return closest_pair", "raw_solution": "import random\nfrom itertools import combinations\nimport math\n\ndef task_func(n):\n    if n < 2:\n        return None\n\n    # Generate n random dots within a unit square\n    dots = [(random.random(), random.random()) for _ in range(n)]\n\n    # Initialize variables to keep track of the closest pair and its distance\n    closest_pair = None\n    min_distance = float('inf')\n\n    # Iterate over all pairs of dots\n    for pair in combinations(dots, 2):\n        # Calculate the Euclidean distance between the pair\n        distance = math.sqrt((pair[0][0] - pair[1][0])**2 + (pair[0][1] - pair[1][1])**2)\n\n        # If this distance is less than the current minimum, update the closest pair and its distance\n        if distance < min_distance:\n            closest_pair = pair\n            min_distance = distance\n\n    return closest_pair"}
{"task_id": "BigCodeBench/278", "solution": "import numpy as np\nfrom sympy import symbols, solve\ndef task_func(precision=2, seed=0):\n    np.random.seed(seed)\n    a, b, c = np.random.randint(-10, 11, 3)\n    x = symbols('x')\n    solutions = solve(a*x**2 + b*x + c, x)\n    solutions = [complex(round(s.evalf(precision), precision)) for s in solutions]\n    return tuple(solutions)", "raw_solution": "import numpy as np\nfrom sympy import symbols, solve\n\ndef task_func(precision=2, seed=0):\n    np.random.seed(seed)\n    a, b, c = np.random.randint(-10, 11, 3)\n    x = symbols('x')\n    solutions = solve(a*x**2 + b*x + c, x)\n    solutions = [complex(round(s.evalf(precision), precision)) for s in solutions]\n    return tuple(solutions)"}
{"task_id": "BigCodeBench/279", "solution": "import random\nfrom collections import Counter\nCARDS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\ndef task_func(x=1):\n    \"\"\"\n    Draws x random 5-card poker hands from a 52-card pack (without suits) and returns the hands along with a counter of the drawn cards.\n    \"\"\"\n    # Initialize an empty list to store the hands\n    hands = []\n\n    # Draw x hands\n    for _ in range(x):\n        # Draw 5 cards without replacement\n        hand = random.sample(CARDS, 5)\n        hands.append(hand)\n\n    # Count the cards in the hands\n    counter = Counter(card for hand in hands for card in hand)\n\n    return (hands, counter)", "raw_solution": "import random\nfrom collections import Counter\n\n# Constants\nCARDS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n\ndef task_func(x=1):\n    \"\"\"\n    Draws x random 5-card poker hands from a 52-card pack (without suits) and returns the hands along with a counter of the drawn cards.\n    \"\"\"\n    # Initialize an empty list to store the hands\n    hands = []\n\n    # Draw x hands\n    for _ in range(x):\n        # Draw 5 cards without replacement\n        hand = random.sample(CARDS, 5)\n        hands.append(hand)\n\n    # Count the cards in the hands\n    counter = Counter(card for hand in hands for card in hand)\n\n    return (hands, counter)"}
{"task_id": "BigCodeBench/280", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\ndef task_func(signal, precision=2, seed=777):\n    np.random.seed(seed)\n    N = len(signal)\n    X = fft(signal)\n    X_rounded = np.round(np.abs(X)/N, precision)\n\n    fig, ax = plt.subplots(2, 1)\n    ax[0].plot(np.arange(N), signal)\n    ax[0].set_title('Original Signal')\n    ax[1].stem(np.arange(N), X_rounded)\n    ax[1].set_title('Transformed Signal')\n    plt.tight_layout()\n\n    return X_rounded, (ax[0], ax[1])", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(signal, precision=2, seed=777):\n    np.random.seed(seed)\n    N = len(signal)\n    X = fft(signal)\n    X_rounded = np.round(np.abs(X)/N, precision)\n\n    fig, ax = plt.subplots(2, 1)\n    ax[0].plot(np.arange(N), signal)\n    ax[0].set_title('Original Signal')\n    ax[1].stem(np.arange(N), X_rounded)\n    ax[1].set_title('Transformed Signal')\n    plt.tight_layout()\n\n    return X_rounded, (ax[0], ax[1])"}
{"task_id": "BigCodeBench/281", "solution": "import re\nimport os\nfrom collections import Counter\ndef task_func(folder_path: str) -> dict:\n    \"\"\"\n    Scans a directory for log files and counts the occurrences of each IP address in all files.\n    The function utilizes a regular expression pattern to identify IP addresses in the log files.\n    The function returns a dictionary with IP addresses as keys and their counts as values.\n    \"\"\"\n    ip_pattern = r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b'\n    ip_counter = Counter()\n\n    for filename in os.listdir(folder_path):\n        if filename.endswith('.log'):\n            with open(os.path.join(folder_path, filename), 'r') as file:\n                for line in file:\n                    ip_matches = re.findall(ip_pattern, line)\n                    ip_counter.update(ip_matches)\n\n    return dict(ip_counter)", "raw_solution": "import re\nimport os\nfrom collections import Counter\n\ndef task_func(folder_path: str) -> dict:\n    \"\"\"\n    Scans a directory for log files and counts the occurrences of each IP address in all files.\n    The function utilizes a regular expression pattern to identify IP addresses in the log files.\n    The function returns a dictionary with IP addresses as keys and their counts as values.\n    \"\"\"\n    ip_pattern = r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b'\n    ip_counter = Counter()\n\n    for filename in os.listdir(folder_path):\n        if filename.endswith('.log'):\n            with open(os.path.join(folder_path, filename), 'r') as file:\n                for line in file:\n                    ip_matches = re.findall(ip_pattern, line)\n                    ip_counter.update(ip_matches)\n\n    return dict(ip_counter)"}
{"task_id": "BigCodeBench/282", "solution": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport cv2\nimport os\ndef task_func(file_path, onpick):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    img = cv2.imread(file_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Calculate color histogram\n    hist = cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n    hist = hist.flatten()\n\n    # Create 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    x = np.arange(0, img.shape[0], 1)\n    y = np.arange(0, img.shape[1], 1)\n    x, y = np.meshgrid(x, y)\n\n    ax.scatter(x, y, hist, picker=True)\n\n    # Connect to pick event\n    ax.callbacks.pick.connect(onpick)\n\n    plt.show()\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport cv2\nimport os\n\ndef task_func(file_path, onpick):\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    img = cv2.imread(file_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Calculate color histogram\n    hist = cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n    hist = hist.flatten()\n\n    # Create 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    x = np.arange(0, img.shape[0], 1)\n    y = np.arange(0, img.shape[1], 1)\n    x, y = np.meshgrid(x, y)\n\n    ax.scatter(x, y, hist, picker=True)\n\n    # Connect to pick event\n    ax.callbacks.pick.connect(onpick)\n\n    plt.show()\n\n    return ax"}
{"task_id": "BigCodeBench/283", "solution": "import os\nimport json\nfrom collections import Counter\ndef task_func(json_files_path='./json_files/', key='name'):\n    \"\"\"\n    Counts the occurrence of a particular key in all json files in a specified directory and returns a dictionary with the values of the specified key and their counts.\n    \"\"\"\n    # Initialize an empty Counter\n    key_counter = Counter()\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(json_files_path):\n        # Check if the file is a JSON file\n        if filename.endswith('.json'):\n            # Open the file and load the JSON data\n            with open(os.path.join(json_files_path, filename), 'r') as file:\n                data = json.load(file)\n\n            # If the key exists in the JSON data, increment the counter\n            if key in data:\n                key_counter[data[key]] += 1\n\n    # Return the Counter as a dictionary\n    return dict(key_counter)", "raw_solution": "import os\nimport json\nfrom collections import Counter\n\ndef task_func(json_files_path='./json_files/', key='name'):\n    \"\"\"\n    Counts the occurrence of a particular key in all json files in a specified directory and returns a dictionary with the values of the specified key and their counts.\n    \"\"\"\n    # Initialize an empty Counter\n    key_counter = Counter()\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(json_files_path):\n        # Check if the file is a JSON file\n        if filename.endswith('.json'):\n            # Open the file and load the JSON data\n            with open(os.path.join(json_files_path, filename), 'r') as file:\n                data = json.load(file)\n\n            # If the key exists in the JSON data, increment the counter\n            if key in data:\n                key_counter[data[key]] += 1\n\n    # Return the Counter as a dictionary\n    return dict(key_counter)"}
{"task_id": "BigCodeBench/284", "solution": "import mechanize\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\ndef task_func(url):\n    browser = mechanize.Browser()\n    browser.set_handle_robots(False)\n    browser.open(url)\n    html = browser.response().read()\n    soup = BeautifulSoup(html, 'html.parser')\n    base_url = url\n    hyperlinks = []\n\n    for link in soup.find_all('a'):\n        href = link.get('href')\n        if href and not href.startswith('http'):\n            href = urljoin(base_url, href)\n        hyperlinks.append(href)\n\n    return hyperlinks", "raw_solution": "import mechanize\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef task_func(url):\n    browser = mechanize.Browser()\n    browser.set_handle_robots(False)\n    browser.open(url)\n    html = browser.response().read()\n    soup = BeautifulSoup(html, 'html.parser')\n    base_url = url\n    hyperlinks = []\n\n    for link in soup.find_all('a'):\n        href = link.get('href')\n        if href and not href.startswith('http'):\n            href = urljoin(base_url, href)\n        hyperlinks.append(href)\n\n    return hyperlinks"}
{"task_id": "BigCodeBench/285", "solution": "import mechanize\nfrom bs4 import BeautifulSoup\ndef task_func(url, form_id, data):\n    \"\"\"\n    Submits a form on a given webpage using mechanize and extracts the title of the response page.\n    If the page has no title, it returns 'No Title'.\n    \"\"\"\n    browser = mechanize.Browser()\n    browser.set_handle_robots(False)\n    browser.open(url)\n\n    form = browser.forms()[form_id]\n    for key, value in data.items():\n        form[key] = value\n\n    response = browser.submit()\n    soup = BeautifulSoup(response, 'html.parser')\n\n    title = soup.title.string if soup.title else 'No Title'\n    return title", "raw_solution": "import mechanize\nfrom bs4 import BeautifulSoup\n\ndef task_func(url, form_id, data):\n    \"\"\"\n    Submits a form on a given webpage using mechanize and extracts the title of the response page.\n    If the page has no title, it returns 'No Title'.\n    \"\"\"\n    browser = mechanize.Browser()\n    browser.set_handle_robots(False)\n    browser.open(url)\n\n    form = browser.forms()[form_id]\n    for key, value in data.items():\n        form[key] = value\n\n    response = browser.submit()\n    soup = BeautifulSoup(response, 'html.parser')\n\n    title = soup.title.string if soup.title else 'No Title'\n    return title"}
{"task_id": "BigCodeBench/286", "solution": "from collections import Counter\nimport os\nimport csv\ndef task_func(output_file, test_directory):\n    \"\"\"\n    Counts the number of words in multiple dictionary files (.txt) in a specific directory,\n    exports the counts to a CSV file, and then returns the total number of words.\n    \"\"\"\n    try:\n        # Initialize a counter\n        word_counter = Counter()\n\n        # Iterate over all files in the directory\n        for filename in os.listdir(test_directory):\n            # Check if the file is a .txt file\n            if filename.endswith('.txt'):\n                # Open the file and read its content\n                with open(os.path.join(test_directory, filename), 'r') as file:\n                    # Split the content into words and update the counter\n                    word_counter.update(file.read().split())\n\n        # Open the output file in write mode\n        with open(output_file, 'w', newline='') as file:\n            # Create a CSV writer\n            writer = csv.writer(file)\n            # Write the header\n            writer.writerow(['Word', 'Count'])\n            # Write the counts\n            writer.writerows(word_counter.items())\n\n        # Return the total number of words\n        return sum(word_counter.values())\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0", "raw_solution": "from collections import Counter\nimport os\nimport csv\n\n# Constants\nFILE_DIR = './yourdictfiles/'\n\ndef task_func(output_file, test_directory):\n    \"\"\"\n    Counts the number of words in multiple dictionary files (.txt) in a specific directory,\n    exports the counts to a CSV file, and then returns the total number of words.\n    \"\"\"\n    try:\n        # Initialize a counter\n        word_counter = Counter()\n\n        # Iterate over all files in the directory\n        for filename in os.listdir(test_directory):\n            # Check if the file is a .txt file\n            if filename.endswith('.txt'):\n                # Open the file and read its content\n                with open(os.path.join(test_directory, filename), 'r') as file:\n                    # Split the content into words and update the counter\n                    word_counter.update(file.read().split())\n\n        # Open the output file in write mode\n        with open(output_file, 'w', newline='') as file:\n            # Create a CSV writer\n            writer = csv.writer(file)\n            # Write the header\n            writer.writerow(['Word', 'Count'])\n            # Write the counts\n            writer.writerows(word_counter.items())\n\n        # Return the total number of words\n        return sum(word_counter.values())\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(directory):\n    \"\"\"\n    Counts the number of words in .txt files within a specified directory,\n    exports the counts to a JSON file, and then returns the total number of words.\n    \"\"\"\n    word_counts = Counter()\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                words = file.read().split()\n                word_counts.update(words)\n\n    # Export the counts to a JSON file\n    with open('word_counts.json', 'w') as file:\n        json.dump(word_counts, file)\n\n    # Return the total number of words\n    return sum(word_counts.values())", "raw_solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(directory):\n    \"\"\"\n    Counts the number of words in .txt files within a specified directory,\n    exports the counts to a JSON file, and then returns the total number of words.\n    \"\"\"\n    word_counts = Counter()\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                words = file.read().split()\n                word_counts.update(words)\n\n    # Export the counts to a JSON file\n    with open('word_counts.json', 'w') as file:\n        json.dump(word_counts, file)\n\n    # Return the total number of words\n    return sum(word_counts.values())"}
{"task_id": "BigCodeBench/288", "solution": "import collections\nimport json\nimport os\ndef task_func(directory_path: str) -> dict:\n    \"\"\"\n    Counts the total appearances of all keys in all JSON files in the specified directory and returns a dictionary with the keys from the JSON files as keys and their respective counts as values.\n    \"\"\"\n    # Initialize an empty dictionary to store the counts\n    counts = collections.defaultdict(int)\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory_path):\n        # Check if the file is a JSON file\n        if filename.endswith('.json'):\n            # Open the file and load the JSON data\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                data = json.load(file)\n\n            # Count the keys in the JSON data\n            for key in data.keys():\n                counts[key] += 1\n\n    # Return the counts dictionary\n    return counts", "raw_solution": "import collections\nimport json\nimport os\n\ndef task_func(directory_path: str) -> dict:\n    \"\"\"\n    Counts the total appearances of all keys in all JSON files in the specified directory and returns a dictionary with the keys from the JSON files as keys and their respective counts as values.\n    \"\"\"\n    # Initialize an empty dictionary to store the counts\n    counts = collections.defaultdict(int)\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory_path):\n        # Check if the file is a JSON file\n        if filename.endswith('.json'):\n            # Open the file and load the JSON data\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                data = json.load(file)\n\n            # Count the keys in the JSON data\n            for key in data.keys():\n                counts[key] += 1\n\n    # Return the counts dictionary\n    return counts"}
{"task_id": "BigCodeBench/289", "solution": "import tensorflow as tf\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(X, y, n_splits, batch_size, epochs):\n    # Preprocessing\n    scaler = MinMaxScaler()\n    X = scaler.fit_transform(X)\n\n    # K-Fold Cross Validation\n    kfold = KFold(n_splits=n_splits, shuffle=True)\n\n    # Model\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(20, activation='relu', input_shape=(X.shape[1],)),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    # List to store the history of each fold\n    histories = []\n\n    for train_index, test_index in kfold.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        # Train the model\n        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), verbose=0)\n\n        histories.append(history)\n\n    return histories", "raw_solution": "import tensorflow as tf\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(X, y, n_splits, batch_size, epochs):\n    # Preprocessing\n    scaler = MinMaxScaler()\n    X = scaler.fit_transform(X)\n\n    # K-Fold Cross Validation\n    kfold = KFold(n_splits=n_splits, shuffle=True)\n\n    # Model\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(20, activation='relu', input_shape=(X.shape[1],)),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    # List to store the history of each fold\n    histories = []\n\n    for train_index, test_index in kfold.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        # Train the model\n        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), verbose=0)\n\n        histories.append(history)\n\n    return histories"}
{"task_id": "BigCodeBench/290", "solution": "import nltk\nfrom collections import Counter\nimport os\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(directory_path):\n    \"\"\"\n    Counts the number of unique non-stop words across all '.txt' files in a specified directory.\n    \"\"\"\n    # Initialize a counter\n    word_counter = Counter()\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory_path):\n        if filename.endswith('.txt'):\n            # Read the file\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                # Tokenize the file content\n                words = nltk.word_tokenize(file.read())\n                # Count the words\n                word_counter.update(word.lower() for word in words if word.isalpha() and word.lower() not in STOPWORDS)\n\n    # Return the total count of unique non-stop words\n    return sum(word_counter.values())", "raw_solution": "import nltk\nnltk.download('stopwords')\nfrom collections import Counter\nimport os\nfrom nltk.corpus import stopwords\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(directory_path):\n    \"\"\"\n    Counts the number of unique non-stop words across all '.txt' files in a specified directory.\n    \"\"\"\n    # Initialize a counter\n    word_counter = Counter()\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory_path):\n        if filename.endswith('.txt'):\n            # Read the file\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                # Tokenize the file content\n                words = nltk.word_tokenize(file.read())\n                # Count the words\n                word_counter.update(word.lower() for word in words if word.isalpha() and word.lower() not in STOPWORDS)\n\n    # Return the total count of unique non-stop words\n    return sum(word_counter.values())"}
{"task_id": "BigCodeBench/291", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\ndef task_func(mu, sigma, seed=0):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, 1000)\n\n    fig, ax = plt.subplots()\n    sns.histplot(data, ax=ax, kde=True, color=\"skyblue\")\n    ax.set_title(f\"Normal Distribution with Mean {mu} and Standard Deviation {sigma}\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ndef task_func(mu, sigma, seed=0):\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, 1000)\n\n    fig, ax = plt.subplots()\n    sns.histplot(data, ax=ax, kde=True, color=\"skyblue\")\n    ax.set_title(f\"Normal Distribution with Mean {mu} and Standard Deviation {sigma}\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return ax"}
{"task_id": "BigCodeBench/292", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Scale 'Age' and 'Income' columns between 0 and 1 for each group by 'id'\n    df_scaled = df.groupby('id').apply(lambda x: MinMaxScaler().fit_transform(x[['Age', 'Income']]))\n    df_scaled.reset_index(drop=True, inplace=True)\n\n    # Create a histogram of the 'Income' column after scaling\n    plt.hist(df_scaled['Income'], bins=10, edgecolor='black')\n    plt.xlabel('Income')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Income')\n\n    return (df_scaled, plt)", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    # Scale 'Age' and 'Income' columns between 0 and 1 for each group by 'id'\n    df_scaled = df.groupby('id').apply(lambda x: MinMaxScaler().fit_transform(x[['Age', 'Income']]))\n    df_scaled.reset_index(drop=True, inplace=True)\n\n    # Create a histogram of the 'Income' column after scaling\n    plt.hist(df_scaled['Income'], bins=10, edgecolor='black')\n    plt.xlabel('Income')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Income')\n\n    return (df_scaled, plt)"}
{"task_id": "BigCodeBench/293", "solution": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(elements, subset_size):\n    # Generate all subsets of a given size\n    subsets = list(itertools.combinations(elements, subset_size))\n\n    # Calculate the sum of each subset\n    sums = [sum(subset) for subset in subsets]\n\n    # Draw a histogram of the sums of the subsets\n    fig, ax = plt.subplots()\n    ax.hist(sums, bins='auto')\n\n    return ax, subsets, sums", "raw_solution": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(elements, subset_size):\n    # Generate all subsets of a given size\n    subsets = list(itertools.combinations(elements, subset_size))\n\n    # Calculate the sum of each subset\n    sums = [sum(subset) for subset in subsets]\n\n    # Draw a histogram of the sums of the subsets\n    fig, ax = plt.subplots()\n    ax.hist(sums, bins='auto')\n\n    return ax, subsets, sums"}
{"task_id": "BigCodeBench/294", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    \"\"\"\n    Standardize the 'age' and 'income' columns for each group by 'id' in a Pandas DataFrame.\n    \"\"\"\n    # Check if the DataFrame has the required columns\n    if not all(col in df.columns for col in ['id', 'age', 'income']):\n        raise ValueError(\"The DataFrame must have 'id', 'age', and 'income' columns.\")\n\n    # Group by 'id' and standardize 'age' and 'income'\n    df_grouped = df.groupby('id')[['age', 'income']].apply(lambda x: (x - x.mean()) / x.std())\n\n    return df_grouped", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    \"\"\"\n    Standardize the 'age' and 'income' columns for each group by 'id' in a Pandas DataFrame.\n    \"\"\"\n    # Check if the DataFrame has the required columns\n    if not all(col in df.columns for col in ['id', 'age', 'income']):\n        raise ValueError(\"The DataFrame must have 'id', 'age', and 'income' columns.\")\n\n    # Group by 'id' and standardize 'age' and 'income'\n    df_grouped = df.groupby('id')[['age', 'income']].apply(lambda x: (x - x.mean()) / x.std())\n\n    return df_grouped"}
{"task_id": "BigCodeBench/295", "solution": "import itertools\nimport statistics\nfrom collections import Counter\ndef task_func(elements, subset_size):\n    # Generate all subsets of a given size\n    subsets = list(itertools.combinations(elements, subset_size))\n\n    # Calculate the sum of each subset\n    sums = [sum(subset) for subset in subsets]\n\n    # Calculate the mean, median, and mode of the sums\n    mean = statistics.mean(sums)\n    median = statistics.median(sums)\n    mode = statistics.mode(sums)\n\n    # Return the results in a dictionary\n    return {'mean': mean, 'median': median, 'mode': mode}", "raw_solution": "import itertools\nimport statistics\nfrom collections import Counter\n\ndef task_func(elements, subset_size):\n    # Generate all subsets of a given size\n    subsets = list(itertools.combinations(elements, subset_size))\n\n    # Calculate the sum of each subset\n    sums = [sum(subset) for subset in subsets]\n\n    # Calculate the mean, median, and mode of the sums\n    mean = statistics.mean(sums)\n    median = statistics.median(sums)\n    mode = statistics.mode(sums)\n\n    # Return the results in a dictionary\n    return {'mean': mean, 'median': median, 'mode': mode}\n\n# Test the function"}
{"task_id": "BigCodeBench/296", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    if 'value' not in df.columns:\n        return plt.figure().gca()\n\n    counts = df['value'].value_counts()\n\n    fig, ax = plt.subplots()\n    ax.bar(counts.index, counts.values)\n    ax.set_title('Value Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Count')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    if 'value' not in df.columns:\n        return plt.figure().gca()\n\n    counts = df['value'].value_counts()\n\n    fig, ax = plt.subplots()\n    ax.bar(counts.index, counts.values)\n    ax.set_title('Value Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Count')\n\n    return ax"}
{"task_id": "BigCodeBench/297", "solution": "import itertools\nimport collections\ndef task_func(elements, subset_size):\n    subsets = list(itertools.combinations(elements, subset_size))\n    sums = [sum(subset) for subset in subsets]\n    count_dict = collections.Counter(sums)\n    return count_dict", "raw_solution": "import itertools\nimport collections\n\ndef task_func(elements, subset_size):\n    subsets = list(itertools.combinations(elements, subset_size))\n    sums = [sum(subset) for subset in subsets]\n    count_dict = collections.Counter(sums)\n    return count_dict"}
{"task_id": "BigCodeBench/298", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    \"\"\"\n    Splits a list in the 'Value' column of a DataFrame into several columns, scales these columns using StandardScaler,\n    and optionally returned the scaled data using a bar chart. The 'Date' column is converted to datetime and used as the index in the plot.\n    \"\"\"\n    if not all(col in df.columns for col in COLUMNS):\n        raise KeyError(\"The DataFrame does not have the 'Date' and 'Value' columns.\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    scaler = StandardScaler()\n    df[['Scaled Value']] = scaler.fit_transform(df[['Value']])\n\n    if plot:\n        fig, ax = plt.subplots(figsize=(10, 6))\n        df[['Scaled Value']].plot(kind='bar', ax=ax)\n        ax.set_title('Scaled Values Over Time')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Scaled Value')\n        plt.show()\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Splits a list in the 'Value' column of a DataFrame into several columns, scales these columns using StandardScaler,\n    and optionally returned the scaled data using a bar chart. The 'Date' column is converted to datetime and used as the index in the plot.\n    \"\"\"\n    if not all(col in df.columns for col in COLUMNS):\n        raise KeyError(\"The DataFrame does not have the 'Date' and 'Value' columns.\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    scaler = StandardScaler()\n    df[['Scaled Value']] = scaler.fit_transform(df[['Value']])\n\n    if plot:\n        fig, ax = plt.subplots(figsize=(10, 6))\n        df[['Scaled Value']].plot(kind='bar', ax=ax)\n        ax.set_title('Scaled Values Over Time')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Scaled Value')\n        plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/299", "solution": "from itertools import combinations\nfrom pandas import Series\nimport math\ndef task_func(elements, subset_size, top_n=2):\n    if subset_size > len(elements):\n        return 1, [1]*top_n\n    if subset_size == 0:\n        return 1, [1]*top_n\n\n    subsets = list(combinations(elements, subset_size))\n    sums = [sum(subset) for subset in subsets]\n    sums.sort(reverse=True)\n\n    product = math.prod(sums[:top_n])\n    top_sums = sums[:top_n]\n\n    return product, Series(top_sums)", "raw_solution": "from itertools import combinations\nfrom pandas import Series\nimport math\n\ndef task_func(elements, subset_size, top_n=2):\n    if subset_size > len(elements):\n        return 1, [1]*top_n\n    if subset_size == 0:\n        return 1, [1]*top_n\n\n    subsets = list(combinations(elements, subset_size))\n    sums = [sum(subset) for subset in subsets]\n    sums.sort(reverse=True)\n\n    product = math.prod(sums[:top_n])\n    top_sums = sums[:top_n]\n\n    return product, Series(top_sums)"}
{"task_id": "BigCodeBench/300", "solution": "import pandas as pd\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if 'Date' not in df.columns or 'Value' not in df.columns:\n        raise KeyError(\"DataFrame does not have the 'Date' and 'Value' columns\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    df_zscore = df['Value'].apply(zscore).to_frame('Z-Score')\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    df_zscore.plot(kind='box', ax=ax)\n    ax.set_title('Z-Scores Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Z-Score')\n\n    return df_zscore, fig", "raw_solution": "import pandas as pd\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if 'Date' not in df.columns or 'Value' not in df.columns:\n        raise KeyError(\"DataFrame does not have the 'Date' and 'Value' columns\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    df_zscore = df['Value'].apply(zscore).to_frame('Z-Score')\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    df_zscore.plot(kind='box', ax=ax)\n    ax.set_title('Z-Scores Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Z-Score')\n\n    return df_zscore, fig"}
{"task_id": "BigCodeBench/301", "solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Convert the date to the target timezone\n    date = date.replace(tzinfo=pytz.timezone(from_tz))\n    date = date.astimezone(pytz.timezone(to_tz))\n\n    # Calculate the years since the closest solar cycle year\n    years_since_solar_cycle = date.year - SOLAR_CYCLE_YEARS.min()\n\n    # Calculate the solar activity using a cosine function\n    solar_activity = 0.5 * (1 + math.cos(2 * math.pi * years_since_solar_cycle / 11))\n\n    return solar_activity", "raw_solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\n\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\n\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Convert the date to the target timezone\n    date = date.replace(tzinfo=pytz.timezone(from_tz))\n    date = date.astimezone(pytz.timezone(to_tz))\n\n    # Calculate the years since the closest solar cycle year\n    years_since_solar_cycle = date.year - SOLAR_CYCLE_YEARS.min()\n\n    # Calculate the solar activity using a cosine function\n    solar_activity = 0.5 * (1 + math.cos(2 * math.pi * years_since_solar_cycle / 11))\n\n    return solar_activity\n\n# Test the function"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally visualizes\n    the correlation matrix using a heatmap.\n    \"\"\"\n    if df.empty or 'Value' not in df.columns:\n        raise ValueError(\"Invalid DataFrame input. It should not be empty and contain 'Value' column.\")\n\n    # Split lists in the 'Value' column into separate columns\n    df[COLUMNS[1]] = df[COLUMNS[1]].apply(lambda x: pd.Series(x))\n    df = pd.concat([df.drop([COLUMNS[1]], axis=1), df[COLUMNS[1]].apply(pd.Series).fillna(0)], axis=1)\n\n    # Calculate the Pearson correlation coefficient\n    corr_matrix = df.corr(method='pearson')\n\n    if plot:\n        # Visualize the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n\n    return corr_matrix", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally visualizes\n    the correlation matrix using a heatmap.\n    \"\"\"\n    if df.empty or 'Value' not in df.columns:\n        raise ValueError(\"Invalid DataFrame input. It should not be empty and contain 'Value' column.\")\n\n    # Split lists in the 'Value' column into separate columns\n    df[COLUMNS[1]] = df[COLUMNS[1]].apply(lambda x: pd.Series(x))\n    df = pd.concat([df.drop([COLUMNS[1]], axis=1), df[COLUMNS[1]].apply(pd.Series).fillna(0)], axis=1)\n\n    # Calculate the Pearson correlation coefficient\n    corr_matrix = df.corr(method='pearson')\n\n    if plot:\n        # Visualize the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n\n    return corr_matrix"}
{"task_id": "BigCodeBench/303", "solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\nMOON_PHASES_YEARS = np.array([1987, 1994, 2001, 2008, 2015, 2022])\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Calculates the moon phase based on the date and time, taking into account the lunar phase cycle of 7 years.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Convert the date to the target timezone\n    date = date.astimezone(pytz.timezone(to_tz))\n\n    # Calculate the number of days since the first moon phase\n    days_since_first_moon = (date.year - MOON_PHASES_YEARS[0]) * 365.25\n\n    # Calculate the phase of the moon\n    phase = (date.timetuple().tm_yday + days_since_first_moon) % 29.53058867\n\n    # Normalize the phase to a value between 0 and 1\n    phase /= 29.53058867\n\n    return phase", "raw_solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\n\nMOON_PHASES_YEARS = np.array([1987, 1994, 2001, 2008, 2015, 2022])\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Calculates the moon phase based on the date and time, taking into account the lunar phase cycle of 7 years.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Convert the date to the target timezone\n    date = date.astimezone(pytz.timezone(to_tz))\n\n    # Calculate the number of days since the first moon phase\n    days_since_first_moon = (date.year - MOON_PHASES_YEARS[0]) * 365.25\n\n    # Calculate the phase of the moon\n    phase = (date.timetuple().tm_yday + days_since_first_moon) % 29.53058867\n\n    # Normalize the phase to a value between 0 and 1\n    phase /= 29.53058867\n\n    return phase\n\n# Test the function"}
{"task_id": "BigCodeBench/304", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if df.empty:\n        return (0.0, 0.0)\n\n    # Convert lists to separate columns\n    df = pd.concat([df.drop(['numbers'], axis=1), df['numbers'].apply(pd.Series)], axis=1)\n\n    # Perform PCA\n    pca = PCA(n_components=len(df.columns))\n    pca.fit(df)\n\n    # Get explained variance ratio\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n    ax.set_title('Explained Variance Ratio of Principal Components')\n    ax.set_xlabel('Principal Component')\n    ax.set_ylabel('Explained Variance Ratio')\n\n    return (explained_variance_ratio, ax)", "raw_solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if df.empty:\n        return (0.0, 0.0)\n\n    # Convert lists to separate columns\n    df = pd.concat([df.drop(['numbers'], axis=1), df['numbers'].apply(pd.Series)], axis=1)\n\n    # Perform PCA\n    pca = PCA(n_components=len(df.columns))\n    pca.fit(df)\n\n    # Get explained variance ratio\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n    ax.set_title('Explained Variance Ratio of Principal Components')\n    ax.set_xlabel('Principal Component')\n    ax.set_ylabel('Explained Variance Ratio')\n\n    return (explained_variance_ratio, ax)"}
{"task_id": "BigCodeBench/305", "solution": "from collections import Counter\nimport itertools\nimport random\nALPHABET = 'abcdefghijklmnopqrstuvwxyz'\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    for i, lst in enumerate(list_of_lists):\n        if not lst:\n            list_of_lists[i] = random.sample(ALPHABET, 5)\n    flattened_list = list(itertools.chain.from_iterable(list_of_lists))\n    counter = Counter(flattened_list)\n    return counter", "raw_solution": "from collections import Counter\nimport itertools\nimport random\n\n# Constants\nALPHABET = 'abcdefghijklmnopqrstuvwxyz'\n\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    for i, lst in enumerate(list_of_lists):\n        if not lst:\n            list_of_lists[i] = random.sample(ALPHABET, 5)\n    flattened_list = list(itertools.chain.from_iterable(list_of_lists))\n    counter = Counter(flattened_list)\n    return counter"}
{"task_id": "BigCodeBench/306", "solution": "import os\nimport logging\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The specified directory {directory} does not exist.\")\n\n    removed_files = []\n    count = 0\n    for filename in os.listdir(directory):\n        if 'jquery' in filename:\n            file_path = os.path.join(directory, filename)\n            if os.path.isfile(file_path):\n                os.remove(file_path)\n                removed_files.append(filename)\n                count += 1\n\n    logging.basicConfig(filename='jquery_removal.log', level=logging.INFO)\n    logging.info(f\"Number of files removed: {count}\")\n    logging.info(f\"Names of removed files: {removed_files}\")\n\n    return (count, removed_files)", "raw_solution": "import os\nimport logging\n\ndef task_func(directory):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The specified directory {directory} does not exist.\")\n\n    removed_files = []\n    count = 0\n    for filename in os.listdir(directory):\n        if 'jquery' in filename:\n            file_path = os.path.join(directory, filename)\n            if os.path.isfile(file_path):\n                os.remove(file_path)\n                removed_files.append(filename)\n                count += 1\n\n    logging.basicConfig(filename='jquery_removal.log', level=logging.INFO)\n    logging.info(f\"Number of files removed: {count}\")\n    logging.info(f\"Names of removed files: {removed_files}\")\n\n    return (count, removed_files)"}
{"task_id": "BigCodeBench/307", "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    combined_data = []\n    for sublist in list_of_lists:\n        if not sublist:\n            sublist = random.sample(range(0, 101), 5)\n        combined_data.extend(sublist)\n\n    plt.figure(figsize=(10, 6))\n    sns.histplot(combined_data, bins=20, kde=False)\n    plt.title('Histogram of Combined Data')\n    plt.xlabel('Data')\n    plt.ylabel('Frequency')\n    return plt.gca()", "raw_solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    combined_data = []\n    for sublist in list_of_lists:\n        if not sublist:\n            sublist = random.sample(range(0, 101), 5)\n        combined_data.extend(sublist)\n\n    plt.figure(figsize=(10, 6))\n    sns.histplot(combined_data, bins=20, kde=False)\n    plt.title('Histogram of Combined Data')\n    plt.xlabel('Data')\n    plt.ylabel('Frequency')\n    return plt.gca()"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n    # Generate random grades for each student in each subject\n    data = {field: [random.randint(0, 100) for _ in range(100)] for field in FIELDS}\n    data['Students'] = STUDENTS\n\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Calculate the average grade for each student\n    df['Average'] = df[FIELDS].mean(axis=1)\n\n    # Calculate the average grade for each subject\n    subject_averages = df[FIELDS].mean()\n\n    # Add the subject averages to the DataFrame\n    df = df.append(subject_averages, ignore_index=True)\n\n    # Rename the last row to 'Average Grade'\n    df.loc[len(df)-1, 'Students'] = 'Average Grade'\n\n    return df", "raw_solution": "import pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    # Generate random grades for each student in each subject\n    data = {field: [random.randint(0, 100) for _ in range(100)] for field in FIELDS}\n    data['Students'] = STUDENTS\n\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Calculate the average grade for each student\n    df['Average'] = df[FIELDS].mean(axis=1)\n\n    # Calculate the average grade for each subject\n    subject_averages = df[FIELDS].mean()\n\n    # Add the subject averages to the DataFrame\n    df = df.append(subject_averages, ignore_index=True)\n\n    # Rename the last row to 'Average Grade'\n    df.loc[len(df)-1, 'Students'] = 'Average Grade'\n\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/309", "solution": "import numpy as np\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(list_of_lists, seed=42):\n    random.seed(seed)\n    for i in range(len(list_of_lists)):\n        if not list_of_lists[i]:\n            list_of_lists[i] = [random.randint(0, 100) for _ in range(5)]\n    scaler = MinMaxScaler()\n    return scaler.fit_transform(list_of_lists).tolist()\nlist_of_lists = [[1, 2, 3], [], [100, 200, 300], [50]]", "raw_solution": "import numpy as np\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(list_of_lists, seed=42):\n    random.seed(seed)\n    for i in range(len(list_of_lists)):\n        if not list_of_lists[i]:\n            list_of_lists[i] = [random.randint(0, 100) for _ in range(5)]\n    scaler = MinMaxScaler()\n    return scaler.fit_transform(list_of_lists).tolist()\n\n# Test the function\nlist_of_lists = [[1, 2, 3], [], [100, 200, 300], [50]]"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Generate data\n    data = []\n    for i in range(PEOPLE_COUNT):\n        name = f'Person{i+1}'\n        age = random.randint(18, 80)\n        height = round(random.normalvariate(170, 10), 2)\n        weight = round(random.normalvariate(70, 10), 2)\n        data.append([name, age, height, weight])\n\n    # Calculate averages\n    avg_age = round(mean(row[1] for row in data), 2)\n    avg_height = round(mean(row[2] for row in data), 2)\n    avg_weight = round(mean(row[3] for row in data), 2)\n    data.append([f'Average', avg_age, avg_height, avg_weight])\n\n    # Write to CSV\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    return filename", "raw_solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    # Generate data\n    data = []\n    for i in range(PEOPLE_COUNT):\n        name = f'Person{i+1}'\n        age = random.randint(18, 80)\n        height = round(random.normalvariate(170, 10), 2)\n        weight = round(random.normalvariate(70, 10), 2)\n        data.append([name, age, height, weight])\n\n    # Calculate averages\n    avg_age = round(mean(row[1] for row in data), 2)\n    avg_height = round(mean(row[2] for row in data), 2)\n    avg_weight = round(mean(row[3] for row in data), 2)\n    data.append([f'Average', avg_age, avg_height, avg_weight])\n\n    # Write to CSV\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    return filename"}
{"task_id": "BigCodeBench/311", "solution": "import numpy as np\nimport random\nfrom scipy import stats\nfrom typing import List, Dict, Union\ndef task_func(list_of_lists: List[List[Union[int, float]]], size: int = 5, seed: int = 0) -> Dict[str, float]:\n    \"\"\"\n    Calculates the mean, median, and mode of values in a list of lists.\n    If a list is empty, it fills it with `size` random integers between 0 and 100,\n    and then calculates the statistics.\n    \"\"\"\n    random.seed(seed)\n\n    # Fill empty lists with random integers\n    list_of_lists = [random.sample(range(0, 101), size) if not lst else lst for lst in list_of_lists]\n\n    # Flatten the list of lists\n    flat_list = [item for sublist in list_of_lists for item in sublist]\n\n    # Calculate statistics\n    mean_val = np.mean(flat_list)\n    median_val = np.median(flat_list)\n    mode_val = stats.mode(flat_list)[0][0]\n\n    return {'mean': mean_val, 'median': median_val, 'mode': mode_val}", "raw_solution": "import numpy as np\nimport random\nfrom scipy import stats\nfrom typing import List, Dict, Union\n\ndef task_func(list_of_lists: List[List[Union[int, float]]], size: int = 5, seed: int = 0) -> Dict[str, float]:\n    \"\"\"\n    Calculates the mean, median, and mode of values in a list of lists.\n    If a list is empty, it fills it with `size` random integers between 0 and 100,\n    and then calculates the statistics.\n    \"\"\"\n    random.seed(seed)\n\n    # Fill empty lists with random integers\n    list_of_lists = [random.sample(range(0, 101), size) if not lst else lst for lst in list_of_lists]\n\n    # Flatten the list of lists\n    flat_list = [item for sublist in list_of_lists for item in sublist]\n\n    # Calculate statistics\n    mean_val = np.mean(flat_list)\n    median_val = np.median(flat_list)\n    mode_val = stats.mode(flat_list)[0][0]\n\n    return {'mean': mean_val, 'median': median_val, 'mode': mode_val}"}
{"task_id": "BigCodeBench/312", "solution": "import random\nimport matplotlib.pyplot as plt\nimport numpy as np\nDISTRIBUTION_SIZE = 1000\ndef task_func(bins=30):\n    \"\"\"\n    Generates a Gaussian distribution and plots its histogram.\n    The function returns a tuple containing the distribution list and the Axes patch object of the histogram plot.\n    \"\"\"\n    # Generate a Gaussian distribution\n    distribution = np.random.normal(0, 1, DISTRIBUTION_SIZE)\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the histogram\n    ax.hist(distribution, bins=bins)\n\n    # Return the distribution and the Axes patch object\n    return distribution, ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nDISTRIBUTION_SIZE = 1000\n\ndef task_func(bins=30):\n    \"\"\"\n    Generates a Gaussian distribution and plots its histogram.\n    The function returns a tuple containing the distribution list and the Axes patch object of the histogram plot.\n    \"\"\"\n    # Generate a Gaussian distribution\n    distribution = np.random.normal(0, 1, DISTRIBUTION_SIZE)\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the histogram\n    ax.hist(distribution, bins=bins)\n\n    # Return the distribution and the Axes patch object\n    return distribution, ax\n\n# Test the function\ndistribution, ax = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    \"\"\"\n    Organizes files in a directory based on the first text that is not enclosed in square brackets.\n    Moves the files to subdirectories named after this text. If no matching text is found, the file is not moved.\n    \"\"\"\n    # Initialize an empty dictionary to store the subdirectories and their corresponding files\n    subdirs = {}\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Use a regular expression to find the first text that is not enclosed in square brackets\n        match = re.search(r'\\[([^\\]]+)\\]', filename)\n\n        # If a match is found, create a subdirectory with the name of the match\n        if match:\n            subdir_name = match.group(1)\n            subdir_path = os.path.join(directory, subdir_name)\n\n            # If the subdirectory does not exist, create it\n            if not os.path.exists(subdir_path):\n                os.makedirs(subdir_path)\n\n            # Add the file to the list of files in the subdirectory\n            if subdir_name in subdirs:\n                subdirs[subdir_name].append(filename)\n            else:\n                subdirs[subdir_name] = [filename]\n\n            # Move the file to the subdirectory\n            shutil.move(os.path.join(directory, filename), subdir_path)\n\n    # Return the directory path and the dictionary of subdirectories and their files\n    return directory, subdirs", "raw_solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    \"\"\"\n    Organizes files in a directory based on the first text that is not enclosed in square brackets.\n    Moves the files to subdirectories named after this text. If no matching text is found, the file is not moved.\n    \"\"\"\n    # Initialize an empty dictionary to store the subdirectories and their corresponding files\n    subdirs = {}\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Use a regular expression to find the first text that is not enclosed in square brackets\n        match = re.search(r'\\[([^\\]]+)\\]', filename)\n\n        # If a match is found, create a subdirectory with the name of the match\n        if match:\n            subdir_name = match.group(1)\n            subdir_path = os.path.join(directory, subdir_name)\n\n            # If the subdirectory does not exist, create it\n            if not os.path.exists(subdir_path):\n                os.makedirs(subdir_path)\n\n            # Add the file to the list of files in the subdirectory\n            if subdir_name in subdirs:\n                subdirs[subdir_name].append(filename)\n            else:\n                subdirs[subdir_name] = [filename]\n\n            # Move the file to the subdirectory\n            shutil.move(os.path.join(directory, filename), subdir_path)\n\n    # Return the directory path and the dictionary of subdirectories and their files\n    return directory, subdirs"}
{"task_id": "BigCodeBench/314", "solution": "import socket\nimport ssl\nimport http.client\ndef task_func(SERVER_NAME, SERVER_PORT, path):\n    try:\n        context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2)\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock = context.wrap_socket(sock, server_hostname=SERVER_NAME)\n        sock.connect((SERVER_NAME, SERVER_PORT))\n        request = f'GET {path} HTTP/1.1\\r\\nHost: {SERVER_NAME}\\r\\n\\r\\n'\n        sock.sendall(request.encode())\n        response = b''\n        while True:\n            chunk = sock.recv(4096)\n            if chunk:\n                response += chunk\n            else:\n                break\n        sock.close()\n        return response.decode()\n    except ssl.SSLError as e:\n        raise Exception(f'SSL Error: {str(e)}')", "raw_solution": "import socket\nimport ssl\nimport http.client\n\ndef task_func(SERVER_NAME, SERVER_PORT, path):\n    try:\n        context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2)\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock = context.wrap_socket(sock, server_hostname=SERVER_NAME)\n        sock.connect((SERVER_NAME, SERVER_PORT))\n        request = f'GET {path} HTTP/1.1\\r\\nHost: {SERVER_NAME}\\r\\n\\r\\n'\n        sock.sendall(request.encode())\n        response = b''\n        while True:\n            chunk = sock.recv(4096)\n            if chunk:\n                response += chunk\n            else:\n                break\n        sock.close()\n        return response.decode()\n    except ssl.SSLError as e:\n        raise Exception(f'SSL Error: {str(e)}')"}
{"task_id": "BigCodeBench/315", "solution": "import os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.exceptions import HTTPError\ndef task_func(dir, api_key, recipient_email):\n    try:\n        files = os.listdir(dir)\n    except FileNotFoundError:\n        return False\n\n    message = Mail(\n        from_email='from_email@example.com',\n        to_emails=recipient_email,\n        subject='List of Files',\n        plain_text_content='\\n'.join(files))\n\n    try:\n        sg = SendGridAPIClient(api_key)\n        response = sg.send(message)\n        return 200 <= response.status_code < 300\n    except HTTPError as e:\n        raise e\n    except Exception as e:\n        raise e", "raw_solution": "import os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.exceptions import HTTPError\n\ndef task_func(dir, api_key, recipient_email):\n    try:\n        files = os.listdir(dir)\n    except FileNotFoundError:\n        return False\n\n    message = Mail(\n        from_email='from_email@example.com',\n        to_emails=recipient_email,\n        subject='List of Files',\n        plain_text_content='\\n'.join(files))\n\n    try:\n        sg = SendGridAPIClient(api_key)\n        response = sg.send(message)\n        return 200 <= response.status_code < 300\n    except HTTPError as e:\n        raise e\n    except Exception as e:\n        raise e"}
{"task_id": "BigCodeBench/316", "solution": "import pandas as pd\nimport random\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\ndef task_func(value_range=(0, 100)):\n    \"\"\"\n    Generates a category distribution within a specified range and returns as a DataFrame.\n    The function outputs a DataFrame with two columns: 'Category' (category names) and 'Count' (count of each category).\n    \"\"\"\n    # Generate a random count for each category within the specified range\n    counts = [random.randint(value_range[0], value_range[1]) for _ in range(len(CATEGORIES))]\n\n    # Create a DataFrame from the categories and counts\n    df = pd.DataFrame({\n        'Category': CATEGORIES,\n        'Count': counts\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(value_range=(0, 100)):\n    \"\"\"\n    Generates a category distribution within a specified range and returns as a DataFrame.\n    The function outputs a DataFrame with two columns: 'Category' (category names) and 'Count' (count of each category).\n    \"\"\"\n    # Generate a random count for each category within the specified range\n    counts = [random.randint(value_range[0], value_range[1]) for _ in range(len(CATEGORIES))]\n\n    # Create a DataFrame from the categories and counts\n    df = pd.DataFrame({\n        'Category': CATEGORIES,\n        'Count': counts\n    })\n\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/317", "solution": "import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\ndef task_func(example_str):\n    # Extract texts not enclosed in square brackets\n    text = re.sub(r'\\[.*?\\]', '', example_str)\n\n    # Calculate TF-IDF values\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform([text])\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Convert TF-IDF matrix to dictionary\n    tfidf_dict = {}\n    for col in range(tfidf_matrix.shape[1]):\n        tfidf_dict[feature_names[col]] = tfidf_matrix[0, col]\n\n    return tfidf_dict", "raw_solution": "import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\ndef task_func(example_str):\n    # Extract texts not enclosed in square brackets\n    text = re.sub(r'\\[.*?\\]', '', example_str)\n\n    # Calculate TF-IDF values\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform([text])\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Convert TF-IDF matrix to dictionary\n    tfidf_dict = {}\n    for col in range(tfidf_matrix.shape[1]):\n        tfidf_dict[feature_names[col]] = tfidf_matrix[0, col]\n\n    return tfidf_dict"}
{"task_id": "BigCodeBench/318", "solution": "import random\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(points_count=1000, radius=1):\n    # Generate points_count random points within a circle of radius radius\n    points = [(radius * math.cos(theta), radius * math.sin(theta)) for theta in [random.uniform(0, 2 * math.pi) for _ in range(points_count)]]\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(*zip(*points))\n\n    # Set aspect ratio to 'equal'\n    ax.set_aspect('equal')\n\n    return ax", "raw_solution": "import random\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(points_count=1000, radius=1):\n    # Generate points_count random points within a circle of radius radius\n    points = [(radius * math.cos(theta), radius * math.sin(theta)) for theta in [random.uniform(0, 2 * math.pi) for _ in range(points_count)]]\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(*zip(*points))\n\n    # Set aspect ratio to 'equal'\n    ax.set_aspect('equal')\n\n    return ax"}
{"task_id": "BigCodeBench/319", "solution": "import re\nimport matplotlib.pyplot as plt\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\ndef task_func(example_str, top_n=30):\n    # Extract all texts that are not enclosed in square brackets\n    text = re.sub(r'\\[.*?\\]', '', example_str)\n\n    # Tokenize the text into words\n    words = word_tokenize(text)\n\n    # Calculate the frequency distribution of the words\n    fdist = FreqDist(words)\n\n    # Plot the frequency distribution\n    fdist.plot(top_n)\n\n    # Get the top_n most common words and their frequencies\n    top_words = dict(Counter(words).most_common(top_n))\n\n    return plt.gca(), top_words", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\n\ndef task_func(example_str, top_n=30):\n    # Extract all texts that are not enclosed in square brackets\n    text = re.sub(r'\\[.*?\\]', '', example_str)\n\n    # Tokenize the text into words\n    words = word_tokenize(text)\n\n    # Calculate the frequency distribution of the words\n    fdist = FreqDist(words)\n\n    # Plot the frequency distribution\n    fdist.plot(top_n)\n\n    # Get the top_n most common words and their frequencies\n    top_words = dict(Counter(words).most_common(top_n))\n\n    return plt.gca(), top_words"}
{"task_id": "BigCodeBench/320", "solution": "import subprocess\nimport os\nimport random\ndef task_func(directory, file_list):\n    if not file_list:\n        return None\n\n    random_file = random.choice(file_list)\n    file_path = os.path.join(directory, random_file)\n\n    if not os.path.isfile(file_path):\n        return None\n\n    process = subprocess.Popen(['python', file_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n\n    if process.returncode == 0:\n        return process.returncode\n    else:\n        return None", "raw_solution": "import subprocess\nimport os\nimport random\n\ndef task_func(directory, file_list):\n    if not file_list:\n        return None\n\n    random_file = random.choice(file_list)\n    file_path = os.path.join(directory, random_file)\n\n    if not os.path.isfile(file_path):\n        return None\n\n    process = subprocess.Popen(['python', file_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n\n    if process.returncode == 0:\n        return process.returncode\n    else:\n        return None"}
{"task_id": "BigCodeBench/321", "solution": "import pandas as pd\nimport re\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Extract names not surrounded by square brackets\n    names = re.findall(r'\\[[^\\]]*\\]|([^\\s\\[\\]]+)', text)\n    names = [name for name in names if name]  # Remove empty strings\n\n    # Count the frequency of each name\n    freqs = pd.Series(names).value_counts()\n\n    # Create a bar chart of the name frequencies\n    if len(freqs) > 0:\n        plot = freqs.plot(kind='bar')\n    else:\n        plot = None\n\n    # Calculate skewness and kurtosis\n    skewness = freqs.skew() if len(freqs) > 0 else None\n    kurtosis = freqs.kurt() if len(freqs) > 0 else None\n\n    return freqs, plot, skewness, kurtosis", "raw_solution": "import pandas as pd\nimport re\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Extract names not surrounded by square brackets\n    names = re.findall(r'\\[[^\\]]*\\]|([^\\s\\[\\]]+)', text)\n    names = [name for name in names if name]  # Remove empty strings\n\n    # Count the frequency of each name\n    freqs = pd.Series(names).value_counts()\n\n    # Create a bar chart of the name frequencies\n    if len(freqs) > 0:\n        plot = freqs.plot(kind='bar')\n    else:\n        plot = None\n\n    # Calculate skewness and kurtosis\n    skewness = freqs.skew() if len(freqs) > 0 else None\n    kurtosis = freqs.kurt() if len(freqs) > 0 else None\n\n    return freqs, plot, skewness, kurtosis"}
{"task_id": "BigCodeBench/322", "solution": "import subprocess\nimport os\nimport shutil\nimport sys\nDIRECTORY = 'c:\\Program Files\\VMware\\VMware Server'\nBACKUP_DIRECTORY = 'c:\\Program Files\\VMware\\VMware Server\\Backup'\ndef task_func(filename):\n    # Check if the backup directory exists, if not, create it\n    if not os.path.exists(BACKUP_DIRECTORY):\n        os.makedirs(BACKUP_DIRECTORY)\n\n    # Construct the full path to the file\n    file_path = os.path.join(DIRECTORY, filename)\n\n    # Check if the file exists\n    if not os.path.isfile(file_path):\n        print(f\"File {file_path} does not exist.\")\n        return -1\n\n    # Construct the full path to the backup file\n    backup_file_path = os.path.join(BACKUP_DIRECTORY, filename)\n\n    # Copy the file to the backup directory\n    try:\n        shutil.copy2(file_path, backup_file_path)\n    except Exception as e:\n        print(f\"Backup failed: {str(e)}\")\n        return -1\n\n    # Execute the file as a subprocess\n    try:\n        exit_code = subprocess.call([sys.executable, backup_file_path])\n        return exit_code\n    except Exception as e:\n        print(f\"Execution failed: {str(e)}\")\n        return -1", "raw_solution": "import subprocess\nimport os\nimport shutil\nimport sys\n\n# Constants\nDIRECTORY = 'c:\\Program Files\\VMware\\VMware Server'\nBACKUP_DIRECTORY = 'c:\\Program Files\\VMware\\VMware Server\\Backup'\n\ndef task_func(filename):\n    # Check if the backup directory exists, if not, create it\n    if not os.path.exists(BACKUP_DIRECTORY):\n        os.makedirs(BACKUP_DIRECTORY)\n\n    # Construct the full path to the file\n    file_path = os.path.join(DIRECTORY, filename)\n\n    # Check if the file exists\n    if not os.path.isfile(file_path):\n        print(f\"File {file_path} does not exist.\")\n        return -1\n\n    # Construct the full path to the backup file\n    backup_file_path = os.path.join(BACKUP_DIRECTORY, filename)\n\n    # Copy the file to the backup directory\n    try:\n        shutil.copy2(file_path, backup_file_path)\n    except Exception as e:\n        print(f\"Backup failed: {str(e)}\")\n        return -1\n\n    # Execute the file as a subprocess\n    try:\n        exit_code = subprocess.call([sys.executable, backup_file_path])\n        return exit_code\n    except Exception as e:\n        print(f\"Execution failed: {str(e)}\")\n        return -1\n\n# Test the function"}
{"task_id": "BigCodeBench/323", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.mixture import GaussianMixture\ndef task_func(text, num_gaussians=1, seed=42):\n    if num_gaussians <= 0:\n        raise ValueError(\"num_gaussians must be greater than 0\")\n\n    # Extract names from the text\n    names = re.findall(r'\\[[^\\]]*\\]', text)\n    names = [name.strip('[]') for name in names]\n\n    # Tokenize the names into words\n    words = re.findall(r'\\b\\w+\\b', ' '.join(names))\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    # Fit a mixture of num_gaussians 1-D Gaussian distributions to the word frequencies\n    gmm = GaussianMixture(n_components=num_gaussians, random_state=seed)\n    X = np.array(list(word_freq.values())).reshape(-1, 1)\n    gmm.fit(X)\n\n    # Return the means and variances of the fitted Gaussians\n    means = gmm.means_.flatten()\n    variances = gmm.covariances_.flatten()\n\n    return {'word_freq': word_freq, 'means': means, 'variances': variances}", "raw_solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.mixture import GaussianMixture\n\ndef task_func(text, num_gaussians=1, seed=42):\n    if num_gaussians <= 0:\n        raise ValueError(\"num_gaussians must be greater than 0\")\n\n    # Extract names from the text\n    names = re.findall(r'\\[[^\\]]*\\]', text)\n    names = [name.strip('[]') for name in names]\n\n    # Tokenize the names into words\n    words = re.findall(r'\\b\\w+\\b', ' '.join(names))\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    # Fit a mixture of num_gaussians 1-D Gaussian distributions to the word frequencies\n    gmm = GaussianMixture(n_components=num_gaussians, random_state=seed)\n    X = np.array(list(word_freq.values())).reshape(-1, 1)\n    gmm.fit(X)\n\n    # Return the means and variances of the fitted Gaussians\n    means = gmm.means_.flatten()\n    variances = gmm.covariances_.flatten()\n\n    return {'word_freq': word_freq, 'means': means, 'variances': variances}"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    \"\"\"\n    Runs files from a list of files as subprocesses at the same time.\n    Returns a list of exit codes of the subprocesses.\n    \"\"\"\n    exit_codes = []\n    threads = []\n\n    def run_file(file):\n        \"\"\"\n        Runs a file as a subprocess and appends the exit code to the exit_codes list.\n        \"\"\"\n        exit_codes.append(subprocess.call([\"python\", file]))\n\n    # Start a new thread for each file\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes", "raw_solution": "import subprocess\nimport time\nimport threading\n\ndef task_func(file_list):\n    \"\"\"\n    Runs files from a list of files as subprocesses at the same time.\n    Returns a list of exit codes of the subprocesses.\n    \"\"\"\n    exit_codes = []\n    threads = []\n\n    def run_file(file):\n        \"\"\"\n        Runs a file as a subprocess and appends the exit code to the exit_codes list.\n        \"\"\"\n        exit_codes.append(subprocess.call([\"python\", file]))\n\n    # Start a new thread for each file\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes"}
{"task_id": "BigCodeBench/325", "solution": "import re\nimport os\nfrom pathlib import Path\nimport glob\ndef task_func(directory_path: str, regex_pattern: str = r'(?<=\\().+?(?=\\))|\\w+') -> dict:\n    \"\"\"\n    Extracts matches from all text files in a specified directory based on a regular expression pattern.\n    It captures whatever is between parentheses as a single match, and any character outside the parentheses as individual matches in the string.\n    The function returns a dictionary where keys are file names (without path) and values are lists of matches extracted from the files.\n    \"\"\"\n    # Initialize an empty dictionary to store the results\n    results = {}\n\n    # Get a list of all text files in the directory\n    text_files = glob.glob(os.path.join(directory_path, '*.txt'))\n\n    # Iterate over each text file\n    for file_path in text_files:\n        # Get the file name (without path)\n        file_name = os.path.basename(file_path)\n\n        # Open the file and read its content\n        with open(file_path, 'r') as file:\n            content = file.read()\n\n        # Use the regular expression pattern to extract matches\n        matches = re.findall(regex_pattern, content)\n\n        # Add the matches to the results dictionary\n        results[file_name] = matches\n\n    # Return the results dictionary\n    return results", "raw_solution": "import re\nimport os\nfrom pathlib import Path\nimport glob\n\ndef task_func(directory_path: str, regex_pattern: str = r'(?<=\\().+?(?=\\))|\\w+') -> dict:\n    \"\"\"\n    Extracts matches from all text files in a specified directory based on a regular expression pattern.\n    It captures whatever is between parentheses as a single match, and any character outside the parentheses as individual matches in the string.\n    The function returns a dictionary where keys are file names (without path) and values are lists of matches extracted from the files.\n    \"\"\"\n    # Initialize an empty dictionary to store the results\n    results = {}\n\n    # Get a list of all text files in the directory\n    text_files = glob.glob(os.path.join(directory_path, '*.txt'))\n\n    # Iterate over each text file\n    for file_path in text_files:\n        # Get the file name (without path)\n        file_name = os.path.basename(file_path)\n\n        # Open the file and read its content\n        with open(file_path, 'r') as file:\n            content = file.read()\n\n        # Use the regular expression pattern to extract matches\n        matches = re.findall(regex_pattern, content)\n\n        # Add the matches to the results dictionary\n        results[file_name] = matches\n\n    # Return the results dictionary\n    return results"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    \"\"\"\n    Finds and runs all .bat files in a given directory, returning their file names and exit codes.\n    If a file cannot be executed, its exit code is None.\n    \"\"\"\n    # Get a list of all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n\n    # Initialize an empty list to store the file names and exit codes\n    results = []\n\n    # Iterate over each .bat file\n    for file in bat_files:\n        try:\n            # Run the file and capture its exit code\n            exit_code = subprocess.call([file], shell=True)\n        except Exception as e:\n            # If an error occurs, set the exit code to None\n            exit_code = None\n\n        # Append the file name and exit code to the results list\n        results.append((file, exit_code))\n\n    return results", "raw_solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    Finds and runs all .bat files in a given directory, returning their file names and exit codes.\n    If a file cannot be executed, its exit code is None.\n    \"\"\"\n    # Get a list of all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n\n    # Initialize an empty list to store the file names and exit codes\n    results = []\n\n    # Iterate over each .bat file\n    for file in bat_files:\n        try:\n            # Run the file and capture its exit code\n            exit_code = subprocess.call([file], shell=True)\n        except Exception as e:\n            # If an error occurs, set the exit code to None\n            exit_code = None\n\n        # Append the file name and exit code to the results list\n        results.append((file, exit_code))\n\n    return results"}
{"task_id": "BigCodeBench/327", "solution": "import csv\nimport re\nfrom collections import Counter\ndef task_func(file_path, regex_pattern=r'\\(.+?\\)|\\w+|[\\W_]+'):\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    matches = []\n    for row in data:\n        for cell in row:\n            matches.extend(re.findall(regex_pattern, cell))\n\n    return dict(Counter(matches))", "raw_solution": "import csv\nimport re\nfrom collections import Counter\n\ndef task_func(file_path, regex_pattern=r'\\(.+?\\)|\\w+|[\\W_]+'):\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    matches = []\n    for row in data:\n        for cell in row:\n            matches.extend(re.findall(regex_pattern, cell))\n\n    return dict(Counter(matches))\n\n# Test the function"}
{"task_id": "BigCodeBench/328", "solution": "import collections\nimport random\nfrom queue import PriorityQueue\ndef task_func(number_teams=5):\n    # Create a dictionary to store team names and points\n    teams = {f\"Team {i}\": random.randint(1, 100) for i in range(1, number_teams + 1)}\n\n    # Create a priority queue to sort the teams by points in descending order\n    pq = PriorityQueue()\n\n    for team, points in teams.items():\n        pq.put((-points, team))\n\n    # Create an ordered dictionary to store the sorted teams\n    sorted_teams = collections.OrderedDict()\n\n    while not pq.empty():\n        points, team = pq.get()\n        sorted_teams[team] = -points\n\n    return sorted_teams", "raw_solution": "import collections\nimport random\nfrom queue import PriorityQueue\n\ndef task_func(number_teams=5):\n    # Create a dictionary to store team names and points\n    teams = {f\"Team {i}\": random.randint(1, 100) for i in range(1, number_teams + 1)}\n\n    # Create a priority queue to sort the teams by points in descending order\n    pq = PriorityQueue()\n\n    for team, points in teams.items():\n        pq.put((-points, team))\n\n    # Create an ordered dictionary to store the sorted teams\n    sorted_teams = collections.OrderedDict()\n\n    while not pq.empty():\n        points, team = pq.get()\n        sorted_teams[team] = -points\n\n    return sorted_teams\n\n# Test the function"}
{"task_id": "BigCodeBench/329", "solution": "import re\nimport json\nimport os\ndef task_func(file_path: str, regex_pattern=r'\\(.+?\\)|\\w') -> dict:\n    result = {}\n    for root, dirs, files in os.walk(file_path):\n        for file in files:\n            if file.endswith('.json'):\n                with open(os.path.join(root, file), 'r') as f:\n                    data = json.load(f)\n                    matches = re.findall(regex_pattern, str(data))\n                    result[file] = matches\n    return result", "raw_solution": "import re\nimport json\nimport os\n\ndef task_func(file_path: str, regex_pattern=r'\\(.+?\\)|\\w') -> dict:\n    result = {}\n    for root, dirs, files in os.walk(file_path):\n        for file in files:\n            if file.endswith('.json'):\n                with open(os.path.join(root, file), 'r') as f:\n                    data = json.load(f)\n                    matches = re.findall(regex_pattern, str(data))\n                    result[file] = matches\n    return result"}
{"task_id": "BigCodeBench/330", "solution": "import heapq\nimport random\nfrom typing import List, Tuple\ndef task_func(list_length: int = 5, k: int = 3) -> Tuple[List[int], List[int]]:\n    \"\"\"\n    Generates a list of random integers and finds the k largest numbers using heapq.\n    \"\"\"\n    # Generate a list of random integers\n    random_list = [random.randint(1, 100) for _ in range(list_length)]\n\n    # Use heapq to find the k largest numbers\n    k_largest = heapq.nlargest(k, random_list)\n\n    return random_list, k_largest", "raw_solution": "import heapq\nimport random\nfrom typing import List, Tuple\n\ndef task_func(list_length: int = 5, k: int = 3) -> Tuple[List[int], List[int]]:\n    \"\"\"\n    Generates a list of random integers and finds the k largest numbers using heapq.\n    \"\"\"\n    # Generate a list of random integers\n    random_list = [random.randint(1, 100) for _ in range(list_length)]\n\n    # Use heapq to find the k largest numbers\n    k_largest = heapq.nlargest(k, random_list)\n\n    return random_list, k_largest\n\n# Test the function\nrandom_list, k_largest = task_func()"}
{"task_id": "BigCodeBench/331", "solution": "import bisect\nimport random\ndef task_func(num, list_length = 5, min_value = 0, max_value = 100):\n    if max_value < min_value:\n        min_value, max_value = max_value, min_value\n    if num < min_value or num > max_value:\n        raise ValueError(\"Number is out of range\")\n\n    random_list = random.sample(range(min_value, max_value + 1), list_length)\n    random_list.sort()\n\n    bisect.insort(random_list, num)\n\n    return (random_list, random_list)", "raw_solution": "import bisect\nimport random\n\ndef task_func(num, list_length = 5, min_value = 0, max_value = 100):\n    if max_value < min_value:\n        min_value, max_value = max_value, min_value\n    if num < min_value or num > max_value:\n        raise ValueError(\"Number is out of range\")\n\n    random_list = random.sample(range(min_value, max_value + 1), list_length)\n    random_list.sort()\n\n    bisect.insort(random_list, num)\n\n    return (random_list, random_list)\n\n# Test the function"}
{"task_id": "BigCodeBench/332", "solution": "import re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\ndef task_func(text: str) -> dict:\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Split the text into words\n    words = text.split()\n\n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in words if word not in stop_words]\n\n    # Count the words\n    word_counts = Counter(words)\n\n    return word_counts", "raw_solution": "import re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\n\ndef task_func(text: str) -> dict:\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Split the text into words\n    words = text.split()\n\n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in words if word not in stop_words]\n\n    # Count the words\n    word_counts = Counter(words)\n\n    return word_counts"}
{"task_id": "BigCodeBench/333", "solution": "import heapq\nimport random\ndef task_func(k, list_length = 5, min_value = 0, max_value = 100):\n    # Generate a list of random integers\n    random_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n\n    # Use heapq to find the k smallest numbers\n    k_smallest = heapq.nsmallest(k, random_list)\n\n    return (random_list, k_smallest)", "raw_solution": "import heapq\nimport random\n\ndef task_func(k, list_length = 5, min_value = 0, max_value = 100):\n    # Generate a list of random integers\n    random_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n\n    # Use heapq to find the k smallest numbers\n    k_smallest = heapq.nsmallest(k, random_list)\n\n    return (random_list, k_smallest)\n\n# Test the function"}
{"task_id": "BigCodeBench/334", "solution": "from nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\ndef task_func(documents):\n    \"\"\"\n    Calculates the TF-IDF score of the words in a list of documents.\n    :param documents: A list of documents (strings)\n    :return: A DataFrame with words as columns and documents as rows, containing the TF-IDF scores.\n    \"\"\"\n    # Tokenize the documents\n    tokens = [word_tokenize(doc) for doc in documents]\n\n    # Create a TfidfVectorizer\n    vectorizer = TfidfVectorizer(input='content', encoding='utf-8', decode_error='ignore',\n                                 tokenizer=lambda doc: doc, lowercase=False)\n\n    # Fit and transform the vectorizer\n    tfidf_matrix = vectorizer.fit_transform(tokens)\n\n    # Get the feature names (words)\n    features = vectorizer.get_feature_names_out()\n\n    # Create a DataFrame from the TF-IDF matrix\n    df = pd.DataFrame(tfidf_matrix.toarray(), columns=features)\n\n    return df", "raw_solution": "from nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\n\ndef task_func(documents):\n    \"\"\"\n    Calculates the TF-IDF score of the words in a list of documents.\n    :param documents: A list of documents (strings)\n    :return: A DataFrame with words as columns and documents as rows, containing the TF-IDF scores.\n    \"\"\"\n    # Tokenize the documents\n    tokens = [word_tokenize(doc) for doc in documents]\n\n    # Create a TfidfVectorizer\n    vectorizer = TfidfVectorizer(input='content', encoding='utf-8', decode_error='ignore',\n                                 tokenizer=lambda doc: doc, lowercase=False)\n\n    # Fit and transform the vectorizer\n    tfidf_matrix = vectorizer.fit_transform(tokens)\n\n    # Get the feature names (words)\n    features = vectorizer.get_feature_names_out()\n\n    # Create a DataFrame from the TF-IDF matrix\n    df = pd.DataFrame(tfidf_matrix.toarray(), columns=features)\n\n    return df"}
{"task_id": "BigCodeBench/335", "solution": "import collections\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e']\ndef task_func(string_length=100):\n    # Generate a random string of given length from the predefined list of letters\n    random_string = ''.join(random.choice(LETTERS) for _ in range(string_length))\n\n    # Count the frequency of each letter in the generated string\n    frequency_dict = collections.Counter(random_string)\n\n    # Sort the dictionary by frequency in descending order\n    sorted_dict = collections.OrderedDict(sorted(frequency_dict.items(), key=lambda x: x[1], reverse=True))\n\n    return sorted_dict", "raw_solution": "import collections\nimport random\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef task_func(string_length=100):\n    # Generate a random string of given length from the predefined list of letters\n    random_string = ''.join(random.choice(LETTERS) for _ in range(string_length))\n\n    # Count the frequency of each letter in the generated string\n    frequency_dict = collections.Counter(random_string)\n\n    # Sort the dictionary by frequency in descending order\n    sorted_dict = collections.OrderedDict(sorted(frequency_dict.items(), key=lambda x: x[1], reverse=True))\n\n    return sorted_dict"}
{"task_id": "BigCodeBench/336", "solution": "import re\nimport os\nimport glob\nfrom pathlib import Path\ndef task_func(pattern, directory, extensions):\n    \"\"\"\n    Find all files in a specific directory that contain a regex pattern in their contents in a case insensitive manner.\n    The function returns a list of absolute file paths that contain the pattern.\n    \"\"\"\n    # Compile the regex pattern\n    pattern = re.compile(pattern, re.IGNORECASE)\n\n    # Get all files with the specified extensions in the directory\n    files = glob.glob(os.path.join(directory, f'*.{extensions}'))\n\n    # List to store the file paths that contain the pattern\n    matched_files = []\n\n    # Iterate over each file\n    for file in files:\n        # Open the file and read its contents\n        with open(file, 'r') as f:\n            contents = f.read()\n\n        # If the pattern is found in the contents, add the file path to the list\n        if pattern.search(contents):\n            matched_files.append(os.path.abspath(file))\n\n    return matched_files", "raw_solution": "import re\nimport os\nimport glob\nfrom pathlib import Path\n\ndef task_func(pattern, directory, extensions):\n    \"\"\"\n    Find all files in a specific directory that contain a regex pattern in their contents in a case insensitive manner.\n    The function returns a list of absolute file paths that contain the pattern.\n    \"\"\"\n    # Compile the regex pattern\n    pattern = re.compile(pattern, re.IGNORECASE)\n\n    # Get all files with the specified extensions in the directory\n    files = glob.glob(os.path.join(directory, f'*.{extensions}'))\n\n    # List to store the file paths that contain the pattern\n    matched_files = []\n\n    # Iterate over each file\n    for file in files:\n        # Open the file and read its contents\n        with open(file, 'r') as f:\n            contents = f.read()\n\n        # If the pattern is found in the contents, add the file path to the list\n        if pattern.search(contents):\n            matched_files.append(os.path.abspath(file))\n\n    return matched_files"}
{"task_id": "BigCodeBench/337", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b']\ndef task_func(df, group_col, value_col):\n    \"\"\"\n    Creates a bar chart of data in multiple groups with error bars.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame containing the data.\n    group_col (str): The name of the column to group by.\n    value_col (str): The name of the column to plot.\n\n    Returns:\n    Axes: A matplotlib axes object with the bar chart.\n\n    Raises:\n    TypeError: If the 'Value' has non-numeric values.\n    \"\"\"\n    # Check if 'Value' has non-numeric values\n    if not np.issubdtype(df[value_col].dtype, np.number):\n        raise TypeError(\"'Value' has non-numeric values\")\n\n    # Group the data by 'Group' and calculate the mean and standard deviation\n    group_data = df.groupby(group_col)[value_col].agg(['mean', 'std'])\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(group_data.index, group_data['mean'], yerr=group_data['std'], color=[COLORS[i % len(COLORS)] for i in range(len(group_data.index))])\n\n    # Set the plot title\n    plt.title(f\"Bar chart of {value_col} by {group_col}\")\n\n    # Set the xlabel and ylabel\n    plt.xlabel(group_col)\n    plt.ylabel(value_col)\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef task_func(df, group_col, value_col):\n    \"\"\"\n    Creates a bar chart of data in multiple groups with error bars.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame containing the data.\n    group_col (str): The name of the column to group by.\n    value_col (str): The name of the column to plot.\n\n    Returns:\n    Axes: A matplotlib axes object with the bar chart.\n\n    Raises:\n    TypeError: If the 'Value' has non-numeric values.\n    \"\"\"\n    # Check if 'Value' has non-numeric values\n    if not np.issubdtype(df[value_col].dtype, np.number):\n        raise TypeError(\"'Value' has non-numeric values\")\n\n    # Group the data by 'Group' and calculate the mean and standard deviation\n    group_data = df.groupby(group_col)[value_col].agg(['mean', 'std'])\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(group_data.index, group_data['mean'], yerr=group_data['std'], color=[COLORS[i % len(COLORS)] for i in range(len(group_data.index))])\n\n    # Set the plot title\n    plt.title(f\"Bar chart of {value_col} by {group_col}\")\n\n    # Set the xlabel and ylabel\n    plt.xlabel(group_col)\n    plt.ylabel(value_col)\n\n    return ax"}
{"task_id": "BigCodeBench/338", "solution": "import random\nimport string\nfrom matplotlib import pyplot as plt\nfrom collections import Counter\ndef task_func(elements, seed=100):\n    random.seed(seed)\n    formatted_elements = []\n    char_counts = Counter()\n\n    for element in elements:\n        pattern = \"% \" + ''.join(random.choices(string.ascii_letters + string.digits, k=5)) + \"%\"\n        formatted_element = pattern.format(element)\n        formatted_elements.append(formatted_element)\n        char_counts.update(formatted_element)\n\n    # Plot histogram\n    ax = plt.figure().gca()\n    ax.bar(char_counts.keys(), char_counts.values())\n    plt.title('Histogram of Character Counts in Formatted Strings')\n    plt.xlabel('Character')\n    plt.ylabel('Count')\n\n    return formatted_elements, ax, dict(char_counts)", "raw_solution": "import random\nimport string\nfrom matplotlib import pyplot as plt\nfrom collections import Counter\n\ndef task_func(elements, seed=100):\n    random.seed(seed)\n    formatted_elements = []\n    char_counts = Counter()\n\n    for element in elements:\n        pattern = \"% \" + ''.join(random.choices(string.ascii_letters + string.digits, k=5)) + \"%\"\n        formatted_element = pattern.format(element)\n        formatted_elements.append(formatted_element)\n        char_counts.update(formatted_element)\n\n    # Plot histogram\n    ax = plt.figure().gca()\n    ax.bar(char_counts.keys(), char_counts.values())\n    plt.title('Histogram of Character Counts in Formatted Strings')\n    plt.xlabel('Character')\n    plt.ylabel('Count')\n\n    return formatted_elements, ax, dict(char_counts)"}
{"task_id": "BigCodeBench/339", "solution": "import json\nimport urllib.parse\nimport hmac\nimport hashlib\ndef task_func(req_data, secret_key):\n    if not isinstance(req_data, dict):\n        raise TypeError(\"`req_data` must be a dictionary\")\n\n    json_data = json.dumps(req_data, sort_keys=True)\n    signature = hmac.new(secret_key.encode(), json_data.encode(), hashlib.sha256).hexdigest()\n    url_encoded_signature = urllib.parse.quote(signature.replace(' ', '+'))\n\n    return url_encoded_signature", "raw_solution": "import json\nimport urllib.parse\nimport hmac\nimport hashlib\n\ndef task_func(req_data, secret_key):\n    if not isinstance(req_data, dict):\n        raise TypeError(\"`req_data` must be a dictionary\")\n\n    json_data = json.dumps(req_data, sort_keys=True)\n    signature = hmac.new(secret_key.encode(), json_data.encode(), hashlib.sha256).hexdigest()\n    url_encoded_signature = urllib.parse.quote(signature.replace(' ', '+'))\n\n    return url_encoded_signature"}
{"task_id": "BigCodeBench/340", "solution": "import json\nimport hashlib\nimport blake3\ndef task_func(req_data):\n    # Convert request data to JSON\n    json_data = json.dumps(req_data)\n\n    # Hash the JSON data with BLAKE3\n    blake3_hash = blake3.hash(json_data.encode())\n\n    # Convert the BLAKE3 hash to hexadecimal\n    blake3_hex = blake3_hash.hexdigest()\n\n    # Generate an MD5 hash of the BLAKE3 hexadecimal representation\n    md5_hash = hashlib.md5(blake3_hex.encode())\n    md5_hex = md5_hash.hexdigest()\n\n    return (blake3_hex, md5_hex)", "raw_solution": "import json\nimport hashlib\nimport blake3\n\ndef task_func(req_data):\n    # Convert request data to JSON\n    json_data = json.dumps(req_data)\n\n    # Hash the JSON data with BLAKE3\n    blake3_hash = blake3.hash(json_data.encode())\n\n    # Convert the BLAKE3 hash to hexadecimal\n    blake3_hex = blake3_hash.hexdigest()\n\n    # Generate an MD5 hash of the BLAKE3 hexadecimal representation\n    md5_hash = hashlib.md5(blake3_hex.encode())\n    md5_hex = md5_hash.hexdigest()\n\n    return (blake3_hex, md5_hex)"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"The input df must be DataFrame, not be empty\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the specified column: {col}\")\n\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n    if df[col].dtype in ['int64', 'float64']:\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.countplot(df[col], ax=axes[0])\n\n    sns.boxplot(df[col], ax=axes[1])\n\n    plt.tight_layout()\n    return fig", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"The input df must be DataFrame, not be empty\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the specified column: {col}\")\n\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n    if df[col].dtype in ['int64', 'float64']:\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.countplot(df[col], ax=axes[0])\n\n    sns.boxplot(df[col], ax=axes[1])\n\n    plt.tight_layout()\n    return fig"}
{"task_id": "BigCodeBench/342", "solution": "import string\nimport random\nimport re\ndef task_func(elements, pattern, seed=100):\n    random.seed(seed)\n    formatted_elements = []\n    for element in elements:\n        replaced_element = \"\".join(random.choice(string.ascii_letters) for _ in range(len(element)))\n        formatted_element = f\"%{replaced_element}%\"\n        formatted_elements.append(formatted_element)\n    search_result = any(re.search(pattern, fe) for fe in formatted_elements)\n    return formatted_elements, search_result", "raw_solution": "import string\nimport random\nimport re\n\ndef task_func(elements, pattern, seed=100):\n    random.seed(seed)\n    formatted_elements = []\n    for element in elements:\n        replaced_element = \"\".join(random.choice(string.ascii_letters) for _ in range(len(element)))\n        formatted_element = f\"%{replaced_element}%\"\n        formatted_elements.append(formatted_element)\n    search_result = any(re.search(pattern, fe) for fe in formatted_elements)\n    return formatted_elements, search_result"}
{"task_id": "BigCodeBench/343", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b', 'y', 'm']\ndef task_func(df, col, title=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be DataFrame\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the column {col}\")\n\n    # Get the unique values and their counts\n    values, counts = df[col].value_counts().items()\n\n    # Create the pie chart\n    fig, ax = plt.subplots()\n    ax.pie(counts, labels=values, colors=COLORS[:len(counts)], autopct='%1.1f%%')\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    if title:\n        ax.set_title(title)\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants for pie chart colors\nCOLORS = ['r', 'g', 'b', 'y', 'm']\n\ndef task_func(df, col, title=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be DataFrame\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the column {col}\")\n\n    # Get the unique values and their counts\n    values, counts = df[col].value_counts().items()\n\n    # Create the pie chart\n    fig, ax = plt.subplots()\n    ax.pie(counts, labels=values, colors=COLORS[:len(counts)], autopct='%1.1f%%')\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    if title:\n        ax.set_title(title)\n\n    return ax"}
{"task_id": "BigCodeBench/344", "solution": "import os\nimport shutil\ndef task_func(src_folder, backup_dir):\n    \"\"\"\n    Backs up a given source folder to the specified backup directory, then deletes the source folder.\n    Raises ValueError if the source folder does not exist. Raises Exception if an error occurs while deleting the source folder.\n    Returns:\n        bool: True if the operation is successful, False otherwise.\n    \"\"\"\n    if not os.path.exists(src_folder):\n        raise ValueError(f\"Source folder {src_folder} does not exist.\")\n\n    try:\n        shutil.make_archive(os.path.join(backup_dir, os.path.basename(src_folder)), 'zip', src_folder)\n    except Exception as e:\n        raise Exception(f\"An error occurred while backing up the source folder: {e}\")\n\n    try:\n        shutil.rmtree(src_folder)\n    except Exception as e:\n        raise Exception(f\"An error occurred while deleting the source folder: {e}\")\n\n    return True", "raw_solution": "import os\nimport shutil\n\ndef task_func(src_folder, backup_dir):\n    \"\"\"\n    Backs up a given source folder to the specified backup directory, then deletes the source folder.\n    Raises ValueError if the source folder does not exist. Raises Exception if an error occurs while deleting the source folder.\n    Returns:\n        bool: True if the operation is successful, False otherwise.\n    \"\"\"\n    if not os.path.exists(src_folder):\n        raise ValueError(f\"Source folder {src_folder} does not exist.\")\n\n    try:\n        shutil.make_archive(os.path.join(backup_dir, os.path.basename(src_folder)), 'zip', src_folder)\n    except Exception as e:\n        raise Exception(f\"An error occurred while backing up the source folder: {e}\")\n\n    try:\n        shutil.rmtree(src_folder)\n    except Exception as e:\n        raise Exception(f\"An error occurred while deleting the source folder: {e}\")\n\n    return True"}
{"task_id": "BigCodeBench/345", "solution": "import pandas as pd\nimport seaborn as sns\ndef task_func(df, col1, col2):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"Input df is empty\")\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(f\"Input df does not contain the specified columns: {col1}, {col2}\")\n    if not pd.api.types.is_numeric_dtype(df[col1]) or not pd.api.types.is_numeric_dtype(df[col2]):\n        raise TypeError(\"df uses non-numeric data\")\n\n    sns.regplot(x=df[col1], y=df[col2])\n    return sns.plt.gca()", "raw_solution": "import pandas as pd\nimport seaborn as sns\n\ndef task_func(df, col1, col2):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"Input df is empty\")\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(f\"Input df does not contain the specified columns: {col1}, {col2}\")\n    if not pd.api.types.is_numeric_dtype(df[col1]) or not pd.api.types.is_numeric_dtype(df[col2]):\n        raise TypeError(\"df uses non-numeric data\")\n\n    sns.regplot(x=df[col1], y=df[col2])\n    return sns.plt.gca()"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Runs a Python script as a process with predefined arguments.\n    By default, waits for the process to complete. If wait is False, the function returns None.\n    Raises:\n        - ValueError: If the script does not exist.\n        - subprocess.CalledProcessError: If the script raises an exception.\n    Returns:\n        int: The return code of the subprocess. If 'wait' is False, returns None.\n    \"\"\"\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n\n    try:\n        if wait:\n            result = subprocess.run([sys.executable, script_path] + list(args), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            return result.returncode\n        else:\n            subprocess.Popen([sys.executable, script_path] + list(args))\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output)", "raw_solution": "import subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Runs a Python script as a process with predefined arguments.\n    By default, waits for the process to complete. If wait is False, the function returns None.\n    Raises:\n        - ValueError: If the script does not exist.\n        - subprocess.CalledProcessError: If the script raises an exception.\n    Returns:\n        int: The return code of the subprocess. If 'wait' is False, returns None.\n    \"\"\"\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n\n    try:\n        if wait:\n            result = subprocess.run([sys.executable, script_path] + list(args), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            return result.returncode\n        else:\n            subprocess.Popen([sys.executable, script_path] + list(args))\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output)"}
{"task_id": "BigCodeBench/347", "solution": "import pandas as pd\nimport re\nimport numpy as np\nPATTERN = r\"([a-fA-F\\d]{32})\"\ndef task_func(df, column):\n    \"\"\"\n    Finds all matches of the regex pattern '([a-fA-F\\d]{32})' in a Pandas DataFrame column and counts the occurrence of any unique match in the data.\n    \"\"\"\n    if column not in df.columns:\n        raise KeyError(f\"The column '{column}' does not exist in the DataFrame.\")\n\n    # Find all matches of the pattern\n    matches = df[column].str.extractall(PATTERN)\n\n    # Count the occurrence of each unique match\n    counts = matches[0].value_counts()\n\n    return counts", "raw_solution": "import pandas as pd\nimport re\nimport numpy as np\n\n# Constants\nPATTERN = r\"([a-fA-F\\d]{32})\"\n\ndef task_func(df, column):\n    \"\"\"\n    Finds all matches of the regex pattern '([a-fA-F\\d]{32})' in a Pandas DataFrame column and counts the occurrence of any unique match in the data.\n    \"\"\"\n    if column not in df.columns:\n        raise KeyError(f\"The column '{column}' does not exist in the DataFrame.\")\n\n    # Find all matches of the pattern\n    matches = df[column].str.extractall(PATTERN)\n\n    # Count the occurrence of each unique match\n    counts = matches[0].value_counts()\n\n    return counts"}
{"task_id": "BigCodeBench/348", "solution": "import subprocess\nimport os\nimport signal\nimport time\ndef task_func(process_name: str) -> int:\n    \"\"\"\n    Stops all running processes with a specific name.\n    \"\"\"\n    # Get a list of all running processes\n    processes = subprocess.check_output('ps -A', shell=True).splitlines()\n\n    # Initialize a counter for the number of processes stopped\n    count = 0\n\n    # Iterate over each process\n    for process in processes:\n        # Convert the process to a string and split it by spaces\n        process = process.decode().split()\n\n        # If the process name matches the target name\n        if process_name in process:\n            # Try to send a termination signal to the process\n            try:\n                # Get the process ID from the process name\n                pid = int(process[1])\n\n                # Send the termination signal to the process\n                os.kill(pid, signal.SIGTERM)\n\n                # Wait for 1 second for the process to terminate\n                time.sleep(1)\n\n                # If the process is still running, send a kill signal\n                if os.path.exists('/proc/%s' % pid):\n                    os.kill(pid, signal.SIGKILL)\n\n                # Increment the counter\n                count += 1\n            except:\n                pass\n\n    # Return the number of processes stopped\n    return count", "raw_solution": "import subprocess\nimport os\nimport signal\nimport time\n\ndef task_func(process_name: str) -> int:\n    \"\"\"\n    Stops all running processes with a specific name.\n    \"\"\"\n    # Get a list of all running processes\n    processes = subprocess.check_output('ps -A', shell=True).splitlines()\n\n    # Initialize a counter for the number of processes stopped\n    count = 0\n\n    # Iterate over each process\n    for process in processes:\n        # Convert the process to a string and split it by spaces\n        process = process.decode().split()\n\n        # If the process name matches the target name\n        if process_name in process:\n            # Try to send a termination signal to the process\n            try:\n                # Get the process ID from the process name\n                pid = int(process[1])\n\n                # Send the termination signal to the process\n                os.kill(pid, signal.SIGTERM)\n\n                # Wait for 1 second for the process to terminate\n                time.sleep(1)\n\n                # If the process is still running, send a kill signal\n                if os.path.exists('/proc/%s' % pid):\n                    os.kill(pid, signal.SIGKILL)\n\n                # Increment the counter\n                count += 1\n            except:\n                pass\n\n    # Return the number of processes stopped\n    return count"}
{"task_id": "BigCodeBench/349", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories):\n    data = []\n    for i, category in enumerate(categories):\n        for j, product in enumerate(product_list):\n            quantity_sold = random.randint(1, 100)\n            revenue = quantity_sold * random.randint(10, 100)\n            data.append([product, category, quantity_sold, revenue])\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue'])\n    return df\nproduct_list = ['Product 1', 'Product 2', 'Product 3']\ncategories = ['Category 1', 'Category 2', 'Category 3']", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories):\n    data = []\n    for i, category in enumerate(categories):\n        for j, product in enumerate(product_list):\n            quantity_sold = random.randint(1, 100)\n            revenue = quantity_sold * random.randint(10, 100)\n            data.append([product, category, quantity_sold, revenue])\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue'])\n    return df\n\n# Test the function\nproduct_list = ['Product 1', 'Product 2', 'Product 3']\ncategories = ['Category 1', 'Category 2', 'Category 3']\ndf = task_func(product_list, categories)"}
{"task_id": "BigCodeBench/350", "solution": "import subprocess\nimport os\nimport shutil\nfrom glob import glob\ndef task_func(src_folder, dst_folder):\n    \"\"\"\n    Compress all files in the specified source folder and move the compressed files to a destination folder.\n    \"\"\"\n    if not os.path.exists(src_folder):\n        return {'success': False, 'message': f'Source folder {src_folder} does not exist.', 'failed_files': []}\n\n    if not os.path.exists(dst_folder):\n        return {'success': False, 'message': f'Destination folder {dst_folder} does not exist.', 'failed_files': []}\n\n    failed_files = []\n    for file in glob(os.path.join(src_folder, '*')):\n        if os.path.isfile(file):\n            try:\n                subprocess.run(['gzip', file], check=True)\n                shutil.move(file + '.gz', dst_folder)\n            except (subprocess.CalledProcessError, shutil.Error) as e:\n                failed_files.append(os.path.basename(file))\n\n    success = len(failed_files) == 0\n    message = 'All files were compressed and moved successfully.' if success else 'Some files failed to compress or move.'\n\n    return {'success': success, 'message': message, 'failed_files': failed_files}", "raw_solution": "import subprocess\nimport os\nimport shutil\nfrom glob import glob\n\ndef task_func(src_folder, dst_folder):\n    \"\"\"\n    Compress all files in the specified source folder and move the compressed files to a destination folder.\n    \"\"\"\n    if not os.path.exists(src_folder):\n        return {'success': False, 'message': f'Source folder {src_folder} does not exist.', 'failed_files': []}\n\n    if not os.path.exists(dst_folder):\n        return {'success': False, 'message': f'Destination folder {dst_folder} does not exist.', 'failed_files': []}\n\n    failed_files = []\n    for file in glob(os.path.join(src_folder, '*')):\n        if os.path.isfile(file):\n            try:\n                subprocess.run(['gzip', file], check=True)\n                shutil.move(file + '.gz', dst_folder)\n            except (subprocess.CalledProcessError, shutil.Error) as e:\n                failed_files.append(os.path.basename(file))\n\n    success = len(failed_files) == 0\n    message = 'All files were compressed and moved successfully.' if success else 'Some files failed to compress or move.'\n\n    return {'success': success, 'message': message, 'failed_files': failed_files}"}
{"task_id": "BigCodeBench/351", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    data = []\n    for category in categories:\n        for product in product_list:\n            quantity_sold = random.randint(min_value, max_value)\n            revenue = quantity_sold * random.uniform(min_value, max_value)\n            data.append([product, category, quantity_sold, revenue])\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue'])\n    return df\nproduct_list = ['Product 1', 'Product 2', 'Product 3']\ncategories = ['Category 1', 'Category 2', 'Category 3']", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    data = []\n    for category in categories:\n        for product in product_list:\n            quantity_sold = random.randint(min_value, max_value)\n            revenue = quantity_sold * random.uniform(min_value, max_value)\n            data.append([product, category, quantity_sold, revenue])\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue'])\n    return df\n\n# Test the function\nproduct_list = ['Product 1', 'Product 2', 'Product 3']\ncategories = ['Category 1', 'Category 2', 'Category 3']\ndf = task_func(product_list, categories)"}
{"task_id": "BigCodeBench/352", "solution": "import matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(text_dict, word_keys, top_k=2):\n    if top_k < 0:\n        raise ValueError(\"top_k cannot be a negative integer\")\n\n    word_freq = Counter(text_dict)\n    top_k_words = word_freq.most_common(top_k)\n\n    # Create a bar chart\n    words = [word[0] for word in top_k_words]\n    frequencies = [word[1] for word in top_k_words]\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('Frequency of top {} words'.format(top_k))\n\n    # Create a dictionary of top k words and their frequencies\n    top_k_dict = {word[0]: word[1] for word in top_k_words}\n\n    return ax, top_k_dict", "raw_solution": "import matplotlib.pyplot as plt\nfrom collections import Counter\n\ndef task_func(text_dict, word_keys, top_k=2):\n    if top_k < 0:\n        raise ValueError(\"top_k cannot be a negative integer\")\n\n    word_freq = Counter(text_dict)\n    top_k_words = word_freq.most_common(top_k)\n\n    # Create a bar chart\n    words = [word[0] for word in top_k_words]\n    frequencies = [word[1] for word in top_k_words]\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('Frequency of top {} words'.format(top_k))\n\n    # Create a dictionary of top k words and their frequencies\n    top_k_dict = {word[0]: word[1] for word in top_k_words}\n\n    return ax, top_k_dict"}
{"task_id": "BigCodeBench/353", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    data = []\n    for category in categories:\n        for product in product_list:\n            quantity_sold = random.randint(min_value, max_value)\n            revenue = quantity_sold * random.uniform(min_value, max_value)\n            total_revenue = quantity_sold * revenue\n            data.append([product, category, quantity_sold, revenue, total_revenue])\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue', 'Total Revenue'])\n    return df\nproduct_list = ['Product1', 'Product2', 'Product3']\ncategories = ['Category1', 'Category2', 'Category3']", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    data = []\n    for category in categories:\n        for product in product_list:\n            quantity_sold = random.randint(min_value, max_value)\n            revenue = quantity_sold * random.uniform(min_value, max_value)\n            total_revenue = quantity_sold * revenue\n            data.append([product, category, quantity_sold, revenue, total_revenue])\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue', 'Total Revenue'])\n    return df\n\n# Test the function\nproduct_list = ['Product1', 'Product2', 'Product3']\ncategories = ['Category1', 'Category2', 'Category3']\ndf = task_func(product_list, categories)"}
{"task_id": "BigCodeBench/354", "solution": "import collections\nimport matplotlib.pyplot as plt\nimport pandas as pd\nWORDS = ['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I']\ndef task_func(sentences_dict, word_keys):\n    \"\"\"\n    This function calculates the occurrence of certain words in a collection of sentences and returns a bar chart.\n    :param sentences_dict: A dictionary where the key is a sentence ID and the value is the sentence.\n    :param word_keys: A list of words to calculate the occurrence of.\n    :return: A matplotlib.axes._axes.Axes object of the bar chart displaying the frequencies.\n    \"\"\"\n    # Initialize a counter\n    word_counter = collections.Counter()\n\n    # Iterate over the sentences\n    for sentence in sentences_dict.values():\n        # Split the sentence into words\n        words = sentence.split()\n        # Count the occurrence of the words\n        word_counter.update(words)\n\n    # Filter the counter to only include the words we're interested in\n    word_counter = {word: count for word, count in word_counter.items() if word in word_keys}\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(list(word_counter.items()), columns=['Word', 'Frequency'])\n\n    # Create a bar chart\n    ax = df.plot(kind='bar', x='Word', y='Frequency', legend=False)\n\n    return ax\nsentences_dict = {\n    1: 'The quick brown fox jumps over the lazy dog',\n    2: 'And the dog was very happy',\n    3: 'The fox was very quick'\n}", "raw_solution": "import collections\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nWORDS = ['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I']\n\ndef task_func(sentences_dict, word_keys):\n    \"\"\"\n    This function calculates the occurrence of certain words in a collection of sentences and returns a bar chart.\n    :param sentences_dict: A dictionary where the key is a sentence ID and the value is the sentence.\n    :param word_keys: A list of words to calculate the occurrence of.\n    :return: A matplotlib.axes._axes.Axes object of the bar chart displaying the frequencies.\n    \"\"\"\n    # Initialize a counter\n    word_counter = collections.Counter()\n\n    # Iterate over the sentences\n    for sentence in sentences_dict.values():\n        # Split the sentence into words\n        words = sentence.split()\n        # Count the occurrence of the words\n        word_counter.update(words)\n\n    # Filter the counter to only include the words we're interested in\n    word_counter = {word: count for word, count in word_counter.items() if word in word_keys}\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(list(word_counter.items()), columns=['Word', 'Frequency'])\n\n    # Create a bar chart\n    ax = df.plot(kind='bar', x='Word', y='Frequency', legend=False)\n\n    return ax\n\n# Test the function\nsentences_dict = {\n    1: 'The quick brown fox jumps over the lazy dog',\n    2: 'And the dog was very happy',\n    3: 'The fox was very quick'\n}\n\nax = task_func(sentences_dict, WORDS)\nplt.show()"}
{"task_id": "BigCodeBench/355", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import get_window\ndef generate_complex_wave(amplitude, frequency, time):\n    \"\"\"\n    Generates a complex wave with a specified amplitude, frequency, and time.\n    The wave is represented as a complex number where the real part is the cosine component,\n    and the imaginary part is the sine component. The function applies a Hann window to reduce\n    edge effects.\n    \"\"\"\n    # Generate the wave\n    wave = amplitude * np.exp(2j * np.pi * frequency * time)\n\n    # Apply a Hann window\n    window = get_window('hann', len(wave))\n    wave *= window\n\n    return wave\ndef plot_complex_wave(wave):\n    \"\"\"\n    Plots a complex wave. The plot displays both the real and imaginary parts of the wave.\n    \"\"\"\n    # Create a figure and axes\n    fig, ax = plt.subplots()\n\n    # Plot the real and imaginary parts\n    ax.plot(np.real(wave), label='Real')\n    ax.plot(np.imag(wave), label='Imaginary')\n\n    # Set the plot title, x-label, and y-label\n    ax.set_title('Complex Wave with Hann Window')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n\n    # Add a legend\n    ax.legend()\n\n    return fig, ax\ndef task_func(amplitude, frequency, time):\n    \"\"\"\n    Generates and plots a complex wave with a specified amplitude, frequency, and time.\n    The function returns both the wave and the plot object.\n    \"\"\"\n    # Generate the wave\n    wave = generate_complex_wave(amplitude, frequency, time)\n\n    # Plot the wave\n    fig, ax = plot_complex_wave(wave)\n\n    return wave, fig, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import get_window\n\ndef generate_complex_wave(amplitude, frequency, time):\n    \"\"\"\n    Generates a complex wave with a specified amplitude, frequency, and time.\n    The wave is represented as a complex number where the real part is the cosine component,\n    and the imaginary part is the sine component. The function applies a Hann window to reduce\n    edge effects.\n    \"\"\"\n    # Generate the wave\n    wave = amplitude * np.exp(2j * np.pi * frequency * time)\n\n    # Apply a Hann window\n    window = get_window('hann', len(wave))\n    wave *= window\n\n    return wave\n\ndef plot_complex_wave(wave):\n    \"\"\"\n    Plots a complex wave. The plot displays both the real and imaginary parts of the wave.\n    \"\"\"\n    # Create a figure and axes\n    fig, ax = plt.subplots()\n\n    # Plot the real and imaginary parts\n    ax.plot(np.real(wave), label='Real')\n    ax.plot(np.imag(wave), label='Imaginary')\n\n    # Set the plot title, x-label, and y-label\n    ax.set_title('Complex Wave with Hann Window')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n\n    # Add a legend\n    ax.legend()\n\n    return fig, ax\n\ndef task_func(amplitude, frequency, time):\n    \"\"\"\n    Generates and plots a complex wave with a specified amplitude, frequency, and time.\n    The function returns both the wave and the plot object.\n    \"\"\"\n    # Generate the wave\n    wave = generate_complex_wave(amplitude, frequency, time)\n\n    # Plot the wave\n    fig, ax = plot_complex_wave(wave)\n\n    return wave, fig, ax"}
{"task_id": "BigCodeBench/356", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport cmath\ndef task_func(x, y):\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"Both x and y must be numpy.ndarray\")\n    if len(x) != len(y):\n        raise ValueError(\"x and y must have the same length\")\n\n    phase = np.angle(x + y*1j)\n\n    fig, ax = plt.subplots()\n    cax = ax.matshow(phase, interpolation='nearest')\n    fig.colorbar(cax)\n\n    return ax, phase", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport cmath\n\ndef task_func(x, y):\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"Both x and y must be numpy.ndarray\")\n    if len(x) != len(y):\n        raise ValueError(\"x and y must have the same length\")\n\n    phase = np.angle(x + y*1j)\n\n    fig, ax = plt.subplots()\n    cax = ax.matshow(phase, interpolation='nearest')\n    fig.colorbar(cax)\n\n    return ax, phase"}
{"task_id": "BigCodeBench/357", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(x):\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"x must be a numpy.ndarray\")\n\n    real_part = norm.rvs(loc=0, scale=1, size=x.shape[0])\n    imag_part = norm.rvs(loc=2, scale=2, size=x.shape[0])\n    complex_distribution = real_part + 1j * imag_part\n\n    return complex_distribution\nx = np.linspace(-10, 10, 1000)", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(x):\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"x must be a numpy.ndarray\")\n\n    real_part = norm.rvs(loc=0, scale=1, size=x.shape[0])\n    imag_part = norm.rvs(loc=2, scale=2, size=x.shape[0])\n    complex_distribution = real_part + 1j * imag_part\n\n    return complex_distribution\n\n# Test the function\nx = np.linspace(-10, 10, 1000)\ny = task_func(x)\n\nplt.figure(figsize=(10, 5))\nplt.plot(x, y.real, label='Real part')\nplt.plot(x, y.imag, label='Imaginary part')\nplt.legend()\nplt.show()"}
{"task_id": "BigCodeBench/358", "solution": "import itertools\nimport json\ndef task_func(json_list, r):\n    try:\n        data = json.loads(json_list)\n        if not isinstance(data, dict) or 'number_list' not in data:\n            raise Exception('Invalid JSON: Missing \"number_list\" key')\n        number_list = data['number_list']\n        if not isinstance(number_list, list) or not number_list:\n            raise Exception('Invalid JSON: \"number_list\" must be a non-empty list')\n        return list(itertools.combinations(number_list, r))\n    except json.JSONDecodeError:\n        raise Exception('Invalid JSON')\njson_list = '{\"number_list\": [1, 2, 3, 4]}'\nr = 2", "raw_solution": "import itertools\nimport json\n\ndef task_func(json_list, r):\n    try:\n        data = json.loads(json_list)\n        if not isinstance(data, dict) or 'number_list' not in data:\n            raise Exception('Invalid JSON: Missing \"number_list\" key')\n        number_list = data['number_list']\n        if not isinstance(number_list, list) or not number_list:\n            raise Exception('Invalid JSON: \"number_list\" must be a non-empty list')\n        return list(itertools.combinations(number_list, r))\n    except json.JSONDecodeError:\n        raise Exception('Invalid JSON')\n\n# Test cases\njson_list = '{\"number_list\": [1, 2, 3, 4]}'\nr = 2"}
{"task_id": "BigCodeBench/359", "solution": "from scipy import stats\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(data_dict, data_keys):\n    \"\"\"\n    Calculates the correlation between two data series and returns a scatter plot along with the correlation coefficient.\n\n    Args:\n    data_dict (dict): A dictionary where keys are data series names and values are lists of data points.\n    data_keys (list): A list of two data series names to calculate the correlation between.\n\n    Returns:\n    tuple:\n    float: The correlation coefficient.\n    Axes: The scatter plot of the two data series.\n    \"\"\"\n    # Extract the data series\n    data1 = np.array(data_dict[data_keys[0]])\n    data2 = np.array(data_dict[data_keys[1]])\n\n    # Calculate the correlation coefficient\n    corr_coef, _ = stats.pearsonr(data1, data2)\n\n    # Create the scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(data1, data2)\n    ax.set_xlabel(data_keys[0])\n    ax.set_ylabel(data_keys[1])\n    ax.set_title(f'Scatter plot of {data_keys[0]} and {data_keys[1]} with correlation coefficient {corr_coef:.2f}')\n\n    return corr_coef, ax", "raw_solution": "from scipy import stats\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(data_dict, data_keys):\n    \"\"\"\n    Calculates the correlation between two data series and returns a scatter plot along with the correlation coefficient.\n\n    Args:\n    data_dict (dict): A dictionary where keys are data series names and values are lists of data points.\n    data_keys (list): A list of two data series names to calculate the correlation between.\n\n    Returns:\n    tuple:\n    float: The correlation coefficient.\n    Axes: The scatter plot of the two data series.\n    \"\"\"\n    # Extract the data series\n    data1 = np.array(data_dict[data_keys[0]])\n    data2 = np.array(data_dict[data_keys[1]])\n\n    # Calculate the correlation coefficient\n    corr_coef, _ = stats.pearsonr(data1, data2)\n\n    # Create the scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(data1, data2)\n    ax.set_xlabel(data_keys[0])\n    ax.set_ylabel(data_keys[1])\n    ax.set_title(f'Scatter plot of {data_keys[0]} and {data_keys[1]} with correlation coefficient {corr_coef:.2f}')\n\n    return corr_coef, ax"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(\"The specified file does not exist.\")\n    if not pd.ExcelFile(file_location).sheet_names.count(sheet_name):\n        raise ValueError(\"The specified sheet does not exist in the workbook.\")\n\n    # Load data from Excel\n    df = pd.read_excel(file_location, sheet_name=sheet_name)\n\n    # Calculate mean and standard deviation\n    stats_dict = {}\n    for col in df.columns:\n        stats_dict[col] = {'mean': df[col].mean(), 'std': df[col].std()}\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(len(stats_dict)), [val['mean'] for val in stats_dict.values()], alpha=0.5, label='Mean')\n    ax.bar(range(len(stats_dict)), [val['std'] for val in stats_dict.values()], alpha=0.5, label='Standard Deviation')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.legend()\n\n    return stats_dict, fig", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(\"The specified file does not exist.\")\n    if not pd.ExcelFile(file_location).sheet_names.count(sheet_name):\n        raise ValueError(\"The specified sheet does not exist in the workbook.\")\n\n    # Load data from Excel\n    df = pd.read_excel(file_location, sheet_name=sheet_name)\n\n    # Calculate mean and standard deviation\n    stats_dict = {}\n    for col in df.columns:\n        stats_dict[col] = {'mean': df[col].mean(), 'std': df[col].std()}\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(len(stats_dict)), [val['mean'] for val in stats_dict.values()], alpha=0.5, label='Mean')\n    ax.bar(range(len(stats_dict)), [val['std'] for val in stats_dict.values()], alpha=0.5, label='Standard Deviation')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.legend()\n\n    return stats_dict, fig"}
{"task_id": "BigCodeBench/361", "solution": "import pandas as pd\nimport logging\nimport os\ndef task_func(sheet_name, excel_file_location=\"test.xlsx\", csv_file_location=\"test.csv\"):\n    \"\"\"\n    Reads data from an Excel spreadsheet, converts it to a CSV file, then calculates the sum of each column in the CSV file.\n    \"\"\"\n    # Check if the Excel file exists\n    if not os.path.isfile(excel_file_location):\n        logging.error(f\"FileNotFoundError: The file {excel_file_location} does not exist.\")\n        raise FileNotFoundError(f\"The file {excel_file_location} does not exist.\")\n\n    # Read the Excel file\n    try:\n        df = pd.read_excel(excel_file_location, sheet_name=sheet_name)\n    except KeyError:\n        logging.error(f\"ValueError: The sheet {sheet_name} does not exist in the file {excel_file_location}.\")\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the file {excel_file_location}.\")\n\n    # Write the data to a CSV file\n    df.to_csv(csv_file_location, index=False)\n\n    # Read the CSV file\n    df = pd.read_csv(csv_file_location)\n\n    # Calculate the sum of each column\n    column_sums = df.sum().to_dict()\n\n    logging.info(f\"The sum of each column is: {column_sums}\")\n\n    return column_sums", "raw_solution": "import pandas as pd\nimport logging\nimport os\n\n# Set up basic configuration for logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef task_func(sheet_name, excel_file_location=\"test.xlsx\", csv_file_location=\"test.csv\"):\n    \"\"\"\n    Reads data from an Excel spreadsheet, converts it to a CSV file, then calculates the sum of each column in the CSV file.\n    \"\"\"\n    # Check if the Excel file exists\n    if not os.path.isfile(excel_file_location):\n        logging.error(f\"FileNotFoundError: The file {excel_file_location} does not exist.\")\n        raise FileNotFoundError(f\"The file {excel_file_location} does not exist.\")\n\n    # Read the Excel file\n    try:\n        df = pd.read_excel(excel_file_location, sheet_name=sheet_name)\n    except KeyError:\n        logging.error(f\"ValueError: The sheet {sheet_name} does not exist in the file {excel_file_location}.\")\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the file {excel_file_location}.\")\n\n    # Write the data to a CSV file\n    df.to_csv(csv_file_location, index=False)\n\n    # Read the CSV file\n    df = pd.read_csv(csv_file_location)\n\n    # Calculate the sum of each column\n    column_sums = df.sum().to_dict()\n\n    logging.info(f\"The sum of each column is: {column_sums}\")\n\n    return column_sums"}
{"task_id": "BigCodeBench/362", "solution": "import pandas as pd\nimport os\ndef task_func(original_file_location=\"test.xlsx\", new_file_location=\"new_test.xlsx\", sheet_name=\"Sheet1\"):\n    \"\"\"\n    Copies data from an Excel spreadsheet into a new Excel file, then reads the new Excel file and returns its contents.\n    \"\"\"\n    if not os.path.exists(original_file_location):\n        raise FileNotFoundError(f\"The file {original_file_location} does not exist.\")\n\n    # Read the original Excel file\n    df = pd.read_excel(original_file_location, sheet_name=sheet_name)\n\n    # Write the data to a new Excel file\n    df.to_excel(new_file_location, index=False)\n\n    # Read the new Excel file\n    df_new = pd.read_excel(new_file_location)\n\n    return df_new", "raw_solution": "import pandas as pd\nimport os\n\ndef task_func(original_file_location=\"test.xlsx\", new_file_location=\"new_test.xlsx\", sheet_name=\"Sheet1\"):\n    \"\"\"\n    Copies data from an Excel spreadsheet into a new Excel file, then reads the new Excel file and returns its contents.\n    \"\"\"\n    if not os.path.exists(original_file_location):\n        raise FileNotFoundError(f\"The file {original_file_location} does not exist.\")\n\n    # Read the original Excel file\n    df = pd.read_excel(original_file_location, sheet_name=sheet_name)\n\n    # Write the data to a new Excel file\n    df.to_excel(new_file_location, index=False)\n\n    # Read the new Excel file\n    df_new = pd.read_excel(new_file_location)\n\n    return df_new"}
{"task_id": "BigCodeBench/363", "solution": "from multiprocessing import Pool\nimport math\ndef calculate_factorial(number: int) -> tuple:\n    if not isinstance(number, int) or number < 0:\n        raise ValueError(\"Input must be a non-negative integer\")\n    return number, math.factorial(number)\ndef task_func(numbers: list) -> dict:\n    with Pool(processes=len(numbers)) as pool:\n        result = pool.map(calculate_factorial, numbers)\n    return dict(result)\nnumbers = [1, 2, 3, 4, 5]", "raw_solution": "from multiprocessing import Pool\nimport math\n\ndef calculate_factorial(number: int) -> tuple:\n    if not isinstance(number, int) or number < 0:\n        raise ValueError(\"Input must be a non-negative integer\")\n    return number, math.factorial(number)\n\ndef task_func(numbers: list) -> dict:\n    with Pool(processes=len(numbers)) as pool:\n        result = pool.map(calculate_factorial, numbers)\n    return dict(result)\n\n# Testing\nnumbers = [1, 2, 3, 4, 5]"}
{"task_id": "BigCodeBench/364", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nFEATURES = ['feature '+str(i) for i in range(1, 11)]\nTARGET = 'target'\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    lr = LinearRegression()\n    lr.fit(X_train, y_train)\n\n    return lr", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nFEATURES = ['feature '+str(i) for i in range(1, 11)]\nTARGET = 'target'\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    lr = LinearRegression()\n    lr.fit(X_train, y_train)\n\n    return lr"}
{"task_id": "BigCodeBench/365", "solution": "from collections import Counter\nimport json\nimport random\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\ndef task_func(n, file_name, seed=77):\n    random.seed(seed)\n    selected_words = random.sample(WORDS, n)\n\n    with open(file_name, 'w') as f:\n        json.dump(selected_words, f)\n\n    return file_name", "raw_solution": "from collections import Counter\nimport json\nimport random\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\n\ndef task_func(n, file_name, seed=77):\n    random.seed(seed)\n    selected_words = random.sample(WORDS, n)\n\n    with open(file_name, 'w') as f:\n        json.dump(selected_words, f)\n\n    return file_name"}
{"task_id": "BigCodeBench/366", "solution": "import matplotlib.pyplot as plt\nimport random\nCOLORS = ['#00bfbf', '#000000', '#0000ff']\ndef task_func(number_list, bins):\n    \"\"\"\n    Generates a histogram plot using Matplotlib. The plot title is set to 'Histogram'.\n    The x-axis label is set to 'Number'. The y-axis label is set to 'Frequency'.\n    The color of the histogram bars is randomly selected from a predefined set of colors.\n    \"\"\"\n    # Generate a random color\n    color = random.choice(COLORS)\n\n    # Create a histogram subplot\n    fig, ax = plt.subplots()\n    ax.hist(number_list, bins=bins, color=color)\n\n    # Set plot title and labels\n    ax.set_title('Histogram')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLORS = ['#00bfbf', '#000000', '#0000ff']\n\ndef task_func(number_list, bins):\n    \"\"\"\n    Generates a histogram plot using Matplotlib. The plot title is set to 'Histogram'.\n    The x-axis label is set to 'Number'. The y-axis label is set to 'Frequency'.\n    The color of the histogram bars is randomly selected from a predefined set of colors.\n    \"\"\"\n    # Generate a random color\n    color = random.choice(COLORS)\n\n    # Create a histogram subplot\n    fig, ax = plt.subplots()\n    ax.hist(number_list, bins=bins, color=color)\n\n    # Set plot title and labels\n    ax.set_title('Histogram')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return ax"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects\")\n\n    day_of_week_counts = defaultdict(int)\n    for activity in activities:\n        day_of_week = activity.strftime(\"%A\")\n        day_of_week_counts[day_of_week] += 1\n\n    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [day_of_week_counts[day] for day in days_of_week]\n\n    plt.bar(days_of_week, counts)\n    plt.xlabel('Day of the Week')\n    plt.ylabel('Number of Activities')\n    plt.title('Weekly Activity')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return plt.gca()", "raw_solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects\")\n\n    day_of_week_counts = defaultdict(int)\n    for activity in activities:\n        day_of_week = activity.strftime(\"%A\")\n        day_of_week_counts[day_of_week] += 1\n\n    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [day_of_week_counts[day] for day in days_of_week]\n\n    plt.bar(days_of_week, counts)\n    plt.xlabel('Day of the Week')\n    plt.ylabel('Number of Activities')\n    plt.title('Weekly Activity')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n    The function returns the name of the file moved.\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n\n    # If there are no files in the source directory, return an empty string\n    if not files:\n        return \"\"\n\n    # Select a random file from the list of files\n    file_name = random.choice(files)\n\n    # Construct the full path to the file\n    src_path = os.path.join(src_dir, file_name)\n\n    # Move the file to the destination directory\n    shutil.move(src_path, dest_dir)\n\n    # Return the name of the file moved\n    return file_name", "raw_solution": "import os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n    The function returns the name of the file moved.\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n\n    # If there are no files in the source directory, return an empty string\n    if not files:\n        return \"\"\n\n    # Select a random file from the list of files\n    file_name = random.choice(files)\n\n    # Construct the full path to the file\n    src_path = os.path.join(src_dir, file_name)\n\n    # Move the file to the destination directory\n    shutil.move(src_path, dest_dir)\n\n    # Return the name of the file moved\n    return file_name"}
{"task_id": "BigCodeBench/369", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(l):\n    # Convert the list to a numpy array\n    arr = np.array(l)\n\n    # Calculate the mean and standard deviation\n    mu, std = stats.norm.fit(arr)\n\n    # Create a histogram\n    plt.hist(arr, bins='auto', alpha=0.7, rwidth=0.85, color='#003f5c', density=True)\n\n    # Create a range of x values\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n\n    # Create a normal distribution with the same mean and standard deviation\n    p = stats.norm.pdf(x, mu, std)\n\n    # Plot the normal distribution\n    plt.plot(x, p, 'k', linewidth=2)\n\n    # Set the title\n    plt.title(f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\")\n\n    return plt.gca()", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(l):\n    # Convert the list to a numpy array\n    arr = np.array(l)\n\n    # Calculate the mean and standard deviation\n    mu, std = stats.norm.fit(arr)\n\n    # Create a histogram\n    plt.hist(arr, bins='auto', alpha=0.7, rwidth=0.85, color='#003f5c', density=True)\n\n    # Create a range of x values\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n\n    # Create a normal distribution with the same mean and standard deviation\n    p = stats.norm.pdf(x, mu, std)\n\n    # Plot the normal distribution\n    plt.plot(x, p, 'k', linewidth=2)\n\n    # Set the title\n    plt.title(f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\")\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/370", "solution": "import os\nimport re\nimport json\nimport glob\ndef task_func(directory_path: str) -> list:\n    \"\"\"\n    Protects all double quotes in all JSON files in the specified directory by prepending them with a double backslash.\n    \"\"\"\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The specified directory {directory_path} does not exist.\")\n\n    json_files = glob.glob(os.path.join(directory_path, \"*.json\"))\n    processed_files = []\n\n    for json_file in json_files:\n        with open(json_file, 'r') as file:\n            data = json.load(file)\n\n        # Convert the data to a string and escape double quotes\n        data_str = json.dumps(data)\n        data_str = re.sub(r'\\\\', '\\\\\\\\', data_str)\n\n        with open(json_file, 'w') as file:\n            file.write(data_str)\n\n        processed_files.append(json_file)\n\n    return processed_files", "raw_solution": "import os\nimport re\nimport json\nimport glob\n\ndef task_func(directory_path: str) -> list:\n    \"\"\"\n    Protects all double quotes in all JSON files in the specified directory by prepending them with a double backslash.\n    \"\"\"\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The specified directory {directory_path} does not exist.\")\n\n    json_files = glob.glob(os.path.join(directory_path, \"*.json\"))\n    processed_files = []\n\n    for json_file in json_files:\n        with open(json_file, 'r') as file:\n            data = json.load(file)\n\n        # Convert the data to a string and escape double quotes\n        data_str = json.dumps(data)\n        data_str = re.sub(r'\\\\', '\\\\\\\\', data_str)\n\n        with open(json_file, 'w') as file:\n            file.write(data_str)\n\n        processed_files.append(json_file)\n\n    return processed_files"}
{"task_id": "BigCodeBench/371", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(l):\n    \"\"\"\n    Scale the input field to the range [0, 1] and display it as a DataFrame.\n    \"\"\"\n    # Convert the input list to a numpy array\n    import numpy as np\n    np_array = np.array(l).reshape(-1, 1)\n\n    # Initialize a MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the numpy array\n    scaled_array = scaler.fit_transform(np_array)\n\n    # Convert the scaled array to a pandas DataFrame\n    df = pd.DataFrame(scaled_array, columns=['Scaled Values'])\n\n    return df", "raw_solution": "from sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(l):\n    \"\"\"\n    Scale the input field to the range [0, 1] and display it as a DataFrame.\n    \"\"\"\n    # Convert the input list to a numpy array\n    import numpy as np\n    np_array = np.array(l).reshape(-1, 1)\n\n    # Initialize a MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the numpy array\n    scaled_array = scaler.fit_transform(np_array)\n\n    # Convert the scaled array to a pandas DataFrame\n    df = pd.DataFrame(scaled_array, columns=['Scaled Values'])\n\n    return df"}
{"task_id": "BigCodeBench/372", "solution": "import re\nimport glob\nfrom docx import Document\ndef task_func(directory_path: str) -> int:\n    \"\"\"\n    Processes all Word (.docx) files in the provided directory, searching for double quotes in the text and adding a backslash before each double quote to \"protect\" it.\n    \"\"\"\n    # Get all .docx files in the directory\n    files = glob.glob(f'{directory_path}/*.docx')\n\n    # Initialize counter\n    count = 0\n\n    # Process each file\n    for file in files:\n        # Open the document\n        doc = Document(file)\n\n        # Process each paragraph in the document\n        for para in doc.paragraphs:\n            # Check if the paragraph contains a double quote\n            if '\"' in para.text:\n                # Add a backslash before each double quote\n                para.text = re.sub(r'\"', r'\\\\\"', para.text)\n\n        # Save the document\n        doc.save(file)\n\n        # Increment the counter\n        count += 1\n\n    # Return the number of files processed\n    return count", "raw_solution": "import re\nimport glob\nfrom docx import Document\n\ndef task_func(directory_path: str) -> int:\n    \"\"\"\n    Processes all Word (.docx) files in the provided directory, searching for double quotes in the text and adding a backslash before each double quote to \"protect\" it.\n    \"\"\"\n    # Get all .docx files in the directory\n    files = glob.glob(f'{directory_path}/*.docx')\n\n    # Initialize counter\n    count = 0\n\n    # Process each file\n    for file in files:\n        # Open the document\n        doc = Document(file)\n\n        # Process each paragraph in the document\n        for para in doc.paragraphs:\n            # Check if the paragraph contains a double quote\n            if '\"' in para.text:\n                # Add a backslash before each double quote\n                para.text = re.sub(r'\"', r'\\\\\"', para.text)\n\n        # Save the document\n        doc.save(file)\n\n        # Increment the counter\n        count += 1\n\n    # Return the number of files processed\n    return count"}
{"task_id": "BigCodeBench/373", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(x_data, y_data, plot=False):\n    def quad_func(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    popt, pcov = curve_fit(quad_func, x_data, y_data)\n    fitted_values = quad_func(x_data, *popt)\n\n    if plot:\n        fig, ax = plt.subplots()\n        ax.scatter(x_data, y_data, color='k', label='Data')\n        ax.plot(x_data, fitted_values, color='r', label='Fitted curve')\n        ax.set_xlabel('x')\n        ax.set_ylabel('y')\n        ax.legend()\n        plt.show()\n\n    return (popt, fitted_values, ax if plot else None)", "raw_solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(x_data, y_data, plot=False):\n    def quad_func(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    popt, pcov = curve_fit(quad_func, x_data, y_data)\n    fitted_values = quad_func(x_data, *popt)\n\n    if plot:\n        fig, ax = plt.subplots()\n        ax.scatter(x_data, y_data, color='k', label='Data')\n        ax.plot(x_data, fitted_values, color='r', label='Fitted curve')\n        ax.set_xlabel('x')\n        ax.set_ylabel('y')\n        ax.legend()\n        plt.show()\n\n    return (popt, fitted_values, ax if plot else None)"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    # Get all .xlsx files in the directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Initialize counter\n    count = 0\n\n    # Process each .xlsx file\n    for file in xlsx_files:\n        # Load the workbook\n        wb = load_workbook(filename=file)\n\n        # Iterate over each worksheet\n        for sheet in wb:\n            # Iterate over each cell\n            for row in sheet.iter_rows():\n                for cell in row:\n                    # If the cell contains a double quote, replace it with a double backslash\n                    if cell.value and '\"' in str(cell.value):\n                        cell.value = re.sub(r'\"', '\\\\\"', str(cell.value))\n\n        # Save the workbook\n        wb.save(file)\n\n        # Increment the counter\n        count += 1\n\n    return count", "raw_solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Get all .xlsx files in the directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Initialize counter\n    count = 0\n\n    # Process each .xlsx file\n    for file in xlsx_files:\n        # Load the workbook\n        wb = load_workbook(filename=file)\n\n        # Iterate over each worksheet\n        for sheet in wb:\n            # Iterate over each cell\n            for row in sheet.iter_rows():\n                for cell in row:\n                    # If the cell contains a double quote, replace it with a double backslash\n                    if cell.value and '\"' in str(cell.value):\n                        cell.value = re.sub(r'\"', '\\\\\"', str(cell.value))\n\n        # Save the workbook\n        wb.save(file)\n\n        # Increment the counter\n        count += 1\n\n    return count"}
{"task_id": "BigCodeBench/375", "solution": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(l):\n    pca = PCA(n_components=2)\n    pca.fit(l)\n    x = pca.components_[0]\n    y = pca.components_[1]\n\n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n    ax.set_title('PCA Result')\n    ax.set_xlabel('First Principal Component')\n    ax.set_ylabel('Second Principal Component')\n\n    return ax", "raw_solution": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(l):\n    pca = PCA(n_components=2)\n    pca.fit(l)\n    x = pca.components_[0]\n    y = pca.components_[1]\n\n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n    ax.set_title('PCA Result')\n    ax.set_xlabel('First Principal Component')\n    ax.set_ylabel('Second Principal Component')\n\n    return ax"}
{"task_id": "BigCodeBench/376", "solution": "import nltk\nimport re\nfrom collections import Counter\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(text):\n    # Split the text into words, convert to lowercase, and remove punctuation\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Calculate the frequency of each word\n    word_freq = Counter(words)\n\n    return word_freq\ntext = \"This is a sample text. It contains several words, some of which are repeated.\"", "raw_solution": "import nltk\nimport re\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef task_func(text):\n    # Split the text into words, convert to lowercase, and remove punctuation\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Calculate the frequency of each word\n    word_freq = Counter(words)\n\n    return word_freq\n\n# Test the function\ntext = \"This is a sample text. It contains several words, some of which are repeated.\""}
{"task_id": "BigCodeBench/377", "solution": "from texttable import Texttable\nimport psutil\ndef task_func():\n    def get_size(bytes, suffix=\"B\"):\n        \"\"\"\n        Scale bytes to its proper format\n        e.g:\n            1253656 => '1.20MB'\n            1253656678 => '1.17GB'\n        \"\"\"\n        factor = 1024\n        for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n            if bytes < factor:\n                return f\"{bytes:.2f}{unit}{suffix}\"\n            bytes /= factor\n\n    def get_cpu_usage():\n        return psutil.cpu_percent()\n\n    def get_memory_usage():\n        svmem = psutil.virtual_memory()\n        return svmem.percent\n\n    def get_disk_usage():\n        disk = psutil.disk_usage('/')\n        return disk.percent\n\n    def generate_table():\n        table = Texttable()\n        table.add_rows([[\"Item\", \"Value\"],\n                        [\"CPU Usage (%)\", get_cpu_usage()],\n                        [\"Memory Usage (%)\", get_memory_usage()],\n                        [\"Disk Usage (%)\", get_disk_usage()]])\n        return table.draw()\n\n    return generate_table()", "raw_solution": "from texttable import Texttable\nimport psutil\n\ndef task_func():\n    def get_size(bytes, suffix=\"B\"):\n        \"\"\"\n        Scale bytes to its proper format\n        e.g:\n            1253656 => '1.20MB'\n            1253656678 => '1.17GB'\n        \"\"\"\n        factor = 1024\n        for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n            if bytes < factor:\n                return f\"{bytes:.2f}{unit}{suffix}\"\n            bytes /= factor\n\n    def get_cpu_usage():\n        return psutil.cpu_percent()\n\n    def get_memory_usage():\n        svmem = psutil.virtual_memory()\n        return svmem.percent\n\n    def get_disk_usage():\n        disk = psutil.disk_usage('/')\n        return disk.percent\n\n    def generate_table():\n        table = Texttable()\n        table.add_rows([[\"Item\", \"Value\"],\n                        [\"CPU Usage (%)\", get_cpu_usage()],\n                        [\"Memory Usage (%)\", get_memory_usage()],\n                        [\"Disk Usage (%)\", get_disk_usage()]])\n        return table.draw()\n\n    return generate_table()"}
{"task_id": "BigCodeBench/378", "solution": "import pandas as pd\nfrom texttable import Texttable\nimport os\nimport glob\ndef task_func(data_dir='./data/'):\n    \"\"\"\n    Generates a summary table of all ascendingly sorted CSV files in a specified directory using Texttable.\n    If an empty CSV file is encountered, a pandas.errors.EmptyDataError is raised.\n    \"\"\"\n    # Check if the directory exists\n    if not os.path.isdir(data_dir):\n        raise FileNotFoundError(f\"The directory {data_dir} does not exist.\")\n\n    # Find all CSV files in the directory\n    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n\n    # If there are no CSV files, raise a ValueError\n    if not csv_files:\n        raise ValueError(f\"No CSV files found in the directory {data_dir}.\")\n\n    # Initialize Texttable\n    t = Texttable()\n    t.add_rows([[\"File Name\", \"Number of Rows\", \"Number of Columns\"]])\n\n    # Process each CSV file\n    for csv_file in csv_files:\n        try:\n            # Read the CSV file\n            df = pd.read_csv(csv_file)\n        except pd.errors.EmptyDataError:\n            raise pd.errors.EmptyDataError(f\"The CSV file {csv_file} is empty.\")\n\n        # Add a row to the table\n        t.add_row([os.path.basename(csv_file), df.shape[0], df.shape[1]])\n\n    # Return the table as a string\n    return t.draw()", "raw_solution": "import pandas as pd\nfrom texttable import Texttable\nimport os\nimport glob\n\ndef task_func(data_dir='./data/'):\n    \"\"\"\n    Generates a summary table of all ascendingly sorted CSV files in a specified directory using Texttable.\n    If an empty CSV file is encountered, a pandas.errors.EmptyDataError is raised.\n    \"\"\"\n    # Check if the directory exists\n    if not os.path.isdir(data_dir):\n        raise FileNotFoundError(f\"The directory {data_dir} does not exist.\")\n\n    # Find all CSV files in the directory\n    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n\n    # If there are no CSV files, raise a ValueError\n    if not csv_files:\n        raise ValueError(f\"No CSV files found in the directory {data_dir}.\")\n\n    # Initialize Texttable\n    t = Texttable()\n    t.add_rows([[\"File Name\", \"Number of Rows\", \"Number of Columns\"]])\n\n    # Process each CSV file\n    for csv_file in csv_files:\n        try:\n            # Read the CSV file\n            df = pd.read_csv(csv_file)\n        except pd.errors.EmptyDataError:\n            raise pd.errors.EmptyDataError(f\"The CSV file {csv_file} is empty.\")\n\n        # Add a row to the table\n        t.add_row([os.path.basename(csv_file), df.shape[0], df.shape[1]])\n\n    # Return the table as a string\n    return t.draw()"}
{"task_id": "BigCodeBench/379", "solution": "import pandas as pd\nimport numpy as np\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length):\n    \"\"\"\n    Generates a pandas DataFrame with specified length and random data.\n    \"\"\"\n    # Generate random data\n    data = np.random.randint(0, 100, size=(length, len(COLUMNS)))\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(length):\n    \"\"\"\n    Generates a pandas DataFrame with specified length and random data.\n    \"\"\"\n    # Generate random data\n    data = np.random.randint(0, 100, size=(length, len(COLUMNS)))\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    return df\n\n# Test the function\ndf = task_func(10)"}
{"task_id": "BigCodeBench/380", "solution": "import os\nimport shutil\ndef task_func(directory):\n    \"\"\"\n    Arranges files in a directory by their extensions.\n    Creates a new directory for each extension and moves the files to the corresponding directories.\n    \"\"\"\n    # Create a dictionary to store files by extension\n    files_by_ext = {}\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Get the extension of the file\n        ext = os.path.splitext(filename)[1]\n\n        # If the extension is not in the dictionary, create a new list for it\n        if ext not in files_by_ext:\n            files_by_ext[ext] = []\n\n        # Add the file to the list for its extension\n        files_by_ext[ext].append(filename)\n\n    # Iterate over all extensions in the dictionary\n    for ext, files in files_by_ext.items():\n        # Create a new directory for the extension\n        new_dir = os.path.join(directory, ext)\n        os.makedirs(new_dir, exist_ok=True)\n\n        # Move the files to the new directory\n        for file in files:\n            old_path = os.path.join(directory, file)\n            new_path = os.path.join(new_dir, file)\n            shutil.move(old_path, new_path)\n\n    return None", "raw_solution": "import os\nimport shutil\n\ndef task_func(directory):\n    \"\"\"\n    Arranges files in a directory by their extensions.\n    Creates a new directory for each extension and moves the files to the corresponding directories.\n    \"\"\"\n    # Create a dictionary to store files by extension\n    files_by_ext = {}\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Get the extension of the file\n        ext = os.path.splitext(filename)[1]\n\n        # If the extension is not in the dictionary, create a new list for it\n        if ext not in files_by_ext:\n            files_by_ext[ext] = []\n\n        # Add the file to the list for its extension\n        files_by_ext[ext].append(filename)\n\n    # Iterate over all extensions in the dictionary\n    for ext, files in files_by_ext.items():\n        # Create a new directory for the extension\n        new_dir = os.path.join(directory, ext)\n        os.makedirs(new_dir, exist_ok=True)\n\n        # Move the files to the new directory\n        for file in files:\n            old_path = os.path.join(directory, file)\n            new_path = os.path.join(new_dir, file)\n            shutil.move(old_path, new_path)\n\n    return None"}
{"task_id": "BigCodeBench/381", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_path='arena.csv', target_column='Index', seed=42):\n    \"\"\"\n    Trains a random forest model on data from a CSV file, using one column as the target variable (y) and the rest as features (X), and visualizes the feature importances in a bar plot.\n    This function also handles missing values by dropping rows with any NaN values.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Load data\n    df = pd.read_csv(file_path)\n\n    # Check if target_column exists\n    if target_column not in df.columns:\n        raise ValueError(f\"The target_column {target_column} does not exist in the CSV file's columns.\")\n\n    # Drop rows with any NaN values\n    df = df.dropna()\n\n    # Check for infinity and large values\n    for col in df.columns:\n        if df[col].isin([float('inf'), float('-inf')]).any():\n            raise ValueError(f\"The column {col} contains infinity values.\")\n        if df[col].max() > 1e30:\n            raise ValueError(f\"The column {col} contains values too large for dtype('float32')\")\n\n    # Split data into features (X) and target (y)\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train random forest model\n    rf = RandomForestRegressor(random_state=seed)\n    rf.fit(X, y)\n\n    # Get feature importances\n    importances = rf.feature_importances_\n\n    # Create bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=X.columns, y=importances)\n    plt.xlabel('Features')\n    plt.ylabel('Importance')\n    plt.title('Feature Importances')\n    ax = plt.gca()\n\n    return ax, importances", "raw_solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_path='arena.csv', target_column='Index', seed=42):\n    \"\"\"\n    Trains a random forest model on data from a CSV file, using one column as the target variable (y) and the rest as features (X), and visualizes the feature importances in a bar plot.\n    This function also handles missing values by dropping rows with any NaN values.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Load data\n    df = pd.read_csv(file_path)\n\n    # Check if target_column exists\n    if target_column not in df.columns:\n        raise ValueError(f\"The target_column {target_column} does not exist in the CSV file's columns.\")\n\n    # Drop rows with any NaN values\n    df = df.dropna()\n\n    # Check for infinity and large values\n    for col in df.columns:\n        if df[col].isin([float('inf'), float('-inf')]).any():\n            raise ValueError(f\"The column {col} contains infinity values.\")\n        if df[col].max() > 1e30:\n            raise ValueError(f\"The column {col} contains values too large for dtype('float32')\")\n\n    # Split data into features (X) and target (y)\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train random forest model\n    rf = RandomForestRegressor(random_state=seed)\n    rf.fit(X, y)\n\n    # Get feature importances\n    importances = rf.feature_importances_\n\n    # Create bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=X.columns, y=importances)\n    plt.xlabel('Features')\n    plt.ylabel('Importance')\n    plt.title('Feature Importances')\n    ax = plt.gca()\n\n    return ax, importances"}
{"task_id": "BigCodeBench/382", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(length):\n    mu, sigma = 0, 1\n    data = np.random.normal(mu, sigma, length)\n\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(data, 30, density=True)\n    ax.plot(bins, norm(mu, sigma).pdf(bins), linewidth=2, color='r')\n\n    return data, ax", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(length):\n    mu, sigma = 0, 1\n    data = np.random.normal(mu, sigma, length)\n\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(data, 30, density=True)\n    ax.plot(bins, norm(mu, sigma).pdf(bins), linewidth=2, color='r')\n\n    return data, ax"}
{"task_id": "BigCodeBench/383", "solution": "import pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nfrom textblob import TextBlob\nfrom matplotlib import pyplot as plt\ndef task_func(text, n, top_k):\n    \"\"\"\n    This function takes a text string, an n-gram size, and a top_k value. It calculates the frequency of n-grams in the text,\n    and then plots the top_k n-grams in descending order of frequency.\n    \"\"\"\n    # Generate n-grams\n    ngrams = zip(*[text[i:] for i in range(n)])\n    ngrams = [' '.join(ngram) for ngram in ngrams]\n\n    # Count n-grams\n    counter = Counter(ngrams)\n\n    # Get top k n-grams\n    top_k_ngrams = counter.most_common(top_k)\n\n    # Create a DataFrame\n    df = pd.DataFrame(top_k_ngrams, columns=['n-gram', 'Frequency'])\n\n    # Plot the DataFrame\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Frequency', y='n-gram', data=df, ci=None, palette='muted',orient='h', )\n    plt.title(f'Top {top_k} {n}-grams')\n    plt.show()\ntext = \"The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\"\nn = 2\ntop_k = 5", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nfrom textblob import TextBlob\nfrom matplotlib import pyplot as plt\n\ndef task_func(text, n, top_k):\n    \"\"\"\n    This function takes a text string, an n-gram size, and a top_k value. It calculates the frequency of n-grams in the text,\n    and then plots the top_k n-grams in descending order of frequency.\n    \"\"\"\n    # Generate n-grams\n    ngrams = zip(*[text[i:] for i in range(n)])\n    ngrams = [' '.join(ngram) for ngram in ngrams]\n\n    # Count n-grams\n    counter = Counter(ngrams)\n\n    # Get top k n-grams\n    top_k_ngrams = counter.most_common(top_k)\n\n    # Create a DataFrame\n    df = pd.DataFrame(top_k_ngrams, columns=['n-gram', 'Frequency'])\n\n    # Plot the DataFrame\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Frequency', y='n-gram', data=df, ci=None, palette='muted',orient='h', )\n    plt.title(f'Top {top_k} {n}-grams')\n    plt.show()\n\n# Test the function\ntext = \"The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\"\nn = 2\ntop_k = 5\ntask_func(text, n, top_k)"}
{"task_id": "BigCodeBench/384", "solution": "import collections\nimport random\nimport itertools\ndef task_func(animal_dict, max_count=10, seed=0):\n    random.seed(seed)\n    reversed_dict = collections.defaultdict(list)\n    counter = collections.Counter()\n\n    for name, animal in animal_dict.items():\n        reversed_dict[animal].append(name)\n        counter[animal] = random.randint(1, max_count)\n\n    return (dict(reversed_dict), counter)\nanimal_dict = {'Alice': 'Cat', 'Bob': 'Dog', 'Charlie': 'Elephant', 'David': 'Lion', 'Eve': 'Tiger', 'Frank': 'Bear', 'Grace': 'Giraffe', 'Helen': 'Horse', 'Ivan': 'Rabbit', 'John': 'Snake', 'Kevin': 'Zebra'}", "raw_solution": "import collections\nimport random\nimport itertools\n\nANIMALS = ['Cat', 'Dog', 'Elephant', 'Lion', 'Tiger', 'Bear', 'Giraffe', 'Horse', 'Rabbit', 'Snake', 'Zebra']\n\ndef task_func(animal_dict, max_count=10, seed=0):\n    random.seed(seed)\n    reversed_dict = collections.defaultdict(list)\n    counter = collections.Counter()\n\n    for name, animal in animal_dict.items():\n        reversed_dict[animal].append(name)\n        counter[animal] = random.randint(1, max_count)\n\n    return (dict(reversed_dict), counter)\n\n# Test the function\nanimal_dict = {'Alice': 'Cat', 'Bob': 'Dog', 'Charlie': 'Elephant', 'David': 'Lion', 'Eve': 'Tiger', 'Frank': 'Bear', 'Grace': 'Giraffe', 'Helen': 'Horse', 'Ivan': 'Rabbit', 'John': 'Snake', 'Kevin': 'Zebra'}"}
{"task_id": "BigCodeBench/385", "solution": "import matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(fruit_dict):\n    # Count the frequency of each fruit\n    fruit_counts = Counter(fruit_dict.values())\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(fruit_counts.keys(), fruit_counts.values())\n    ax.set_xlabel('Fruit')\n    ax.set_ylabel('Count')\n    ax.set_title('Fruit Counts')\n\n    return fruit_counts, ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom collections import Counter\n\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry', 'Fig', 'Grape', 'Honeydew', 'Indian Prune', 'Jackfruit']\n\ndef task_func(fruit_dict):\n    # Count the frequency of each fruit\n    fruit_counts = Counter(fruit_dict.values())\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(fruit_counts.keys(), fruit_counts.values())\n    ax.set_xlabel('Fruit')\n    ax.set_ylabel('Count')\n    ax.set_title('Fruit Counts')\n\n    return fruit_counts, ax"}
{"task_id": "BigCodeBench/386", "solution": "import numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length, min_value = 0, max_value = 100):\n    \"\"\"\n    Generates a pandas DataFrame with specified ranges and length, and calculates the cumulative distribution function (CDF).\n    \"\"\"\n    # Generate a DataFrame with random values\n    df = pd.DataFrame(np.random.randint(min_value, max_value, size=(length, len(COLUMNS))), columns=COLUMNS)\n\n    # Calculate the cumulative distribution function (CDF)\n    cdf = df.apply(lambda x: x.sort_values().values)\n\n    return cdf", "raw_solution": "import numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(length, min_value = 0, max_value = 100):\n    \"\"\"\n    Generates a pandas DataFrame with specified ranges and length, and calculates the cumulative distribution function (CDF).\n    \"\"\"\n    # Generate a DataFrame with random values\n    df = pd.DataFrame(np.random.randint(min_value, max_value, size=(length, len(COLUMNS))), columns=COLUMNS)\n\n    # Calculate the cumulative distribution function (CDF)\n    cdf = df.apply(lambda x: x.sort_values().values)\n\n    return cdf"}
{"task_id": "BigCodeBench/387", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney', 'Paris', 'Berlin', 'Moscow', 'Madrid', 'Rome']\ndef task_func(city_dict, max_range=1000000, seed=0):\n    np.random.seed(seed)\n    city_population = {}\n    for name, city in city_dict.items():\n        if city in CITIES:\n            city_population[city] = np.random.randint(1, max_range)\n        else:\n            city_population[city] = -1\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.bar(city_population.keys(), city_population.values())\n    ax.set_xlabel('Cities')\n    ax.set_ylabel('Population')\n    ax.set_title('City Population')\n    plt.show()\n\n    return city_population, ax\ncity_dict = {'Alice': 'New York', 'Bob': 'London', 'Charlie': 'Beijing', 'David': 'Tokyo', 'Eve': 'Sydney'}", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney', 'Paris', 'Berlin', 'Moscow', 'Madrid', 'Rome']\n\ndef task_func(city_dict, max_range=1000000, seed=0):\n    np.random.seed(seed)\n    city_population = {}\n    for name, city in city_dict.items():\n        if city in CITIES:\n            city_population[city] = np.random.randint(1, max_range)\n        else:\n            city_population[city] = -1\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.bar(city_population.keys(), city_population.values())\n    ax.set_xlabel('Cities')\n    ax.set_ylabel('Population')\n    ax.set_title('City Population')\n    plt.show()\n\n    return city_population, ax\n\n# Testing\ncity_dict = {'Alice': 'New York', 'Bob': 'London', 'Charlie': 'Beijing', 'David': 'Tokyo', 'Eve': 'Sydney'}\npopulation_dict, ax = task_func(city_dict, max_range=1000000, seed=0)"}
{"task_id": "BigCodeBench/388", "solution": "import collections\nimport pandas as pd\ndef task_func(my_tuple, path_csv_files):\n    \"\"\"\n    Count the occurrences of each value in the specified columns in multiple CSV files.\n    \"\"\"\n    result = {}\n    for file in path_csv_files:\n        df = pd.read_csv(file)\n        for column in my_tuple:\n            if column in df.columns:\n                column_data = df[column].values\n                counter = collections.Counter(column_data)\n                result[column] = dict(counter)\n    return result", "raw_solution": "import collections\nimport pandas as pd\n\ndef task_func(my_tuple, path_csv_files):\n    \"\"\"\n    Count the occurrences of each value in the specified columns in multiple CSV files.\n    \"\"\"\n    result = {}\n    for file in path_csv_files:\n        df = pd.read_csv(file)\n        for column in my_tuple:\n            if column in df.columns:\n                column_data = df[column].values\n                counter = collections.Counter(column_data)\n                result[column] = dict(counter)\n    return result"}
{"task_id": "BigCodeBench/389", "solution": "import re\nimport os\nimport shutil\ndef task_func(directory):\n    \"\"\"\n    Finds files with filenames that contain \"like\" or \"what\" in a directory,\n    creates a new subdirectory called \"Interesting Files\" and moves those files to the new subdirectory.\n    \"\"\"\n    # Create the \"Interesting Files\" subdirectory if it doesn't exist\n    if not os.path.exists(os.path.join(directory, \"Interesting Files\")):\n        os.makedirs(os.path.join(directory, \"Interesting Files\"))\n\n    # List to store the names of the files moved\n    files_moved = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the filename contains \"like\" or \"what\"\n        if re.search(r'like|what', filename):\n            # Move the file to the \"Interesting Files\" subdirectory\n            shutil.move(os.path.join(directory, filename), os.path.join(directory, \"Interesting Files\", filename))\n            # Add the filename to the list of files moved\n            files_moved.append(filename)\n\n    # Return the list of files moved\n    return files_moved", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(directory):\n    \"\"\"\n    Finds files with filenames that contain \"like\" or \"what\" in a directory,\n    creates a new subdirectory called \"Interesting Files\" and moves those files to the new subdirectory.\n    \"\"\"\n    # Create the \"Interesting Files\" subdirectory if it doesn't exist\n    if not os.path.exists(os.path.join(directory, \"Interesting Files\")):\n        os.makedirs(os.path.join(directory, \"Interesting Files\"))\n\n    # List to store the names of the files moved\n    files_moved = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the filename contains \"like\" or \"what\"\n        if re.search(r'like|what', filename):\n            # Move the file to the \"Interesting Files\" subdirectory\n            shutil.move(os.path.join(directory, filename), os.path.join(directory, \"Interesting Files\", filename))\n            # Add the filename to the list of files moved\n            files_moved.append(filename)\n\n    # Return the list of files moved\n    return files_moved"}
{"task_id": "BigCodeBench/390", "solution": "import pandas as pd\nimport requests\nfrom io import StringIO\ndef task_func(csv_url_dict, sort_by_column=\"title\"):\n    if not csv_url_dict:\n        raise ValueError(\"The dictionary is empty.\")\n    if \"URL\" not in csv_url_dict:\n        raise ValueError('The key \"URL\" does not exist in the dictionary.')\n\n    url = csv_url_dict[\"URL\"]\n    response = requests.get(url)\n    data = response.text\n    df = pd.read_csv(StringIO(data))\n    sorted_df = df.sort_values(by=[sort_by_column])\n\n    return sorted_df", "raw_solution": "import pandas as pd\nimport requests\nfrom io import StringIO\n\ndef task_func(csv_url_dict, sort_by_column=\"title\"):\n    if not csv_url_dict:\n        raise ValueError(\"The dictionary is empty.\")\n    if \"URL\" not in csv_url_dict:\n        raise ValueError('The key \"URL\" does not exist in the dictionary.')\n\n    url = csv_url_dict[\"URL\"]\n    response = requests.get(url)\n    data = response.text\n    df = pd.read_csv(StringIO(data))\n    sorted_df = df.sort_values(by=[sort_by_column])\n\n    return sorted_df"}
{"task_id": "BigCodeBench/391", "solution": "import os\nimport glob\nimport shutil\ndef task_func(directory, archive_dir='archive'):\n    \"\"\"\n    Archives all JSON files in a given directory by moving them to a specified archive directory.\n    The function returns a tuple containing a boolean value and a list of error messages.\n    The boolean is True if all files are successfully moved, and False otherwise.\n    The list contains error messages for each file that failed to move.\n    \"\"\"\n    # Create the archive directory if it doesn't exist\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n\n    # Get a list of all JSON files in the directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n\n    # Initialize error messages list\n    error_messages = []\n\n    # Move each JSON file to the archive directory\n    for file in json_files:\n        try:\n            shutil.move(file, os.path.join(archive_dir, os.path.basename(file)))\n        except Exception as e:\n            error_messages.append(f\"Failed to move file {file}: {str(e)}\")\n\n    # If there were no errors, return True, otherwise return False\n    return (not error_messages, error_messages)", "raw_solution": "import os\nimport glob\nimport shutil\n\ndef task_func(directory, archive_dir='archive'):\n    \"\"\"\n    Archives all JSON files in a given directory by moving them to a specified archive directory.\n    The function returns a tuple containing a boolean value and a list of error messages.\n    The boolean is True if all files are successfully moved, and False otherwise.\n    The list contains error messages for each file that failed to move.\n    \"\"\"\n    # Create the archive directory if it doesn't exist\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n\n    # Get a list of all JSON files in the directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n\n    # Initialize error messages list\n    error_messages = []\n\n    # Move each JSON file to the archive directory\n    for file in json_files:\n        try:\n            shutil.move(file, os.path.join(archive_dir, os.path.basename(file)))\n        except Exception as e:\n            error_messages.append(f\"Failed to move file {file}: {str(e)}\")\n\n    # If there were no errors, return True, otherwise return False\n    return (not error_messages, error_messages)"}
{"task_id": "BigCodeBench/392", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nCOLORS = ['r', 'g', 'b']\ndef task_func(df, group_col, value_col, group_name):\n    \"\"\"\n    Creates a bar subplot of a specific group from the input dataframe.\n    \"\"\"\n    if group_name not in df[group_col].values:\n        raise ValueError(f\"The group_name {group_name} does not exist in df.\")\n\n    group_data = df[df[group_col] == group_name][value_col]\n\n    fig, ax = plt.subplots()\n    ax.bar(group_name, group_data.mean(), color=COLORS[0])\n    ax.set_title(f'Bar chart of {value_col} for {group_name}')\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef task_func(df, group_col, value_col, group_name):\n    \"\"\"\n    Creates a bar subplot of a specific group from the input dataframe.\n    \"\"\"\n    if group_name not in df[group_col].values:\n        raise ValueError(f\"The group_name {group_name} does not exist in df.\")\n\n    group_data = df[df[group_col] == group_name][value_col]\n\n    fig, ax = plt.subplots()\n    ax.bar(group_name, group_data.mean(), color=COLORS[0])\n    ax.set_title(f'Bar chart of {value_col} for {group_name}')\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n\n    return ax"}
{"task_id": "BigCodeBench/393", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mu, sigma, num_samples=1000, seed=77):\n    np.random.seed(seed)\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n\n    # Histogram\n    ax[0].hist(samples, bins=30, density=True)\n    ax[0].set_title('Histogram')\n    ax[0].set_xlabel('Value')\n    ax[0].set_ylabel('Frequency')\n\n    # Q-Q plot\n    stats.probplot(samples, plot=ax[1])\n    ax[1].set_title('Q-Q Plot')\n\n    plt.tight_layout()\n    return fig", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(mu, sigma, num_samples=1000, seed=77):\n    np.random.seed(seed)\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n\n    # Histogram\n    ax[0].hist(samples, bins=30, density=True)\n    ax[0].set_title('Histogram')\n    ax[0].set_xlabel('Value')\n    ax[0].set_ylabel('Frequency')\n\n    # Q-Q plot\n    stats.probplot(samples, plot=ax[1])\n    ax[1].set_title('Q-Q Plot')\n\n    plt.tight_layout()\n    return fig"}
{"task_id": "BigCodeBench/394", "solution": "import collections\nimport string\nimport random\ndef task_func(length, seed=0):\n    random.seed(seed)\n    random_string = ''.join(random.choices(string.ascii_letters, k=length))\n    frequency = collections.Counter(random_string)\n    return dict(frequency)", "raw_solution": "import collections\nimport string\nimport random\n\ndef task_func(length, seed=0):\n    random.seed(seed)\n    random_string = ''.join(random.choices(string.ascii_letters, k=length))\n    frequency = collections.Counter(random_string)\n    return dict(frequency)\n\n# Test the function"}
{"task_id": "BigCodeBench/395", "solution": "import re\nimport os\nimport glob\nimport natsort\nimport pandas as pd\ndef task_func(directory='./', file_pattern='*.txt', regex=r'([0-9]+)'):\n    \"\"\"\n    Extracts numeric data from all text files matching a given pattern in a directory and compiles it into a Pandas DataFrame.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The specified directory {directory} does not exist.\")\n\n    file_list = glob.glob(os.path.join(directory, file_pattern))\n    if not file_list:\n        raise ValueError(f\"No files matching the pattern {file_pattern} were found in the directory {directory}.\")\n\n    file_list = natsort.natsorted(file_list)\n\n    data = []\n    for file in file_list:\n        with open(file, 'r') as f:\n            content = f.read()\n            numeric_data = re.findall(regex, content)\n            if numeric_data:\n                data.append({'Filename': file, 'Numeric Data': numeric_data})\n\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import re\nimport os\nimport glob\nimport natsort\nimport pandas as pd\n\ndef task_func(directory='./', file_pattern='*.txt', regex=r'([0-9]+)'):\n    \"\"\"\n    Extracts numeric data from all text files matching a given pattern in a directory and compiles it into a Pandas DataFrame.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The specified directory {directory} does not exist.\")\n\n    file_list = glob.glob(os.path.join(directory, file_pattern))\n    if not file_list:\n        raise ValueError(f\"No files matching the pattern {file_pattern} were found in the directory {directory}.\")\n\n    file_list = natsort.natsorted(file_list)\n\n    data = []\n    for file in file_list:\n        with open(file, 'r') as f:\n            content = f.read()\n            numeric_data = re.findall(regex, content)\n            if numeric_data:\n                data.append({'Filename': file, 'Numeric Data': numeric_data})\n\n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/396", "solution": "import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mu, sigma, sample_size, seed=0):\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n    density = stats.gaussian_kde(sample)\n    xs = np.linspace(sample.min(), sample.max(), 200)\n    plt.plot(xs, density(xs))\n    plt.title(f'Gaussian Kernel Density Estimate (Mean: {mu}, Std Dev: {sigma}, Sample Size: {sample_size})')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    return plt.gca()", "raw_solution": "import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(mu, sigma, sample_size, seed=0):\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n    density = stats.gaussian_kde(sample)\n    xs = np.linspace(sample.min(), sample.max(), 200)\n    plt.plot(xs, density(xs))\n    plt.title(f'Gaussian Kernel Density Estimate (Mean: {mu}, Std Dev: {sigma}, Sample Size: {sample_size})')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    return plt.gca()"}
{"task_id": "BigCodeBench/397", "solution": "import re\nimport urllib.request\nimport json", "raw_solution": "import re\nimport urllib.request\nimport json\n\n# Constants\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef get_public_ip(API_URL):\n    \"\"\"\n    Fetches the public IP address of the current host from an API.\n    If the API request fails, the function will return the error message.\n    \"\"\"\n    try:\n        response = urllib.request.urlopen(API_URL)\n        data = response.read()\n        json_data = json.loads(data)\n\n        ip_address = json_data['ip']\n\n        if re.match(IP_REGEX, ip_address):\n            return ip_address\n        else:\n            return \"Invalid IP address\"\n\n    except urllib.error.URLError as e:\n        return str(e)\n\n# Test the function\nAPI_URL = 'https://api.ipify.org?format=json'"}
{"task_id": "BigCodeBench/398", "solution": "import json\nimport os\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return False\n\n    with open(file_path, 'r') as file:\n        try:\n            data = json.load(file)\n        except json.JSONDecodeError:\n            return False\n\n    if not isinstance(data, list):\n        return False\n\n    for item in data:\n        if not isinstance(item, dict):\n            return False\n\n    return True", "raw_solution": "import json\nimport os\n\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return False\n\n    with open(file_path, 'r') as file:\n        try:\n            data = json.load(file)\n        except json.JSONDecodeError:\n            return False\n\n    if not isinstance(data, list):\n        return False\n\n    for item in data:\n        if not isinstance(item, dict):\n            return False\n\n    return True"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n\n    x = np.linspace(0, 1, sample_size, False)\n    y_sin = np.sin(2 * np.pi * frequency * x)\n    y_cos = np.cos(2 * np.pi * frequency * x)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y_sin, label='Sine Wave')\n    ax.plot(x, y_cos, label='Cosine Wave')\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency}')\n\n    return fig, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n\n    x = np.linspace(0, 1, sample_size, False)\n    y_sin = np.sin(2 * np.pi * frequency * x)\n    y_cos = np.cos(2 * np.pi * frequency * x)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y_sin, label='Sine Wave')\n    ax.plot(x, y_cos, label='Cosine Wave')\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency}')\n\n    return fig, ax"}
{"task_id": "BigCodeBench/400", "solution": "import json\nimport os\ndef task_func(directory, string):\n    \"\"\"\n    Searches for a specific string within the JSON data of files in a given directory and its subdirectories.\n    \"\"\"\n    # Initialize an empty list to store the file paths\n    file_paths = []\n\n    # Use os.walk to recursively traverse the directory and its subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file is a JSON file\n            if file.endswith('.json'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    try:\n                        # Try to load the JSON data from the file\n                        data = json.load(f)\n\n                        # Use the json.dumps method to convert the data to a string\n                        # This is necessary because the json.load method returns a Python data structure\n                        # We then check if the string contains the given string\n                        if string in json.dumps(data):\n                            file_paths.append(file_path)\n                    except json.JSONDecodeError:\n                        # If the file is not a valid JSON file, skip it\n                        continue\n\n    return file_paths", "raw_solution": "import json\nimport os\n\ndef task_func(directory, string):\n    \"\"\"\n    Searches for a specific string within the JSON data of files in a given directory and its subdirectories.\n    \"\"\"\n    # Initialize an empty list to store the file paths\n    file_paths = []\n\n    # Use os.walk to recursively traverse the directory and its subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file is a JSON file\n            if file.endswith('.json'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    try:\n                        # Try to load the JSON data from the file\n                        data = json.load(f)\n\n                        # Use the json.dumps method to convert the data to a string\n                        # This is necessary because the json.load method returns a Python data structure\n                        # We then check if the string contains the given string\n                        if string in json.dumps(data):\n                            file_paths.append(file_path)\n                    except json.JSONDecodeError:\n                        # If the file is not a valid JSON file, skip it\n                        continue\n\n    return file_paths"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables\n    app.config['MAIL_SERVER'] = os.environ.get('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.environ.get('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = os.environ.get('MAIL_USE_TLS', False) in ['True', 'true', '1']\n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD', None)\n\n    mail = Mail(app)\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']", "raw_solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables\n    app.config['MAIL_SERVER'] = os.environ.get('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.environ.get('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = os.environ.get('MAIL_USE_TLS', False) in ['True', 'true', '1']\n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD', None)\n\n    mail = Mail(app)\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']"}
{"task_id": "BigCodeBench/402", "solution": "import re\nimport requests\nimport json\nimport csv\nimport os\nAPI_URL = 'https://api.example.com/data'\ndef task_func(pattern):\n    # Make a GET request to the API\n    response = requests.get(API_URL)\n\n    # Extract the data from the JSON response\n    data = json.loads(response.text)['data']\n\n    # Initialize a list to store the matched data\n    matched_data = []\n\n    # Iterate over the data and match it with the RegEx pattern\n    for item in data:\n        if re.search(pattern, item):\n            matched_data.append(item)\n\n    # Get the absolute path to the CSV file\n    csv_file_path = os.path.join(os.getcwd(), 'matched_data.csv')\n\n    # Write the matched data to the CSV file\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        for row in matched_data:\n            writer.writerow([row])\n\n    return csv_file_path", "raw_solution": "import re\nimport requests\nimport json\nimport csv\nimport os\n\n# Constants\nAPI_URL = 'https://api.example.com/data'\n\ndef task_func(pattern):\n    # Make a GET request to the API\n    response = requests.get(API_URL)\n\n    # Extract the data from the JSON response\n    data = json.loads(response.text)['data']\n\n    # Initialize a list to store the matched data\n    matched_data = []\n\n    # Iterate over the data and match it with the RegEx pattern\n    for item in data:\n        if re.search(pattern, item):\n            matched_data.append(item)\n\n    # Get the absolute path to the CSV file\n    csv_file_path = os.path.join(os.getcwd(), 'matched_data.csv')\n\n    # Write the matched data to the CSV file\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        for row in matched_data:\n            writer.writerow([row])\n\n    return csv_file_path"}
{"task_id": "BigCodeBench/403", "solution": "from PIL import Image, ImageFilter\nimport cv2\nimport numpy as np\nimport os\ndef task_func(img_path, blur_radius=5):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The image file {img_path} does not exist.\")\n\n    # Open the image\n    img = Image.open(img_path)\n\n    # Apply a blur filter\n    img_blur = img.filter(ImageFilter.GaussianBlur(blur_radius))\n\n    # Convert the image to grayscale\n    img_gray = img_blur.convert('L')\n\n    # Convert the images to numpy arrays\n    img_np = np.array(img)\n    img_blur_gray_np = np.array(img_gray)\n\n    # Display the original and the edited images side by side\n    cv2.imshow('Original Image', img_np)\n    cv2.imshow('Blurred and Grayscaled Image', img_blur_gray_np)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n    return (img_np, img_blur_gray_np)", "raw_solution": "from PIL import Image, ImageFilter\nimport cv2\nimport numpy as np\nimport os\n\ndef task_func(img_path, blur_radius=5):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The image file {img_path} does not exist.\")\n\n    # Open the image\n    img = Image.open(img_path)\n\n    # Apply a blur filter\n    img_blur = img.filter(ImageFilter.GaussianBlur(blur_radius))\n\n    # Convert the image to grayscale\n    img_gray = img_blur.convert('L')\n\n    # Convert the images to numpy arrays\n    img_np = np.array(img)\n    img_blur_gray_np = np.array(img_gray)\n\n    # Display the original and the edited images side by side\n    cv2.imshow('Original Image', img_np)\n    cv2.imshow('Blurred and Grayscaled Image', img_blur_gray_np)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n    return (img_np, img_blur_gray_np)"}
{"task_id": "BigCodeBench/404", "solution": "import cv2\nimport numpy as np\nimport os\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(\"The image file does not exist at the specified path.\")\n\n    # Load the image\n    img = cv2.imread(img_path)\n\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Find contours in the image\n    contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    return (img, contours)", "raw_solution": "import cv2\nimport numpy as np\nimport os\n\ndef task_func(img_path):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(\"The image file does not exist at the specified path.\")\n\n    # Load the image\n    img = cv2.imread(img_path)\n\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Find contours in the image\n    contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    return (img, contours)"}
{"task_id": "BigCodeBench/405", "solution": "import random\nimport matplotlib.pyplot as plt\ndef task_func(points: int):\n    \"\"\"\n    Generates a list of random numbers and a matplotlib Axes object representing the plot.\n    The indices of the list are on the x-axis and the generated numbers are on the y-axis.\n    \"\"\"\n    random_numbers = [random.random() for _ in range(points)]\n    fig, ax = plt.subplots()\n    ax.plot(range(points), random_numbers)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Random Number')\n    ax.set_title('Random Numbers Plot')\n    return random_numbers, ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\ndef task_func(points: int):\n    \"\"\"\n    Generates a list of random numbers and a matplotlib Axes object representing the plot.\n    The indices of the list are on the x-axis and the generated numbers are on the y-axis.\n    \"\"\"\n    random_numbers = [random.random() for _ in range(points)]\n    fig, ax = plt.subplots()\n    ax.plot(range(points), random_numbers)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Random Number')\n    ax.set_title('Random Numbers Plot')\n    return random_numbers, ax"}
{"task_id": "BigCodeBench/406", "solution": "from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\ndef task_func(img_path, angle):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The image file {img_path} does not exist.\")\n\n    img = Image.open(img_path)\n    img_array = np.array(img)\n\n    rotated_img = img.rotate(angle, expand=True)\n    rotated_img_array = np.array(rotated_img)\n\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(img_array)\n    axs[0].set_title('Original Image')\n    axs[1].imshow(rotated_img_array)\n    axs[1].set_title(f'Rotated Image by {angle} degrees')\n    plt.show()\n\n    return img_array, rotated_img_array", "raw_solution": "from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\ndef task_func(img_path, angle):\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The image file {img_path} does not exist.\")\n\n    img = Image.open(img_path)\n    img_array = np.array(img)\n\n    rotated_img = img.rotate(angle, expand=True)\n    rotated_img_array = np.array(rotated_img)\n\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(img_array)\n    axs[0].set_title('Original Image')\n    axs[1].imshow(rotated_img_array)\n    axs[1].set_title(f'Rotated Image by {angle} degrees')\n    plt.show()\n\n    return img_array, rotated_img_array"}
{"task_id": "BigCodeBench/407", "solution": "import os\nimport csv\nfrom openpyxl import load_workbook\ndef task_func(file_name, excel_file_path, csv_file_path) -> str:\n    \"\"\"\n    Converts an Excel file (.xls or .xlsx) to a CSV file by reading the contents of the Excel file and writing them to a new CSV file with the same name but a different extension.\n    Allows specifying separate paths for the Excel file source and the CSV file destination.\n    This function assumes the active sheet is the one to be converted.\n    \"\"\"\n    # Load the workbook\n    workbook = load_workbook(filename=os.path.join(excel_file_path, file_name), read_only=True)\n\n    # Get the active sheet\n    worksheet = workbook.active\n\n    # Create the CSV file path\n    csv_file_name = f\"{file_name.split('.')[0]}.csv\"\n    csv_file_path = os.path.join(csv_file_path, csv_file_name)\n\n    # Open the CSV file for writing\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n\n        # Write the header row\n        writer.writerow([cell.value for cell in worksheet[1]])\n\n        # Write the data rows\n        for row in worksheet.iter_rows(min_row=2, values_only=True):\n            writer.writerow(row)\n\n    return csv_file_name", "raw_solution": "import os\nimport csv\nfrom openpyxl import load_workbook\n\ndef task_func(file_name, excel_file_path, csv_file_path) -> str:\n    \"\"\"\n    Converts an Excel file (.xls or .xlsx) to a CSV file by reading the contents of the Excel file and writing them to a new CSV file with the same name but a different extension.\n    Allows specifying separate paths for the Excel file source and the CSV file destination.\n    This function assumes the active sheet is the one to be converted.\n    \"\"\"\n    # Load the workbook\n    workbook = load_workbook(filename=os.path.join(excel_file_path, file_name), read_only=True)\n\n    # Get the active sheet\n    worksheet = workbook.active\n\n    # Create the CSV file path\n    csv_file_name = f\"{file_name.split('.')[0]}.csv\"\n    csv_file_path = os.path.join(csv_file_path, csv_file_name)\n\n    # Open the CSV file for writing\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n\n        # Write the header row\n        writer.writerow([cell.value for cell in worksheet[1]])\n\n        # Write the data rows\n        for row in worksheet.iter_rows(min_row=2, values_only=True):\n            writer.writerow(row)\n\n    return csv_file_name"}
{"task_id": "BigCodeBench/408", "solution": "import sqlite3\nimport pandas as pd\nfrom typing import List, Dict", "raw_solution": "import sqlite3\nimport pandas as pd\nfrom typing import List, Dict\n\ndef query_sqlite_db(db_file: str, query: str) -> pd.DataFrame:\n    \"\"\"\n    Queries an SQLite database and returns the results as a pandas DataFrame.\n\n    :param db_file: The path to the SQLite database file.\n    :param query: The SQL query to execute.\n    :return: A pandas DataFrame containing the results of the executed query.\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # Execute the SQL query\n    df = pd.read_sql_query(query, conn)\n\n    # Close the database connection\n    conn.close()\n\n    # Return the results as a pandas DataFrame\n    return df"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculates the mean, median, and standard deviation of the data from a specific column in an Excel file.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.exists(excel_file_path + file_name):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path.\")\n\n    # Read the Excel file\n    df = pd.read_excel(excel_file_path + file_name)\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file.\")\n\n    # Calculate the mean, median, and standard deviation\n    mean_val = np.mean(df[column_name])\n    median_val = np.median(df[column_name])\n    std_dev_val = np.std(df[column_name])\n\n    # Return the results in a dictionary\n    return {\n        'mean': mean_val,\n        'median': median_val,\n        'standard_deviation': std_dev_val\n    }", "raw_solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculates the mean, median, and standard deviation of the data from a specific column in an Excel file.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.exists(excel_file_path + file_name):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path.\")\n\n    # Read the Excel file\n    df = pd.read_excel(excel_file_path + file_name)\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file.\")\n\n    # Calculate the mean, median, and standard deviation\n    mean_val = np.mean(df[column_name])\n    median_val = np.median(df[column_name])\n    std_dev_val = np.std(df[column_name])\n\n    # Return the results in a dictionary\n    return {\n        'mean': mean_val,\n        'median': median_val,\n        'standard_deviation': std_dev_val\n    }"}
{"task_id": "BigCodeBench/410", "solution": "import os\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(excel_directory: str, file_name: str, column_name: str, start_date: str, end_date: str) -> pd.DataFrame:\n    \"\"\"\n    Filters data in a specific date range from a column in an Excel file and returns a Pandas DataFrame of the filtered data.\n    \"\"\"\n    # Check if the file exists\n    file_path = os.path.join(excel_directory, file_name)\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The specified Excel file {file_name} does not exist.\")\n\n    # Load the Excel file\n    df = pd.read_excel(file_path)\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame.\")\n\n    # Check if the start_date and end_date are in the correct format\n    try:\n        start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n    except ValueError:\n        raise ValueError(\"start_date and end_date should be in the format 'YYYY-MM-DD'.\")\n\n    # Filter the data\n    df = df[df[column_name].between(start_date, end_date)]\n\n    return df", "raw_solution": "import os\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(excel_directory: str, file_name: str, column_name: str, start_date: str, end_date: str) -> pd.DataFrame:\n    \"\"\"\n    Filters data in a specific date range from a column in an Excel file and returns a Pandas DataFrame of the filtered data.\n    \"\"\"\n    # Check if the file exists\n    file_path = os.path.join(excel_directory, file_name)\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The specified Excel file {file_name} does not exist.\")\n\n    # Load the Excel file\n    df = pd.read_excel(file_path)\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame.\")\n\n    # Check if the start_date and end_date are in the correct format\n    try:\n        start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n    except ValueError:\n        raise ValueError(\"start_date and end_date should be in the format 'YYYY-MM-DD'.\")\n\n    # Filter the data\n    df = df[df[column_name].between(start_date, end_date)]\n\n    return df"}
{"task_id": "BigCodeBench/411", "solution": "import pandas as pd\nimport json\nimport os\ndef task_func(data: dict, output_path: str = \"./default_data_output.json\") -> str:\n    \"\"\"\n    Converts the given DataFrame to a dictionary, dropping the column named 'c' if it exists,\n    and then saves it as a JSON file.\n    \"\"\"\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Drop the column named 'c' if it exists\n    if 'c' in df.columns:\n        df = df.drop(columns='c')\n\n    # Convert the DataFrame to a dictionary\n    df_dict = df.to_dict(orient='records')\n\n    # Save the dictionary as a JSON file\n    with open(output_path, 'w') as f:\n        json.dump(df_dict, f)\n\n    # Return the path where the JSON file was saved\n    return os.path.abspath(output_path)", "raw_solution": "import pandas as pd\nimport json\nimport os\n\ndef task_func(data: dict, output_path: str = \"./default_data_output.json\") -> str:\n    \"\"\"\n    Converts the given DataFrame to a dictionary, dropping the column named 'c' if it exists,\n    and then saves it as a JSON file.\n    \"\"\"\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Drop the column named 'c' if it exists\n    if 'c' in df.columns:\n        df = df.drop(columns='c')\n\n    # Convert the DataFrame to a dictionary\n    df_dict = df.to_dict(orient='records')\n\n    # Save the dictionary as a JSON file\n    with open(output_path, 'w') as f:\n        json.dump(df_dict, f)\n\n    # Return the path where the JSON file was saved\n    return os.path.abspath(output_path)"}
{"task_id": "BigCodeBench/412", "solution": "import json\nimport base64\nimport unicodedata\ndef task_func(json_file: str) -> dict:\n    \"\"\"\n    This function reads a JSON file where each key is a unique identifier, and the corresponding value is a base64 encoded string.\n    After decoding, it applies Unicode normalization form C (NFC) to each decoded string to ensure the canonical composition of characters.\n    The function returns a dictionary where the keys are preserved, and the values are the normalized, decoded strings. Decoding is performed using the UTF-8 encoding scheme.\n    \"\"\"\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    decoded_data = {key: base64.b64decode(value).decode('utf-8') for key, value in data.items()}\n    normalized_data = {key: unicodedata.normalize('NFC', value) for key, value in decoded_data.items()}\n\n    return normalized_data", "raw_solution": "import json\nimport base64\nimport unicodedata\n\ndef task_func(json_file: str) -> dict:\n    \"\"\"\n    This function reads a JSON file where each key is a unique identifier, and the corresponding value is a base64 encoded string.\n    After decoding, it applies Unicode normalization form C (NFC) to each decoded string to ensure the canonical composition of characters.\n    The function returns a dictionary where the keys are preserved, and the values are the normalized, decoded strings. Decoding is performed using the UTF-8 encoding scheme.\n    \"\"\"\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    decoded_data = {key: base64.b64decode(value).decode('utf-8') for key, value in data.items()}\n    normalized_data = {key: unicodedata.normalize('NFC', value) for key, value in decoded_data.items()}\n\n    return normalized_data"}
{"task_id": "BigCodeBench/413", "solution": "import os\nfrom flask import Flask\nfrom flask_mail import Mail\ndef task_func(app):\n    mail = Mail(app)\n\n    app.config['MAIL_SERVER'] = os.environ.get('MAIL_SERVER', 'smtp.example.com')\n    app.config['MAIL_PORT'] = int(os.environ.get('MAIL_PORT', '587'))\n    app.config['MAIL_USE_TLS'] = os.environ.get('MAIL_USE_TLS', True)\n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME', 'username')\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD', 'password')\n    app.config['MAIL_DEFAULT_SENDER'] = os.environ.get('MAIL_DEFAULT_SENDER', 'sender@example.com')\n\n    return mail, app.config['MAIL']\napp = Flask(__name__)", "raw_solution": "import os\nfrom flask import Flask\nfrom flask_mail import Mail\n\ndef task_func(app):\n    mail = Mail(app)\n\n    app.config['MAIL_SERVER'] = os.environ.get('MAIL_SERVER', 'smtp.example.com')\n    app.config['MAIL_PORT'] = int(os.environ.get('MAIL_PORT', '587'))\n    app.config['MAIL_USE_TLS'] = os.environ.get('MAIL_USE_TLS', True)\n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME', 'username')\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD', 'password')\n    app.config['MAIL_DEFAULT_SENDER'] = os.environ.get('MAIL_DEFAULT_SENDER', 'sender@example.com')\n\n    return mail, app.config['MAIL']\n\n# Testing\napp = Flask(__name__)\nmail, mail_config = task_func(app)"}
{"task_id": "BigCodeBench/414", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data, column=\"c\"):\n    if column in data.columns:\n        data = data.drop(column, axis=1)\n    if any(data.dtypes == np.float64) or any(data.dtypes == np.int64):\n        fig, ax = plt.subplots()\n        data.plot(ax=ax)\n        plt.tight_layout()\n        return data, ax\n    else:\n        return data, None\ndata = pd.DataFrame({\n    'a': [1, 2, 3, 4, 5],\n    'b': ['a', 'b', 'c', 'd', 'e'],\n    'c': [1.0, 2.0, 3.0, 4.0, 5.0],\n    'd': [10, 20, 30, 40, 50]\n})", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column=\"c\"):\n    if column in data.columns:\n        data = data.drop(column, axis=1)\n    if any(data.dtypes == np.float64) or any(data.dtypes == np.int64):\n        fig, ax = plt.subplots()\n        data.plot(ax=ax)\n        plt.tight_layout()\n        return data, ax\n    else:\n        return data, None\n\n# Test the function\ndata = pd.DataFrame({\n    'a': [1, 2, 3, 4, 5],\n    'b': ['a', 'b', 'c', 'd', 'e'],\n    'c': [1.0, 2.0, 3.0, 4.0, 5.0],\n    'd': [10, 20, 30, 40, 50]\n})\n\ndf, ax = task_func(data, column=\"c\")\nassert df.shape[1] == 3\nassert ax is not None\n\ndf, ax = task_func(data, column=\"b\")\nassert df.shape[1] == 3\nassert ax is None"}
{"task_id": "BigCodeBench/415", "solution": "import pandas as pd\nimport codecs\ndef task_func(dataframe: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input should be a Pandas DataFrame\")\n    if \"UnicodeString\" not in dataframe.columns:\n        raise KeyError(\"Column 'UnicodeString' does not exist in the DataFrame\")\n\n    dataframe['UnicodeString'] = dataframe['UnicodeString'].apply(lambda x: codecs.decode(x.encode('utf-8'), 'unicode_escape'))\n    return dataframe", "raw_solution": "import pandas as pd\nimport codecs\n\ndef task_func(dataframe: pd.DataFrame) -> pd.DataFrame:\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input should be a Pandas DataFrame\")\n    if \"UnicodeString\" not in dataframe.columns:\n        raise KeyError(\"Column 'UnicodeString' does not exist in the DataFrame\")\n\n    dataframe['UnicodeString'] = dataframe['UnicodeString'].apply(lambda x: codecs.decode(x.encode('utf-8'), 'unicode_escape'))\n    return dataframe"}
{"task_id": "BigCodeBench/416", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(data, column=\"c\"):\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Data should be a pandas DataFrame\")\n    if not isinstance(column, str):\n        raise TypeError(\"Column should be a string\")\n    if data.empty or not any(data.dtypes[data.dtypes != \"object\"].values):\n        return None\n\n    # Remove the column\n    data = data.drop(column, axis=1)\n\n    # Calculate the correlation matrix\n    corr = data.corr()\n\n    # Create a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Create a figure and an axes\n    fig, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(data, column=\"c\"):\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Data should be a pandas DataFrame\")\n    if not isinstance(column, str):\n        raise TypeError(\"Column should be a string\")\n    if data.empty or not any(data.dtypes[data.dtypes != \"object\"].values):\n        return None\n\n    # Remove the column\n    data = data.drop(column, axis=1)\n\n    # Calculate the correlation matrix\n    corr = data.corr()\n\n    # Create a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Create a figure and an axes\n    fig, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n    return ax"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y, learning_rate=0.01):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct the Sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model\n    sgd = SGD(lr=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # Train the model\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plot the training and validation loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    return model, plt.gca()", "raw_solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y, learning_rate=0.01):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct the Sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model\n    sgd = SGD(lr=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # Train the model\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plot the training and validation loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_shape=(X_train.shape[1],), activation='sigmoid'),\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.01), metrics=['AUC'])\n\n    # Fit the model to the training data\n    history = model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities on the test set\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model, plt.gca()", "raw_solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_shape=(X_train.shape[1],), activation='sigmoid'),\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.01), metrics=['AUC'])\n\n    # Fit the model to the training data\n    history = model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities on the test set\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/419", "solution": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Construct the model\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_dim=X.shape[1], activation='sigmoid'),\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\n    # Fit the model\n    history = model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities\n    Y_pred_prob = model.predict(X_test)\n\n    # Calculate precision and recall\n    precision, recall, _ = precision_recall_curve(Y_test, Y_pred_prob)\n\n    # Plot the Precision-Recall curve\n    plt.plot(recall, precision)\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    ax = plt.gca()\n\n    return model, ax", "raw_solution": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Construct the model\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_dim=X.shape[1], activation='sigmoid'),\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\n    # Fit the model\n    history = model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities\n    Y_pred_prob = model.predict(X_test)\n\n    # Calculate precision and recall\n    precision, recall, _ = precision_recall_curve(Y_test, Y_pred_prob)\n\n    # Plot the Precision-Recall curve\n    plt.plot(recall, precision)\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    ax = plt.gca()\n\n    return model, ax"}
{"task_id": "BigCodeBench/420", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data):\n    \"\"\"\n    Scales numeric columns of a data dictionary using the StandardScaler.\n    Non-numeric columns remain unchanged. If a column contains mixed data types,\n    it tries to convert the entire column to float. If any value in the column\n    cannot be converted to float, the entire column is left unchanged.\n    \"\"\"\n    scaler = StandardScaler()\n    numeric_cols = data.select_dtypes(include=['int', 'float']).columns\n\n    for col in numeric_cols:\n        try:\n            data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n        except ValueError:\n            continue\n\n    return data", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data):\n    \"\"\"\n    Scales numeric columns of a data dictionary using the StandardScaler.\n    Non-numeric columns remain unchanged. If a column contains mixed data types,\n    it tries to convert the entire column to float. If any value in the column\n    cannot be converted to float, the entire column is left unchanged.\n    \"\"\"\n    scaler = StandardScaler()\n    numeric_cols = data.select_dtypes(include=['int', 'float']).columns\n\n    for col in numeric_cols:\n        try:\n            data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n        except ValueError:\n            continue\n\n    return data"}
{"task_id": "BigCodeBench/421", "solution": "import requests\nimport os\nimport json\nimport time\nHEADERS = {\n    'accept': 'text/json',\n    'Content-Type': 'application/json'\n}\ndef task_func(url, directory, metadata):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"Directory {directory} does not exist\")\n    if not isinstance(url, str):\n        raise TypeError(\"URL must be a string\")\n\n    status_codes = []\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        with open(file_path, 'r') as file:\n            file_data = file.read()\n\n        data = {\n            'file_name': filename,\n            'file_data': file_data,\n            'metadata': metadata\n        }\n\n        response = requests.post(url, headers=HEADERS, data=json.dumps(data))\n        status_codes.append(response.status_code)\n\n        time.sleep(1)  # Pause for 1 second\n\n    return status_codes", "raw_solution": "import requests\nimport os\nimport json\nimport time\n\nHEADERS = {\n    'accept': 'text/json',\n    'Content-Type': 'application/json'\n}\n\ndef task_func(url, directory, metadata):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"Directory {directory} does not exist\")\n    if not isinstance(url, str):\n        raise TypeError(\"URL must be a string\")\n\n    status_codes = []\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        with open(file_path, 'r') as file:\n            file_data = file.read()\n\n        data = {\n            'file_name': filename,\n            'file_data': file_data,\n            'metadata': metadata\n        }\n\n        response = requests.post(url, headers=HEADERS, data=json.dumps(data))\n        status_codes.append(response.status_code)\n\n        time.sleep(1)  # Pause for 1 second\n\n    return status_codes"}
{"task_id": "BigCodeBench/422", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef task_func(df, target_column, column_to_remove=\"c\", test_size=0.2):\n    if column_to_remove in df.columns:\n        df = df.drop(column_to_remove, axis=1)\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    return X_train, X_test, y_train, y_test", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(df, target_column, column_to_remove=\"c\", test_size=0.2):\n    if column_to_remove in df.columns:\n        df = df.drop(column_to_remove, axis=1)\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    return X_train, X_test, y_train, y_test"}
{"task_id": "BigCodeBench/423", "solution": "import numpy as np\nimport cv2\nimport os\ndef task_func(image_path='image.jpg', threshold=128):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    if not isinstance(threshold, int) or threshold < 0 or threshold > 255:\n        raise ValueError(f\"The threshold value {threshold} is not an integer or not in the range 0-255.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Binarize the image using the given threshold\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    return gray_image, binary_image", "raw_solution": "import numpy as np\nimport cv2\nimport os\n\ndef task_func(image_path='image.jpg', threshold=128):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    if not isinstance(threshold, int) or threshold < 0 or threshold > 255:\n        raise ValueError(f\"The threshold value {threshold} is not an integer or not in the range 0-255.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Binarize the image using the given threshold\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    return gray_image, binary_image"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(\"Image file does not exist at the specified path.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' must be a positive integer.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Reshape the image to a 2D array of pixels and RGB components\n    image_array = np.reshape(image, (-1, 3))\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed).fit(image_array)\n\n    # Get the cluster labels for each pixel\n    labels = kmeans.labels_\n\n    # Replace each pixel's color with the centroid of the cluster it belongs to\n    segmented_array = kmeans.cluster_centers_[labels]\n\n    # Reshape the segmented array back to the original image shape\n    segmented_image = np.reshape(segmented_array, image.shape)\n\n    # Convert the image arrays to uint8 for OpenCV\n    image_array = image_array.astype(np.uint8)\n    segmented_image = segmented_image.astype(np.uint8)\n\n    return image_array, segmented_image", "raw_solution": "import cv2\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(\"Image file does not exist at the specified path.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' must be a positive integer.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Reshape the image to a 2D array of pixels and RGB components\n    image_array = np.reshape(image, (-1, 3))\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed).fit(image_array)\n\n    # Get the cluster labels for each pixel\n    labels = kmeans.labels_\n\n    # Replace each pixel's color with the centroid of the cluster it belongs to\n    segmented_array = kmeans.cluster_centers_[labels]\n\n    # Reshape the segmented array back to the original image shape\n    segmented_image = np.reshape(segmented_array, image.shape)\n\n    # Convert the image arrays to uint8 for OpenCV\n    image_array = image_array.astype(np.uint8)\n    segmented_image = segmented_image.astype(np.uint8)\n\n    return image_array, segmented_image"}
{"task_id": "BigCodeBench/425", "solution": "import cv2\nimport os\nfrom matplotlib import pyplot as plt\ndef task_func(image_path='image.jpg', histogram_path='histogram.png'):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n\n    # Read the image\n    image = cv2.imread(image_path, 0)\n\n    # Create a histogram\n    histogram = cv2.calcHist([image], [0], None, [256], [0, 256])\n\n    # Plot the histogram\n    plt.figure(figsize=(12, 6))\n    plt.subplot(121), plt.imshow(image, cmap='gray')\n    plt.title('Image'), plt.xticks([]), plt.yticks([])\n    plt.subplot(122), plt.plot(histogram)\n    plt.title('Histogram'), plt.xlim([0, 256])\n\n    # Save the histogram plot as a PNG file\n    plt.savefig(histogram_path)\n\n    return plt.gca()", "raw_solution": "import cv2\nimport os\nfrom matplotlib import pyplot as plt\n\ndef task_func(image_path='image.jpg', histogram_path='histogram.png'):\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n\n    # Read the image\n    image = cv2.imread(image_path, 0)\n\n    # Create a histogram\n    histogram = cv2.calcHist([image], [0], None, [256], [0, 256])\n\n    # Plot the histogram\n    plt.figure(figsize=(12, 6))\n    plt.subplot(121), plt.imshow(image, cmap='gray')\n    plt.title('Image'), plt.xticks([]), plt.yticks([])\n    plt.subplot(122), plt.plot(histogram)\n    plt.title('Histogram'), plt.xlim([0, 256])\n\n    # Save the histogram plot as a PNG file\n    plt.savefig(histogram_path)\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/426", "solution": "import numpy as np\nimport cv2\nimport os\ndef task_func(image_path='image.jpg', threshold=128):\n    if not isinstance(threshold, int) or threshold < 0 or threshold > 255:\n        raise ValueError(\"Threshold must be an integer between 0 and 255\")\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(\"Image file does not exist at the specified path\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Binarize the image using the given threshold\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    # Save the binarized image\n    cv2.imwrite('binary_image.jpg', binary_image)\n\n    return gray_image, binary_image", "raw_solution": "import numpy as np\nimport cv2\nimport os\n\ndef task_func(image_path='image.jpg', threshold=128):\n    if not isinstance(threshold, int) or threshold < 0 or threshold > 255:\n        raise ValueError(\"Threshold must be an integer between 0 and 255\")\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(\"Image file does not exist at the specified path\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Binarize the image using the given threshold\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    # Save the binarized image\n    cv2.imwrite('binary_image.jpg', binary_image)\n\n    return gray_image, binary_image"}
{"task_id": "BigCodeBench/427", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2, features=[\"feature1\", \"feature2\", \"feature3\"], target=\"target\"):\n    # Merge the two dataframes based on 'id' column\n    df = pd.merge(df1, df2, on='id')\n\n    # Perform linear regression\n    X = df[features]\n    y = df[target]\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Get the regression coefficients and intercept\n    coefficients = model.coef_.tolist()\n    intercept = model.intercept_\n\n    # Predict the target values\n    y_pred = model.predict(X)\n\n    # Calculate the residuals\n    residuals = y - y_pred\n\n    # Plot the residuals\n    plt.figure(figsize=(8, 6))\n    plt.scatter(y_pred, residuals)\n    plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors='black')\n    plt.title('Residuals Plot')\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Residuals')\n    residuals_plot = plt.gca()\n\n    # Return the results\n    return {\n        'coefficients': coefficients,\n        'intercept': intercept,\n        'residuals_plot': residuals_plot\n    }", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2, features=[\"feature1\", \"feature2\", \"feature3\"], target=\"target\"):\n    # Merge the two dataframes based on 'id' column\n    df = pd.merge(df1, df2, on='id')\n\n    # Perform linear regression\n    X = df[features]\n    y = df[target]\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Get the regression coefficients and intercept\n    coefficients = model.coef_.tolist()\n    intercept = model.intercept_\n\n    # Predict the target values\n    y_pred = model.predict(X)\n\n    # Calculate the residuals\n    residuals = y - y_pred\n\n    # Plot the residuals\n    plt.figure(figsize=(8, 6))\n    plt.scatter(y_pred, residuals)\n    plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors='black')\n    plt.title('Residuals Plot')\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Residuals')\n    residuals_plot = plt.gca()\n\n    # Return the results\n    return {\n        'coefficients': coefficients,\n        'intercept': intercept,\n        'residuals_plot': residuals_plot\n    }"}
{"task_id": "BigCodeBench/428", "solution": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df1, df2):\n    # Merge the two dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on='id', how='outer')\n\n    # Scale the numeric features\n    scaler = StandardScaler()\n    merged_df[merged_df.columns] = scaler.fit_transform(merged_df[merged_df.columns])\n\n    # Create a pair plot of the scaled features\n    pair_plot = sns.pairplot(merged_df)\n\n    return merged_df, pair_plot", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df1, df2):\n    # Merge the two dataframes on 'id' column\n    merged_df = pd.merge(df1, df2, on='id', how='outer')\n\n    # Scale the numeric features\n    scaler = StandardScaler()\n    merged_df[merged_df.columns] = scaler.fit_transform(merged_df[merged_df.columns])\n\n    # Create a pair plot of the scaled features\n    pair_plot = sns.pairplot(merged_df)\n\n    return merged_df, pair_plot"}
{"task_id": "BigCodeBench/429", "solution": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2):\n    \"\"\"\n    Performs feature selection with SelectKBest (k=2) and returns a heatmap of the feature correlations.\n    \"\"\"\n    # Merge the two dataframes\n    df = pd.concat([df1, df2], axis=1)\n\n    # Perform feature selection\n    selector = SelectKBest(score_func=f_classif, k=2)\n    selector.fit(df.iloc[:, :-1], df.iloc[:, -1])\n\n    # Get the selected features\n    selected_features = df.columns[selector.get_support()]\n\n    # Calculate the correlation matrix\n    corr = df[selected_features].corr()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.title('Heatmap of the feature correlations')\n    plt.show()\n\n    return (selected_features.tolist(), plt.gca())", "raw_solution": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2):\n    \"\"\"\n    Performs feature selection with SelectKBest (k=2) and returns a heatmap of the feature correlations.\n    \"\"\"\n    # Merge the two dataframes\n    df = pd.concat([df1, df2], axis=1)\n\n    # Perform feature selection\n    selector = SelectKBest(score_func=f_classif, k=2)\n    selector.fit(df.iloc[:, :-1], df.iloc[:, -1])\n\n    # Get the selected features\n    selected_features = df.columns[selector.get_support()]\n\n    # Calculate the correlation matrix\n    corr = df[selected_features].corr()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.title('Heatmap of the feature correlations')\n    plt.show()\n\n    return (selected_features.tolist(), plt.gca())"}
{"task_id": "BigCodeBench/430", "solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge the two dataframes\n    df = pd.concat([df1, df2], ignore_index=True)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2, n_init=10)\n    labels = kmeans.fit_predict(df[[column1, column2]])\n\n    # Create a scatterplot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df[column1], df[column2], c=labels)\n\n    return labels, ax", "raw_solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    # Merge the two dataframes\n    df = pd.concat([df1, df2], ignore_index=True)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2, n_init=10)\n    labels = kmeans.fit_predict(df[[column1, column2]])\n\n    # Create a scatterplot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df[column1], df[column2], c=labels)\n\n    return labels, ax"}
{"task_id": "BigCodeBench/431", "solution": "import cv2\nimport os\nimport numpy as np\ndef task_func(image_file: str) -> np.ndarray:\n    \"\"\"\n    Creates a histogram of the pixel values of a grayscale image.\n    The function assumes the image is in grayscale format. The histogram array is 1D with a size of 256, where each index corresponds to a pixel value, and the value at each index represents the count of pixels in the image with that pixel value.\n    The function raises FileNotFoundError if the specified image file does not exist and ValueError if the image file is not a valid image.\n    The function returns a 1D numpy array representing the histogram of the image, with 256 bins corresponding to the pixel values in the range [0, 256). Each entry in the array represents the frequency of a pixel value in the grayscale image.\n    \"\"\"\n    if not os.path.isfile(image_file):\n        raise FileNotFoundError(f\"The specified image file {image_file} does not exist.\")\n\n    try:\n        img = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n    except cv2.error:\n        raise ValueError(f\"The image file {image_file} is not a valid image.\")\n\n    hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n    return hist.flatten()", "raw_solution": "import cv2\nimport os\nimport numpy as np\n\ndef task_func(image_file: str) -> np.ndarray:\n    \"\"\"\n    Creates a histogram of the pixel values of a grayscale image.\n    The function assumes the image is in grayscale format. The histogram array is 1D with a size of 256, where each index corresponds to a pixel value, and the value at each index represents the count of pixels in the image with that pixel value.\n    The function raises FileNotFoundError if the specified image file does not exist and ValueError if the image file is not a valid image.\n    The function returns a 1D numpy array representing the histogram of the image, with 256 bins corresponding to the pixel values in the range [0, 256). Each entry in the array represents the frequency of a pixel value in the grayscale image.\n    \"\"\"\n    if not os.path.isfile(image_file):\n        raise FileNotFoundError(f\"The specified image file {image_file} does not exist.\")\n\n    try:\n        img = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n    except cv2.error:\n        raise ValueError(f\"The image file {image_file} is not a valid image.\")\n\n    hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n    return hist.flatten()"}
{"task_id": "BigCodeBench/432", "solution": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    \"\"\"\n    Merges two dataframes based on the 'id' column, performs a chi-square independence test on the merged dataframe, and draws a heatmap of the contingency table created from the features in column1, column2.\n    \"\"\"\n    # Merge the two dataframes based on 'id'\n    merged_df = pd.merge(df1, df2, on='id', how='inner')\n\n    # Create a contingency table\n    contingency_table = pd.crosstab(merged_df[column1], merged_df[column2])\n\n    # Perform the chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    # Create a heatmap of the contingency table\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(contingency_table, annot=True, cmap='YlGnBu')\n    plt.title('Heatmap of Contingency Table')\n    plt.xlabel(column2)\n    plt.ylabel(column1)\n\n    return (p, plt.gca())", "raw_solution": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency\n\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    \"\"\"\n    Merges two dataframes based on the 'id' column, performs a chi-square independence test on the merged dataframe, and draws a heatmap of the contingency table created from the features in column1, column2.\n    \"\"\"\n    # Merge the two dataframes based on 'id'\n    merged_df = pd.merge(df1, df2, on='id', how='inner')\n\n    # Create a contingency table\n    contingency_table = pd.crosstab(merged_df[column1], merged_df[column2])\n\n    # Perform the chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    # Create a heatmap of the contingency table\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(contingency_table, annot=True, cmap='YlGnBu')\n    plt.title('Heatmap of Contingency Table')\n    plt.xlabel(column2)\n    plt.ylabel(column1)\n\n    return (p, plt.gca())"}
{"task_id": "BigCodeBench/433", "solution": "import base64\nimport hashlib\nimport hmac\nimport binascii\ndef task_func(s, signature, secret_key):\n    \"\"\"\n    Validates the HMAC SHA-1 signature of a base64-encoded message against a provided signature using a specified secret key.\n    \"\"\"\n    # Decode the base64-encoded message\n    message = base64.b64decode(s)\n\n    # Compute the HMAC SHA-1 hash using the provided secret key\n    secret_key = bytes(secret_key, 'utf-8')\n    computed_signature = hmac.new(secret_key, msg=message, digestmod=hashlib.sha1).digest()\n\n    # Convert the provided signature from hex to bytes\n    provided_signature = binascii.unhexlify(signature)\n\n    # Compare the computed and provided signatures\n    return hmac.compare_digest(computed_signature, provided_signature)", "raw_solution": "import base64\nimport hashlib\nimport hmac\nimport binascii\n\ndef task_func(s, signature, secret_key):\n    \"\"\"\n    Validates the HMAC SHA-1 signature of a base64-encoded message against a provided signature using a specified secret key.\n    \"\"\"\n    # Decode the base64-encoded message\n    message = base64.b64decode(s)\n\n    # Compute the HMAC SHA-1 hash using the provided secret key\n    secret_key = bytes(secret_key, 'utf-8')\n    computed_signature = hmac.new(secret_key, msg=message, digestmod=hashlib.sha1).digest()\n\n    # Convert the provided signature from hex to bytes\n    provided_signature = binascii.unhexlify(signature)\n\n    # Compare the computed and provided signatures\n    return hmac.compare_digest(computed_signature, provided_signature)\n\n# Test the function\nassert task_func('SGVsbG8gV29ybGQ=', '7468697320697320612074657374', 'my_secret_key') == True\nassert task_func('SGVsbG8gV29ybGQ=', 'incorrect_signature', 'my_secret_key') == False"}
{"task_id": "BigCodeBench/434", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(s: str, seed: int = 0) -> pd.DataFrame:\n    random.seed(seed)\n    product_names = ['Apple', 'Banana', 'Orange', 'Pear', 'Grape']\n    data = [line.split() for line in s.split('\\n')]\n    data_df = pd.DataFrame(data, columns=['ID', 'Quantity', 'Code', 'Price', 'Description'])\n    data_df['Quantity'] = data_df['Quantity'].astype(int)\n    data_df['Price'] = data_df['Price'].astype(int)\n    data_df['Product'] = data_df['Code'].apply(lambda x: random.choice(product_names))\n    data_df['Description'] = data_df['Description'].apply(lambda x: x.rstrip())\n    return data_df\ns = '1 10 A10B 100 This is a description with spaces\\n2 20 B20C 200 Another description example'", "raw_solution": "import pandas as pd\nimport re\nimport random\n\ndef task_func(s: str, seed: int = 0) -> pd.DataFrame:\n    random.seed(seed)\n    product_names = ['Apple', 'Banana', 'Orange', 'Pear', 'Grape']\n    data = [line.split() for line in s.split('\\n')]\n    data_df = pd.DataFrame(data, columns=['ID', 'Quantity', 'Code', 'Price', 'Description'])\n    data_df['Quantity'] = data_df['Quantity'].astype(int)\n    data_df['Price'] = data_df['Price'].astype(int)\n    data_df['Product'] = data_df['Code'].apply(lambda x: random.choice(product_names))\n    data_df['Description'] = data_df['Description'].apply(lambda x: x.rstrip())\n    return data_df\n\ns = '1 10 A10B 100 This is a description with spaces\\n2 20 B20C 200 Another description example'\ndf = task_func(s)"}
{"task_id": "BigCodeBench/435", "solution": "import pandas as pd\nfrom random import randint\ndef task_func(name: str, age: int, code: str, salary: float, bio: str) -> pd.DataFrame:\n    job_titles = ['Engineer', 'Manager', 'Analyst', 'Developer', 'Tester']\n    job_title = job_titles[randint(0, len(job_titles)-1)]\n    data = {'Name': name, 'Age': age, 'Code': code, 'Salary': salary, 'Bio': bio, 'Job Title': job_title}\n    data_df = pd.DataFrame(data, index=[0])\n    return data_df\nname = \"John Doe\"\nage = 30\ncode = \"JD123\"\nsalary = 50000.00\nbio = \"A software engineer with 5 years of experience.\"", "raw_solution": "import pandas as pd\nfrom random import randint\n\ndef task_func(name: str, age: int, code: str, salary: float, bio: str) -> pd.DataFrame:\n    job_titles = ['Engineer', 'Manager', 'Analyst', 'Developer', 'Tester']\n    job_title = job_titles[randint(0, len(job_titles)-1)]\n    data = {'Name': name, 'Age': age, 'Code': code, 'Salary': salary, 'Bio': bio, 'Job Title': job_title}\n    data_df = pd.DataFrame(data, index=[0])\n    return data_df\n\n# Test the function\nname = \"John Doe\"\nage = 30\ncode = \"JD123\"\nsalary = 50000.00\nbio = \"A software engineer with 5 years of experience.\"\n"}
{"task_id": "BigCodeBench/436", "solution": "import string\nimport matplotlib.pyplot as plt\ndef task_func(s):\n    if not isinstance(s, str):\n        raise TypeError(\"Input must be a string\")\n\n    s = s.lower()\n    frequency = {char: s.count(char) for char in string.ascii_lowercase if char in s}\n\n    plt.figure(figsize=(10, 5))\n    plt.bar(frequency.keys(), frequency.values())\n    plt.xlabel('Letters')\n    plt.ylabel('Frequency')\n    plt.title('Letter Frequencies')\n    plt.show()\n\n    return (frequency, plt)", "raw_solution": "import string\nimport matplotlib.pyplot as plt\n\ndef task_func(s):\n    if not isinstance(s, str):\n        raise TypeError(\"Input must be a string\")\n\n    s = s.lower()\n    frequency = {char: s.count(char) for char in string.ascii_lowercase if char in s}\n\n    plt.figure(figsize=(10, 5))\n    plt.bar(frequency.keys(), frequency.values())\n    plt.xlabel('Letters')\n    plt.ylabel('Frequency')\n    plt.title('Letter Frequencies')\n    plt.show()\n\n    return (frequency, plt)"}
{"task_id": "BigCodeBench/437", "solution": "import pickle\nimport os\nimport pandas as pd\ndef task_func(df, file_name=\"save.pkl\"):\n    \"\"\"\n    Saves a Pandas DataFrame to a pickle file, reads it back, and deletes the intermediate file.\n    \"\"\"\n    # Save DataFrame to pickle file\n    df.to_pickle(file_name)\n\n    # Load DataFrame from pickle file\n    loaded_df = pd.read_pickle(file_name)\n\n    # Delete intermediate file\n    os.remove(file_name)\n\n    return loaded_df", "raw_solution": "import pickle\nimport os\nimport pandas as pd\n\ndef task_func(df, file_name=\"save.pkl\"):\n    \"\"\"\n    Saves a Pandas DataFrame to a pickle file, reads it back, and deletes the intermediate file.\n    \"\"\"\n    # Save DataFrame to pickle file\n    df.to_pickle(file_name)\n\n    # Load DataFrame from pickle file\n    loaded_df = pd.read_pickle(file_name)\n\n    # Delete intermediate file\n    os.remove(file_name)\n\n    return loaded_df"}
{"task_id": "BigCodeBench/438", "solution": "import pickle\nimport os\nimport matplotlib.pyplot as plt\ndef task_func(numbers, file_path=\"save.pkl\"):\n    if not isinstance(numbers, list) or not all(isinstance(i, (int, float)) for i in numbers):\n        raise TypeError(\"Input should be a list of numbers\")\n\n    # Generate a matplotlib figure\n    fig, ax = plt.subplots()\n    ax.plot(numbers)\n\n    # Save the figure to a pickle file\n    with open(file_path, 'wb') as f:\n        pickle.dump(fig, f)\n\n    # Load the figure from the pickle file\n    with open(file_path, 'rb') as f:\n        loaded_fig = pickle.load(f)\n\n    # Delete the pickle file\n    os.remove(file_path)\n\n    return loaded_fig", "raw_solution": "import pickle\nimport os\nimport matplotlib.pyplot as plt\n\ndef task_func(numbers, file_path=\"save.pkl\"):\n    if not isinstance(numbers, list) or not all(isinstance(i, (int, float)) for i in numbers):\n        raise TypeError(\"Input should be a list of numbers\")\n\n    # Generate a matplotlib figure\n    fig, ax = plt.subplots()\n    ax.plot(numbers)\n\n    # Save the figure to a pickle file\n    with open(file_path, 'wb') as f:\n        pickle.dump(fig, f)\n\n    # Load the figure from the pickle file\n    with open(file_path, 'rb') as f:\n        loaded_fig = pickle.load(f)\n\n    # Delete the pickle file\n    os.remove(file_path)\n\n    return loaded_fig"}
{"task_id": "BigCodeBench/439", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(P, T):\n    \"\"\"\n    Calculates the product of a matrix 'P' and a 3D tensor 'T' using numpy and visualizes the results as a heatmap.\n    \"\"\"\n    # Perform matrix-tensor multiplication\n    result = np.matmul(P, T)\n\n    # Create a heatmap using seaborn\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(result, ax=ax)\n\n    return result, ax\nP = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nT = np.array([[[10, 11, 12], [13, 14, 15], [16, 17, 18]],\n              [[19, 20, 21], [22, 23, 24], [25, 26, 27]],\n              [[28, 29, 30], [31, 32, 33], [34, 35, 36]]])", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(P, T):\n    \"\"\"\n    Calculates the product of a matrix 'P' and a 3D tensor 'T' using numpy and visualizes the results as a heatmap.\n    \"\"\"\n    # Perform matrix-tensor multiplication\n    result = np.matmul(P, T)\n\n    # Create a heatmap using seaborn\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(result, ax=ax)\n\n    return result, ax\n\n# Test the function\nP = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nT = np.array([[[10, 11, 12], [13, 14, 15], [16, 17, 18]],\n              [[19, 20, 21], [22, 23, 24], [25, 26, 27]],\n              [[28, 29, 30], [31, 32, 33], [34, 35, 36]]])\n\nresult, ax = task_func(P, T)"}
{"task_id": "BigCodeBench/440", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(P, T):\n    # Check if the shapes of P and T are compatible for multiplication\n    if P.shape[1] != T.shape[0]:\n        raise ValueError(\"Shapes of P and T are not compatible for multiplication\")\n\n    # Perform matrix-tensor multiplication\n    result = np.matmul(P, T)\n\n    # Flatten the result to 1D array\n    result = result.flatten()\n\n    # Normalize the result using StandardScaler\n    scaler = StandardScaler()\n    result = scaler.fit_transform(result.reshape(-1, 1))\n\n    # Convert the result to pandas DataFrame\n    df = pd.DataFrame(result, columns=['feature_0'])\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(P, T):\n    # Check if the shapes of P and T are compatible for multiplication\n    if P.shape[1] != T.shape[0]:\n        raise ValueError(\"Shapes of P and T are not compatible for multiplication\")\n\n    # Perform matrix-tensor multiplication\n    result = np.matmul(P, T)\n\n    # Flatten the result to 1D array\n    result = result.flatten()\n\n    # Normalize the result using StandardScaler\n    scaler = StandardScaler()\n    result = scaler.fit_transform(result.reshape(-1, 1))\n\n    # Convert the result to pandas DataFrame\n    df = pd.DataFrame(result, columns=['feature_0'])\n\n    return df"}
{"task_id": "BigCodeBench/441", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(P, T):\n    \"\"\"\n    Calculates the product of a matrix P and a 3D tensor T with numpy and visualizes the result in 3D with matplotlib.\n    The product of the matrix and tensor is based on the Einstein summation.\n    \"\"\"\n    # Calculate the product\n    result = np.einsum('ij,ajk->aij', P, T)\n\n    # Create a 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(result[:, 0], result[:, 1], result[:, 2])\n\n    return result, ax\nP = np.array([[1, 2], [3, 4], [5, 6]])\nT = np.array([[[7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24]]])", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef task_func(P, T):\n    \"\"\"\n    Calculates the product of a matrix P and a 3D tensor T with numpy and visualizes the result in 3D with matplotlib.\n    The product of the matrix and tensor is based on the Einstein summation.\n    \"\"\"\n    # Calculate the product\n    result = np.einsum('ij,ajk->aij', P, T)\n\n    # Create a 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(result[:, 0], result[:, 1], result[:, 2])\n\n    return result, ax\n\n# Test the function\nP = np.array([[1, 2], [3, 4], [5, 6]])\nT = np.array([[[7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24]]])\nresult, ax = task_func(P, T)\n\n# Print the result and show the plot"}
{"task_id": "BigCodeBench/442", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(P, T, tensor_shape=(3, 3, 3)):\n    # Calculate the product of a matrix \"P\" and a 3D tensor \"T\"\n    T = T.reshape(tensor_shape)\n    result = np.matmul(P, T)\n    result = result.reshape(-1, result.shape[-1])\n\n    # Apply PCA to reduce the dimensionality of the result\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(result)\n\n    # Visualize the PCA result\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Result Visualization')\n\n    return pca_result, ax", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(P, T, tensor_shape=(3, 3, 3)):\n    # Calculate the product of a matrix \"P\" and a 3D tensor \"T\"\n    T = T.reshape(tensor_shape)\n    result = np.matmul(P, T)\n    result = result.reshape(-1, result.shape[-1])\n\n    # Apply PCA to reduce the dimensionality of the result\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(result)\n\n    # Visualize the PCA result\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Result Visualization')\n\n    return pca_result, ax"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculates the product of a matrix 'P' and a 3D tensor 'T', flattens the result, applies KMeans clustering to the flattened data, and visualizes it.\n    \"\"\"\n    # Calculate the product of P and T\n    product = np.matmul(P, T)\n\n    # Flatten the result\n    flattened = product.flatten()\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n\n    # Visualize the KMeans clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result)\n    ax.set_title('KMeans Clustering Visualization')\n\n    return cluster_result, ax", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculates the product of a matrix 'P' and a 3D tensor 'T', flattens the result, applies KMeans clustering to the flattened data, and visualizes it.\n    \"\"\"\n    # Calculate the product of P and T\n    product = np.matmul(P, T)\n\n    # Flatten the result\n    flattened = product.flatten()\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n\n    # Visualize the KMeans clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result)\n    ax.set_title('KMeans Clustering Visualization')\n\n    return cluster_result, ax"}
{"task_id": "BigCodeBench/444", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(n_points=100, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    points = np.random.rand(n_points, 3)\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n\n    return (points, ax)", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef task_func(n_points=100, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    points = np.random.rand(n_points, 3)\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n\n    return (points, ax)\n\n# Test the function\npoints, ax = task_func(100, 0)\nassert points.shape == (100, 3)\nassert isinstance(ax, Axes3D)\nplt.show()"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculates the Voronoi diagram for a number of points in 2D and plots it.\n    Jittering is applied prior to plotting.\n\n    Args:\n        points (numpy.ndarray): A 2D array of shape (n, 2) representing the points.\n        seed (int, optional): The seed for the random number generator. Defaults to 0.\n\n    Returns:\n        tuple: A tuple containing:\n            vor (Voronoi): A Voronoi object representing the Voronoi diagram of the points.\n            ax (Axes): The axes of the plotted Voronoi diagram.\n    \"\"\"\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"'points' must be a numpy.ndarray\")\n    if points.shape[1] != 2:\n        raise ValueError(\"'points' must be a 2D array of shape (n, 2)\")\n\n    np.random.seed(seed)\n    jitter = np.random.rand(*points.shape)\n    points += jitter / 1000.0\n\n    vor = Voronoi(points)\n\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n\n    return vor, ax", "raw_solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculates the Voronoi diagram for a number of points in 2D and plots it.\n    Jittering is applied prior to plotting.\n\n    Args:\n        points (numpy.ndarray): A 2D array of shape (n, 2) representing the points.\n        seed (int, optional): The seed for the random number generator. Defaults to 0.\n\n    Returns:\n        tuple: A tuple containing:\n            vor (Voronoi): A Voronoi object representing the Voronoi diagram of the points.\n            ax (Axes): The axes of the plotted Voronoi diagram.\n    \"\"\"\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"'points' must be a numpy.ndarray\")\n    if points.shape[1] != 2:\n        raise ValueError(\"'points' must be a 2D array of shape (n, 2)\")\n\n    np.random.seed(seed)\n    jitter = np.random.rand(*points.shape)\n    points += jitter / 1000.0\n\n    vor = Voronoi(points)\n\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n\n    return vor, ax"}
{"task_id": "BigCodeBench/446", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\ndef task_func(n_samples=100, centers=3, n_features=2, random_seed=42):\n    np.random.seed(random_seed)\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=random_seed)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n\n    return (X, y, ax)", "raw_solution": "import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\ndef task_func(n_samples=100, centers=3, n_features=2, random_seed=42):\n    np.random.seed(random_seed)\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=random_seed)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n\n    return (X, y, ax)\n\n# Test the function\nX, y, ax = task_func()\nassert isinstance(X, np.ndarray)\nassert isinstance(y, np.ndarray)\nassert isinstance(ax, plt.Axes)"}
{"task_id": "BigCodeBench/447", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(data, n_components=2, random_state=None):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the provided dataset to reduce its dimensionality, and visualizes the results using a scatter plot.\n    \"\"\"\n    pca = PCA(n_components=n_components, random_state=random_state)\n    transformed_data = pca.fit_transform(data)\n\n    fig, ax = plt.subplots()\n    if n_components == 1:\n        ax.scatter(transformed_data, np.zeros_like(transformed_data))\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n    elif n_components == 2:\n        ax.scatter(transformed_data[:, 0], transformed_data[:, 1])\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n    else:\n        ax.scatter(transformed_data[:, 0], transformed_data[:, 1])\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n\n    return {\"transformed_data\": transformed_data, \"ax\": ax}", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(data, n_components=2, random_state=None):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the provided dataset to reduce its dimensionality, and visualizes the results using a scatter plot.\n    \"\"\"\n    pca = PCA(n_components=n_components, random_state=random_state)\n    transformed_data = pca.fit_transform(data)\n\n    fig, ax = plt.subplots()\n    if n_components == 1:\n        ax.scatter(transformed_data, np.zeros_like(transformed_data))\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n    elif n_components == 2:\n        ax.scatter(transformed_data[:, 0], transformed_data[:, 1])\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n    else:\n        ax.scatter(transformed_data[:, 0], transformed_data[:, 1])\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n\n    return {\"transformed_data\": transformed_data, \"ax\": ax}"}
{"task_id": "BigCodeBench/448", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(mu=0, sigma=1):\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title(f'Normal Distribution with Mean {mu} and Standard Deviation {sigma}')\n    ax.set_xlabel('x')\n    ax.set_ylabel('Probability Density')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(mu=0, sigma=1):\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title(f'Normal Distribution with Mean {mu} and Standard Deviation {sigma}')\n    ax.set_xlabel('x')\n    ax.set_ylabel('Probability Density')\n\n    return ax"}
{"task_id": "BigCodeBench/449", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, list):\n    \"\"\"\n    Standardizes the features of a pandas DataFrame using sklearn's StandardScaler,\n    and then draws a histogram for each feature with 20 bins.\n    \"\"\"\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Create a list to store the Axes objects\n    axes_list = []\n\n    # Plot histograms for each feature\n    for i, column in enumerate(standardized_data.columns):\n        # Create a new figure and axis\n        fig, ax = plt.subplots()\n\n        # Add the histogram to the axis\n        ax.hist(standardized_data[column], bins=20)\n\n        # Add the axis to the list\n        axes_list.append(ax)\n\n    return standardized_data, axes_list", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, list):\n    \"\"\"\n    Standardizes the features of a pandas DataFrame using sklearn's StandardScaler,\n    and then draws a histogram for each feature with 20 bins.\n    \"\"\"\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Create a list to store the Axes objects\n    axes_list = []\n\n    # Plot histograms for each feature\n    for i, column in enumerate(standardized_data.columns):\n        # Create a new figure and axis\n        fig, ax = plt.subplots()\n\n        # Add the histogram to the axis\n        ax.hist(standardized_data[column], bins=20)\n\n        # Add the axis to the list\n        axes_list.append(ax)\n\n    return standardized_data, axes_list"}
{"task_id": "BigCodeBench/450", "solution": "from scipy.spatial.distance import cdist\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(n_samples=200, centers=4, plot_path=None, random_seed=None):\n    # Generate synthetic 2D dataset\n    X, y = make_blobs(n_samples=n_samples, centers=centers, random_state=random_seed)\n\n    # Calculate Euclidean distance between individual samples\n    dist_matrix = cdist(X, X, metric='euclidean')\n\n    # Plot the dataset\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    plt.colorbar(scatter, ax=ax)\n\n    if plot_path is not None:\n        plt.savefig(plot_path)\n        return None\n    else:\n        return dist_matrix, plt.gca()", "raw_solution": "from scipy.spatial.distance import cdist\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(n_samples=200, centers=4, plot_path=None, random_seed=None):\n    # Generate synthetic 2D dataset\n    X, y = make_blobs(n_samples=n_samples, centers=centers, random_state=random_seed)\n\n    # Calculate Euclidean distance between individual samples\n    dist_matrix = cdist(X, X, metric='euclidean')\n\n    # Plot the dataset\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    plt.colorbar(scatter, ax=ax)\n\n    if plot_path is not None:\n        plt.savefig(plot_path)\n        return None\n    else:\n        return dist_matrix, plt.gca()"}
{"task_id": "BigCodeBench/451", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(n_components=2, N_SAMPLES=500, N_FEATURES=50, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate a high-dimensional dataset\n    X = np.random.randn(N_SAMPLES, N_FEATURES)\n\n    # Run PCA to reduce its dimensionality\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(X)\n\n    # Draw a heatmap of the covariance matrix of the transformed data\n    if n_components > 1:\n        covariance_matrix = np.cov(transformed_data, rowvar=False)\n        fig, heatmap_axes = plt.subplots(1, 1, figsize=(6, 6))\n        sns.heatmap(covariance_matrix, ax=heatmap_axes, cmap=\"YlGnBu\")\n        heatmap_axes.set_title(\"Covariance Matrix of Transformed Data\")\n    else:\n        heatmap_axes = None\n\n    return transformed_data, heatmap_axes", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(n_components=2, N_SAMPLES=500, N_FEATURES=50, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate a high-dimensional dataset\n    X = np.random.randn(N_SAMPLES, N_FEATURES)\n\n    # Run PCA to reduce its dimensionality\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(X)\n\n    # Draw a heatmap of the covariance matrix of the transformed data\n    if n_components > 1:\n        covariance_matrix = np.cov(transformed_data, rowvar=False)\n        fig, heatmap_axes = plt.subplots(1, 1, figsize=(6, 6))\n        sns.heatmap(covariance_matrix, ax=heatmap_axes, cmap=\"YlGnBu\")\n        heatmap_axes.set_title(\"Covariance Matrix of Transformed Data\")\n    else:\n        heatmap_axes = None\n\n    return transformed_data, heatmap_axes"}
{"task_id": "BigCodeBench/452", "solution": "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\ndef task_func(n_samples=100, n_features=10, random_seed=None):\n    # Generate synthetic data\n    X, y = datasets.make_regression(n_samples=n_samples, n_features=n_features, random_state=random_seed)\n\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n\n    # Fit a linear regression model to the data\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Get the predicted values\n    predictions = model.predict(X_test)\n\n    # Get the coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n\n    # Calculate the mean squared error\n    mse = mean_squared_error(y_test, predictions)\n\n    return predictions, coefficients, intercept, mse", "raw_solution": "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\ndef task_func(n_samples=100, n_features=10, random_seed=None):\n    # Generate synthetic data\n    X, y = datasets.make_regression(n_samples=n_samples, n_features=n_features, random_state=random_seed)\n\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n\n    # Fit a linear regression model to the data\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Get the predicted values\n    predictions = model.predict(X_test)\n\n    # Get the coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n\n    # Calculate the mean squared error\n    mse = mean_squared_error(y_test, predictions)\n\n    return predictions, coefficients, intercept, mse"}
{"task_id": "BigCodeBench/453", "solution": "import re\nimport string\nfrom random import choice\ndef task_func(n, pattern):\n    while True:\n        random_string = ''.join(choice(string.ascii_letters) for _ in range(n))\n        if re.match(pattern, random_string):\n            return random_string", "raw_solution": "import re\nimport string\nfrom random import choice\n\ndef task_func(n, pattern):\n    while True:\n        random_string = ''.join(choice(string.ascii_letters) for _ in range(n))\n        if re.match(pattern, random_string):\n            return random_string\n\n# Test the function"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n    If a file with the same name already exists in the destination directory, it is not moved.\n    \"\"\"\n    # Check if source and destination directories exist\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory {src_dir} does not exist\")\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"Destination directory {dest_dir} does not exist\")\n\n    # Get a list of files with the specified extension in the source directory\n    files = glob.glob(os.path.join(src_dir, f\"*.{ext}\"))\n\n    # Initialize an empty list to store the full paths of files that were successfully moved\n    moved_files = []\n\n    # Move each file to the destination directory\n    for file in files:\n        # Get the file name without the extension\n        file_name = os.path.splitext(os.path.basename(file))[0]\n        # Construct the full path of the file in the destination directory\n        dest_file = os.path.join(dest_dir, f\"{file_name}.{ext}\")\n        # If the file does not already exist in the destination directory, move it\n        if not os.path.isfile(dest_file):\n            shutil.move(file, dest_file)\n            moved_files.append(dest_file)\n\n    # Return the list of files that were successfully moved\n    return moved_files", "raw_solution": "import os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n    If a file with the same name already exists in the destination directory, it is not moved.\n    \"\"\"\n    # Check if source and destination directories exist\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory {src_dir} does not exist\")\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"Destination directory {dest_dir} does not exist\")\n\n    # Get a list of files with the specified extension in the source directory\n    files = glob.glob(os.path.join(src_dir, f\"*.{ext}\"))\n\n    # Initialize an empty list to store the full paths of files that were successfully moved\n    moved_files = []\n\n    # Move each file to the destination directory\n    for file in files:\n        # Get the file name without the extension\n        file_name = os.path.splitext(os.path.basename(file))[0]\n        # Construct the full path of the file in the destination directory\n        dest_file = os.path.join(dest_dir, f\"{file_name}.{ext}\")\n        # If the file does not already exist in the destination directory, move it\n        if not os.path.isfile(dest_file):\n            shutil.move(file, dest_file)\n            moved_files.append(dest_file)\n\n    # Return the list of files that were successfully moved\n    return moved_files"}
{"task_id": "BigCodeBench/455", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mean, std_dev, n):\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, n)\n\n    # Plot histogram\n    plt.hist(samples, bins=30, density=True)\n    plt.show()\n\n    # Plot probability density function\n    x = np.linspace(mean - 4*std_dev, mean + 4*std_dev, 100)\n    plt.plot(x, stats.norm.pdf(x, mean, std_dev))\n    plt.show()\n\n    return samples", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, n):\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, n)\n\n    # Plot histogram\n    plt.hist(samples, bins=30, density=True)\n    plt.show()\n\n    # Plot probability density function\n    x = np.linspace(mean - 4*std_dev, mean + 4*std_dev, 100)\n    plt.plot(x, stats.norm.pdf(x, mean, std_dev))\n    plt.show()\n\n    return samples\n\n# Test the function\nsamples = task_func(5, 2, 500)\nassert len(samples) == 500"}
{"task_id": "BigCodeBench/456", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, plt.Axes):\n    \"\"\"\n    Normalizes the data to a range [0, 1] and visualizes it using a seaborn heatmap.\n    \"\"\"\n    # Normalize the data\n    scaler = MinMaxScaler()\n    normalized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(normalized_data, cmap=\"YlGnBu\", cbar_kws={\"label\": \"Normalized Value\"})\n\n    return normalized_data, heatmap.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, plt.Axes):\n    \"\"\"\n    Normalizes the data to a range [0, 1] and visualizes it using a seaborn heatmap.\n    \"\"\"\n    # Normalize the data\n    scaler = MinMaxScaler()\n    normalized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(normalized_data, cmap=\"YlGnBu\", cbar_kws={\"label\": \"Normalized Value\"})\n\n    return normalized_data, heatmap.figure"}
{"task_id": "BigCodeBench/457", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    if not isinstance(L, list):\n        raise TypeError(\"Input should be a list\")\n    for sublist in L:\n        if not isinstance(sublist, list):\n            raise TypeError(\"Input should be a list of lists\")\n        for item in sublist:\n            if not isinstance(item, int):\n                raise TypeError(\"All elements in the list should be integers\")\n\n    # Flatten the list and convert to numpy array\n    flat_list = [item for sublist in L for item in sublist]\n    arr = np.array(flat_list)\n\n    # Create a pandas Series from the numpy array\n    series = pd.Series(arr)\n\n    # Plot the histogram\n    ax = series.plot.hist(bins=range(min(arr), max(arr) + 2), rwidth=0.8)\n\n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    if not isinstance(L, list):\n        raise TypeError(\"Input should be a list\")\n    for sublist in L:\n        if not isinstance(sublist, list):\n            raise TypeError(\"Input should be a list of lists\")\n        for item in sublist:\n            if not isinstance(item, int):\n                raise TypeError(\"All elements in the list should be integers\")\n\n    # Flatten the list and convert to numpy array\n    flat_list = [item for sublist in L for item in sublist]\n    arr = np.array(flat_list)\n\n    # Create a pandas Series from the numpy array\n    series = pd.Series(arr)\n\n    # Plot the histogram\n    ax = series.plot.hist(bins=range(min(arr), max(arr) + 2), rwidth=0.8)\n\n    return ax"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    if not json_str:\n        return pd.DataFrame()\n\n    data_dict = json.loads(json_str)\n\n    def normalize_value(value):\n        if isinstance(value, list):\n            return [float(i)*2 if isinstance(i, (int, float)) else float(i)*2 if re.match(r'^-?\\d+(?:\\.\\d+)?$', i) else i for i in value]\n        elif isinstance(value, str):\n            return float(value)*2 if re.match(r'^-?\\d+(?:\\.\\d+)?$', value) else value\n        elif isinstance(value, (int, float)):\n            return float(value)*2\n        else:\n            return value\n\n    data_dict = {key: normalize_value(value) for key, value in data_dict.items()}\n\n    return pd.DataFrame(data_dict)", "raw_solution": "import json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    if not json_str:\n        return pd.DataFrame()\n\n    data_dict = json.loads(json_str)\n\n    def normalize_value(value):\n        if isinstance(value, list):\n            return [float(i)*2 if isinstance(i, (int, float)) else float(i)*2 if re.match(r'^-?\\d+(?:\\.\\d+)?$', i) else i for i in value]\n        elif isinstance(value, str):\n            return float(value)*2 if re.match(r'^-?\\d+(?:\\.\\d+)?$', value) else value\n        elif isinstance(value, (int, float)):\n            return float(value)*2\n        else:\n            return value\n\n    data_dict = {key: normalize_value(value) for key, value in data_dict.items()}\n\n    return pd.DataFrame(data_dict)"}
{"task_id": "BigCodeBench/459", "solution": "import subprocess\nimport os\nimport time\nfrom datetime import datetime\ndef task_func(script_dir, scripts, delay):\n    if delay < 0:\n        raise ValueError(\"Delay cannot be negative\")\n    if not scripts:\n        raise ValueError(\"No scripts provided\")\n\n    timestamps = []\n\n    for script in scripts:\n        script_path = os.path.join(script_dir, script)\n        if not os.path.exists(script_path):\n            raise FileNotFoundError(f\"Script {script} not found in {script_dir}\")\n\n        start_time = datetime.now()\n        timestamps.append(start_time)\n\n        print(f\"Executing {script} at {start_time}\")\n        subprocess.run([\"bash\", script_path])\n\n        print(f\"Finished executing {script} at {datetime.now()}\")\n\n        if delay > 0:\n            print(f\"Waiting for {delay} seconds before next script\")\n            time.sleep(delay)\n\n    return timestamps", "raw_solution": "import subprocess\nimport os\nimport time\nfrom datetime import datetime\n\ndef task_func(script_dir, scripts, delay):\n    if delay < 0:\n        raise ValueError(\"Delay cannot be negative\")\n    if not scripts:\n        raise ValueError(\"No scripts provided\")\n\n    timestamps = []\n\n    for script in scripts:\n        script_path = os.path.join(script_dir, script)\n        if not os.path.exists(script_path):\n            raise FileNotFoundError(f\"Script {script} not found in {script_dir}\")\n\n        start_time = datetime.now()\n        timestamps.append(start_time)\n\n        print(f\"Executing {script} at {start_time}\")\n        subprocess.run([\"bash\", script_path])\n\n        print(f\"Finished executing {script} at {datetime.now()}\")\n\n        if delay > 0:\n            print(f\"Waiting for {delay} seconds before next script\")\n            time.sleep(delay)\n\n    return timestamps"}
{"task_id": "BigCodeBench/460", "solution": "import subprocess\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(script_path, output_file_path):\n    try:\n        # Execute the script\n        subprocess.run(['python', script_path, output_file_path], check=True)\n    except subprocess.CalledProcessError:\n        raise ValueError(\"Script execution failed\")\n\n    # Read the CSV into a DataFrame\n    try:\n        df = pd.read_csv(output_file_path)\n    except pd.errors.ParserError:\n        raise ValueError(\"Invalid CSV file\")\n\n    # Check if the CSV has exactly 2 columns\n    if df.shape[1] != 2:\n        raise ValueError(\"CSV does not contain exactly 2 columns\")\n\n    # Plot a bar graph\n    fig, ax = plt.subplots()\n    ax.bar(df.iloc[:, 0], df.iloc[:, 1])\n    ax.set_xlabel(df.columns[0])\n    ax.set_ylabel(df.columns[1])\n\n    return df, ax.figure, ax", "raw_solution": "import subprocess\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(script_path, output_file_path):\n    try:\n        # Execute the script\n        subprocess.run(['python', script_path, output_file_path], check=True)\n    except subprocess.CalledProcessError:\n        raise ValueError(\"Script execution failed\")\n\n    # Read the CSV into a DataFrame\n    try:\n        df = pd.read_csv(output_file_path)\n    except pd.errors.ParserError:\n        raise ValueError(\"Invalid CSV file\")\n\n    # Check if the CSV has exactly 2 columns\n    if df.shape[1] != 2:\n        raise ValueError(\"CSV does not contain exactly 2 columns\")\n\n    # Plot a bar graph\n    fig, ax = plt.subplots()\n    ax.bar(df.iloc[:, 0], df.iloc[:, 1])\n    ax.set_xlabel(df.columns[0])\n    ax.set_ylabel(df.columns[1])\n\n    return df, ax.figure, ax"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n\n    start_time = time.time()\n    process = subprocess.Popen(['python', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    cpu_usage = 0\n    memory_usage = 0\n\n    while True:\n        if time.time() - start_time > timeout:\n            process.terminate()\n            break\n\n        if psutil.pid_exists(process.pid):\n            process_info = psutil.Process(process.pid)\n            cpu_usage += process_info.cpu_percent(interval=1)\n            memory_usage += process_info.memory_info().rss\n        else:\n            break\n\n    return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}", "raw_solution": "import subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n\n    start_time = time.time()\n    process = subprocess.Popen(['python', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    cpu_usage = 0\n    memory_usage = 0\n\n    while True:\n        if time.time() - start_time > timeout:\n            process.terminate()\n            break\n\n        if psutil.pid_exists(process.pid):\n            process_info = psutil.Process(process.pid)\n            cpu_usage += process_info.cpu_percent(interval=1)\n            memory_usage += process_info.memory_info().rss\n        else:\n            break\n\n    return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}"}
{"task_id": "BigCodeBench/462", "solution": "import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(num_rows=100, categories=[\"a\", \"b\", \"c\", \"d\", \"e\"], random_seed=42):\n    if num_rows < 1:\n        raise ValueError(\"num_rows must be greater than 0\")\n\n    random.seed(random_seed)\n\n    data = {\n        \"Category\": [random.choice(categories) for _ in range(num_rows)],\n        \"Value\": [random.randint(1, 100) for _ in range(num_rows)]\n    }\n\n    df = pd.DataFrame(data)\n\n    counts = df['Category'].value_counts()\n\n    fig, ax = plt.subplots()\n    counts.plot(kind='bar', ax=ax)\n    ax.set_title('Category Counts')\n\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(num_rows=100, categories=[\"a\", \"b\", \"c\", \"d\", \"e\"], random_seed=42):\n    if num_rows < 1:\n        raise ValueError(\"num_rows must be greater than 0\")\n\n    random.seed(random_seed)\n\n    data = {\n        \"Category\": [random.choice(categories) for _ in range(num_rows)],\n        \"Value\": [random.randint(1, 100) for _ in range(num_rows)]\n    }\n\n    df = pd.DataFrame(data)\n\n    counts = df['Category'].value_counts()\n\n    fig, ax = plt.subplots()\n    counts.plot(kind='bar', ax=ax)\n    ax.set_title('Category Counts')\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/463", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_str, separator=\",\", bins=20):\n    if not data_str:\n        raise ValueError(\"Data is empty\")\n\n    data = data_str.split(separator)\n    try:\n        series = pd.Series(data).astype(int)\n    except ValueError:\n        raise ValueError(\"Failed to convert data to integers\")\n\n    fig, ax = plt.subplots()\n    ax.hist(series, bins=bins, grid=True, rwidth=0.9, color='#607c8e')\n    plt.show()\n\n    return series, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_str, separator=\",\", bins=20):\n    if not data_str:\n        raise ValueError(\"Data is empty\")\n\n    data = data_str.split(separator)\n    try:\n        series = pd.Series(data).astype(int)\n    except ValueError:\n        raise ValueError(\"Failed to convert data to integers\")\n\n    fig, ax = plt.subplots()\n    ax.hist(series, bins=bins, grid=True, rwidth=0.9, color='#607c8e')\n    plt.show()\n\n    return series, ax"}
{"task_id": "BigCodeBench/464", "solution": "import json\nfrom datetime import datetime\nfrom decimal import Decimal\nclass DateTimeEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        return super().default(obj)\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=DateTimeEncoder)", "raw_solution": "import json\nfrom datetime import datetime\nfrom decimal import Decimal\n\nclass DateTimeEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        return super().default(obj)\n\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=DateTimeEncoder)\n\n# Testing\nassert task_func({'name': 'Alice', 'age': 30}) == '{\"name\": \"Alice\", \"age\": 30}'\nassert task_func({'name': 'Bob', 'birthday': datetime(1985, 12, 25)}) == '{\"name\": \"Bob\", \"birthday\": \"1985-12-25T00:00:00\"}'\nassert task_func({'name': 'Charlie', 'salary': Decimal('123456.789')}) == '{\"name\": \"Charlie\", \"salary\": \"123456.789\"}'"}
{"task_id": "BigCodeBench/465", "solution": "import json\nfrom datetime import datetime\nimport numpy as np\nfrom decimal import Decimal\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        else:\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)", "raw_solution": "import json\nfrom datetime import datetime\nimport numpy as np\nfrom decimal import Decimal\n\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        else:\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)\n\n# Testing\nassert task_func({'name': 'Alice', 'age': 30}) == '{\"name\": \"Alice\", \"age\": 30}'\nassert task_func({'name': 'Bob', 'age': 40, 'birthday': datetime(1980, 1, 1)}) == '{\"name\": \"Bob\", \"age\": 40, \"birthday\": \"1980-01-01T00:00:00\"}'\nassert task_func({'name': 'Charlie', 'age': 50, 'data': np.array([1, 2, 3])}) == '{\"name\": \"Charlie\", \"age\": 50, \"data\": [1, 2, 3]}'\nassert task_func({'name': 'David', 'age': 60, 'balance': Decimal('1000.00')}) == '{\"name\": \"David\", \"age\": 60, \"balance\": \"1000.00\"}'\n\ntry:\n    task_func({'name': 'Eve', 'age': 70, 'address': lambda: None})\nexcept TypeError as e:\n    assert str(e) == \"Object of type <class 'function'> is not JSON serializable\""}
{"task_id": "BigCodeBench/466", "solution": "import json\nfrom enum import Enum\nfrom typing import Any\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj: Any) -> Any:\n        if isinstance(obj, Enum):\n            return obj.name\n        return super().default(obj)\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)", "raw_solution": "import json\nfrom enum import Enum\nfrom typing import Any\n\nclass Color(Enum):\n    RED = 1\n    GREEN = 2\n    BLUE = 3\n\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj: Any) -> Any:\n        if isinstance(obj, Enum):\n            return obj.name\n        return super().default(obj)\n\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)\n\n# Testing\nassert task_func({'name': 'Alice', 'age': 30}) == '{\"name\": \"Alice\", \"age\": 30}'\nassert task_func({'color': Color.RED}) == '{\"color\": \"RED\"}'"}
{"task_id": "BigCodeBench/467", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(n, seed=0):\n    np.random.seed(seed)\n    points = [(x, y) for x, y in np.random.rand(n, 2)]\n\n    fig, ax = plt.subplots()\n    ax.scatter([x for x, y in points], [y for x, y in points])\n    ax.set_title(\"Scatter plot of random points\")\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n\n    return fig, points", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(n, seed=0):\n    np.random.seed(seed)\n    points = [(x, y) for x, y in np.random.rand(n, 2)]\n\n    fig, ax = plt.subplots()\n    ax.scatter([x for x, y in points], [y for x, y in points])\n    ax.set_title(\"Scatter plot of random points\")\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n\n    return fig, points"}
{"task_id": "BigCodeBench/468", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(file_path=\"data.csv\", columns=[\"A\", \"B\", \"C\"]):\n    # Read CSV file into DataFrame\n    df = pd.read_csv(file_path)\n\n    # Convert numeric values into floats\n    df[columns] = df[columns].apply(pd.to_numeric, errors='coerce')\n\n    # Draw a line chart of data in the specified columns\n    fig, ax = plt.subplots()\n    df[columns].plot(ax=ax)\n\n    # Compute the cube-root of the data\n    cube_root_series = np.cbrt(df[columns])\n\n    return df, ax.figure, cube_root_series", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(file_path=\"data.csv\", columns=[\"A\", \"B\", \"C\"]):\n    # Read CSV file into DataFrame\n    df = pd.read_csv(file_path)\n\n    # Convert numeric values into floats\n    df[columns] = df[columns].apply(pd.to_numeric, errors='coerce')\n\n    # Draw a line chart of data in the specified columns\n    fig, ax = plt.subplots()\n    df[columns].plot(ax=ax)\n\n    # Compute the cube-root of the data\n    cube_root_series = np.cbrt(df[columns])\n\n    return df, ax.figure, cube_root_series"}
{"task_id": "BigCodeBench/469", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom typing import Tuple\ndef task_func(student_grades, possible_grades=[\"A\", \"B\", \"C\", \"D\", \"F\"]):\n    # Normalize grades to uppercase and remove whitespaces\n    student_grades = [grade.upper().strip() for grade in student_grades]\n\n    # Filter out grades not in possible grades\n    student_grades = [grade for grade in student_grades if grade in possible_grades]\n\n    # Count each grade\n    grade_counts = Counter(student_grades)\n\n    # Create a DataFrame\n    df = pd.DataFrame(list(grade_counts.items()), columns=['Grade', 'Count'])\n    df.set_index('Grade', inplace=True)\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Grade')\n    ax.set_ylabel('Number of Students')\n    ax.set_title('Grade Distribution')\n\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom typing import Tuple\n\ndef task_func(student_grades, possible_grades=[\"A\", \"B\", \"C\", \"D\", \"F\"]):\n    # Normalize grades to uppercase and remove whitespaces\n    student_grades = [grade.upper().strip() for grade in student_grades]\n\n    # Filter out grades not in possible grades\n    student_grades = [grade for grade in student_grades if grade in possible_grades]\n\n    # Count each grade\n    grade_counts = Counter(student_grades)\n\n    # Create a DataFrame\n    df = pd.DataFrame(list(grade_counts.items()), columns=['Grade', 'Count'])\n    df.set_index('Grade', inplace=True)\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Grade')\n    ax.set_ylabel('Number of Students')\n    ax.set_title('Grade Distribution')\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/470", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(myList):\n    # Create a histogram\n    bins = np.arange(min(myList), max(myList) + 1) - 0.5\n    ax = plt.hist(myList, bins=bins, edgecolor='black', color='skyblue')\n\n    # Set labels and title\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Values')\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(myList):\n    # Create a histogram\n    bins = np.arange(min(myList), max(myList) + 1) - 0.5\n    ax = plt.hist(myList, bins=bins, edgecolor='black', color='skyblue')\n\n    # Set labels and title\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Values')\n\n    return ax"}
{"task_id": "BigCodeBench/471", "solution": "from collections import Counter\nimport pandas as pd\ndef task_func(myList):\n    \"\"\"\n    Counts the frequency of each word in a list and returns a DataFrame of words and their number.\n    \"\"\"\n    word_counts = Counter(myList)\n    df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n    return df", "raw_solution": "from collections import Counter\nimport pandas as pd\n\ndef task_func(myList):\n    \"\"\"\n    Counts the frequency of each word in a list and returns a DataFrame of words and their number.\n    \"\"\"\n    word_counts = Counter(myList)\n    df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n    return df"}
{"task_id": "BigCodeBench/472", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\ndef task_func(myList, n_clusters):\n    \"\"\"\n    Clusters a list of 2D points using KMeans and visualizes the clusters.\n    Args:\n        myList (List[List[float]]): A list of 2D points.\n        n_clusters (int): The number of clusters.\n    Returns:\n        matplotlib.axes._axes.Axes: Axes object with the plotted clusters.\n    \"\"\"\n    # Validate inputs\n    if not all(isinstance(i, list) and len(i) == 2 for i in myList):\n        raise ValueError(\"myList should be a list of 2D points\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters should be a positive integer\")\n\n    # Convert list of lists to numpy array\n    X = np.array(myList)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    kmeans.fit(X)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\n\n    # Plot cluster centers\n    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', s=200, linewidths=3, color='r')\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef task_func(myList, n_clusters):\n    \"\"\"\n    Clusters a list of 2D points using KMeans and visualizes the clusters.\n    Args:\n        myList (List[List[float]]): A list of 2D points.\n        n_clusters (int): The number of clusters.\n    Returns:\n        matplotlib.axes._axes.Axes: Axes object with the plotted clusters.\n    \"\"\"\n    # Validate inputs\n    if not all(isinstance(i, list) and len(i) == 2 for i in myList):\n        raise ValueError(\"myList should be a list of 2D points\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters should be a positive integer\")\n\n    # Convert list of lists to numpy array\n    X = np.array(myList)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    kmeans.fit(X)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\n\n    # Plot cluster centers\n    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', s=200, linewidths=3, color='r')\n\n    return ax"}
{"task_id": "BigCodeBench/473", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(n_walks, n_steps, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    if n_walks < 1 or n_steps < 1:\n        raise ValueError(\"n_walks and n_steps must be greater than 0\")\n\n    # Define the step sizes\n    step_sizes = np.array([-1, 1])\n\n    # Generate the random walks\n    walks = np.random.choice(a=step_sizes, size=(n_walks, n_steps))\n\n    # Calculate the cumulative sum of the walks to get the position of each step\n    positions = np.cumsum(walks, axis=1)\n\n    # Define the colors for the walks\n    colors = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n\n    # Plot the walks\n    fig, ax = plt.subplots()\n    for i in range(n_walks):\n        ax.plot(positions[i], color=next(colors))\n\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Position')\n    ax.set_title('Random Walks')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef task_func(n_walks, n_steps, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    if n_walks < 1 or n_steps < 1:\n        raise ValueError(\"n_walks and n_steps must be greater than 0\")\n\n    # Define the step sizes\n    step_sizes = np.array([-1, 1])\n\n    # Generate the random walks\n    walks = np.random.choice(a=step_sizes, size=(n_walks, n_steps))\n\n    # Calculate the cumulative sum of the walks to get the position of each step\n    positions = np.cumsum(walks, axis=1)\n\n    # Define the colors for the walks\n    colors = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n\n    # Plot the walks\n    fig, ax = plt.subplots()\n    for i in range(n_walks):\n        ax.plot(positions[i], color=next(colors))\n\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Position')\n    ax.set_title('Random Walks')\n\n    return ax"}
{"task_id": "BigCodeBench/474", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(n_samples=1000, mu=0, sigma=1, random_seed=0):\n    np.random.seed(random_seed)\n    samples = np.random.normal(mu, sigma, n_samples)\n\n    fig, ax = plt.subplots(1, 1)\n\n    # Histogram\n    ax.hist(samples, bins=30, density=True, alpha=0.6, color='b', label='Histogram')\n\n    # PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'r-', linewidth=2, label='PDF')\n\n    ax.legend()\n    plt.show()\n\n    return ax, samples", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(n_samples=1000, mu=0, sigma=1, random_seed=0):\n    np.random.seed(random_seed)\n    samples = np.random.normal(mu, sigma, n_samples)\n\n    fig, ax = plt.subplots(1, 1)\n\n    # Histogram\n    ax.hist(samples, bins=30, density=True, alpha=0.6, color='b', label='Histogram')\n\n    # PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'r-', linewidth=2, label='PDF')\n\n    ax.legend()\n    plt.show()\n\n    return ax, samples"}
{"task_id": "BigCodeBench/475", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\ndef task_func(data, date_format, country, country_codes=None):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n    if not isinstance(date_format, str):\n        raise ValueError(\"'date_format' should be a string\")\n    if country not in country_codes:\n        raise ValueError(\"'country' is not in 'country_codes'\")\n    if not isinstance(country_codes, dict):\n        raise ValueError(\"'country_codes' should be a dictionary\")\n\n    data['date'] = pd.to_datetime(data['date'], format=date_format)\n    data = data[data['country'] == country]\n\n    fig, ax = plt.subplots()\n    ax.hist(data['date'], bins=30, color='blue', edgecolor='black')\n    ax.set_title('Date Distribution')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\ndef task_func(data, date_format, country, country_codes=None):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n    if not isinstance(date_format, str):\n        raise ValueError(\"'date_format' should be a string\")\n    if country not in country_codes:\n        raise ValueError(\"'country' is not in 'country_codes'\")\n    if not isinstance(country_codes, dict):\n        raise ValueError(\"'country_codes' should be a dictionary\")\n\n    data['date'] = pd.to_datetime(data['date'], format=date_format)\n    data = data[data['country'] == country]\n\n    fig, ax = plt.subplots()\n    ax.hist(data['date'], bins=30, color='blue', edgecolor='black')\n    ax.set_title('Date Distribution')\n    ax.set_ylabel('Frequency')\n\n    return ax"}
{"task_id": "BigCodeBench/476", "solution": "import matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nimport numpy as np\ndef task_func(X, Y):\n    def quad_func(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    popt, _ = curve_fit(quad_func, X, Y)\n    a, b, c = popt\n\n    x_fit = np.linspace(min(X), max(X), 500)\n    y_fit = quad_func(x_fit, a, b, c)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X, Y, label='Data')\n    ax.plot(x_fit, y_fit, label='Fit')\n    ax.legend()\n    plt.show()\n\n    return (a, b, c), ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nimport numpy as np\n\ndef task_func(X, Y):\n    def quad_func(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    popt, _ = curve_fit(quad_func, X, Y)\n    a, b, c = popt\n\n    x_fit = np.linspace(min(X), max(X), 500)\n    y_fit = quad_func(x_fit, a, b, c)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X, Y, label='Data')\n    ax.plot(x_fit, y_fit, label='Fit')\n    ax.legend()\n    plt.show()\n\n    return (a, b, c), ax"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n\n    # Generate DataFrame\n    df = pd.DataFrame({\n        \"x\": np.random.rand(N),\n        \"y\": np.random.rand(N),\n    })\n\n    # Generate categories\n    if N >= len(CATEGORIES):\n        df[\"category\"] = np.random.choice(CATEGORIES, N)\n    else:\n        df[\"category\"] = np.random.choice(CATEGORIES, N, replace=False)\n\n    # Scatter plot\n    fig, ax = plt.subplots()\n    for category in CATEGORIES:\n        df_category = df[df[\"category\"] == category]\n        ax.scatter(df_category[\"x\"], df_category[\"y\"], label=category)\n    ax.legend()\n\n    return (df, ax)", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n\n    # Generate DataFrame\n    df = pd.DataFrame({\n        \"x\": np.random.rand(N),\n        \"y\": np.random.rand(N),\n    })\n\n    # Generate categories\n    if N >= len(CATEGORIES):\n        df[\"category\"] = np.random.choice(CATEGORIES, N)\n    else:\n        df[\"category\"] = np.random.choice(CATEGORIES, N, replace=False)\n\n    # Scatter plot\n    fig, ax = plt.subplots()\n    for category in CATEGORIES:\n        df_category = df[df[\"category\"] == category]\n        ax.scatter(df_category[\"x\"], df_category[\"y\"], label=category)\n    ax.legend()\n\n    return (df, ax)"}
{"task_id": "BigCodeBench/478", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(data_list, seed=None):\n    \"\"\"\n    Removes a random comma-separated value (treated as a \"substring\") from each string in a list and returns a pandas DataFrame containing the original and modified strings.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    original_strings = []\n    modified_strings = []\n\n    for string in data_list:\n        original_strings.append(string)\n        # Split the string by comma and join it back without a random element\n        modified_string = ','.join(string.split(','))\n        modified_strings.append(modified_string)\n\n    df = pd.DataFrame({\n        'Original String': original_strings,\n        'Modified String': modified_strings\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nimport random\n\ndef task_func(data_list, seed=None):\n    \"\"\"\n    Removes a random comma-separated value (treated as a \"substring\") from each string in a list and returns a pandas DataFrame containing the original and modified strings.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    original_strings = []\n    modified_strings = []\n\n    for string in data_list:\n        original_strings.append(string)\n        # Split the string by comma and join it back without a random element\n        modified_string = ','.join(string.split(','))\n        modified_strings.append(modified_string)\n\n    df = pd.DataFrame({\n        'Original String': original_strings,\n        'Modified String': modified_strings\n    })\n\n    return df"}
{"task_id": "BigCodeBench/479", "solution": "import random\nimport string\nimport pandas as pd\ndef task_func(data_list, seed=0):\n    random.seed(seed)\n    result = []\n    for data in data_list:\n        if data:\n            start = random.randint(0, len(data) - 1)\n            end = random.randint(start, len(data))\n            substring = data[start:end]\n            replacement = ''.join(random.choices(string.ascii_lowercase, k=len(substring)))\n            modified_data = data.replace(substring, replacement)\n            result.append({'Original String': data, 'Modified String': modified_data})\n        else:\n            result.append({'Original String': data, 'Modified String': data})\n    return pd.DataFrame(result)\ndata_list = ['hello,world', 'python,programming', 'data,science', '']", "raw_solution": "import random\nimport string\nimport pandas as pd\n\ndef task_func(data_list, seed=0):\n    random.seed(seed)\n    result = []\n    for data in data_list:\n        if data:\n            start = random.randint(0, len(data) - 1)\n            end = random.randint(start, len(data))\n            substring = data[start:end]\n            replacement = ''.join(random.choices(string.ascii_lowercase, k=len(substring)))\n            modified_data = data.replace(substring, replacement)\n            result.append({'Original String': data, 'Modified String': modified_data})\n        else:\n            result.append({'Original String': data, 'Modified String': data})\n    return pd.DataFrame(result)\n\n# Test the function\ndata_list = ['hello,world', 'python,programming', 'data,science', '']\ndf = task_func(data_list)"}
{"task_id": "BigCodeBench/480", "solution": "import re\nimport random\nimport pandas as pd\ndef task_func(data_list, seed=None):\n    \"\"\"\n    Shuffles the substrings within each string in a given list.\n    \"\"\"\n    # Set the seed for the random number generator\n    if seed is not None:\n        random.seed(seed)\n\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over each string in the list\n    for data in data_list:\n        # Split the string into substrings based on commas\n        substrings = [s.strip() for s in re.split(',', data)]\n\n        # Shuffle the substrings\n        random.shuffle(substrings)\n\n        # Join the substrings back into a string\n        shuffled_string = ', '.join(substrings)\n\n        # Append the original string and the shuffled string to the results list\n        results.append({'Original String': data, 'Shuffled String': shuffled_string})\n\n    # Convert the results list to a pandas DataFrame\n    df = pd.DataFrame(results)\n\n    return df", "raw_solution": "import re\nimport random\nimport pandas as pd\n\ndef task_func(data_list, seed=None):\n    \"\"\"\n    Shuffles the substrings within each string in a given list.\n    \"\"\"\n    # Set the seed for the random number generator\n    if seed is not None:\n        random.seed(seed)\n\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over each string in the list\n    for data in data_list:\n        # Split the string into substrings based on commas\n        substrings = [s.strip() for s in re.split(',', data)]\n\n        # Shuffle the substrings\n        random.shuffle(substrings)\n\n        # Join the substrings back into a string\n        shuffled_string = ', '.join(substrings)\n\n        # Append the original string and the shuffled string to the results list\n        results.append({'Original String': data, 'Shuffled String': shuffled_string})\n\n    # Convert the results list to a pandas DataFrame\n    df = pd.DataFrame(results)\n\n    return df"}
{"task_id": "BigCodeBench/481", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(data_list, seed=42):\n    random.seed(seed)\n    result = []\n    for data in data_list:\n        words = data.split(',')\n        random.shuffle(words)\n        randomized_data = ' '.join(words).strip()\n        result.append((data, randomized_data))\n    df = pd.DataFrame(result, columns=['Original String', 'Randomized String'])\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport re\n\ndef task_func(data_list, seed=42):\n    random.seed(seed)\n    result = []\n    for data in data_list:\n        words = data.split(',')\n        random.shuffle(words)\n        randomized_data = ' '.join(words).strip()\n        result.append((data, randomized_data))\n    df = pd.DataFrame(result, columns=['Original String', 'Randomized String'])\n    return df"}
{"task_id": "BigCodeBench/482", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(data_list, seed=None):\n    random.seed(seed)\n    operations = ['remove', 'replace', 'shuffle', 'randomize']\n    df = pd.DataFrame(data_list, columns=['Original String'])\n    df['Modified String'] = df['Original String'].apply(lambda x: random.choice(operations))\n    df['Modified String'] = df.apply(lambda row: modify_string(row['Original String'], row['Modified String']), axis=1)\n    return df\ndef modify_string(original, operation):\n    if operation == 'remove':\n        return remove_substring(original)\n    elif operation == 'replace':\n        return replace_substring(original)\n    elif operation == 'shuffle':\n        return shuffle_substrings(original)\n    elif operation == 'randomize':\n        return randomize_substrings(original)\ndef remove_substring(s):\n    substrings = s.split(',')\n    if len(substrings) > 1:\n        substrings.remove(random.choice(substrings))\n    return ','.join(substrings).strip()\ndef replace_substring(s):\n    substrings = s.split(',')\n    replace_with = 'random_string'\n    substrings[random.randint(0, len(substrings) - 1)] = replace_with\n    return ','.join(substrings).strip()\ndef shuffle_substrings(s):\n    substrings = s.split(',')\n    random.shuffle(substrings)\n    return ','.join(substrings).strip()\ndef randomize_substrings(s):\n    substrings = s.split(',')\n    random.shuffle(substrings)\n    return ','.join(substrings).strip()\ndata_list = ['apple, banana, cherry', 'orange, pineapple, strawberry', 'grape, melon, kiwi']", "raw_solution": "import pandas as pd\nimport random\nimport re\n\ndef task_func(data_list, seed=None):\n    random.seed(seed)\n    operations = ['remove', 'replace', 'shuffle', 'randomize']\n    df = pd.DataFrame(data_list, columns=['Original String'])\n    df['Modified String'] = df['Original String'].apply(lambda x: random.choice(operations))\n    df['Modified String'] = df.apply(lambda row: modify_string(row['Original String'], row['Modified String']), axis=1)\n    return df\n\ndef modify_string(original, operation):\n    if operation == 'remove':\n        return remove_substring(original)\n    elif operation == 'replace':\n        return replace_substring(original)\n    elif operation == 'shuffle':\n        return shuffle_substrings(original)\n    elif operation == 'randomize':\n        return randomize_substrings(original)\n\ndef remove_substring(s):\n    substrings = s.split(',')\n    if len(substrings) > 1:\n        substrings.remove(random.choice(substrings))\n    return ','.join(substrings).strip()\n\ndef replace_substring(s):\n    substrings = s.split(',')\n    replace_with = 'random_string'\n    substrings[random.randint(0, len(substrings) - 1)] = replace_with\n    return ','.join(substrings).strip()\n\ndef shuffle_substrings(s):\n    substrings = s.split(',')\n    random.shuffle(substrings)\n    return ','.join(substrings).strip()\n\ndef randomize_substrings(s):\n    substrings = s.split(',')\n    random.shuffle(substrings)\n    return ','.join(substrings).strip()\n\n# Test the function\ndata_list = ['apple, banana, cherry', 'orange, pineapple, strawberry', 'grape, melon, kiwi']"}
{"task_id": "BigCodeBench/483", "solution": "import re\nimport pandas as pd\ndef task_func(df: pd.DataFrame, column_name: str, pattern: str) -> pd.DataFrame:\n    \"\"\"\n    Reverses the order of words in a specific column of a pandas DataFrame where the words match a user-specified regular expression pattern.\n    Words are considered to be whitespace-separated strings. This function maintains the original order of non-matching words.\n    \"\"\"\n    if not pattern:\n        return df.copy()\n\n    def reverse_words(text):\n        \"\"\"\n        Helper function to reverse the order of words in a string that match the pattern.\n        \"\"\"\n        words = text.split()\n        for i, word in enumerate(words):\n            if re.search(pattern, word):\n                words[i] = word[::-1]\n        return ' '.join(words)\n\n    df_copy = df.copy()\n    df_copy[column_name] = df[column_name].apply(reverse_words)\n    return df_copy", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame, column_name: str, pattern: str) -> pd.DataFrame:\n    \"\"\"\n    Reverses the order of words in a specific column of a pandas DataFrame where the words match a user-specified regular expression pattern.\n    Words are considered to be whitespace-separated strings. This function maintains the original order of non-matching words.\n    \"\"\"\n    if not pattern:\n        return df.copy()\n\n    def reverse_words(text):\n        \"\"\"\n        Helper function to reverse the order of words in a string that match the pattern.\n        \"\"\"\n        words = text.split()\n        for i, word in enumerate(words):\n            if re.search(pattern, word):\n                words[i] = word[::-1]\n        return ' '.join(words)\n\n    df_copy = df.copy()\n    df_copy[column_name] = df[column_name].apply(reverse_words)\n    return df_copy"}
{"task_id": "BigCodeBench/484", "solution": "import math\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport pandas as pd\ndef task_func(\n    start_time,\n    end_time,\n    step,\n    columns=[\"Timestamp\", \"Sensor1\", \"Sensor2\", \"Sensor3\", \"SensorStatus\"],\n    sensor_statuses=[\"OK\", \"MAINTENANCE_REQUIRED\", \"ERROR\"],\n    random_seed=42,\n):\n    np.random.seed(random_seed)\n\n    # Generate timestamps\n    timestamps = pd.date_range(start=start_time, end=end_time, freq=step)\n\n    # Generate sensor readings\n    sensor_readings = pd.DataFrame(columns=columns)\n    sensor_readings[\"Timestamp\"] = timestamps\n    sensor_readings[\"Sensor1\"] = np.sin(timestamps.view(int)/1000000000) + np.random.normal(0, 0.1, len(timestamps))\n    sensor_readings[\"Sensor2\"] = np.cos(timestamps.view(int)/1000000000) + np.random.normal(0, 0.1, len(timestamps))\n    sensor_readings[\"Sensor3\"] = np.tan(timestamps.view(int)/1000000000) + np.random.normal(0, 0.1, len(timestamps))\n\n    # Generate sensor statuses\n    sensor_readings[\"SensorStatus\"] = np.random.choice(sensor_statuses, len(timestamps))\n\n    return sensor_readings", "raw_solution": "import math\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport pandas as pd\n\ndef task_func(\n    start_time,\n    end_time,\n    step,\n    columns=[\"Timestamp\", \"Sensor1\", \"Sensor2\", \"Sensor3\", \"SensorStatus\"],\n    sensor_statuses=[\"OK\", \"MAINTENANCE_REQUIRED\", \"ERROR\"],\n    random_seed=42,\n):\n    np.random.seed(random_seed)\n\n    # Generate timestamps\n    timestamps = pd.date_range(start=start_time, end=end_time, freq=step)\n\n    # Generate sensor readings\n    sensor_readings = pd.DataFrame(columns=columns)\n    sensor_readings[\"Timestamp\"] = timestamps\n    sensor_readings[\"Sensor1\"] = np.sin(timestamps.view(int)/1000000000) + np.random.normal(0, 0.1, len(timestamps))\n    sensor_readings[\"Sensor2\"] = np.cos(timestamps.view(int)/1000000000) + np.random.normal(0, 0.1, len(timestamps))\n    sensor_readings[\"Sensor3\"] = np.tan(timestamps.view(int)/1000000000) + np.random.normal(0, 0.1, len(timestamps))\n\n    # Generate sensor statuses\n    sensor_readings[\"SensorStatus\"] = np.random.choice(sensor_statuses, len(timestamps))\n\n    return sensor_readings"}
{"task_id": "BigCodeBench/485", "solution": "from datetime import datetime, timedelta\nimport pytz\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time):\n    # Define the time zones\n    time_zones = [\"UTC\", \"America/Los_Angeles\", \"Europe/Paris\", \"Asia/Kolkata\", \"Australia/Sydney\"]\n    colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\n\n    # Convert start_time and end_time to datetime objects\n    start_time = datetime.strptime(start_time, \"%Y-%m-%d\")\n    end_time = datetime.strptime(end_time, \"%Y-%m-%d\")\n\n    # Generate a list of dates between start_time and end_time\n    dates = [start_time + timedelta(days=x) for x in range((end_time-start_time).days + 1)]\n\n    # Initialize a figure\n    fig, ax = plt.subplots()\n\n    # Plot the time difference for each time zone\n    for i, time_zone in enumerate(time_zones):\n        # Create a timezone object\n        tz = pytz.timezone(time_zone)\n\n        # Calculate the time difference for each date\n        time_diffs = [(datetime(date.year, date.month, date.day, tzinfo=tz) - datetime(date.year, date.month, date.day)).seconds / 3600 for date in dates]\n\n        # Plot the time difference\n        ax.plot(dates, time_diffs, color=colors[i % len(colors)], label=time_zone)\n\n    # Set the x-axis label and rotate the tick labels\n    ax.set_xlabel(\"Date\")\n    plt.xticks(rotation=45)\n\n    # Set the y-axis label\n    ax.set_ylabel(\"Time Difference (hours)\")\n\n    # Add a legend\n    ax.legend()\n\n    return ax", "raw_solution": "from datetime import datetime, timedelta\nimport pytz\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time):\n    # Define the time zones\n    time_zones = [\"UTC\", \"America/Los_Angeles\", \"Europe/Paris\", \"Asia/Kolkata\", \"Australia/Sydney\"]\n    colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\n\n    # Convert start_time and end_time to datetime objects\n    start_time = datetime.strptime(start_time, \"%Y-%m-%d\")\n    end_time = datetime.strptime(end_time, \"%Y-%m-%d\")\n\n    # Generate a list of dates between start_time and end_time\n    dates = [start_time + timedelta(days=x) for x in range((end_time-start_time).days + 1)]\n\n    # Initialize a figure\n    fig, ax = plt.subplots()\n\n    # Plot the time difference for each time zone\n    for i, time_zone in enumerate(time_zones):\n        # Create a timezone object\n        tz = pytz.timezone(time_zone)\n\n        # Calculate the time difference for each date\n        time_diffs = [(datetime(date.year, date.month, date.day, tzinfo=tz) - datetime(date.year, date.month, date.day)).seconds / 3600 for date in dates]\n\n        # Plot the time difference\n        ax.plot(dates, time_diffs, color=colors[i % len(colors)], label=time_zone)\n\n    # Set the x-axis label and rotate the tick labels\n    ax.set_xlabel(\"Date\")\n    plt.xticks(rotation=45)\n\n    # Set the y-axis label\n    ax.set_ylabel(\"Time Difference (hours)\")\n\n    # Add a legend\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    np.random.seed(seed)\n\n    # Generate timestamps\n    start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n    end_time = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n    time_range = pd.date_range(start=start_time, end=end_time, freq=f'{step}S')\n\n    # Generate values from normal distribution\n    values = np.random.normal(0, 1, len(time_range))\n\n    # Add trend\n    time_range_seconds = (time_range - time_range[0]).total_seconds()\n    values += trend * time_range_seconds\n\n    # Create DataFrame\n    df = pd.DataFrame({'Time': time_range, 'Value': values})\n\n    # Plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Time'], df['Value'])\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n\n    return ax", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    np.random.seed(seed)\n\n    # Generate timestamps\n    start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n    end_time = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n    time_range = pd.date_range(start=start_time, end=end_time, freq=f'{step}S')\n\n    # Generate values from normal distribution\n    values = np.random.normal(0, 1, len(time_range))\n\n    # Add trend\n    time_range_seconds = (time_range - time_range[0]).total_seconds()\n    values += trend * time_range_seconds\n\n    # Create DataFrame\n    df = pd.DataFrame({'Time': time_range, 'Value': values})\n\n    # Plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Time'], df['Value'])\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n\n    return ax"}
{"task_id": "BigCodeBench/487", "solution": "import os\nimport pandas as pd\nimport re\ndef task_func(file_path: str) -> pd.DataFrame:\n    \"\"\"\n    Parses a log file to extract log entries into a DataFrame.\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The specified log file {file_path} does not exist.\")\n\n    log_entries = []\n    with open(file_path, 'r') as log_file:\n        for line in log_file:\n            match = re.match(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+) - (\\w+) - (.+)', line)\n            if match:\n                timestamp, level, message = match.groups()\n                log_entries.append({'Timestamp': timestamp, 'Level': level, 'Message': message})\n\n    if not log_entries:\n        return pd.DataFrame(columns=['Timestamp', 'Level', 'Message'])\n\n    return pd.DataFrame(log_entries)", "raw_solution": "import os\nimport pandas as pd\nimport re\n\ndef task_func(file_path: str) -> pd.DataFrame:\n    \"\"\"\n    Parses a log file to extract log entries into a DataFrame.\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The specified log file {file_path} does not exist.\")\n\n    log_entries = []\n    with open(file_path, 'r') as log_file:\n        for line in log_file:\n            match = re.match(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+) - (\\w+) - (.+)', line)\n            if match:\n                timestamp, level, message = match.groups()\n                log_entries.append({'Timestamp': timestamp, 'Level': level, 'Message': message})\n\n    if not log_entries:\n        return pd.DataFrame(columns=['Timestamp', 'Level', 'Message'])\n\n    return pd.DataFrame(log_entries)"}
{"task_id": "BigCodeBench/488", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_time_series(start_time, end_time, step, amplitude, period, seed=0):\n    np.random.seed(seed)\n    time_range = pd.date_range(start=start_time, end=end_time, freq=step)\n    seasonal_component = amplitude * np.sin(2 * np.pi * (time_range - time_range.min()) / period)\n    time_series = np.random.normal(0, 1, len(time_range)) + seasonal_component\n    df = pd.DataFrame({'Timestamp': time_range, 'Value': time_series})\n    return df\n\ndef plot_time_series(df):\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['Timestamp'], df['Value'], marker='o')\n    plt.xlabel('Timestamp')\n    plt.ylabel('Value')\n    plt.title('Time Series with Seasonality')\n    plt.grid(True)\n    return plt.gca()\n\n# Test the function\nstart_time = datetime.now()\nend_time = datetime.now() + timedelta(days=10)\nstep = '1H'\namplitude = 5\nperiod = '10H'\ndf = generate_time_series(start_time, end_time, step, amplitude, period)\nax = plot_time_series(df)\nplt.show()"}
{"task_id": "BigCodeBench/489", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\ndef task_func(epoch_milliseconds, seed=0):\n    \"\"\"\n    Generates user activity logs from a given epoch time to the current time.\n    \"\"\"\n    # Check if the start time is after the current system time\n    if epoch_milliseconds > int(time.time() * 1000):\n        raise ValueError(\"Start time cannot be after the current system time.\")\n\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Define the list of users and activities\n    users = ['user1', 'user2', 'user3', 'user4', 'user5']\n    activities = ['login', 'logout', 'browse', 'search', 'purchase']\n\n    # Initialize the DataFrame to store the logs\n    logs = pd.DataFrame(columns=['User', 'Activity', 'Time'])\n\n    # Convert the epoch time to a datetime object\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n\n    # Generate logs incrementally until the current system time\n    current_time = start_time\n    while True:\n        # Generate a random number of seconds to increment the time\n        sleep_time = random.randint(1, 10)\n\n        # Increment the current time by the random number of seconds\n        current_time += timedelta(seconds=sleep_time)\n\n        # If the current time is after the current system time, break the loop\n        if current_time > datetime.now():\n            break\n\n        # Generate a random user and activity\n        user = random.choice(users)\n        activity = random.choice(activities)\n\n        # Append the log to the DataFrame\n        logs = logs.append({'User': user, 'Activity': activity, 'Time': current_time}, ignore_index=True)\n\n    return logs", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\n\ndef task_func(epoch_milliseconds, seed=0):\n    \"\"\"\n    Generates user activity logs from a given epoch time to the current time.\n    \"\"\"\n    # Check if the start time is after the current system time\n    if epoch_milliseconds > int(time.time() * 1000):\n        raise ValueError(\"Start time cannot be after the current system time.\")\n\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Define the list of users and activities\n    users = ['user1', 'user2', 'user3', 'user4', 'user5']\n    activities = ['login', 'logout', 'browse', 'search', 'purchase']\n\n    # Initialize the DataFrame to store the logs\n    logs = pd.DataFrame(columns=['User', 'Activity', 'Time'])\n\n    # Convert the epoch time to a datetime object\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n\n    # Generate logs incrementally until the current system time\n    current_time = start_time\n    while True:\n        # Generate a random number of seconds to increment the time\n        sleep_time = random.randint(1, 10)\n\n        # Increment the current time by the random number of seconds\n        current_time += timedelta(seconds=sleep_time)\n\n        # If the current time is after the current system time, break the loop\n        if current_time > datetime.now():\n            break\n\n        # Generate a random user and activity\n        user = random.choice(users)\n        activity = random.choice(activities)\n\n        # Append the log to the DataFrame\n        logs = logs.append({'User': user, 'Activity': activity, 'Time': current_time}, ignore_index=True)\n\n    return logs"}
{"task_id": "BigCodeBench/490", "solution": "import xmltodict\nimport json\ndef task_func(s, file_path):\n    \"\"\"\n    Converts an XML string into a dictionary representation and saves it as a JSON file.\n    \"\"\"\n    # Convert XML to dictionary\n    dict_data = xmltodict.parse(s)\n\n    # Save dictionary as JSON file\n    with open(file_path, 'w') as f:\n        json.dump(dict_data, f)\n\n    return dict_data", "raw_solution": "import xmltodict\nimport json\n\ndef task_func(s, file_path):\n    \"\"\"\n    Converts an XML string into a dictionary representation and saves it as a JSON file.\n    \"\"\"\n    # Convert XML to dictionary\n    dict_data = xmltodict.parse(s)\n\n    # Save dictionary as JSON file\n    with open(file_path, 'w') as f:\n        json.dump(dict_data, f)\n\n    return dict_data"}
{"task_id": "BigCodeBench/491", "solution": "import random\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\ndef task_func(epoch_milliseconds, seed=None):\n    if epoch_milliseconds < 0 or epoch_milliseconds > int(datetime.now().timestamp() * 1000):\n        raise ValueError(\"Invalid start time. It should be a positive number and not after the current time.\")\n\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports']\n    random.seed(seed)\n\n    sales_data = {}\n    for category in categories:\n        sales_data[category] = [random.randint(10, 50) for _ in range((datetime.now() - start_time).days + 1)]\n\n    days = [(start_time + timedelta(days=i)).strftime('%Y-%m-%d') for i in range((datetime.now() - start_time).days + 1)]\n\n    fig, ax = plt.subplots()\n    for category in categories:\n        ax.plot(days, sales_data[category], label=category)\n\n    ax.set_xlabel('Days since start date')\n    ax.set_ylabel('Sales (units)')\n    ax.legend()\n    plt.show()\n\n    return sales_data, ax", "raw_solution": "import random\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\ndef task_func(epoch_milliseconds, seed=None):\n    if epoch_milliseconds < 0 or epoch_milliseconds > int(datetime.now().timestamp() * 1000):\n        raise ValueError(\"Invalid start time. It should be a positive number and not after the current time.\")\n\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports']\n    random.seed(seed)\n\n    sales_data = {}\n    for category in categories:\n        sales_data[category] = [random.randint(10, 50) for _ in range((datetime.now() - start_time).days + 1)]\n\n    days = [(start_time + timedelta(days=i)).strftime('%Y-%m-%d') for i in range((datetime.now() - start_time).days + 1)]\n\n    fig, ax = plt.subplots()\n    for category in categories:\n        ax.plot(days, sales_data[category], label=category)\n\n    ax.set_xlabel('Days since start date')\n    ax.set_ylabel('Sales (units)')\n    ax.legend()\n    plt.show()\n\n    return sales_data, ax"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generates sales data for five products from a given epoch time up to the current time.\n    \"\"\"\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or not isinstance(random_seed, int) or not isinstance(products, list) or len(products) != 5:\n        raise ValueError(\"Invalid input. Please provide a valid epoch time (in milliseconds), a random seed (integer), and a list of 5 products.\")\n\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n\n    # Convert epoch time to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n\n    # Generate sales data\n    data = []\n    for single_date in pd.date_range(start=start_date, end=end_date):\n        for product in products:\n            data.append({\n                \"Product\": product,\n                \"Date\": single_date,\n                \"Sales\": random.randint(10, 50),\n            })\n\n    # Convert to DataFrame and return\n    return pd.DataFrame(data)\nrandom_seed = 0\nepoch_milliseconds = int(time.mktime(datetime(2022, 1, 1).timetuple()) * 1000)", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generates sales data for five products from a given epoch time up to the current time.\n    \"\"\"\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or not isinstance(random_seed, int) or not isinstance(products, list) or len(products) != 5:\n        raise ValueError(\"Invalid input. Please provide a valid epoch time (in milliseconds), a random seed (integer), and a list of 5 products.\")\n\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n\n    # Convert epoch time to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n\n    # Generate sales data\n    data = []\n    for single_date in pd.date_range(start=start_date, end=end_date):\n        for product in products:\n            data.append({\n                \"Product\": product,\n                \"Date\": single_date,\n                \"Sales\": random.randint(10, 50),\n            })\n\n    # Convert to DataFrame and return\n    return pd.DataFrame(data)\n\n# Test the function\nrandom_seed = 0\nepoch_milliseconds = int(time.mktime(datetime(2022, 1, 1).timetuple()) * 1000)\ndf = task_func(epoch_milliseconds, random_seed)"}
{"task_id": "BigCodeBench/493", "solution": "from datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(\n    epoch_milliseconds,\n    teams=[\"Team1\", \"Team2\", \"Team3\", \"Team4\", \"Team5\"],\n    random_seed=0,\n):\n    random.seed(random_seed)\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    today = datetime.today()\n    days = (today - start_date).days\n\n    performance_data = {}\n    for team in teams:\n        performance_data[team] = [random.uniform(0.1, 1) for _ in range(days)]\n\n    fig, ax = plt.subplots()\n    for team, performance in performance_data.items():\n        ax.plot([i for i in range(days)], performance, label=team)\n\n    ax.set_xlabel('Days since start date')\n    ax.set_ylabel('Performance')\n    ax.legend()\n\n    return performance_data, fig", "raw_solution": "from datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    epoch_milliseconds,\n    teams=[\"Team1\", \"Team2\", \"Team3\", \"Team4\", \"Team5\"],\n    random_seed=0,\n):\n    random.seed(random_seed)\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    today = datetime.today()\n    days = (today - start_date).days\n\n    performance_data = {}\n    for team in teams:\n        performance_data[team] = [random.uniform(0.1, 1) for _ in range(days)]\n\n    fig, ax = plt.subplots()\n    for team, performance in performance_data.items():\n        ax.plot([i for i in range(days)], performance, label=team)\n\n    ax.set_xlabel('Days since start date')\n    ax.set_ylabel('Performance')\n    ax.legend()\n\n    return performance_data, fig"}
{"task_id": "BigCodeBench/494", "solution": "from datetime import datetime\nimport pytz\nimport re\nfrom faker import Faker\ndef task_func(epoch_milliseconds, seed=0, timezones=[\"UTC\"]):\n    Faker.seed(seed)\n    faker = Faker()\n    faker.seed_instance(seed)\n\n    # Convert epoch to datetime\n    dt = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n\n    # Validate timezones\n    valid_timezones = []\n    for tz in timezones:\n        if tz in pytz.all_timezones:\n            valid_timezones.append(tz)\n        elif re.match(r\"^UTC[+-]\\d\\d:\\d\\d$\", tz):\n            valid_timezones.append(tz)\n\n    # If no valid timezones, select UTC\n    if not valid_timezones:\n        timezone = pytz.timezone(\"UTC\")\n    else:\n        # Randomly select a valid timezone\n        timezone = pytz.timezone(faker.random_element(elements=valid_timezones))\n\n    # Convert to the selected timezone\n    dt = dt.astimezone(timezone)\n\n    # Generate a fake event name\n    event_name = faker.bs()\n\n    # Return a dictionary with the event name and event details\n    return {\n        event_name: [\n            {\n                \"date\": dt.strftime(\"%Y-%m-%d\"),\n                \"time\": dt.strftime(\"%H:%M:%S\"),\n                \"timezone\": timezone.zone\n            }\n        ]\n    }", "raw_solution": "from datetime import datetime\nimport pytz\nimport re\nfrom faker import Faker\n\ndef task_func(epoch_milliseconds, seed=0, timezones=[\"UTC\"]):\n    Faker.seed(seed)\n    faker = Faker()\n    faker.seed_instance(seed)\n\n    # Convert epoch to datetime\n    dt = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n\n    # Validate timezones\n    valid_timezones = []\n    for tz in timezones:\n        if tz in pytz.all_timezones:\n            valid_timezones.append(tz)\n        elif re.match(r\"^UTC[+-]\\d\\d:\\d\\d$\", tz):\n            valid_timezones.append(tz)\n\n    # If no valid timezones, select UTC\n    if not valid_timezones:\n        timezone = pytz.timezone(\"UTC\")\n    else:\n        # Randomly select a valid timezone\n        timezone = pytz.timezone(faker.random_element(elements=valid_timezones))\n\n    # Convert to the selected timezone\n    dt = dt.astimezone(timezone)\n\n    # Generate a fake event name\n    event_name = faker.bs()\n\n    # Return a dictionary with the event name and event details\n    return {\n        event_name: [\n            {\n                \"date\": dt.strftime(\"%Y-%m-%d\"),\n                \"time\": dt.strftime(\"%H:%M:%S\"),\n                \"timezone\": timezone.zone\n            }\n        ]\n    }"}
{"task_id": "BigCodeBench/495", "solution": "import pandas as pd\nimport numpy as np\nfrom faker import Faker", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom faker import Faker\n\ndef generate_spending_report(days, random_seed=0):\n    \"\"\"\n    Generates a spending report DataFrame for the given number of days.\n    Each day on or after '2023-01-01' has its own row. The DataFrame has five columns:\n    Groceries, Entertainment, Rent, Utilities, and Miscellaneous, with their integer values\n    independently randomly sampled from 0 to 100.\n    \"\"\"\n    np.random.seed(random_seed)\n    fake = Faker()\n    fake.seed_instance(random_seed)\n\n    date_range = pd.date_range(start='2023-01-01', periods=days)\n    data = {\n        'Groceries': np.random.randint(0, 101, days),\n        'Entertainment': np.random.randint(0, 101, days),\n        'Rent': np.random.randint(0, 101, days),\n        'Utilities': np.random.randint(0, 101, days),\n        'Miscellaneous': np.random.randint(0, 101, days)\n    }\n\n    df = pd.DataFrame(data, index=date_range)\n\n    return df"}
{"task_id": "BigCodeBench/496", "solution": "from datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    Generates a graph of temperature trends over the past week using randomly generated data.\n    The function generates random integer temperatures in Celcius with a low of 15 and high of 35.\n    The function plots date on the x-axis and temperature on the y-axis.\n    The function raises ValueError if days_in_past is less than 1.\n    \"\"\"\n    if days_in_past < 1:\n        raise ValueError(\"days_in_past must be greater than 0\")\n\n    np.random.seed(random_seed)\n    temperatures = np.random.randint(15, 35, days_in_past)\n\n    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_in_past)]\n\n    fig, ax = plt.subplots()\n    ax.plot(dates, temperatures)\n\n    ax.set(xlabel='Date', ylabel='Temperature (\u00b0C)',\n           title='Temperature Trend')\n    ax.grid()\n\n    return ax", "raw_solution": "from datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    Generates a graph of temperature trends over the past week using randomly generated data.\n    The function generates random integer temperatures in Celcius with a low of 15 and high of 35.\n    The function plots date on the x-axis and temperature on the y-axis.\n    The function raises ValueError if days_in_past is less than 1.\n    \"\"\"\n    if days_in_past < 1:\n        raise ValueError(\"days_in_past must be greater than 0\")\n\n    np.random.seed(random_seed)\n    temperatures = np.random.randint(15, 35, days_in_past)\n\n    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_in_past)]\n\n    fig, ax = plt.subplots()\n    ax.plot(dates, temperatures)\n\n    ax.set(xlabel='Date', ylabel='Temperature (\u00b0C)',\n           title='Temperature Trend')\n    ax.grid()\n\n    return ax"}
{"task_id": "BigCodeBench/497", "solution": "from datetime import datetime, timedelta\nimport pytz\nimport calendar\ndef task_func(days_in_past=7):\n    if days_in_past < 0:\n        raise ValueError(\"'days_in_past' cannot be negative\")\n\n    today = datetime.now(pytz.utc)\n    target_date = today - timedelta(days=days_in_past)\n    weekday = calendar.day_name[target_date.weekday()]\n\n    return weekday", "raw_solution": "from datetime import datetime, timedelta\nimport pytz\nimport calendar\n\ndef task_func(days_in_past=7):\n    if days_in_past < 0:\n        raise ValueError(\"'days_in_past' cannot be negative\")\n\n    today = datetime.now(pytz.utc)\n    target_date = today - timedelta(days=days_in_past)\n    weekday = calendar.day_name[target_date.weekday()]\n\n    return weekday"}
{"task_id": "BigCodeBench/498", "solution": "import xmltodict\nimport json\nimport os\ndef task_func(s, save_json=False, json_file_path=None):\n    if not s.strip():\n        raise ValueError(\"Input XML string is empty or contains only whitespace\")\n\n    result = xmltodict.parse(s)\n\n    if save_json and json_file_path:\n        with open(json_file_path, 'w') as f:\n            json.dump(result, f)\n\n    return result", "raw_solution": "import xmltodict\nimport json\nimport os\n\ndef task_func(s, save_json=False, json_file_path=None):\n    if not s.strip():\n        raise ValueError(\"Input XML string is empty or contains only whitespace\")\n\n    result = xmltodict.parse(s)\n\n    if save_json and json_file_path:\n        with open(json_file_path, 'w') as f:\n            json.dump(result, f)\n\n    return result"}
{"task_id": "BigCodeBench/499", "solution": "import xlwt\nimport os\nimport io\nimport csv\ndef task_func(csv_content, filename):\n    \"\"\"\n    Converts CSV content into an Excel file and saves it with the given filename.\n    \"\"\"\n    # Create a new workbook and add a sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet 1')\n\n    # Write the CSV content into the workbook\n    lines = csv_content.split('\\n')\n    for row_index, line in enumerate(lines):\n        fields = line.split(',')\n        for col_index, field in enumerate(fields):\n            sheet.write(row_index, col_index, field)\n\n    # Save the workbook as an Excel file\n    workbook.save(filename)\n\n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)\ncsv_content = 'Hello'\nfilename = 'single_cell.xls'", "raw_solution": "import xlwt\nimport os\nimport io\nimport csv\n\ndef task_func(csv_content, filename):\n    \"\"\"\n    Converts CSV content into an Excel file and saves it with the given filename.\n    \"\"\"\n    # Create a new workbook and add a sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet 1')\n\n    # Write the CSV content into the workbook\n    lines = csv_content.split('\\n')\n    for row_index, line in enumerate(lines):\n        fields = line.split(',')\n        for col_index, field in enumerate(fields):\n            sheet.write(row_index, col_index, field)\n\n    # Save the workbook as an Excel file\n    workbook.save(filename)\n\n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)\n\n# Test the function\ncsv_content = 'Hello'\nfilename = 'single_cell.xls'\nassert os.path.isfile(task_func(csv_content, filename))"}
{"task_id": "BigCodeBench/500", "solution": "import xlwt\nimport os\nfrom collections import OrderedDict\nFIELDS = ['ID', 'Name', 'Age']\ndef task_func(values, filename):\n    # Create a new workbook and add a sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet1')\n\n    # Write the field names to the first row\n    for i, field in enumerate(FIELDS):\n        sheet.write(0, i, field)\n\n    # Write the data to the subsequent rows\n    for i, value in enumerate(values, start=1):\n        for j, key in enumerate(FIELDS):\n            sheet.write(i, j, value.get(key, ''))\n\n    # Save the workbook to the specified file\n    workbook.save(filename)\n\n    # Return the absolute path of the created file\n    return os.path.abspath(filename)\nempty_data = []", "raw_solution": "import xlwt\nimport os\nfrom collections import OrderedDict\n\n# Constants\nFIELDS = ['ID', 'Name', 'Age']\n\ndef task_func(values, filename):\n    # Create a new workbook and add a sheet\n    workbook = xlwt.Workbook()\n    sheet = workbook.add_sheet('Sheet1')\n\n    # Write the field names to the first row\n    for i, field in enumerate(FIELDS):\n        sheet.write(0, i, field)\n\n    # Write the data to the subsequent rows\n    for i, value in enumerate(values, start=1):\n        for j, key in enumerate(FIELDS):\n            sheet.write(i, j, value.get(key, ''))\n\n    # Save the workbook to the specified file\n    workbook.save(filename)\n\n    # Return the absolute path of the created file\n    return os.path.abspath(filename)\n\n# Test the function\nempty_data = []\npath = task_func(empty_data, 'empty_data.xls')\nassert os.path.exists(path) and 'empty_data.xls' in path"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Converts a JSON string to an Excel file.\n    If the JSON string represents an empty array, the function creates an Excel file with no data rows.\n    \"\"\"\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"json_str is not valid JSON\")\n\n    if not isinstance(data, list):\n        raise ValueError(\"json_str must represent a JSON array\")\n\n    df = pd.DataFrame(data)\n\n    try:\n        writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n        df.to_excel(writer, sheet_name=sheet_name, index=False)\n        writer.save()\n    except Exception as e:\n        raise Exception(\"Error writing to file: {}\".format(e))\n\n    return os.path.abspath(filename)", "raw_solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Converts a JSON string to an Excel file.\n    If the JSON string represents an empty array, the function creates an Excel file with no data rows.\n    \"\"\"\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"json_str is not valid JSON\")\n\n    if not isinstance(data, list):\n        raise ValueError(\"json_str must represent a JSON array\")\n\n    df = pd.DataFrame(data)\n\n    try:\n        writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n        df.to_excel(writer, sheet_name=sheet_name, index=False)\n        writer.save()\n    except Exception as e:\n        raise Exception(\"Error writing to file: {}\".format(e))\n\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_in_past)]\n\n    data = []\n    for date in dates:\n        for activity in activities:\n            data.append([date, activity, random.randint(0, 120)])\n\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', ax=ax)\n    ax.set_title('Daily Activity Durations')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Duration')\n\n    return (ax, df)", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_in_past)]\n\n    data = []\n    for date in dates:\n        for activity in activities:\n            data.append([date, activity, random.randint(0, 120)])\n\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', ax=ax)\n    ax.set_title('Daily Activity Durations')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Duration')\n\n    return (ax, df)"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    np.random.seed(random_seed)\n    dates = pd.date_range(start=datetime.now() - timedelta(days=days_in_past), periods=days_in_past).tolist()\n    prices = np.random.rand(days_in_past, len(stock_names)).tolist()\n    df = pd.DataFrame(prices, index=dates, columns=stock_names)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    np.random.seed(random_seed)\n    dates = pd.date_range(start=datetime.now() - timedelta(days=days_in_past), periods=days_in_past).tolist()\n    prices = np.random.rand(days_in_past, len(stock_names)).tolist()\n    df = pd.DataFrame(prices, index=dates, columns=stock_names)\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/504", "solution": "import hashlib\nimport rsa\nimport base64\nfrom typing import Optional\ndef task_func(file_path: str) -> Optional[str]:\n    \"\"\"\n    Generates a signed hash of a file's contents using RSA encryption.\n    The file's contents are hashed using SHA-256, and then the hash is signed with a private RSA key stored in 'private.pem'.\n    The signed hash is encoded in base64.\n    \"\"\"\n    try:\n        with open(file_path, 'rb') as file:\n            data = file.read()\n\n        # Hash the file's contents using SHA-256\n        file_hash = hashlib.sha256(data).digest()\n\n        # Load the private RSA key\n        with open('private.pem', 'rb') as private_key_file:\n            private_key = rsa.PrivateKey.load_pkcs1(private_key_file.read())\n\n        # Sign the hash\n        signed_hash = rsa.sign_hash(file_hash, private_key, 'SHA-256')\n\n        # Encode the signed hash in base64\n        encoded_signed_hash = base64.b64encode(signed_hash).decode()\n\n        return encoded_signed_hash\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None", "raw_solution": "import hashlib\nimport rsa\nimport base64\nfrom typing import Optional\n\ndef task_func(file_path: str) -> Optional[str]:\n    \"\"\"\n    Generates a signed hash of a file's contents using RSA encryption.\n    The file's contents are hashed using SHA-256, and then the hash is signed with a private RSA key stored in 'private.pem'.\n    The signed hash is encoded in base64.\n    \"\"\"\n    try:\n        with open(file_path, 'rb') as file:\n            data = file.read()\n\n        # Hash the file's contents using SHA-256\n        file_hash = hashlib.sha256(data).digest()\n\n        # Load the private RSA key\n        with open('private.pem', 'rb') as private_key_file:\n            private_key = rsa.PrivateKey.load_pkcs1(private_key_file.read())\n\n        # Sign the hash\n        signed_hash = rsa.sign_hash(file_hash, private_key, 'SHA-256')\n\n        # Encode the signed hash in base64\n        encoded_signed_hash = base64.b64encode(signed_hash).decode()\n\n        return encoded_signed_hash\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/505", "solution": "import hashlib\nimport hmac\ndef task_func(secret, message):\n    \"\"\"\n    Generates an HMAC signature for a given message using a secret key.\n    The function uses SHA-256 as the hash function to create the HMAC signature.\n    \"\"\"\n    secret = secret.encode()\n    message = message.encode()\n    hmac_signature = hmac.new(secret, message, hashlib.sha256).hexdigest()\n    return hmac_signature", "raw_solution": "import hashlib\nimport hmac\n\ndef task_func(secret, message):\n    \"\"\"\n    Generates an HMAC signature for a given message using a secret key.\n    The function uses SHA-256 as the hash function to create the HMAC signature.\n    \"\"\"\n    secret = secret.encode()\n    message = message.encode()\n    hmac_signature = hmac.new(secret, message, hashlib.sha256).hexdigest()\n    return hmac_signature\n\n# Test the function\nassert len(task_func('mysecretkey', 'Goodbye, world!')) == 64"}
{"task_id": "BigCodeBench/506", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    df = pd.DataFrame(data, columns=['date', 'temperature', 'humidity', 'wind_speed', 'precipitation'])\n    if df.empty:\n        return {\n            'sum': np.nan,\n            'mean': np.nan,\n            'min': np.inf,\n            'max': -np.inf,\n            'plot': None\n        }\n    else:\n        result = {\n            'sum': df[column].sum(),\n            'mean': df[column].mean(),\n            'min': df[column].min(),\n            'max': df[column].max(),\n            'plot': None\n        }\n        if column in df.columns:\n            plt.hist(df[column], bins=10)\n            plt.title(f'Histogram of {column}')\n            plt.xlabel(column)\n            plt.ylabel('Frequency')\n            result['plot'] = plt.gca()\n        return result", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    df = pd.DataFrame(data, columns=['date', 'temperature', 'humidity', 'wind_speed', 'precipitation'])\n    if df.empty:\n        return {\n            'sum': np.nan,\n            'mean': np.nan,\n            'min': np.inf,\n            'max': -np.inf,\n            'plot': None\n        }\n    else:\n        result = {\n            'sum': df[column].sum(),\n            'mean': df[column].mean(),\n            'min': df[column].min(),\n            'max': df[column].max(),\n            'plot': None\n        }\n        if column in df.columns:\n            plt.hist(df[column], bins=10)\n            plt.title(f'Histogram of {column}')\n            plt.xlabel(column)\n            plt.ylabel('Frequency')\n            result['plot'] = plt.gca()\n        return result"}
{"task_id": "BigCodeBench/507", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(column, data):\n    if column not in data.columns:\n        raise ValueError(f\"Invalid column name: {column}\")\n\n    if data.empty:\n        return {\n            'sum': 0,\n            'mean': np.nan,\n            'min': np.nan,\n            'max': np.nan\n        }\n\n    return {\n        'sum': data[column].sum(),\n        'mean': data[column].mean(),\n        'min': data[column].min(),\n        'max': data[column].max()\n    }", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(column, data):\n    if column not in data.columns:\n        raise ValueError(f\"Invalid column name: {column}\")\n\n    if data.empty:\n        return {\n            'sum': 0,\n            'mean': np.nan,\n            'min': np.nan,\n            'max': np.nan\n        }\n\n    return {\n        'sum': data[column].sum(),\n        'mean': data[column].mean(),\n        'min': data[column].min(),\n        'max': data[column].max()\n    }"}
{"task_id": "BigCodeBench/508", "solution": "import hashlib\nimport io\nimport os\ndef task_func(file_path1, file_path2):\n    \"\"\"\n    Compares two files to determine if they are identical by computing and comparing their MD5 hash values.\n    \"\"\"\n    if not os.path.exists(file_path1) or not os.path.exists(file_path2):\n        raise FileNotFoundError(\"One or both files do not exist.\")\n\n    with io.FileIO(file_path1, 'r') as file1, io.FileIO(file_path2, 'r') as file2:\n        hash1 = hashlib.md5(file1.read()).hexdigest()\n        file1.seek(0)\n        hash2 = hashlib.md5(file2.read()).hexdigest()\n        file2.seek(0)\n\n    return hash1 == hash2", "raw_solution": "import hashlib\nimport io\nimport os\n\ndef task_func(file_path1, file_path2):\n    \"\"\"\n    Compares two files to determine if they are identical by computing and comparing their MD5 hash values.\n    \"\"\"\n    if not os.path.exists(file_path1) or not os.path.exists(file_path2):\n        raise FileNotFoundError(\"One or both files do not exist.\")\n\n    with io.FileIO(file_path1, 'r') as file1, io.FileIO(file_path2, 'r') as file2:\n        hash1 = hashlib.md5(file1.read()).hexdigest()\n        file1.seek(0)\n        hash2 = hashlib.md5(file2.read()).hexdigest()\n        file2.seek(0)\n\n    return hash1 == hash2"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r') as file1, open(file_path2, 'r') as file2:\n            lines1 = file1.readlines()\n            lines2 = file2.readlines()\n\n        if not lines1 or not lines2:\n            raise ValueError(\"One or both of the files are empty.\")\n\n        diff_lines = list(ndiff(lines1, lines2))\n\n        diff_data = []\n        for i, line in enumerate(diff_lines, start=1):\n            status = line[0]\n            content = line[2:]\n            diff_data.append({'Line Number': i, 'Status': status, 'Content': content})\n\n        df = pd.DataFrame(diff_data)\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files cannot be found.\")\n    except Exception as e:\n        raise Exception(\"An error occurred: {}\".format(e))", "raw_solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r') as file1, open(file_path2, 'r') as file2:\n            lines1 = file1.readlines()\n            lines2 = file2.readlines()\n\n        if not lines1 or not lines2:\n            raise ValueError(\"One or both of the files are empty.\")\n\n        diff_lines = list(ndiff(lines1, lines2))\n\n        diff_data = []\n        for i, line in enumerate(diff_lines, start=1):\n            status = line[0]\n            content = line[2:]\n            diff_data.append({'Line Number': i, 'Status': status, 'Content': content})\n\n        df = pd.DataFrame(diff_data)\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files cannot be found.\")\n    except Exception as e:\n        raise Exception(\"An error occurred: {}\".format(e))"}
{"task_id": "BigCodeBench/510", "solution": "import difflib\nimport gzip\nimport io\ndef task_func(file_path1, file_path2):\n    \"\"\"\n    Compares the contents of two gzip files and returns a string describing the differences between them.\n    \"\"\"\n    # Read the contents of each file\n    with gzip.open(file_path1, 'rb') as f1, gzip.open(file_path2, 'rb') as f2:\n        content1 = f1.read().decode('utf-8')\n        content2 = f2.read().decode('utf-8')\n\n    # Use difflib to compute and return the differences\n    diff = difflib.Differ()\n    diffs = diff.compare(content1.splitlines(keepends=True), content2.splitlines(keepends=True))\n    diffs = [line for line in diffs if line.startswith('- ') or line.startswith('+ ')]\n\n    # Return the differences as a string\n    return ''.join(diffs)", "raw_solution": "import difflib\nimport gzip\nimport io\n\ndef task_func(file_path1, file_path2):\n    \"\"\"\n    Compares the contents of two gzip files and returns a string describing the differences between them.\n    \"\"\"\n    # Read the contents of each file\n    with gzip.open(file_path1, 'rb') as f1, gzip.open(file_path2, 'rb') as f2:\n        content1 = f1.read().decode('utf-8')\n        content2 = f2.read().decode('utf-8')\n\n    # Use difflib to compute and return the differences\n    diff = difflib.Differ()\n    diffs = diff.compare(content1.splitlines(keepends=True), content2.splitlines(keepends=True))\n    diffs = [line for line in diffs if line.startswith('- ') or line.startswith('+ ')]\n\n    # Return the differences as a string\n    return ''.join(diffs)"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if data.empty:\n        stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n        fig, ax = plt.subplots()\n        ax.pie([], labels=[])\n        return stats, ax\n\n    stats = {\n        'sum': data[column].sum(),\n        'mean': data[column].mean(),\n        'min': data[column].min(),\n        'max': data[column].max()\n    }\n\n    fig, ax = plt.subplots()\n    ax.pie(data[column].value_counts(), labels=data[column].value_counts().index)\n    ax.set_title(f'{column} distribution')\n\n    return stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if data.empty:\n        stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n        fig, ax = plt.subplots()\n        ax.pie([], labels=[])\n        return stats, ax\n\n    stats = {\n        'sum': data[column].sum(),\n        'mean': data[column].mean(),\n        'min': data[column].min(),\n        'max': data[column].max()\n    }\n\n    fig, ax = plt.subplots()\n    ax.pie(data[column].value_counts(), labels=data[column].value_counts().index)\n    ax.set_title(f'{column} distribution')\n\n    return stats, ax"}
{"task_id": "BigCodeBench/512", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Data should be a pandas DataFrame\")\n    if not isinstance(column, str):\n        raise TypeError(\"Column should be a string\")\n    if column not in data.columns:\n        raise ValueError(\"Column not found in data\")\n    if (data[column] < 0).any():\n        raise ValueError(\"Quantity sold or total sales cannot be negative\")\n\n    summary_stats = {\n        'sum': data[column].sum(),\n        'mean': data[column].mean(),\n        'min': data[column].min(),\n        'max': data[column].max()\n    }\n\n    fig, ax = plt.subplots()\n    ax.bar(data['Product'], data[column])\n    ax.set_xlabel('Product')\n    ax.set_ylabel(column)\n    ax.set_title(f'Bar Chart of {column}')\n\n    return (summary_stats, ax)", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Data should be a pandas DataFrame\")\n    if not isinstance(column, str):\n        raise TypeError(\"Column should be a string\")\n    if column not in data.columns:\n        raise ValueError(\"Column not found in data\")\n    if (data[column] < 0).any():\n        raise ValueError(\"Quantity sold or total sales cannot be negative\")\n\n    summary_stats = {\n        'sum': data[column].sum(),\n        'mean': data[column].mean(),\n        'min': data[column].min(),\n        'max': data[column].max()\n    }\n\n    fig, ax = plt.subplots()\n    ax.bar(data['Product'], data[column])\n    ax.set_xlabel('Product')\n    ax.set_ylabel(column)\n    ax.set_title(f'Bar Chart of {column}')\n\n    return (summary_stats, ax)"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not data:\n        raise ValueError(\"Data list is empty\")\n\n    df = pd.DataFrame(data)\n\n    if column not in df.columns:\n        raise KeyError(f\"Specified column '{column}' is not valid\")\n\n    if df[column].min() < 0:\n        raise ValueError(f\"Negative value found in column '{column}'\")\n\n    summary_stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max(),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column])\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    ax.set_title(f'Line Chart of {column}')\n\n    return (summary_stats, ax)", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if not data:\n        raise ValueError(\"Data list is empty\")\n\n    df = pd.DataFrame(data)\n\n    if column not in df.columns:\n        raise KeyError(f\"Specified column '{column}' is not valid\")\n\n    if df[column].min() < 0:\n        raise ValueError(f\"Negative value found in column '{column}'\")\n\n    summary_stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max(),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column])\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    ax.set_title(f'Line Chart of {column}')\n\n    return (summary_stats, ax)"}
{"task_id": "BigCodeBench/514", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(array):\n    COLUMNS = ['A', 'B', 'C', 'D', 'E']\n    df = pd.DataFrame(array, columns=COLUMNS)\n    sums = df.sum()\n    fig, ax = plt.subplots()\n    ax.bar(COLUMNS, sums)\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Sum')\n    ax.set_title('Sum of each column')\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(array):\n    COLUMNS = ['A', 'B', 'C', 'D', 'E']\n    df = pd.DataFrame(array, columns=COLUMNS)\n    sums = df.sum()\n    fig, ax = plt.subplots()\n    ax.bar(COLUMNS, sums)\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Sum')\n    ax.set_title('Sum of each column')\n    return df, ax"}
{"task_id": "BigCodeBench/515", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(array):\n    if not array:\n        raise ValueError(\"Input array is empty\")\n    if not all(len(sublist) == 5 for sublist in array):\n        raise ValueError(\"Input array contains sublists of varying lengths\")\n\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'E'])\n\n    correlation_matrix = df.corr()\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.title(\"Correlation Matrix Heatmap\")\n    plt.show()\n\n    return df, correlation_matrix", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(array):\n    if not array:\n        raise ValueError(\"Input array is empty\")\n    if not all(len(sublist) == 5 for sublist in array):\n        raise ValueError(\"Input array contains sublists of varying lengths\")\n\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'E'])\n\n    correlation_matrix = df.corr()\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.title(\"Correlation Matrix Heatmap\")\n    plt.show()\n\n    return df, correlation_matrix"}
{"task_id": "BigCodeBench/516", "solution": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\ndef task_func(\n    array: list, random_seed: int = 0\n) -> (pd.DataFrame, sm.regression.linear_model.RegressionResultsWrapper):\n    \"\"\"\n    Generates a Pandas DataFrame from a 2D list and performs a multiple linear regression.\n    The function first validates the input list, creates a DataFrame, separates independent\n    and dependent variables, adds a constant to the model, and fits a linear regression using\n    statsmodels. The function also sets a random seed for reproducibility in numpy for\n    statsmodels.\n\n    Args:\n        array (list): A 2D list where each inner list has 5 elements. The first 4 elements\n            are independent variables and the last element is the dependent variable.\n        random_seed (int, optional): A seed for reproducibility in numpy for statsmodels.\n            Defaults to 0.\n\n    Returns:\n        df (pd.DataFrame): DataFrame with columns 'A', 'B', 'C', 'D', 'Response'.\n        results (statsmodels.RegressionResults): Results of the linear regression.\n    \"\"\"\n    # Validate the input list\n    if not all(len(row) == 5 for row in array):\n        raise ValueError(\"Each inner list should have 5 elements\")\n\n    # Create a DataFrame\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'Response'])\n\n    # Separate independent and dependent variables\n    X = df[['A', 'B', 'C', 'D']]\n    y = df['Response']\n\n    # Add a constant to the model\n    X = sm.add_constant(X)\n\n    # Set a random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Fit a linear regression using statsmodels\n    model = sm.OLS(y, X)\n    results = model.fit()\n\n    return df, results", "raw_solution": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndef task_func(\n    array: list, random_seed: int = 0\n) -> (pd.DataFrame, sm.regression.linear_model.RegressionResultsWrapper):\n    \"\"\"\n    Generates a Pandas DataFrame from a 2D list and performs a multiple linear regression.\n    The function first validates the input list, creates a DataFrame, separates independent\n    and dependent variables, adds a constant to the model, and fits a linear regression using\n    statsmodels. The function also sets a random seed for reproducibility in numpy for\n    statsmodels.\n\n    Args:\n        array (list): A 2D list where each inner list has 5 elements. The first 4 elements\n            are independent variables and the last element is the dependent variable.\n        random_seed (int, optional): A seed for reproducibility in numpy for statsmodels.\n            Defaults to 0.\n\n    Returns:\n        df (pd.DataFrame): DataFrame with columns 'A', 'B', 'C', 'D', 'Response'.\n        results (statsmodels.RegressionResults): Results of the linear regression.\n    \"\"\"\n    # Validate the input list\n    if not all(len(row) == 5 for row in array):\n        raise ValueError(\"Each inner list should have 5 elements\")\n\n    # Create a DataFrame\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'Response'])\n\n    # Separate independent and dependent variables\n    X = df[['A', 'B', 'C', 'D']]\n    y = df['Response']\n\n    # Add a constant to the model\n    X = sm.add_constant(X)\n\n    # Set a random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Fit a linear regression using statsmodels\n    model = sm.OLS(y, X)\n    results = model.fit()\n\n    return df, results"}
{"task_id": "BigCodeBench/517", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\ndef task_func(array: list, random_seed: int = 42) -> (pd.DataFrame, np.ndarray):\n    \"\"\"\n    Converts a 2D list into a pandas DataFrame and applies PCA for dimensionality reduction.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Convert the 2D list into a DataFrame\n    df = pd.DataFrame(array)\n\n    # Apply PCA to reduce the dataset to its two main components\n    pca = PCA(n_components=2)\n    pca_data = pca.fit_transform(df)\n\n    return df, pca_data", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\ndef task_func(array: list, random_seed: int = 42) -> (pd.DataFrame, np.ndarray):\n    \"\"\"\n    Converts a 2D list into a pandas DataFrame and applies PCA for dimensionality reduction.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Convert the 2D list into a DataFrame\n    df = pd.DataFrame(array)\n\n    # Apply PCA to reduce the dataset to its two main components\n    pca = PCA(n_components=2)\n    pca_data = pca.fit_transform(df)\n\n    return df, pca_data"}
{"task_id": "BigCodeBench/518", "solution": "import pandas as pd\nfrom scipy.spatial.distance import pdist, squareform\ndef task_func(array):\n    # Convert 2D list to DataFrame\n    df = pd.DataFrame(array, columns=[chr(65 + i) for i in range(len(array[0]))])\n\n    # Calculate Euclidean distance matrix\n    distance_matrix = pd.DataFrame(squareform(pdist(df)), index=df.index, columns=df.index)\n\n    return df, distance_matrix", "raw_solution": "import pandas as pd\nfrom scipy.spatial.distance import pdist, squareform\n\ndef task_func(array):\n    # Convert 2D list to DataFrame\n    df = pd.DataFrame(array, columns=[chr(65 + i) for i in range(len(array[0]))])\n\n    # Calculate Euclidean distance matrix\n    distance_matrix = pd.DataFrame(squareform(pdist(df)), index=df.index, columns=df.index)\n\n    return df, distance_matrix"}
{"task_id": "BigCodeBench/519", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Fill NA/NaN values with 0\n    df.fillna(0, inplace=True)\n\n    # Generate a line chart of sales\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['Time'], df['Sales Quantity'])\n    plt.title('Fruit Sales over Time')\n    plt.xlabel('Time')\n    plt.ylabel('Sales Quantity')\n\n    return plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Fill NA/NaN values with 0\n    df.fillna(0, inplace=True)\n\n    # Generate a line chart of sales\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['Time'], df['Sales Quantity'])\n    plt.title('Fruit Sales over Time')\n    plt.xlabel('Time')\n    plt.ylabel('Sales Quantity')\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/520", "solution": "import collections\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not all(isinstance(i, dict) for i in data):\n        raise TypeError(\"All elements in data must be dictionaries\")\n    if not all(i.keys() == data[0].keys() for i in data):\n        raise ValueError(\"All dictionaries must have the same keys\")\n    if not all(isinstance(v, int) and v >= 0 for v in collections.ChainMap(*data).values()):\n        raise ValueError(\"Sales quantity must not be negative\")\n\n    total_sales = collections.defaultdict(int)\n    for d in data:\n        for fruit, sales in d.items():\n            total_sales[fruit] += sales\n\n    colors = ['red', 'yellow', 'green', 'blue', 'purple']\n    ax = None\n    if total_sales:\n        fig, ax = plt.subplots()\n        ax.bar(total_sales.keys(), total_sales.values(), color=colors[:len(total_sales)])\n        ax.set_xlabel('Fruit')\n        ax.set_ylabel('Total Sales')\n\n    return total_sales, ax", "raw_solution": "import collections\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not all(isinstance(i, dict) for i in data):\n        raise TypeError(\"All elements in data must be dictionaries\")\n    if not all(i.keys() == data[0].keys() for i in data):\n        raise ValueError(\"All dictionaries must have the same keys\")\n    if not all(isinstance(v, int) and v >= 0 for v in collections.ChainMap(*data).values()):\n        raise ValueError(\"Sales quantity must not be negative\")\n\n    total_sales = collections.defaultdict(int)\n    for d in data:\n        for fruit, sales in d.items():\n            total_sales[fruit] += sales\n\n    colors = ['red', 'yellow', 'green', 'blue', 'purple']\n    ax = None\n    if total_sales:\n        fig, ax = plt.subplots()\n        ax.bar(total_sales.keys(), total_sales.values(), color=colors[:len(total_sales)])\n        ax.set_xlabel('Fruit')\n        ax.set_ylabel('Total Sales')\n\n    return total_sales, ax"}
{"task_id": "BigCodeBench/521", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_list):\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(data_list)\n    df = df.set_index('Test')\n\n    # Plot the data\n    ax = df.plot(kind='line', legend=True)\n\n    # Set the x-axis label and y-axis label\n    ax.set_xlabel('Test Number')\n    ax.set_ylabel('Score')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_list):\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(data_list)\n    df = df.set_index('Test')\n\n    # Plot the data\n    ax = df.plot(kind='line', legend=True)\n\n    # Set the x-axis label and y-axis label\n    ax.set_xlabel('Test Number')\n    ax.set_ylabel('Score')\n\n    return ax"}
{"task_id": "BigCodeBench/522", "solution": "import collections\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        return None\n\n    # Combine dictionaries with differing keys\n    combined_data = collections.defaultdict(list)\n    for d in data:\n        for k, v in d.items():\n            combined_data[k].append(v)\n\n    # Calculate average score for each student\n    averages = {}\n    for k, v in combined_data.items():\n        scores = [s for s in v if s is not None]\n        if not scores:\n            averages[k] = 0\n        else:\n            averages[k] = sum(scores) / len(scores)\n\n    # Check for negative scores\n    for k, v in averages.items():\n        if v < 0:\n            raise ValueError(\"Negative scores are not allowed\")\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(averages.keys(), averages.values(), color=['red', 'yellow', 'green', 'blue', 'purple'])\n    ax.set_xlabel('Student')\n    ax.set_ylabel('Average Score')\n    ax.set_title('Average Student Scores')\n\n    return ax", "raw_solution": "import collections\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not data:\n        return None\n\n    # Combine dictionaries with differing keys\n    combined_data = collections.defaultdict(list)\n    for d in data:\n        for k, v in d.items():\n            combined_data[k].append(v)\n\n    # Calculate average score for each student\n    averages = {}\n    for k, v in combined_data.items():\n        scores = [s for s in v if s is not None]\n        if not scores:\n            averages[k] = 0\n        else:\n            averages[k] = sum(scores) / len(scores)\n\n    # Check for negative scores\n    for k, v in averages.items():\n        if v < 0:\n            raise ValueError(\"Negative scores are not allowed\")\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(averages.keys(), averages.values(), color=['red', 'yellow', 'green', 'blue', 'purple'])\n    ax.set_xlabel('Student')\n    ax.set_ylabel('Average Score')\n    ax.set_title('Average Student Scores')\n\n    return ax"}
{"task_id": "BigCodeBench/523", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        return None\n\n    # Combine the dictionaries with the same keys\n    combined_dict = {}\n    for d in data:\n        for key, value in d.items():\n            if key in combined_dict:\n                combined_dict[key].append(value)\n            else:\n                combined_dict[key] = [value]\n\n    # Convert the combined dictionary to a DataFrame\n    df = pd.DataFrame(combined_dict)\n\n    # Create a line plot of the data\n    ax = df.plot(kind='line', x='Time', y='Data Points')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Data Points')\n    ax.set_title('Data over Time')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not data:\n        return None\n\n    # Combine the dictionaries with the same keys\n    combined_dict = {}\n    for d in data:\n        for key, value in d.items():\n            if key in combined_dict:\n                combined_dict[key].append(value)\n            else:\n                combined_dict[key] = [value]\n\n    # Convert the combined dictionary to a DataFrame\n    df = pd.DataFrame(combined_dict)\n\n    # Create a line plot of the data\n    ax = df.plot(kind='line', x='Time', y='Data Points')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Data Points')\n    ax.set_title('Data over Time')\n\n    return ax"}
{"task_id": "BigCodeBench/524", "solution": "from collections import defaultdict\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not isinstance(data, list):\n        raise TypeError(\"Input data should be a list of dictionaries\")\n    if not data:\n        raise ValueError(\"Input data should not be empty\")\n\n    stats = defaultdict(list)\n    axes = []\n\n    for d in data:\n        if not isinstance(d, dict):\n            raise TypeError(\"All elements in the list should be dictionaries\")\n        for k, v in d.items():\n            if not isinstance(v, (int, float)):\n                raise TypeError(\"All values in the dictionaries should be numeric\")\n            stats[k].append(v)\n\n    for k, v in stats.items():\n        mean = np.mean(v)\n        std_dev = np.std(v)\n        stats[k] = (mean, std_dev)\n\n        fig, ax = plt.subplots()\n        ax.bar(['Mean', 'Std Dev'], [mean, std_dev], color=['blue', 'orange'])\n        ax.set_title(f'Statistics for key: {k}')\n        ax.set_ylabel('Value')\n        ax.set_xticks(['Mean', 'Std Dev'])\n        axes.append(ax)\n\n    return stats, axes", "raw_solution": "from collections import defaultdict\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not isinstance(data, list):\n        raise TypeError(\"Input data should be a list of dictionaries\")\n    if not data:\n        raise ValueError(\"Input data should not be empty\")\n\n    stats = defaultdict(list)\n    axes = []\n\n    for d in data:\n        if not isinstance(d, dict):\n            raise TypeError(\"All elements in the list should be dictionaries\")\n        for k, v in d.items():\n            if not isinstance(v, (int, float)):\n                raise TypeError(\"All values in the dictionaries should be numeric\")\n            stats[k].append(v)\n\n    for k, v in stats.items():\n        mean = np.mean(v)\n        std_dev = np.std(v)\n        stats[k] = (mean, std_dev)\n\n        fig, ax = plt.subplots()\n        ax.bar(['Mean', 'Std Dev'], [mean, std_dev], color=['blue', 'orange'])\n        ax.set_title(f'Statistics for key: {k}')\n        ax.set_ylabel('Value')\n        ax.set_xticks(['Mean', 'Std Dev'])\n        axes.append(ax)\n\n    return stats, axes"}
{"task_id": "BigCodeBench/525", "solution": "import json\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(input_file):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    result = defaultdict(dict)\n    plots = []\n\n    for key in data[0].keys():\n        values = [d[key] for d in data if key in d]\n        mean = np.mean(values)\n        median = np.median(values)\n        result[key]['mean'] = mean\n        result[key]['median'] = median\n\n        fig, ax = plt.subplots()\n        ax.bar(['Mean', 'Median'], [mean, median], color=['b', 'r'])\n        ax.set_title(f'Mean and Median of {key}')\n        plots.append(ax)\n\n    return result, plots", "raw_solution": "import json\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(input_file):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    result = defaultdict(dict)\n    plots = []\n\n    for key in data[0].keys():\n        values = [d[key] for d in data if key in d]\n        mean = np.mean(values)\n        median = np.median(values)\n        result[key]['mean'] = mean\n        result[key]['median'] = median\n\n        fig, ax = plt.subplots()\n        ax.bar(['Mean', 'Median'], [mean, median], color=['b', 'r'])\n        ax.set_title(f'Mean and Median of {key}')\n        plots.append(ax)\n\n    return result, plots"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom typing import List, Dict, Any\ndef task_func(input_file: str = \"data.json\") -> pd.DataFrame:\n    \"\"\"\n    Reads a list of dictionaries from a JSON file, calculates the mean and median for each key (ignoring non-numeric or missing values),\n    and converts the results into a Pandas DataFrame.\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n\n    # Initialize defaultdict to store mean and median values\n    stats = defaultdict(lambda: {'mean': None, 'median': None})\n\n    # Iterate over each dictionary in the list\n    for item in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in item.items():\n            # Ignore non-numeric or missing values\n            if isinstance(value, (int, float)) and not np.isnan(value):\n                # Update mean and median values\n                stats[key]['mean'] = stats[key]['mean'] if stats[key]['mean'] is not None else value\n                stats[key]['median'] = stats[key]['median'] if stats[key]['median'] is not None else value\n                stats[key]['mean'] += value\n                stats[key]['median'] += value\n\n    # Calculate mean and median for each key\n    for key, value in stats.items():\n        stats[key]['mean'] /= len(data)\n        stats[key]['median'] = np.median(data, key)\n\n    # Convert the results into a Pandas DataFrame\n    df = pd.DataFrame.from_dict(stats, orient='index')\n    df.sort_index(inplace=True)\n\n    return df", "raw_solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom typing import List, Dict, Any\n\ndef task_func(input_file: str = \"data.json\") -> pd.DataFrame:\n    \"\"\"\n    Reads a list of dictionaries from a JSON file, calculates the mean and median for each key (ignoring non-numeric or missing values),\n    and converts the results into a Pandas DataFrame.\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n\n    # Initialize defaultdict to store mean and median values\n    stats = defaultdict(lambda: {'mean': None, 'median': None})\n\n    # Iterate over each dictionary in the list\n    for item in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in item.items():\n            # Ignore non-numeric or missing values\n            if isinstance(value, (int, float)) and not np.isnan(value):\n                # Update mean and median values\n                stats[key]['mean'] = stats[key]['mean'] if stats[key]['mean'] is not None else value\n                stats[key]['median'] = stats[key]['median'] if stats[key]['median'] is not None else value\n                stats[key]['mean'] += value\n                stats[key]['median'] += value\n\n    # Calculate mean and median for each key\n    for key, value in stats.items():\n        stats[key]['mean'] /= len(data)\n        stats[key]['median'] = np.median(data, key)\n\n    # Convert the results into a Pandas DataFrame\n    df = pd.DataFrame.from_dict(stats, orient='index')\n    df.sort_index(inplace=True)\n\n    return df"}
{"task_id": "BigCodeBench/527", "solution": "import json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file: str) -> dict:\n    \"\"\"\n    Reads a list of dictionaries from a JSON file, calculates the mean and median for each key,\n    converts the input data into a pandas DataFrame, and returns the results and a box plot.\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Convert data to pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Calculate mean and median for each key\n    results = {}\n    for key in df.columns:\n        mean = df[key].mean()\n        median = df[key].median()\n        results[key] = {'mean': mean, 'median': median}\n\n    # Create box plot\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=df)\n    plt.title('Box plot of aggregated values for each key')\n    ax = plt.gca()\n\n    return results, ax", "raw_solution": "import json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file: str) -> dict:\n    \"\"\"\n    Reads a list of dictionaries from a JSON file, calculates the mean and median for each key,\n    converts the input data into a pandas DataFrame, and returns the results and a box plot.\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Convert data to pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Calculate mean and median for each key\n    results = {}\n    for key in df.columns:\n        mean = df[key].mean()\n        median = df[key].median()\n        results[key] = {'mean': mean, 'median': median}\n\n    # Create box plot\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=df)\n    plt.title('Box plot of aggregated values for each key')\n    ax = plt.gca()\n\n    return results, ax"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File must be a CSV file\")\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    duplicates = [item for item in data if data.count(item) > 1]\n    duplicates_count = Counter(tuple(sorted(item)) for item in duplicates)\n\n    # Convert to pandas DataFrame\n    df = pd.DataFrame(list(duplicates_count.items()), columns=['Duplicates', 'Count'])\n    df['Duplicates'] = df['Duplicates'].apply(lambda x: list(eval(x)))\n\n    # Plotting\n    fig, ax = plt.subplots()\n    df['Count'].plot(kind='bar', ax=ax)\n    ax.set_title('Count of Duplicate Rows')\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Count')\n\n    return duplicates_count, ax", "raw_solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File must be a CSV file\")\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    duplicates = [item for item in data if data.count(item) > 1]\n    duplicates_count = Counter(tuple(sorted(item)) for item in duplicates)\n\n    # Convert to pandas DataFrame\n    df = pd.DataFrame(list(duplicates_count.items()), columns=['Duplicates', 'Count'])\n    df['Duplicates'] = df['Duplicates'].apply(lambda x: list(eval(x)))\n\n    # Plotting\n    fig, ax = plt.subplots()\n    df['Count'].plot(kind='bar', ax=ax)\n    ax.set_title('Count of Duplicate Rows')\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Count')\n\n    return duplicates_count, ax"}
{"task_id": "BigCodeBench/529", "solution": "from collections import Counter\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(num_rolls, num_dice, plot_path=None, random_seed=0):\n    random.seed(random_seed)\n    dice_rolls = [sum(random.randint(1, 6) for _ in range(num_dice)) for _ in range(num_rolls)]\n    counter = Counter(dice_rolls)\n\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values())\n    ax.set_xlabel('Sum of Dice Roll')\n    ax.set_ylabel('Count')\n    ax.set_title('Distribution of Dice Roll Sums')\n\n    if plot_path is not None:\n        plt.savefig(plot_path)\n\n    return counter, ax", "raw_solution": "from collections import Counter\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(num_rolls, num_dice, plot_path=None, random_seed=0):\n    random.seed(random_seed)\n    dice_rolls = [sum(random.randint(1, 6) for _ in range(num_dice)) for _ in range(num_rolls)]\n    counter = Counter(dice_rolls)\n\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values())\n    ax.set_xlabel('Sum of Dice Roll')\n    ax.set_ylabel('Count')\n    ax.set_title('Distribution of Dice Roll Sums')\n\n    if plot_path is not None:\n        plt.savefig(plot_path)\n\n    return counter, ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age cannot be negative\")\n\n    df['age'] = df['age'].apply(np.floor)\n    df['age'] = df['age'].astype(int)\n\n    duplicate_names = df[df.duplicated(subset='name', keep=False)]\n    if duplicate_names.empty:\n        return Counter(), None\n\n    age_distribution = duplicate_names['age'].value_counts()\n    min_age, max_age = duplicate_names['age'].min(), duplicate_names['age'].max()\n\n    plt.figure(figsize=(10, 6))\n    sns.histplot(duplicate_names['age'], bins=range(min_age, max_age + 2), color='skyblue')\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Age Distribution for Duplicate Names')\n    plt.grid(True)\n\n    return age_distribution, plt.gca()", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age cannot be negative\")\n\n    df['age'] = df['age'].apply(np.floor)\n    df['age'] = df['age'].astype(int)\n\n    duplicate_names = df[df.duplicated(subset='name', keep=False)]\n    if duplicate_names.empty:\n        return Counter(), None\n\n    age_distribution = duplicate_names['age'].value_counts()\n    min_age, max_age = duplicate_names['age'].min(), duplicate_names['age'].max()\n\n    plt.figure(figsize=(10, 6))\n    sns.histplot(duplicate_names['age'], bins=range(min_age, max_age + 2), color='skyblue')\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Age Distribution for Duplicate Names')\n    plt.grid(True)\n\n    return age_distribution, plt.gca()"}
{"task_id": "BigCodeBench/531", "solution": "from collections import Counter\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df, n_clusters=3, random_state=None, n_init=10):\n    \"\"\"\n    Identify duplicate points in a DataFrame, perform KMeans clustering on the unique points, and record the clusters.\n    \"\"\"\n    # Identify duplicate points\n    duplicate_points = df[df.duplicated()]\n    duplicate_counts = Counter(map(tuple, duplicate_points.values))\n\n    # Perform KMeans clustering on unique points\n    unique_points = df.drop_duplicates()\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    kmeans.fit(unique_points)\n\n    # Record the clusters\n    unique_points['cluster'] = kmeans.labels_\n\n    # Plot the clustered data\n    plt.scatter(unique_points['x'], unique_points['y'], c=unique_points['cluster'])\n    plt.title('KMeans Clustering')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.show()\n\n    return duplicate_counts, unique_points", "raw_solution": "from collections import Counter\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df, n_clusters=3, random_state=None, n_init=10):\n    \"\"\"\n    Identify duplicate points in a DataFrame, perform KMeans clustering on the unique points, and record the clusters.\n    \"\"\"\n    # Identify duplicate points\n    duplicate_points = df[df.duplicated()]\n    duplicate_counts = Counter(map(tuple, duplicate_points.values))\n\n    # Perform KMeans clustering on unique points\n    unique_points = df.drop_duplicates()\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    kmeans.fit(unique_points)\n\n    # Record the clusters\n    unique_points['cluster'] = kmeans.labels_\n\n    # Plot the clustered data\n    plt.scatter(unique_points['x'], unique_points['y'], c=unique_points['cluster'])\n    plt.title('KMeans Clustering')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.show()\n\n    return duplicate_counts, unique_points"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    # Identify and count duplicate values in the 'value' column\n    counts = Counter(df['value'])\n\n    # Plot a histogram for all values in the 'value' column\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, color='green', alpha=0.6, edgecolor='black')\n\n    # Overlay a normal distribution curve on the histogram\n    mu, sigma = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set plot title and labels\n    ax.set_title(\"Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return counts, ax", "raw_solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=4):\n    # Identify and count duplicate values in the 'value' column\n    counts = Counter(df['value'])\n\n    # Plot a histogram for all values in the 'value' column\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, color='green', alpha=0.6, edgecolor='black')\n\n    # Overlay a normal distribution curve on the histogram\n    mu, sigma = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set plot title and labels\n    ax.set_title(\"Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return counts, ax"}
{"task_id": "BigCodeBench/533", "solution": "import numpy as np\nimport secrets\nimport hashlib\nimport base64\ndef task_func(num, from_base, to_base, alphabet):\n    if from_base < 2 or to_base < 2:\n        raise ValueError(\"Invalid base for conversion\")\n\n    try:\n        decimal_num = int(num, from_base)\n    except ValueError:\n        raise ValueError(\"Invalid number format for conversion\")\n\n    salt = secrets.token_hex(16)\n    hashed_num = hashlib.sha256((str(decimal_num) + salt).encode()).digest()\n    encoded_num = base64.b64encode(hashed_num).decode('utf-8')\n\n    return (encoded_num, salt)\nalphabet = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789+/'", "raw_solution": "import numpy as np\nimport secrets\nimport hashlib\nimport base64\n\ndef task_func(num, from_base, to_base, alphabet):\n    if from_base < 2 or to_base < 2:\n        raise ValueError(\"Invalid base for conversion\")\n\n    try:\n        decimal_num = int(num, from_base)\n    except ValueError:\n        raise ValueError(\"Invalid number format for conversion\")\n\n    salt = secrets.token_hex(16)\n    hashed_num = hashlib.sha256((str(decimal_num) + salt).encode()).digest()\n    encoded_num = base64.b64encode(hashed_num).decode('utf-8')\n\n    return (encoded_num, salt)\n\n# Testing\nalphabet = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789+/'\nresult1, salt1 = task_func('FF', 16, 8, alphabet)\nresult2, salt2 = task_func('FF', 16, 8, alphabet)\nassert result1 != result2\nassert salt1 != salt2"}
{"task_id": "BigCodeBench/534", "solution": "from cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.backends import default_backend\nimport base64\nimport binascii\ndef task_func(num, from_base, to_base, private_key, alphabet):\n    # Convert number from one base to another\n    num = int(num, from_base)\n    num = int(str(num), to_base)\n\n    # Sign the number with RSA private key\n    private_key = serialization.load_pem_private_key(\n        private_key,\n        password=None,\n        backend=default_backend()\n    )\n    signature = private_key.sign(\n        int(num).to_bytes(len(num), byteorder='big'),\n        padding.PSS(\n            mgf=padding.MGF1(hashes.SHA256()),\n            salt_length=padding.PSS.MAX_LENGTH\n        ),\n        hashes.SHA256()\n    )\n\n    # Encode the signed number in base64 with a custom alphabet\n    encoded = base64.b64encode(signature).decode('utf-8')\n    table = str.maketrans('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/', alphabet)\n    encoded = encoded.translate(table)\n\n    return encoded", "raw_solution": "from cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.backends import default_backend\nimport base64\nimport binascii\n\ndef task_func(num, from_base, to_base, private_key, alphabet):\n    # Convert number from one base to another\n    num = int(num, from_base)\n    num = int(str(num), to_base)\n\n    # Sign the number with RSA private key\n    private_key = serialization.load_pem_private_key(\n        private_key,\n        password=None,\n        backend=default_backend()\n    )\n    signature = private_key.sign(\n        int(num).to_bytes(len(num), byteorder='big'),\n        padding.PSS(\n            mgf=padding.MGF1(hashes.SHA256()),\n            salt_length=padding.PSS.MAX_LENGTH\n        ),\n        hashes.SHA256()\n    )\n\n    # Encode the signed number in base64 with a custom alphabet\n    encoded = base64.b64encode(signature).decode('utf-8')\n    table = str.maketrans('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/', alphabet)\n    encoded = encoded.translate(table)\n\n    return encoded"}
{"task_id": "BigCodeBench/535", "solution": "import sqlite3\nimport numpy as np\nfrom random import choice, seed\ndef task_func(db_path, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries cannot be negative\")\n\n    NAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = list(range(18, 65))\n    HEIGHTS = list(range(150, 200))\n\n    seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    c.execute(f'''CREATE TABLE IF NOT EXISTS {table_name}\n                   (name TEXT, age INTEGER, height INTEGER)''')\n\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = choice(AGES)\n        height = choice(HEIGHTS)\n        c.execute(f'''INSERT INTO {table_name} VALUES (?, ?, ?)''', (name, age, height))\n\n    conn.commit()\n    conn.close()\n\n    return num_entries", "raw_solution": "import sqlite3\nimport numpy as np\nfrom random import choice, seed\n\ndef task_func(db_path, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries cannot be negative\")\n\n    NAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = list(range(18, 65))\n    HEIGHTS = list(range(150, 200))\n\n    seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    c.execute(f'''CREATE TABLE IF NOT EXISTS {table_name}\n                   (name TEXT, age INTEGER, height INTEGER)''')\n\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = choice(AGES)\n        height = choice(HEIGHTS)\n        c.execute(f'''INSERT INTO {table_name} VALUES (?, ?, ?)''', (name, age, height))\n\n    conn.commit()\n    conn.close()\n\n    return num_entries"}
{"task_id": "BigCodeBench/536", "solution": "import sqlite3\nimport pandas as pd\nimport os\ndef task_func(db_name, table_name, csv_path=\"data.csv\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n\n    # Read the table from the database into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n\n    # Export the DataFrame to a CSV file\n    df.to_csv(csv_path, index=False)\n\n    # Get the absolute path of the exported CSV file\n    csv_abs_path = os.path.abspath(csv_path)\n\n    return csv_abs_path", "raw_solution": "import sqlite3\nimport pandas as pd\nimport os\n\ndef task_func(db_name, table_name, csv_path=\"data.csv\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n\n    # Read the table from the database into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n\n    # Export the DataFrame to a CSV file\n    df.to_csv(csv_path, index=False)\n\n    # Get the absolute path of the exported CSV file\n    csv_abs_path = os.path.abspath(csv_path)\n\n    return csv_abs_path"}
{"task_id": "BigCodeBench/537", "solution": "import sqlite3\nimport pandas as pd\nimport seaborn as sns\ndef task_func(db_name=\"test.db\", table_name=\"People\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n\n    # Execute a query to fetch data from the table\n    query = f\"SELECT * FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n\n    # Check for negative age values\n    if (df['age'] < 0).any():\n        raise ValueError(\"The data contains negative age values.\")\n\n    # Draw the age distribution\n    ax = sns.histplot(data=df, x='age', bins=30, kde=True)\n\n    return ax", "raw_solution": "import sqlite3\nimport pandas as pd\nimport seaborn as sns\n\ndef task_func(db_name=\"test.db\", table_name=\"People\"):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n\n    # Execute a query to fetch data from the table\n    query = f\"SELECT * FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n\n    # Check for negative age values\n    if (df['age'] < 0).any():\n        raise ValueError(\"The data contains negative age values.\")\n\n    # Draw the age distribution\n    ax = sns.histplot(data=df, x='age', bins=30, kde=True)\n\n    return ax"}
{"task_id": "BigCodeBench/538", "solution": "import sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(db_name, table_name):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n\n    # Query the table\n    query = f\"SELECT * FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n\n    # Close the connection\n    conn.close()\n\n    # Exclude 'id' column\n    df = df.drop(columns=['id'])\n\n    # Get the column names\n    cols = df.columns\n\n    # Check if there are at least two numerical columns\n    if len(df.select_dtypes(include=['int64', 'float64']).columns) < 2:\n        raise ValueError(\"The table has less than two numerical columns.\")\n\n    # Plot the relationship between the first two numerical columns\n    plt.scatter(df[cols[0]], df[cols[1]])\n    plt.xlabel(cols[0])\n    plt.ylabel(cols[1])\n    plt.title(f\"Relationship between {cols[0]} and {cols[1]}\")\n    plt.show()\n\n    return plt.gca()", "raw_solution": "import sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(db_name, table_name):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n\n    # Query the table\n    query = f\"SELECT * FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n\n    # Close the connection\n    conn.close()\n\n    # Exclude 'id' column\n    df = df.drop(columns=['id'])\n\n    # Get the column names\n    cols = df.columns\n\n    # Check if there are at least two numerical columns\n    if len(df.select_dtypes(include=['int64', 'float64']).columns) < 2:\n        raise ValueError(\"The table has less than two numerical columns.\")\n\n    # Plot the relationship between the first two numerical columns\n    plt.scatter(df[cols[0]], df[cols[1]])\n    plt.xlabel(cols[0])\n    plt.ylabel(cols[1])\n    plt.title(f\"Relationship between {cols[0]} and {cols[1]}\")\n    plt.show()\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/539", "solution": "import sqlite3\nfrom random import choice, seed, randint\nimport os\ndef task_func(db_name, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries cannot be negative\")\n\n    NAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = range(18, 65)\n    HEIGHTS = range(150, 200)\n\n    if random_seed is not None:\n        seed(random_seed)\n\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n\n    cursor.execute(f'''\n        CREATE TABLE {table_name} (\n            name TEXT,\n            age INTEGER,\n            height INTEGER\n        )\n    ''')\n\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = randint(18, 65)\n        height = randint(150, 200)\n        cursor.execute(f'''\n            INSERT INTO {table_name} VALUES (?, ?, ?)\n        ''', (name, age, height))\n\n    conn.commit()\n    conn.close()\n\n    return os.path.abspath(db_name)", "raw_solution": "import sqlite3\nfrom random import choice, seed, randint\nimport os\n\ndef task_func(db_name, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries cannot be negative\")\n\n    NAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = range(18, 65)\n    HEIGHTS = range(150, 200)\n\n    if random_seed is not None:\n        seed(random_seed)\n\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n\n    cursor.execute(f'''\n        CREATE TABLE {table_name} (\n            name TEXT,\n            age INTEGER,\n            height INTEGER\n        )\n    ''')\n\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = randint(18, 65)\n        height = randint(150, 200)\n        cursor.execute(f'''\n            INSERT INTO {table_name} VALUES (?, ?, ?)\n        ''', (name, age, height))\n\n    conn.commit()\n    conn.close()\n\n    return os.path.abspath(db_name)"}
{"task_id": "BigCodeBench/540", "solution": "from collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(list_of_menuitems, title=\"Menu Distribution\", color=\"blue\", width=1.0):\n    # Flatten the list using itertools.chain\n    flat_list = list(itertools.chain(*list_of_menuitems))\n\n    # Count the occurrences of each item\n    item_counts = Counter(flat_list)\n\n    # Sort the items alphabetically\n    sorted_items = sorted(item_counts.keys())\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.bar(sorted_items, [item_counts[item] for item in sorted_items], color=color, width=width)\n\n    # Set labels and title\n    ax.set_xlabel(\"Menu Items\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_title(title)\n\n    return ax", "raw_solution": "from collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef task_func(list_of_menuitems, title=\"Menu Distribution\", color=\"blue\", width=1.0):\n    # Flatten the list using itertools.chain\n    flat_list = list(itertools.chain(*list_of_menuitems))\n\n    # Count the occurrences of each item\n    item_counts = Counter(flat_list)\n\n    # Sort the items alphabetically\n    sorted_items = sorted(item_counts.keys())\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.bar(sorted_items, [item_counts[item] for item in sorted_items], color=color, width=width)\n\n    # Set labels and title\n    ax.set_xlabel(\"Menu Items\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_title(title)\n\n    return ax"}
{"task_id": "BigCodeBench/541", "solution": "import os\nimport sys\nimport importlib\nfrom pkgutil import iter_modules\ndef task_func(package_name):\n    \"\"\"\n    Adds all modules of a specified package to the system path.\n    \"\"\"\n    try:\n        package = importlib.import_module(package_name)\n    except ImportError:\n        raise ImportError(f\"{package_name} is not installed or cannot be found. Please install it using `pip install {package_name}`.\")\n\n    package_path = os.path.dirname(package.__file__)\n    sys.path.append(package_path)\n\n    modules = [name for _, name, _ in iter_modules([package_path])]\n\n    return modules", "raw_solution": "import os\nimport sys\nimport importlib\nfrom pkgutil import iter_modules\n\ndef task_func(package_name):\n    \"\"\"\n    Adds all modules of a specified package to the system path.\n    \"\"\"\n    try:\n        package = importlib.import_module(package_name)\n    except ImportError:\n        raise ImportError(f\"{package_name} is not installed or cannot be found. Please install it using `pip install {package_name}`.\")\n\n    package_path = os.path.dirname(package.__file__)\n    sys.path.append(package_path)\n\n    modules = [name for _, name, _ in iter_modules([package_path])]\n\n    return modules"}
{"task_id": "BigCodeBench/542", "solution": "import hashlib\nimport random\nimport struct\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_keys=KEYS, seed=42):\n    random.seed(seed)\n    hex_key = random.choice(hex_keys)\n    try:\n        float_key = struct.unpack('>f', bytes.fromhex(hex_key))[0]\n    except (ValueError, TypeError):\n        raise ValueError(\"Invalid hexadecimal string\")\n    md5_hash = hashlib.md5(struct.pack('>f', float_key)).hexdigest()\n    return md5_hash", "raw_solution": "import hashlib\nimport random\nimport struct\n\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef task_func(hex_keys=KEYS, seed=42):\n    random.seed(seed)\n    hex_key = random.choice(hex_keys)\n    try:\n        float_key = struct.unpack('>f', bytes.fromhex(hex_key))[0]\n    except (ValueError, TypeError):\n        raise ValueError(\"Invalid hexadecimal string\")\n    md5_hash = hashlib.md5(struct.pack('>f', float_key)).hexdigest()\n    return md5_hash"}
{"task_id": "BigCodeBench/543", "solution": "import base64\nimport os\nimport struct\ndef task_func():\n    # Generate a random float number\n    random_float = os.urandom(8)\n    # Convert the float to a hexadecimal string\n    hex_string = random_float.hex()\n    # Encode the hexadecimal representation in base64\n    base64_string = base64.b64encode(bytes.fromhex(hex_string)).decode()\n    return base64_string", "raw_solution": "import base64\nimport os\nimport struct\n\ndef task_func():\n    # Generate a random float number\n    random_float = os.urandom(8)\n    # Convert the float to a hexadecimal string\n    hex_string = random_float.hex()\n    # Encode the hexadecimal representation in base64\n    base64_string = base64.b64encode(bytes.fromhex(hex_string)).decode()\n    return base64_string"}
{"task_id": "BigCodeBench/544", "solution": "import struct\nimport zlib\nKEY = '470FC614'\ndef task_func(hex_string=KEY):\n    # Convert hex string to float\n    float_number = struct.unpack('f', bytes.fromhex(hex_string))[0]\n\n    # Compress the float number to bytes\n    compressed_float = zlib.compress(struct.pack('f', float_number))\n\n    return compressed_float", "raw_solution": "import struct\nimport zlib\n\n# Constants\nKEY = '470FC614'\n\ndef task_func(hex_string=KEY):\n    # Convert hex string to float\n    float_number = struct.unpack('f', bytes.fromhex(hex_string))[0]\n\n    # Compress the float number to bytes\n    compressed_float = zlib.compress(struct.pack('f', float_number))\n\n    return compressed_float\n\n# Test the function\nassert task_func(KEY) == b'\\x7f\\x85\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00"}
{"task_id": "BigCodeBench/545", "solution": "import codecs\nimport random\nimport struct\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_keys=KEYS):\n    # Generate a random float number\n    float_num = random.uniform(0, 1)\n\n    # Convert the float number to bytes\n    float_bytes = struct.pack('f', float_num)\n\n    # Encode the bytes in utf-8\n    utf8_bytes = codecs.encode(float_bytes, 'utf-8')\n\n    return utf8_bytes", "raw_solution": "import codecs\nimport random\nimport struct\n\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef task_func(hex_keys=KEYS):\n    # Generate a random float number\n    float_num = random.uniform(0, 1)\n\n    # Convert the float number to bytes\n    float_bytes = struct.pack('f', float_num)\n\n    # Encode the bytes in utf-8\n    utf8_bytes = codecs.encode(float_bytes, 'utf-8')\n\n    return utf8_bytes\n\n# Test the function"}
{"task_id": "BigCodeBench/546", "solution": "from collections import OrderedDict\nfrom prettytable import PrettyTable\ndef task_func(my_dict):\n    # Sort the dictionary by keys\n    sorted_dict = OrderedDict(sorted(my_dict.items()))\n\n    # Create a PrettyTable object\n    pt = PrettyTable()\n\n    # Add columns to the table\n    pt.field_names = [\"Key\", \"Value\"]\n\n    # Add rows to the table\n    for key, value in sorted_dict.items():\n        pt.add_row([key, value])\n\n    return pt", "raw_solution": "from collections import OrderedDict\nfrom prettytable import PrettyTable\n\ndef task_func(my_dict):\n    # Sort the dictionary by keys\n    sorted_dict = OrderedDict(sorted(my_dict.items()))\n\n    # Create a PrettyTable object\n    pt = PrettyTable()\n\n    # Add columns to the table\n    pt.field_names = [\"Key\", \"Value\"]\n\n    # Add rows to the table\n    for key, value in sorted_dict.items():\n        pt.add_row([key, value])\n\n    return pt\n\n# Test the function\nassert str(task_func({})).startswith('+')"}
{"task_id": "BigCodeBench/547", "solution": "import hashlib\nimport os\nimport base64\ndef task_func(password: str, salt_length: int = 8) -> str:\n    # Generate a random salt\n    salt = os.urandom(salt_length)\n\n    # Combine the password and salt\n    password_salt = password.encode() + salt\n\n    # Hash the password and salt\n    hashed_password = hashlib.sha256(password_salt).digest()\n\n    # Encode the hashed password in base64\n    encoded_password = base64.b64encode(hashed_password)\n\n    return encoded_password.decode()", "raw_solution": "import hashlib\nimport os\nimport base64\n\ndef task_func(password: str, salt_length: int = 8) -> str:\n    # Generate a random salt\n    salt = os.urandom(salt_length)\n\n    # Combine the password and salt\n    password_salt = password.encode() + salt\n\n    # Hash the password and salt\n    hashed_password = hashlib.sha256(password_salt).digest()\n\n    # Encode the hashed password in base64\n    encoded_password = base64.b64encode(hashed_password)\n\n    return encoded_password.decode()"}
{"task_id": "BigCodeBench/548", "solution": "import random\nimport string\nimport base64\nimport zlib\ndef task_func(string_length=100):\n    # Generate a random string of uppercase letters and digits\n    random_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=string_length))\n\n    # Compress the string with zlib\n    compressed_string = zlib.compress(random_string.encode())\n\n    # Encode the compressed string in base64\n    base64_string = base64.b64encode(compressed_string).decode()\n\n    return base64_string", "raw_solution": "import random\nimport string\nimport base64\nimport zlib\n\ndef task_func(string_length=100):\n    # Generate a random string of uppercase letters and digits\n    random_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=string_length))\n\n    # Compress the string with zlib\n    compressed_string = zlib.compress(random_string.encode())\n\n    # Encode the compressed string in base64\n    base64_string = base64.b64encode(compressed_string).decode()\n\n    return base64_string"}
{"task_id": "BigCodeBench/549", "solution": "import base64\nimport pandas as pd\nfrom io import StringIO\ndef task_func(data_dict):\n    \"\"\"\n    Encodes a dict of list as a Base64 string. The dict is first converted to a Pandas DataFrame.\n    Then convert the data frame to CSV format and encoded to bytes, finally encoded it to a Base64 string.\n    \"\"\"\n    # Convert the dict to a DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Convert the DataFrame to CSV format\n    csv_str = df.to_csv(index=False)\n\n    # Encode the CSV string to bytes\n    csv_bytes = csv_str.encode('utf-8')\n\n    # Encode the bytes to a Base64 string\n    base64_str = base64.b64encode(csv_bytes).decode('utf-8')\n\n    return base64_str", "raw_solution": "import base64\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(data_dict):\n    \"\"\"\n    Encodes a dict of list as a Base64 string. The dict is first converted to a Pandas DataFrame.\n    Then convert the data frame to CSV format and encoded to bytes, finally encoded it to a Base64 string.\n    \"\"\"\n    # Convert the dict to a DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Convert the DataFrame to CSV format\n    csv_str = df.to_csv(index=False)\n\n    # Encode the CSV string to bytes\n    csv_bytes = csv_str.encode('utf-8')\n\n    # Encode the bytes to a Base64 string\n    base64_str = base64.b64encode(csv_bytes).decode('utf-8')\n\n    return base64_str"}
{"task_id": "BigCodeBench/550", "solution": "from collections import Counter\nimport pandas as pd\ndef task_func(list_of_menuitems):\n    # Flatten the list\n    flat_list = [item for sublist in list_of_menuitems for item in sublist]\n\n    # Count the occurrences of each menu item\n    count_dict = dict(Counter(flat_list))\n\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(count_dict.items()), columns=['MenuItem', 'Count'])\n    df.set_index('MenuItem', inplace=True)\n\n    return df", "raw_solution": "from collections import Counter\nimport pandas as pd\n\ndef task_func(list_of_menuitems):\n    # Flatten the list\n    flat_list = [item for sublist in list_of_menuitems for item in sublist]\n\n    # Count the occurrences of each menu item\n    count_dict = dict(Counter(flat_list))\n\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(count_dict.items()), columns=['MenuItem', 'Count'])\n    df.set_index('MenuItem', inplace=True)\n\n    return df"}
{"task_id": "BigCodeBench/551", "solution": "from collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Given a nested list of menu items, this function flattens the list and visualizes the frequency of each menu item using a seaborn barplot.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in list_of_menuitems for item in sublist]\n\n    # Count the frequency of each menu item\n    counter = Counter(flat_list)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(list(counter.items()), columns=['Menu Item', 'Frequency'])\n\n    # Create a seaborn barplot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='Menu Item', y='Frequency', data=df)\n\n    return ax", "raw_solution": "from collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Given a nested list of menu items, this function flattens the list and visualizes the frequency of each menu item using a seaborn barplot.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in list_of_menuitems for item in sublist]\n\n    # Count the frequency of each menu item\n    counter = Counter(flat_list)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(list(counter.items()), columns=['Menu Item', 'Frequency'])\n\n    # Create a seaborn barplot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='Menu Item', y='Frequency', data=df)\n\n    return ax"}
{"task_id": "BigCodeBench/552", "solution": "import collections\nimport itertools\nimport matplotlib.pyplot as plt\nITEMS = ['apple', 'banana']\ndef task_func(a, b, items=ITEMS):\n    # Combine the two lists\n    combined = list(itertools.chain(a, b))\n\n    # Count the frequency of each item in the combined list\n    counter = collections.Counter(combined)\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values())\n    ax.set_xlabel('Items')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Items in Combined List')\n\n    return ax", "raw_solution": "import collections\nimport itertools\nimport matplotlib.pyplot as plt\n\n# Constants\nITEMS = ['apple', 'banana']\n\ndef task_func(a, b, items=ITEMS):\n    # Combine the two lists\n    combined = list(itertools.chain(a, b))\n\n    # Count the frequency of each item in the combined list\n    counter = collections.Counter(combined)\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values())\n    ax.set_xlabel('Items')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Items in Combined List')\n\n    return ax"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b', and plots it as a bar chart.\n    'a' sets the DataFrame's row indices, while the length of 'b' determines the number of columns.\n    The function returns the Axes object of the plotted bar chart.\n    \"\"\"\n    # Generate random values\n    data = np.random.rand(len(a), len(b))\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:len(b)])\n\n    # Plot DataFrame as bar chart\n    ax = df.plot(kind='bar')\n\n    return ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b', and plots it as a bar chart.\n    'a' sets the DataFrame's row indices, while the length of 'b' determines the number of columns.\n    The function returns the Axes object of the plotted bar chart.\n    \"\"\"\n    # Generate random values\n    data = np.random.rand(len(a), len(b))\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:len(b)])\n\n    # Plot DataFrame as bar chart\n    ax = df.plot(kind='bar')\n\n    return ax"}
{"task_id": "BigCodeBench/554", "solution": "import numpy as np\nimport random\ndef task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL):\n    # Generate a random number of words between MIN_WORDS and MAX_WORDS\n    num_words = random.randint(MIN_WORDS, MAX_WORDS)\n\n    # Select random words from the pool\n    words = [random.choice(WORDS_POOL) for _ in range(num_words)]\n\n    # Combine the words into a sentence\n    sentence = ' '.join(words)\n\n    # Add the reversed words to the sentence to create a palindrome\n    sentence += ' ' + ' '.join(words[::-1])\n\n    return sentence\nMIN_WORDS = 5\nMAX_WORDS = 10\nWORDS_POOL = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew', 'ice', 'jackfruit', 'kiwi', 'lemon', 'mango', 'nectarine', 'orange', 'pineapple', 'quince', 'raspberry', 'strawberry', 'tangerine', 'ugli fruit', 'victoria', 'watermelon', 'xigua', 'yellow', 'zucchini']", "raw_solution": "import numpy as np\nimport random\n\ndef task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL):\n    # Generate a random number of words between MIN_WORDS and MAX_WORDS\n    num_words = random.randint(MIN_WORDS, MAX_WORDS)\n\n    # Select random words from the pool\n    words = [random.choice(WORDS_POOL) for _ in range(num_words)]\n\n    # Combine the words into a sentence\n    sentence = ' '.join(words)\n\n    # Add the reversed words to the sentence to create a palindrome\n    sentence += ' ' + ' '.join(words[::-1])\n\n    return sentence\n\n# Test the function\nMIN_WORDS = 5\nMAX_WORDS = 10\nWORDS_POOL = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew', 'ice', 'jackfruit', 'kiwi', 'lemon', 'mango', 'nectarine', 'orange', 'pineapple', 'quince', 'raspberry', 'strawberry', 'tangerine', 'ugli fruit', 'victoria', 'watermelon', 'xigua', 'yellow', 'zucchini']\n\nsentence = task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL)"}
{"task_id": "BigCodeBench/555", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\ndef task_func(a, b):\n    \"\"\"\n    Calculates the Pearson correlation coefficient of two lists, generates a Pandas DataFrame from these lists, and then draws a scatter plot with a regression line.\n    \"\"\"\n    # Calculate Pearson correlation coefficient\n    correlation, _ = stats.pearsonr(a, b)\n\n    # Create a DataFrame from the lists\n    df = pd.DataFrame({'a': a, 'b': b})\n\n    # Create a scatter plot with a regression line\n    fig, ax = plt.subplots()\n    ax.scatter(df['a'], df['b'])\n    m, b = np.polyfit(df['a'], df['b'], 1)\n    ax.plot(df['a'], m*df['a'] + b, color='red')\n\n    return correlation, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndef task_func(a, b):\n    \"\"\"\n    Calculates the Pearson correlation coefficient of two lists, generates a Pandas DataFrame from these lists, and then draws a scatter plot with a regression line.\n    \"\"\"\n    # Calculate Pearson correlation coefficient\n    correlation, _ = stats.pearsonr(a, b)\n\n    # Create a DataFrame from the lists\n    df = pd.DataFrame({'a': a, 'b': b})\n\n    # Create a scatter plot with a regression line\n    fig, ax = plt.subplots()\n    ax.scatter(df['a'], df['b'])\n    m, b = np.polyfit(df['a'], df['b'], 1)\n    ax.plot(df['a'], m*df['a'] + b, color='red')\n\n    return correlation, ax"}
{"task_id": "BigCodeBench/556", "solution": "import numpy as np\nimport random\nfrom difflib import SequenceMatcher\ndef task_func(s, min_length, max_length, letters):\n    def similarity(a, b):\n        return SequenceMatcher(None, a, b).ratio()\n\n    length = random.randint(min_length, max_length)\n    random_string = ''.join(random.choice(letters) for _ in range(length))\n    return random_string, similarity(random_string, s) >= 0.5", "raw_solution": "import numpy as np\nimport random\nfrom difflib import SequenceMatcher\n\ndef task_func(s, min_length, max_length, letters):\n    def similarity(a, b):\n        return SequenceMatcher(None, a, b).ratio()\n\n    length = random.randint(min_length, max_length)\n    random_string = ''.join(random.choice(letters) for _ in range(length))\n    return random_string, similarity(random_string, s) >= 0.5"}
{"task_id": "BigCodeBench/557", "solution": "import numpy as np\nfrom difflib import SequenceMatcher\nimport matplotlib.pyplot as plt\ndef task_func(s_list, plot_path=None):\n    if not isinstance(s_list, list):\n        raise ValueError(\"s_list must be a list of strings\")\n    if not all(isinstance(i, str) for i in s_list):\n        raise ValueError(\"s_list must be a list of strings\")\n    if len(s_list) == 1:\n        return np.nan\n\n    scores = []\n    for i in range(len(s_list)):\n        avg_score = sum(SequenceMatcher(None, s_list[i], s_list[j]).ratio() for j in range(len(s_list))) / len(s_list)\n        scores.append(avg_score)\n\n    if plot_path:\n        plt.bar(range(len(s_list)), scores)\n        plt.xlabel('String Index')\n        plt.ylabel('Average Similarity Score')\n        plt.savefig(plot_path)\n        plt.clf()\n\n    return scores", "raw_solution": "import numpy as np\nfrom difflib import SequenceMatcher\nimport matplotlib.pyplot as plt\n\ndef task_func(s_list, plot_path=None):\n    if not isinstance(s_list, list):\n        raise ValueError(\"s_list must be a list of strings\")\n    if not all(isinstance(i, str) for i in s_list):\n        raise ValueError(\"s_list must be a list of strings\")\n    if len(s_list) == 1:\n        return np.nan\n\n    scores = []\n    for i in range(len(s_list)):\n        avg_score = sum(SequenceMatcher(None, s_list[i], s_list[j]).ratio() for j in range(len(s_list))) / len(s_list)\n        scores.append(avg_score)\n\n    if plot_path:\n        plt.bar(range(len(s_list)), scores)\n        plt.xlabel('String Index')\n        plt.ylabel('Average Similarity Score')\n        plt.savefig(plot_path)\n        plt.clf()\n\n    return scores"}
{"task_id": "BigCodeBench/558", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(a, b, columns=['A', 'B']):\n    \"\"\"\n    Standardizes two lists of numbers using the StandardScaler from sklearn and visualizes the standardized values using a bar plot.\n    \"\"\"\n    # Create a DataFrame from the input lists\n    df = pd.DataFrame({\n        columns[0]: a,\n        columns[1]: b\n    })\n\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Fit and transform the DataFrame\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=columns)\n\n    # Create a bar plot\n    ax = df_standardized.plot(kind='bar', figsize=(10, 6))\n\n    # Set plot title and labels\n    ax.set_title('Standardized Values')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Standardized Value')\n\n    return df_standardized, ax.figure", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(a, b, columns=['A', 'B']):\n    \"\"\"\n    Standardizes two lists of numbers using the StandardScaler from sklearn and visualizes the standardized values using a bar plot.\n    \"\"\"\n    # Create a DataFrame from the input lists\n    df = pd.DataFrame({\n        columns[0]: a,\n        columns[1]: b\n    })\n\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Fit and transform the DataFrame\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=columns)\n\n    # Create a bar plot\n    ax = df_standardized.plot(kind='bar', figsize=(10, 6))\n\n    # Set plot title and labels\n    ax.set_title('Standardized Values')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Standardized Value')\n\n    return df_standardized, ax.figure"}
{"task_id": "BigCodeBench/559", "solution": "import pandas as pd\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\ndef task_func(a, b):\n    # Calculate Euclidean distance\n    euclidean_distance = distance.euclidean(a, b)\n\n    # Create DataFrame\n    df = pd.DataFrame({'A': a, 'B': b}, index=['A', 'B'])\n\n    # Create plot\n    fig, ax = plt.subplots()\n    ax.plot(df)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend(['A', 'B'])\n    ax.set_title('Euclidean Distance: {:.2f}'.format(euclidean_distance))\n\n    return euclidean_distance, df, ax", "raw_solution": "import pandas as pd\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\n\ndef task_func(a, b):\n    # Calculate Euclidean distance\n    euclidean_distance = distance.euclidean(a, b)\n\n    # Create DataFrame\n    df = pd.DataFrame({'A': a, 'B': b}, index=['A', 'B'])\n\n    # Create plot\n    fig, ax = plt.subplots()\n    ax.plot(df)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend(['A', 'B'])\n    ax.set_title('Euclidean Distance: {:.2f}'.format(euclidean_distance))\n\n    return euclidean_distance, df, ax"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): A DataFrame with 'date' and 'value' columns. 'date' should be in 'yyyy-mm-dd' format.\n\n    Returns:\n    Axes object: A matplotlib.axes.Axes object representing the plot.\n    \"\"\"\n    # Convert 'date' column to datetime\n    data['date'] = pd.to_datetime(data['date'])\n\n    # Extract year and month from 'date'\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n\n    # Group by 'year' and 'month', and calculate the mean 'value'\n    grouped_data = data.groupby(['year', 'month']).mean().reset_index()\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(grouped_data['month'], grouped_data['value'])\n\n    # Set title and labels\n    ax.set_title(f\"Monthly Data for {grouped_data['year'].iloc[0]}\")\n    ax.set_xlabel(\"Month\")\n    ax.set_ylabel(\"Value\")\n\n    return ax", "raw_solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): A DataFrame with 'date' and 'value' columns. 'date' should be in 'yyyy-mm-dd' format.\n\n    Returns:\n    Axes object: A matplotlib.axes.Axes object representing the plot.\n    \"\"\"\n    # Convert 'date' column to datetime\n    data['date'] = pd.to_datetime(data['date'])\n\n    # Extract year and month from 'date'\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n\n    # Group by 'year' and 'month', and calculate the mean 'value'\n    grouped_data = data.groupby(['year', 'month']).mean().reset_index()\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(grouped_data['month'], grouped_data['value'])\n\n    # Set title and labels\n    ax.set_title(f\"Monthly Data for {grouped_data['year'].iloc[0]}\")\n    ax.set_xlabel(\"Month\")\n    ax.set_ylabel(\"Value\")\n\n    return ax"}
{"task_id": "BigCodeBench/561", "solution": "from datetime import datetime\nfrom dateutil import parser\nfrom pytz import timezone\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Converts a date time from one timezone to another.\n    \"\"\"\n    # Parse the date string into a datetime object\n    dt = parser.parse(date_str)\n\n    # Convert the datetime object to the from_tz timezone\n    dt = dt.replace(tzinfo=timezone(from_tz))\n\n    # Convert the datetime object to the to_tz timezone\n    dt = dt.astimezone(timezone(to_tz))\n\n    # Format the datetime object into a string in \"yyyy-mm-dd hh:mm:ss\" format\n    dt_str = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    return dt_str", "raw_solution": "from datetime import datetime\nfrom dateutil import parser\nfrom pytz import timezone\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Converts a date time from one timezone to another.\n    \"\"\"\n    # Parse the date string into a datetime object\n    dt = parser.parse(date_str)\n\n    # Convert the datetime object to the from_tz timezone\n    dt = dt.replace(tzinfo=timezone(from_tz))\n\n    # Convert the datetime object to the to_tz timezone\n    dt = dt.astimezone(timezone(to_tz))\n\n    # Format the datetime object into a string in \"yyyy-mm-dd hh:mm:ss\" format\n    dt_str = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    return dt_str\n\n# Test the function\nassert task_func(\"2022-01-01 12:00:00\", \"UTC\", \"US/Pacific\") == \"2022-01-01 08:00:00\"\nassert task_func(\"2022-01-01 12:00:00\", \"US/Pacific\", \"UTC\") == \"2022-01-01 04:00:00\"\nassert task_func(\"2022-01-01 12:00:00\", \"UTC\", \"Asia/Tokyo\") == \"2022-01-01 23:00:00\"\nassert task_func(\"2022-01-01 12:00:00\", \"Asia/Tokyo\", \"UTC\") == \"2022-01-01 03:00:00\""}
{"task_id": "BigCodeBench/562", "solution": "import os\nimport ctypes\nimport sys\nimport subprocess\ndef task_func(filepath):\n    if not isinstance(filepath, str):\n        raise TypeError(\"Input filepath must be a string\")\n    if not filepath:\n        raise OSError(\"Input filepath is invalid or empty\")\n\n    # Load DLL file\n    try:\n        ctypes.CDLL(filepath)\n    except OSError:\n        raise OSError(\"Failed to load DLL file\")\n\n    # Retrieve system information\n    system_name = os.name\n    node_name = os.uname().nodename\n    release = os.uname().release\n    version = os.uname().version\n    machine = os.uname().machine\n    python_version = sys.version.split()[0]\n\n    # Retrieve PIP version\n    pip_version = subprocess.run([\"pip\", \"--version\"], capture_output=True, text=True).stdout.split()[1]\n\n    # Print system information\n    print(f\"System: {system_name} Node Name: {node_name} Release: {release} Version: {version} Machine: {machine} Python Version: {python_version} PIP Version: {pip_version}\")\n\n    # Return the name of the loaded DLL file\n    return os.path.basename(filepath)", "raw_solution": "import os\nimport ctypes\nimport sys\nimport subprocess\n\ndef task_func(filepath):\n    if not isinstance(filepath, str):\n        raise TypeError(\"Input filepath must be a string\")\n    if not filepath:\n        raise OSError(\"Input filepath is invalid or empty\")\n\n    # Load DLL file\n    try:\n        ctypes.CDLL(filepath)\n    except OSError:\n        raise OSError(\"Failed to load DLL file\")\n\n    # Retrieve system information\n    system_name = os.name\n    node_name = os.uname().nodename\n    release = os.uname().release\n    version = os.uname().version\n    machine = os.uname().machine\n    python_version = sys.version.split()[0]\n\n    # Retrieve PIP version\n    pip_version = subprocess.run([\"pip\", \"--version\"], capture_output=True, text=True).stdout.split()[1]\n\n    # Print system information\n    print(f\"System: {system_name} Node Name: {node_name} Release: {release} Version: {version} Machine: {machine} Python Version: {python_version} PIP Version: {pip_version}\")\n\n    # Return the name of the loaded DLL file\n    return os.path.basename(filepath)"}
{"task_id": "BigCodeBench/563", "solution": "import ctypes\nimport os\nimport shutil\nimport glob\ndef task_func(filepath, destination_dir):\n    \"\"\"\n    Loads a DLL file specified by the given filepath and moves all DLL files in the same directory to another specified directory.\n    \"\"\"\n    # Load the DLL file\n    dll = ctypes.CDLL(filepath)\n\n    # Get the name of the loaded DLL file\n    dll_name = os.path.basename(filepath)\n\n    # Move all DLL files in the same directory to the destination directory\n    for dll_file in glob.glob(os.path.join(os.path.dirname(filepath), \"*.dll\")):\n        if dll_file != filepath:\n            shutil.move(dll_file, destination_dir)\n\n    return dll_name", "raw_solution": "import ctypes\nimport os\nimport shutil\nimport glob\n\ndef task_func(filepath, destination_dir):\n    \"\"\"\n    Loads a DLL file specified by the given filepath and moves all DLL files in the same directory to another specified directory.\n    \"\"\"\n    # Load the DLL file\n    dll = ctypes.CDLL(filepath)\n\n    # Get the name of the loaded DLL file\n    dll_name = os.path.basename(filepath)\n\n    # Move all DLL files in the same directory to the destination directory\n    for dll_file in glob.glob(os.path.join(os.path.dirname(filepath), \"*.dll\")):\n        if dll_file != filepath:\n            shutil.move(dll_file, destination_dir)\n\n    return dll_name"}
{"task_id": "BigCodeBench/564", "solution": "import os\nimport ctypes\nfrom datetime import datetime\nimport pytz\ndef task_func(filepath):\n    \"\"\"\n    Loads a DLL file from the specified filepath and returns its metadata, including creation time, modification time, and file size.\n    The times are displayed in UTC format.\n    \"\"\"\n    # Load the DLL file\n    dll = ctypes.CDLL(filepath)\n\n    # Get the file size\n    file_size = os.path.getsize(filepath)\n\n    # Get the creation and modification times\n    creation_time = os.path.getctime(filepath)\n    modification_time = os.path.getmtime(filepath)\n\n    # Convert the times to UTC\n    creation_time_utc = datetime.fromtimestamp(creation_time, pytz.utc)\n    modification_time_utc = datetime.fromtimestamp(modification_time, pytz.utc)\n\n    # Create a dictionary to store the metadata\n    metadata = {\n        'Creation Time': creation_time_utc.strftime('%Y-%m-%d %H:%M:%S'),\n        'Modification Time': modification_time_utc.strftime('%Y-%m-%d %H:%M:%S'),\n        'Size': file_size\n    }\n\n    return os.path.basename(filepath), metadata", "raw_solution": "import os\nimport ctypes\nfrom datetime import datetime\nimport pytz\n\ndef task_func(filepath):\n    \"\"\"\n    Loads a DLL file from the specified filepath and returns its metadata, including creation time, modification time, and file size.\n    The times are displayed in UTC format.\n    \"\"\"\n    # Load the DLL file\n    dll = ctypes.CDLL(filepath)\n\n    # Get the file size\n    file_size = os.path.getsize(filepath)\n\n    # Get the creation and modification times\n    creation_time = os.path.getctime(filepath)\n    modification_time = os.path.getmtime(filepath)\n\n    # Convert the times to UTC\n    creation_time_utc = datetime.fromtimestamp(creation_time, pytz.utc)\n    modification_time_utc = datetime.fromtimestamp(modification_time, pytz.utc)\n\n    # Create a dictionary to store the metadata\n    metadata = {\n        'Creation Time': creation_time_utc.strftime('%Y-%m-%d %H:%M:%S'),\n        'Modification Time': modification_time_utc.strftime('%Y-%m-%d %H:%M:%S'),\n        'Size': file_size\n    }\n\n    return os.path.basename(filepath), metadata"}
{"task_id": "BigCodeBench/565", "solution": "import ctypes\nimport hashlib\nimport binascii\ndef task_func(filepath):\n    # Load the DLL file\n    dll = ctypes.CDLL(filepath)\n\n    # Get the DLL file name\n    dll_name = dll._name\n\n    # Open the DLL file in binary mode\n    with open(filepath, 'rb') as f:\n        file_data = f.read()\n\n    # Calculate MD5 and SHA256 hashes\n    md5_hash = hashlib.md5(file_data).hexdigest()\n    sha256_hash = hashlib.sha256(file_data).hexdigest()\n\n    # Print the hashes\n    print(f\"DLL file: {dll_name}\")\n    print(f\"MD5 hash: {md5_hash}\")\n    print(f\"SHA256 hash: {sha256_hash}\")\n\n    return dll_name", "raw_solution": "import ctypes\nimport hashlib\nimport binascii\n\ndef task_func(filepath):\n    # Load the DLL file\n    dll = ctypes.CDLL(filepath)\n\n    # Get the DLL file name\n    dll_name = dll._name\n\n    # Open the DLL file in binary mode\n    with open(filepath, 'rb') as f:\n        file_data = f.read()\n\n    # Calculate MD5 and SHA256 hashes\n    md5_hash = hashlib.md5(file_data).hexdigest()\n    sha256_hash = hashlib.sha256(file_data).hexdigest()\n\n    # Print the hashes\n    print(f\"DLL file: {dll_name}\")\n    print(f\"MD5 hash: {md5_hash}\")\n    print(f\"SHA256 hash: {sha256_hash}\")\n\n    return dll_name"}
{"task_id": "BigCodeBench/566", "solution": "import inspect\nimport types\ndef task_func(f):\n    if not inspect.isfunction(f):\n        raise TypeError(\"Input must be a function\")\n\n    spec = {\n        \"name\": f.__name__,\n        \"is_lambda\": isinstance(f, types.LambdaType),\n        \"arguments\": list(inspect.signature(f).parameters.keys()),\n        \"default_values\": [v.default for v in inspect.signature(f).parameters.values() if v.default is not inspect.Parameter.empty],\n        \"annotations\": [v.annotation for v in inspect.signature(f).parameters.values() if v.annotation is not inspect.Parameter.empty]\n    }\n\n    return spec", "raw_solution": "import inspect\nimport types\n\ndef task_func(f):\n    if not inspect.isfunction(f):\n        raise TypeError(\"Input must be a function\")\n\n    spec = {\n        \"name\": f.__name__,\n        \"is_lambda\": isinstance(f, types.LambdaType),\n        \"arguments\": list(inspect.signature(f).parameters.keys()),\n        \"default_values\": [v.default for v in inspect.signature(f).parameters.values() if v.default is not inspect.Parameter.empty],\n        \"annotations\": [v.annotation for v in inspect.signature(f).parameters.values() if v.annotation is not inspect.Parameter.empty]\n    }\n\n    return spec\n\n# Testing\ndef test_task_func():\n    def test_func(a, b=1, c=2):\n        \"\"\"\n        This is a test function.\n        \"\"\"\n        return a + b + c\n\n    result = task_func(test_func)\n    expected = {\n        \"name\": \"test_func\",\n        \"is_lambda\": False,\n        \"arguments\": [\"a\", \"b\", \"c\"],\n        \"default_values\": [1, 2],\n        \"annotations\": [None, None, None]\n    }\n\n    assert result == expected, f\"Expected {expected}, but got {result}\"\n\ntest_task_func()"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    This function draws a histogram to visualize the frequency distribution of numeric values provided in a string format.\n    The function takes a string of numeric values separated by commas as input, converts it to a numpy array, and then uses\n    matplotlib to create a histogram. The histogram uses bins calculated as `np.arange(data.min(), data.max()+2) - 0.5`.\n    The function returns the Axes object of the created histogram.\n    \"\"\"\n    # Convert the string of numeric values to a numpy array\n    data = np.array([float(x) for x in data.split(',')])\n\n    # Calculate the bins\n    bins = np.arange(data.min(), data.max()+2) - 0.5\n\n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, edgecolor='black')\n\n    # Set the title and labels\n    ax.set_title('Histogram of Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    This function draws a histogram to visualize the frequency distribution of numeric values provided in a string format.\n    The function takes a string of numeric values separated by commas as input, converts it to a numpy array, and then uses\n    matplotlib to create a histogram. The histogram uses bins calculated as `np.arange(data.min(), data.max()+2) - 0.5`.\n    The function returns the Axes object of the created histogram.\n    \"\"\"\n    # Convert the string of numeric values to a numpy array\n    data = np.array([float(x) for x in data.split(',')])\n\n    # Calculate the bins\n    bins = np.arange(data.min(), data.max()+2) - 0.5\n\n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, edgecolor='black')\n\n    # Set the title and labels\n    ax.set_title('Histogram of Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax"}
{"task_id": "BigCodeBench/568", "solution": "import inspect\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(f_list):\n    if not all(inspect.isfunction(f) for f in f_list):\n        raise ValueError(\"All elements in the input list must be functions\")\n\n    data = [(f.__name__, len(inspect.signature(f).parameters)) for f in f_list]\n    df = pd.DataFrame(data, columns=[\"Function Name\", \"Number of Arguments\"])\n    df.set_index(\"Function Name\", inplace=True)\n\n    df.plot(kind=\"bar\", legend=None)\n    plt.ylabel(\"Number of Arguments\")\n    plt.show()\n\n    return df", "raw_solution": "import inspect\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(f_list):\n    if not all(inspect.isfunction(f) for f in f_list):\n        raise ValueError(\"All elements in the input list must be functions\")\n\n    data = [(f.__name__, len(inspect.signature(f).parameters)) for f in f_list]\n    df = pd.DataFrame(data, columns=[\"Function Name\", \"Number of Arguments\"])\n    df.set_index(\"Function Name\", inplace=True)\n\n    df.plot(kind=\"bar\", legend=None)\n    plt.ylabel(\"Number of Arguments\")\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/569", "solution": "import inspect\nimport types\nimport math\ndef task_func(f):\n    sig = inspect.signature(f)\n    num_args = len(sig.parameters)\n    lambda_count = sum(1 for param in sig.parameters.values() if isinstance(param.default, types.LambdaType))\n    return {\n        'function_name': f.__name__,\n        'sqrt_num_args': math.sqrt(num_args),\n        'lambda_count': lambda_count,\n    }", "raw_solution": "import inspect\nimport types\nimport math\n\ndef task_func(f):\n    sig = inspect.signature(f)\n    num_args = len(sig.parameters)\n    lambda_count = sum(1 for param in sig.parameters.values() if isinstance(param.default, types.LambdaType))\n    return {\n        'function_name': f.__name__,\n        'sqrt_num_args': math.sqrt(num_args),\n        'lambda_count': lambda_count,\n    }\n\n# Testing\ndef test_task_func():\n    def func1(a, b, c=lambda: 1, d=lambda: 2):\n        pass\n\n    assert task_func(func1) == {\n        'function_name': 'func1',\n        'sqrt_num_args': math.sqrt(4),\n        'lambda_count': 2,\n    }\n\n    def func2(a, b, c=lambda: 1):\n        pass\n\n    assert task_func(func2) == {\n        'function_name': 'func2',\n        'sqrt_num_args': math.sqrt(3),\n        'lambda_count': 1,\n    }\n\n    def func3(a, b):\n        pass\n\n    assert task_func(func3) == {\n        'function_name': 'func3',\n        'sqrt_num_args': math.sqrt(2),\n        'lambda_count': 0,\n    }\n\n    print(\"All tests passed.\")\n\ntest_task_func()"}
{"task_id": "BigCodeBench/570", "solution": "import inspect\nimport types\nimport json\ndef task_func(f):\n    if isinstance(f, types.FunctionType):\n        specs = {\n            \"name\": f.__name__,\n            \"args\": list(inspect.signature(f).parameters.keys()),\n            \"defaults\": [str(v.default) if v.default is not inspect._empty else None for v in inspect.signature(f).parameters.values()],\n            \"annotations\": str(f.__annotations__),\n            \"is_lambda\": isinstance(f, types.LambdaType)\n        }\n        return json.dumps(specs, indent=4)\n    else:\n        raise TypeError(\"Input must be a function\")", "raw_solution": "import inspect\nimport types\nimport json\n\ndef task_func(f):\n    if isinstance(f, types.FunctionType):\n        specs = {\n            \"name\": f.__name__,\n            \"args\": list(inspect.signature(f).parameters.keys()),\n            \"defaults\": [str(v.default) if v.default is not inspect._empty else None for v in inspect.signature(f).parameters.values()],\n            \"annotations\": str(f.__annotations__),\n            \"is_lambda\": isinstance(f, types.LambdaType)\n        }\n        return json.dumps(specs, indent=4)\n    else:\n        raise TypeError(\"Input must be a function\")\n\n# Testing\ndef test_func(a: int, b: str = 'test') -> str:\n    \"\"\"\n    This is a test function.\n    \"\"\"\n    return a + b\n"}
{"task_id": "BigCodeBench/571", "solution": "import inspect\nimport pandas as pd\ndef task_func(f_list, file_path):\n    if not isinstance(f_list, list) or not all(inspect.isfunction(f) for f in f_list):\n        raise ValueError(\"'f_list' should be a list of functions\")\n    if not f_list:\n        raise ValueError(\"'f_list' should not be empty\")\n    if not isinstance(file_path, str):\n        raise ValueError(\"'file_path' should be a string\")\n\n    data = []\n    for f in f_list:\n        defaults = inspect.getfullargspec(f).defaults\n        if defaults is not None:\n            defaults = list(defaults)\n        annotations = inspect.getfullargspec(f).annotations\n        if annotations is not None:\n            annotations = list(annotations.items())\n        row = {\n            'Function Name': f.__name__,\n            'Number of Arguments': len(inspect.getfullargspec(f).args),\n            'Defaults': defaults,\n            'Annotations': annotations,\n            'Is Lambda': inspect.isfunction(f)\n        }\n        data.append(row)\n\n    df = pd.DataFrame(data)\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"Error writing to the specified file path\")", "raw_solution": "import inspect\nimport pandas as pd\n\ndef task_func(f_list, file_path):\n    if not isinstance(f_list, list) or not all(inspect.isfunction(f) for f in f_list):\n        raise ValueError(\"'f_list' should be a list of functions\")\n    if not f_list:\n        raise ValueError(\"'f_list' should not be empty\")\n    if not isinstance(file_path, str):\n        raise ValueError(\"'file_path' should be a string\")\n\n    data = []\n    for f in f_list:\n        defaults = inspect.getfullargspec(f).defaults\n        if defaults is not None:\n            defaults = list(defaults)\n        annotations = inspect.getfullargspec(f).annotations\n        if annotations is not None:\n            annotations = list(annotations.items())\n        row = {\n            'Function Name': f.__name__,\n            'Number of Arguments': len(inspect.getfullargspec(f).args),\n            'Defaults': defaults,\n            'Annotations': annotations,\n            'Is Lambda': inspect.isfunction(f)\n        }\n        data.append(row)\n\n    df = pd.DataFrame(data)\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"Error writing to the specified file path\")"}
{"task_id": "BigCodeBench/572", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100):\n    # Generate two arrays of random integers\n    array1 = [randint(0, 100) for _ in range(array_length)]\n    array2 = [randint(0, 100) for _ in range(array_length)]\n\n    # Find the maximum values of the respective elements of the two arrays\n    max_values = [max(array1[i], array2[i]) for i in range(array_length)]\n\n    # Draw a line diagram with the maximum values of the respective elements of the two arrays\n    plt.plot(max_values)\n    plt.ylabel('Maximum Values')\n    plt.xlabel('Index')\n    plt.title('Line diagram of maximum values of the respective elements of the two arrays')\n    plt.grid(True)\n\n    return plt.gca()", "raw_solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100):\n    # Generate two arrays of random integers\n    array1 = [randint(0, 100) for _ in range(array_length)]\n    array2 = [randint(0, 100) for _ in range(array_length)]\n\n    # Find the maximum values of the respective elements of the two arrays\n    max_values = [max(array1[i], array2[i]) for i in range(array_length)]\n\n    # Draw a line diagram with the maximum values of the respective elements of the two arrays\n    plt.plot(max_values)\n    plt.ylabel('Maximum Values')\n    plt.xlabel('Index')\n    plt.title('Line diagram of maximum values of the respective elements of the two arrays')\n    plt.grid(True)\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/573", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(array_length=100):\n    np.random.seed(0)\n    array1 = np.random.rand(array_length)\n    array2 = np.random.rand(array_length)\n\n    statistics = pd.DataFrame({\n        'Array1': [np.mean(array1), np.median(array1), np.std(array1)],\n        'Array2': [np.mean(array2), np.median(array2), np.std(array2)]\n    }, index=['Mean', 'Median', 'Standard Deviation'])\n\n    statistics.plot(kind='bar', figsize=(10, 5))\n    plt.ylabel('Statistics')\n    plt.show()\n\n    return statistics", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(array_length=100):\n    np.random.seed(0)\n    array1 = np.random.rand(array_length)\n    array2 = np.random.rand(array_length)\n\n    statistics = pd.DataFrame({\n        'Array1': [np.mean(array1), np.median(array1), np.std(array1)],\n        'Array2': [np.mean(array2), np.median(array2), np.std(array2)]\n    }, index=['Mean', 'Median', 'Standard Deviation'])\n\n    statistics.plot(kind='bar', figsize=(10, 5))\n    plt.ylabel('Statistics')\n    plt.show()\n\n    return statistics"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    def sine_func(x, a, b, c):\n        return a * np.sin(b * x) + c\n\n    x = np.linspace(0, 1, array_length)\n    y = sine_func(x, 1, 2 * np.pi, 0) + np.random.normal(0, noise_level, array_length)\n\n    popt, _ = curve_fit(sine_func, x, y)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y, 'b-', label='Noisy Sine Wave')\n    plt.plot(x, sine_func(x, *popt), 'r-', label='Fitted Curve')\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Sine Wave with Noise and Fitted Curve')\n    return plt.gca()", "raw_solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    def sine_func(x, a, b, c):\n        return a * np.sin(b * x) + c\n\n    x = np.linspace(0, 1, array_length)\n    y = sine_func(x, 1, 2 * np.pi, 0) + np.random.normal(0, noise_level, array_length)\n\n    popt, _ = curve_fit(sine_func, x, y)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y, 'b-', label='Noisy Sine Wave')\n    plt.plot(x, sine_func(x, *popt), 'r-', label='Fitted Curve')\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Sine Wave with Noise and Fitted Curve')\n    return plt.gca()"}
{"task_id": "BigCodeBench/575", "solution": "from random import shuffle\nimport pandas as pd\nimport numpy as np\ndef task_func(l, n_groups = 5):\n    # Shuffle the list\n    shuffle(l)\n\n    # Construct a dataframe from the shuffled list\n    df = pd.DataFrame(l, columns=['Value'])\n\n    # Move the first n_groups elements to the end of the same row\n    df['Value'] = df['Value'].apply(lambda x: [x[-n_groups:], x[:-n_groups]] if len(x) > n_groups else [x, []])\n\n    return df\nl = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]", "raw_solution": "from random import shuffle\nimport pandas as pd\nimport numpy as np\n\ndef task_func(l, n_groups = 5):\n    # Shuffle the list\n    shuffle(l)\n\n    # Construct a dataframe from the shuffled list\n    df = pd.DataFrame(l, columns=['Value'])\n\n    # Move the first n_groups elements to the end of the same row\n    df['Value'] = df['Value'].apply(lambda x: [x[-n_groups:], x[:-n_groups]] if len(x) > n_groups else [x, []])\n\n    return df\n\n# Test the function\nl = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ndf = task_func(l, n_groups=3)"}
{"task_id": "BigCodeBench/576", "solution": "from random import shuffle, randint\nimport pandas as pd\ndef task_func(l, n_groups = 5):\n    shuffle(l)\n    series = pd.Series(l)\n    for i in range(len(series)):\n        series[i] = series[i][n_groups:] + series[i][:n_groups]\n    return series\nl = ['abcdefghijklmnopqrstuvwxyz', '1234567890', 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', '!@#$%^&*()', 'Hello, World!']", "raw_solution": "from random import shuffle, randint\nimport pandas as pd\n\ndef task_func(l, n_groups = 5):\n    shuffle(l)\n    series = pd.Series(l)\n    for i in range(len(series)):\n        series[i] = series[i][n_groups:] + series[i][:n_groups]\n    return series\n\n# Test the function\nl = ['abcdefghijklmnopqrstuvwxyz', '1234567890', 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', '!@#$%^&*()', 'Hello, World!']"}
{"task_id": "BigCodeBench/577", "solution": "import os\nimport pathlib\nfrom hashlib import md5\nimport unicodedata\ndef task_func(directory):\n    \"\"\"\n    Processes all files within the specified directory, normalizes their filenames to ASCII,\n    calculates their MD5 hashes, and retrieves their sizes. It returns a dictionary where each key\n    is the normalized file name and each value is another dictionary with the file's size and MD5 hash.\n    This method is useful for file integrity checks and file organization tasks.\n    \"\"\"\n    result = {}\n\n    for filename in os.listdir(directory):\n        # Normalize the filename to ASCII\n        normalized_filename = unicodedata.normalize('NFKD', filename).encode('ASCII', 'ignore').decode()\n\n        # Calculate the MD5 hash\n        with open(os.path.join(directory, filename), 'rb') as file:\n            file_hash = md5(file.read()).hexdigest()\n\n        # Retrieve the file size\n        file_size = pathlib.Path(os.path.join(directory, filename)).stat().st_size\n\n        # Add the file information to the result dictionary\n        result[normalized_filename] = {'Size': file_size, 'MD5 Hash': file_hash}\n\n    return result", "raw_solution": "import os\nimport pathlib\nfrom hashlib import md5\nimport unicodedata\n\ndef task_func(directory):\n    \"\"\"\n    Processes all files within the specified directory, normalizes their filenames to ASCII,\n    calculates their MD5 hashes, and retrieves their sizes. It returns a dictionary where each key\n    is the normalized file name and each value is another dictionary with the file's size and MD5 hash.\n    This method is useful for file integrity checks and file organization tasks.\n    \"\"\"\n    result = {}\n\n    for filename in os.listdir(directory):\n        # Normalize the filename to ASCII\n        normalized_filename = unicodedata.normalize('NFKD', filename).encode('ASCII', 'ignore').decode()\n\n        # Calculate the MD5 hash\n        with open(os.path.join(directory, filename), 'rb') as file:\n            file_hash = md5(file.read()).hexdigest()\n\n        # Retrieve the file size\n        file_size = pathlib.Path(os.path.join(directory, filename)).stat().st_size\n\n        # Add the file information to the result dictionary\n        result[normalized_filename] = {'Size': file_size, 'MD5 Hash': file_hash}\n\n    return result"}
{"task_id": "BigCodeBench/578", "solution": "import unicodedata\nimport requests\nfrom requests.exceptions import HTTPError\nURL = 'https://api.github.com/users/'\ndef task_func(username):\n    try:\n        response = requests.get(URL + username)\n        response.raise_for_status()\n    except HTTPError as http_err:\n        print(f'HTTP error occurred: {http_err}')\n        return None\n    except Exception as err:\n        print(f'Other error occurred: {err}')\n        return None\n    else:\n        data = response.json()\n        normalized_data = {\n            'id': unicodedata.normalize('NFKD', str(data.get('id', ''))).encode('ascii', 'ignore').decode(),\n            'name': unicodedata.normalize('NFKD', str(data.get('name', ''))).encode('ascii', 'ignore').decode(),\n            'blog': unicodedata.normalize('NFKD', str(data.get('blog', ''))).encode('ascii', 'ignore').decode(),\n            'email': unicodedata.normalize('NFKD', str(data.get('email', ''))).encode('ascii', 'ignore').decode(),\n            'company': unicodedata.normalize('NFKD', str(data.get('company', ''))).encode('ascii', 'ignore').decode(),\n            'location': unicodedata.normalize('NFKD', str(data.get('location', ''))).encode('ascii', 'ignore').decode(),\n            'bio': unicodedata.normalize('NFKD', str(data.get('bio', ''))).encode('ascii', 'ignore').decode(),\n        }\n        return normalized_data", "raw_solution": "import unicodedata\nimport requests\nfrom requests.exceptions import HTTPError\n\nURL = 'https://api.github.com/users/'\n\ndef task_func(username):\n    try:\n        response = requests.get(URL + username)\n        response.raise_for_status()\n    except HTTPError as http_err:\n        print(f'HTTP error occurred: {http_err}')\n        return None\n    except Exception as err:\n        print(f'Other error occurred: {err}')\n        return None\n    else:\n        data = response.json()\n        normalized_data = {\n            'id': unicodedata.normalize('NFKD', str(data.get('id', ''))).encode('ascii', 'ignore').decode(),\n            'name': unicodedata.normalize('NFKD', str(data.get('name', ''))).encode('ascii', 'ignore').decode(),\n            'blog': unicodedata.normalize('NFKD', str(data.get('blog', ''))).encode('ascii', 'ignore').decode(),\n            'email': unicodedata.normalize('NFKD', str(data.get('email', ''))).encode('ascii', 'ignore').decode(),\n            'company': unicodedata.normalize('NFKD', str(data.get('company', ''))).encode('ascii', 'ignore').decode(),\n            'location': unicodedata.normalize('NFKD', str(data.get('location', ''))).encode('ascii', 'ignore').decode(),\n            'bio': unicodedata.normalize('NFKD', str(data.get('bio', ''))).encode('ascii', 'ignore').decode(),\n        }\n        return normalized_data"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            data = [row[0] for row in reader]\n    except FileNotFoundError:\n        raise FileNotFoundError(\"File not found at the specified path.\")\n    except IOError:\n        raise IOError(\"There was an error in reading the file.\")\n\n    # Normalize the text to ASCII\n    data = [unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') for text in data]\n\n    # Split the text into words\n    words = ' '.join(data).split()\n\n    # Count the words\n    word_counts = Counter(words)\n\n    # Get the 10 most common words\n    common_words = word_counts.most_common(10)\n\n    # Create a bar plot\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('10 Most Common Words')\n\n    return (ax, common_words)", "raw_solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            data = [row[0] for row in reader]\n    except FileNotFoundError:\n        raise FileNotFoundError(\"File not found at the specified path.\")\n    except IOError:\n        raise IOError(\"There was an error in reading the file.\")\n\n    # Normalize the text to ASCII\n    data = [unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') for text in data]\n\n    # Split the text into words\n    words = ' '.join(data).split()\n\n    # Count the words\n    word_counts = Counter(words)\n\n    # Get the 10 most common words\n    common_words = word_counts.most_common(10)\n\n    # Create a bar plot\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('10 Most Common Words')\n\n    return (ax, common_words)"}
{"task_id": "BigCodeBench/580", "solution": "import pandas as pd\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\nRANGE = 10000\nSIZE = 1000\nBIN_WIDTH = 100\ndef task_func():\n    # Generate random numbers\n    random_numbers = [random.randint(0, RANGE) for _ in range(SIZE)]\n\n    # Calculate moving average\n    moving_average = [sum(random_numbers[max(0, i-5):i+1])/min(5, i+1) for i in range(SIZE)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Random Numbers\": random_numbers,\n        \"Moving Average\": moving_average\n    })\n\n    # Plot histogram\n    plt.hist(df[\"Random Numbers\"], bins=BIN_WIDTH)\n    plt.title(\"Histogram of Random Numbers\")\n    plt.xlabel(\"Random Number\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nRANGE = 10000  # The range within which random numbers are generated\nSIZE = 1000  # The number of random numbers to generate\nBIN_WIDTH = 100  # The width of bins for the histogram\n\ndef task_func():\n    # Generate random numbers\n    random_numbers = [random.randint(0, RANGE) for _ in range(SIZE)]\n\n    # Calculate moving average\n    moving_average = [sum(random_numbers[max(0, i-5):i+1])/min(5, i+1) for i in range(SIZE)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Random Numbers\": random_numbers,\n        \"Moving Average\": moving_average\n    })\n\n    # Plot histogram\n    plt.hist(df[\"Random Numbers\"], bins=BIN_WIDTH)\n    plt.title(\"Histogram of Random Numbers\")\n    plt.xlabel(\"Random Number\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/581", "solution": "import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nRANGE = 10000\nSIZE = 1000\nPI = np.pi\ndef task_func(size=SIZE, frequency=1):\n    \"\"\"\n    Generates a list of random sinusoidal values and plots them in a graph.\n    \"\"\"\n    # Generate a list of random x values\n    x = np.linspace(0, 2 * PI, size)\n\n    # Generate a list of random y values\n    y = [random.uniform(-RANGE, RANGE) for _ in range(size)]\n\n    # Generate the sinusoidal values\n    y = [math.sin(x[i] * frequency) + y[i] for i in range(size)]\n\n    # Plot the sinusoidal wave\n    plt.plot(x, y)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Sinusoidal Wave')\n    plt.grid(True)\n\n    return plt", "raw_solution": "import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\n# Constants\nRANGE = 10000\nSIZE = 1000\nPI = np.pi\n\ndef task_func(size=SIZE, frequency=1):\n    \"\"\"\n    Generates a list of random sinusoidal values and plots them in a graph.\n    \"\"\"\n    # Generate a list of random x values\n    x = np.linspace(0, 2 * PI, size)\n\n    # Generate a list of random y values\n    y = [random.uniform(-RANGE, RANGE) for _ in range(size)]\n\n    # Generate the sinusoidal values\n    y = [math.sin(x[i] * frequency) + y[i] for i in range(size)]\n\n    # Plot the sinusoidal wave\n    plt.plot(x, y)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Sinusoidal Wave')\n    plt.grid(True)\n\n    return plt\n\n# Test the function\ntask_func()\nplt.show()"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(0, 1, size)\n\n    # Create a figure\n    fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n\n    # Plot histogram\n    ax[0].hist(data, bins=30, density=True)\n    ax[0].set_title('Histogram')\n\n    # Plot PDF\n    x = np.linspace(-4, 4, 1000)\n    ax[1].plot(x, stats.norm.pdf(x, 0, 1))\n    ax[1].set_title('Probability Density Function')\n\n    return fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(0, 1, size)\n\n    # Create a figure\n    fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n\n    # Plot histogram\n    ax[0].hist(data, bins=30, density=True)\n    ax[0].set_title('Histogram')\n\n    # Plot PDF\n    x = np.linspace(-4, 4, 1000)\n    ax[1].plot(x, stats.norm.pdf(x, 0, 1))\n    ax[1].set_title('Probability Density Function')\n\n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\nfrom Crypto.Util.Padding import pad, unpad", "raw_solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\nfrom Crypto.Util.Padding import pad, unpad\n\ndef generate_rsa_keys_and_encrypt_private_key(password: str) -> dict:\n    \"\"\"\n    Generates an RSA public and private key pair and saves the private key in a file after encrypting it with a password using AES encryption.\n    Returns the public key and the filename of the encrypted private key, along with encryption details for testing.\n    \"\"\"\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Convert private key to bytes\n    privkey_bytes = rsa.key.exportKey('PEM')\n\n    # Generate random bytes for filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Encrypt private key with AES\n    cipher = AES.new(password.encode(), AES.MODE_EAX)\n    ciphertext, tag = cipher.encrypt_and_digest(privkey_bytes)\n\n    # Save encrypted private key to file\n    with open(filename, 'wb') as file:\n        [file.write(x) for x in (cipher.nonce, ciphertext, tag)]\n\n    return {\n        \"rsa.PublicKey\": pubkey,\n        \"filename\": filename,\n        \"encryption_password\": password.encode(),\n        \"encryption_nonce\": cipher.nonce\n    }\n\ndef test_generate_rsa_keys_and_encrypt_private_key():\n    \"\"\"\n    Tests the `generate_rsa_keys_and_encrypt_private_key` function.\n    \"\"\"\n    password = \"test_password\"\n    result = generate_rsa_keys_and_encrypt_private_key(password)\n\n    # Check if the function returns the correct keys\n    assert isinstance(result[\"rsa.PublicKey\"], rsa.key.PublicKey)\n    assert isinstance(result[\"filename\"], str)\n    assert isinstance(result[\"encryption_password\"], bytes)\n    assert isinstance(result[\"encryption_nonce\"], bytes)\n\n    # Check if the private key file exists\n    assert os.path.isfile(result[\"filename\"])\n\n    # Check if the private key can be decrypted correctly\n    with open(result[\"filename\"], 'rb') as file:\n        nonce = file.read(16)\n        ciphertext = file.read(16)\n        tag = file.read(16)\n\n    cipher = AES.new(password.encode(), AES.MODE_EAX, nonce=nonce)\n    privkey_bytes = unpad(cipher.decrypt_and_verify(ciphertext, tag), AES.block_size)\n    privkey = rsa.key.importKey(privkey_bytes)\n\n    assert isinstance(privkey, rsa.key.PrivateKey)\n\n    # Clean up\n    os.remove(result[\"filename\"])\n"}
{"task_id": "BigCodeBench/584", "solution": "import rsa\nimport urllib.request\nfrom hashlib import sha256\ndef task_func(url):\n    try:\n        # Generate RSA public and private keys\n        (pubkey, privkey) = rsa.newkeys(512)\n\n        # Retrieve the content from the specified URL\n        response = urllib.request.urlopen(url)\n        content = response.read()\n\n        # Calculate the SHA256 hash of the content\n        content_hash = sha256(content).digest()\n\n        # Sign the hash with the private key\n        signature = rsa.sign_hash(content_hash, privkey, 'SHA-256')\n\n        # Convert the signature to a hexadecimal string\n        signature_hex = signature.hex()\n\n        return pubkey, signature_hex, content_hash\n\n    except urllib.error.URLError as e:\n        print(f\"URL Error: {e}\")\n        raise\n    except urllib.error.HTTPError as e:\n        print(f\"HTTP Error: {e}\")\n        raise\n    except rsa.pkcs1.VerificationError as e:\n        print(f\"Verification Error: {e}\")\n        raise\n    except Exception as e:\n        print(f\"Unexpected Error: {e}\")\n        raise", "raw_solution": "import rsa\nimport urllib.request\nfrom hashlib import sha256\n\ndef task_func(url):\n    try:\n        # Generate RSA public and private keys\n        (pubkey, privkey) = rsa.newkeys(512)\n\n        # Retrieve the content from the specified URL\n        response = urllib.request.urlopen(url)\n        content = response.read()\n\n        # Calculate the SHA256 hash of the content\n        content_hash = sha256(content).digest()\n\n        # Sign the hash with the private key\n        signature = rsa.sign_hash(content_hash, privkey, 'SHA-256')\n\n        # Convert the signature to a hexadecimal string\n        signature_hex = signature.hex()\n\n        return pubkey, signature_hex, content_hash\n\n    except urllib.error.URLError as e:\n        print(f\"URL Error: {e}\")\n        raise\n    except urllib.error.HTTPError as e:\n        print(f\"HTTP Error: {e}\")\n        raise\n    except rsa.pkcs1.VerificationError as e:\n        print(f\"Verification Error: {e}\")\n        raise\n    except Exception as e:\n        print(f\"Unexpected Error: {e}\")\n        raise"}
{"task_id": "BigCodeBench/585", "solution": "import rsa\nimport os\nimport zipfile\nfrom base64 import b64encode\ndef task_func(directory):\n    \"\"\"\n    Generates RSA public and private keys, encrypts all files in the specified directory using the public key, and saves the encrypted files into a zip file.\n    It returns the public key and the name of the zip file.\n    \"\"\"\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Create a zip file\n    zip_filename = \"encrypted_files.zip\"\n    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n        # Iterate over all files in the directory\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                # Read the file\n                with open(os.path.join(root, file), 'rb') as f:\n                    file_data = f.read()\n\n                # Encrypt the file data\n                encrypted_data = rsa.encrypt(file_data, pubkey)\n\n                # Write the encrypted data to the zip file\n                zipf.writestr(file, b64encode(encrypted_data).decode())\n\n    # Return the public key and the name of the zip file\n    return pubkey, zip_filename", "raw_solution": "import rsa\nimport os\nimport zipfile\nfrom base64 import b64encode\n\ndef task_func(directory):\n    \"\"\"\n    Generates RSA public and private keys, encrypts all files in the specified directory using the public key, and saves the encrypted files into a zip file.\n    It returns the public key and the name of the zip file.\n    \"\"\"\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Create a zip file\n    zip_filename = \"encrypted_files.zip\"\n    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n        # Iterate over all files in the directory\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                # Read the file\n                with open(os.path.join(root, file), 'rb') as f:\n                    file_data = f.read()\n\n                # Encrypt the file data\n                encrypted_data = rsa.encrypt(file_data, pubkey)\n\n                # Write the encrypted data to the zip file\n                zipf.writestr(file, b64encode(encrypted_data).decode())\n\n    # Return the public key and the name of the zip file\n    return pubkey, zip_filename"}
{"task_id": "BigCodeBench/586", "solution": "import rsa\nfrom cryptography.fernet import Fernet\nfrom base64 import b64encode\nimport os\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Read file contents\n    with open(file_path, 'rb') as f:\n        file_contents = f.read()\n\n    # Generate Fernet key\n    fernet_key = Fernet.generate_key()\n\n    # Encrypt file contents with Fernet\n    cipher_suite = Fernet(fernet_key)\n    encrypted_file_contents = cipher_suite.encrypt(file_contents)\n\n    # Encrypt Fernet key with RSA\n    encrypted_fernet_key = rsa.encrypt(fernet_key, pubkey)\n\n    # Save encrypted file contents and encrypted Fernet key\n    encrypted_file_path = f\"{file_path}.encrypted\"\n    encrypted_key_path = f\"{file_path}.key.encrypted\"\n\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_file_contents)\n\n    with open(encrypted_key_path, 'wb') as f:\n        f.write(b64encode(encrypted_fernet_key))\n\n    return pubkey, encrypted_file_path, encrypted_key_path", "raw_solution": "import rsa\nfrom cryptography.fernet import Fernet\nfrom base64 import b64encode\nimport os\n\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Read file contents\n    with open(file_path, 'rb') as f:\n        file_contents = f.read()\n\n    # Generate Fernet key\n    fernet_key = Fernet.generate_key()\n\n    # Encrypt file contents with Fernet\n    cipher_suite = Fernet(fernet_key)\n    encrypted_file_contents = cipher_suite.encrypt(file_contents)\n\n    # Encrypt Fernet key with RSA\n    encrypted_fernet_key = rsa.encrypt(fernet_key, pubkey)\n\n    # Save encrypted file contents and encrypted Fernet key\n    encrypted_file_path = f\"{file_path}.encrypted\"\n    encrypted_key_path = f\"{file_path}.key.encrypted\"\n\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_file_contents)\n\n    with open(encrypted_key_path, 'wb') as f:\n        f.write(b64encode(encrypted_fernet_key))\n\n    return pubkey, encrypted_file_path, encrypted_key_path"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Read file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # AES encryption\n    aes_key = os.urandom(32)  # 256-bit key\n    cipher = Cipher(algorithms.AES(aes_key), modes.ECB())\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    padded_content = padder.update(file_content) + padder.finalize()\n    encrypted_file_content = encryptor.update(padded_content) + encryptor.finalize()\n\n    # Save encrypted file\n    encrypted_file_path = 'encrypted_file.bin'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_file_content)\n\n    # Encrypt AES key with RSA\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save encrypted AES key\n    encrypted_aes_key_path = 'encrypted_aes_key.bin'\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pubkey, encrypted_file_path, encrypted_aes_key_path", "raw_solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Read file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # AES encryption\n    aes_key = os.urandom(32)  # 256-bit key\n    cipher = Cipher(algorithms.AES(aes_key), modes.ECB())\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    padded_content = padder.update(file_content) + padder.finalize()\n    encrypted_file_content = encryptor.update(padded_content) + encryptor.finalize()\n\n    # Save encrypted file\n    encrypted_file_path = 'encrypted_file.bin'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_file_content)\n\n    # Encrypt AES key with RSA\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save encrypted AES key\n    encrypted_aes_key_path = 'encrypted_aes_key.bin'\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pubkey, encrypted_file_path, encrypted_aes_key_path"}
{"task_id": "BigCodeBench/588", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nRANGE = 100\nSIZE = 1000\ndef task_func():\n    # Generate a DataFrame with 'X' and 'Y' columns filled with random integers within the specified range\n    df = pd.DataFrame({\n        'X': np.random.randint(0, RANGE, SIZE),\n        'Y': np.random.randint(0, RANGE, SIZE)\n    })\n\n    # Plot the points using a scatter plot with Seaborn\n    sns.scatterplot(x='X', y='Y', data=df)\n    plt.show()\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants defining the range of random integers and the size of the DataFrame\nRANGE = 100\nSIZE = 1000\n\ndef task_func():\n    # Generate a DataFrame with 'X' and 'Y' columns filled with random integers within the specified range\n    df = pd.DataFrame({\n        'X': np.random.randint(0, RANGE, SIZE),\n        'Y': np.random.randint(0, RANGE, SIZE)\n    })\n\n    # Plot the points using a scatter plot with Seaborn\n    sns.scatterplot(x='X', y='Y', data=df)\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/589", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\nRANGE = 100\nSIZE = 1000\nCLUSTERS = 5\ndef task_func():\n    # Generate random 2D points within the specified range and size\n    data = np.random.rand(SIZE, 2) * RANGE\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=CLUSTERS, random_state=0).fit(data)\n\n    # Plot the clustered points with each cluster displayed in a different color\n    plt.scatter(data[:, 0], data[:, 1], c=kmeans.labels_, cmap='viridis')\n\n    # Highlight the centroids of the clusters\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X')\n\n    plt.show()\n\n    return data, kmeans", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n# Constants for configuration\nRANGE = 100\nSIZE = 1000\nCLUSTERS = 5\n\ndef task_func():\n    # Generate random 2D points within the specified range and size\n    data = np.random.rand(SIZE, 2) * RANGE\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=CLUSTERS, random_state=0).fit(data)\n\n    # Plot the clustered points with each cluster displayed in a different color\n    plt.scatter(data[:, 0], data[:, 1], c=kmeans.labels_, cmap='viridis')\n\n    # Highlight the centroids of the clusters\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X')\n\n    plt.show()\n\n    return data, kmeans"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL is invalid or empty\")\n\n    try:\n        response = urllib.request.urlopen(url)\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"Network connectivity or server issue\") from e\n\n    html = response.read().decode('utf-8')\n    doc = pq(html)\n\n    data = []\n    for tag in doc('a'):\n        text = tag.text\n        href = tag.attrib['href']\n        data.append({'text': text, 'href': href})\n\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    return df", "raw_solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL is invalid or empty\")\n\n    try:\n        response = urllib.request.urlopen(url)\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"Network connectivity or server issue\") from e\n\n    html = response.read().decode('utf-8')\n    doc = pq(html)\n\n    data = []\n    for tag in doc('a'):\n        text = tag.text\n        href = tag.attrib['href']\n        data.append({'text': text, 'href': href})\n\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    return df"}
{"task_id": "BigCodeBench/591", "solution": "from datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTEMP_CATEGORIES = ['Cold', 'Normal', 'Hot']\nFILE_PATH = 'custom_data.csv'\ndef task_func(hours, file_path=FILE_PATH):\n    # Generate temperature data\n    data = {'Time': [], 'Temperature': [], 'Category': []}\n    for i in range(hours):\n        time = datetime.now() + timedelta(hours=i)\n        temp = randint(0, 100)\n        category = TEMP_CATEGORIES[0] if temp < 30 else TEMP_CATEGORIES[1] if temp < 60 else TEMP_CATEGORIES[2]\n        data['Time'].append(time)\n        data['Temperature'].append(temp)\n        data['Category'].append(category)\n\n    # Save data to CSV file\n    df = pd.DataFrame(data)\n    df.to_csv(file_path, index=False)\n\n    # Plot data\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Temperature'], color='blue')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Temperature')\n    ax.set_title('Temperature over Time')\n\n    return file_path, ax", "raw_solution": "from datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nTEMP_CATEGORIES = ['Cold', 'Normal', 'Hot']\nFILE_PATH = 'custom_data.csv'\n\ndef task_func(hours, file_path=FILE_PATH):\n    # Generate temperature data\n    data = {'Time': [], 'Temperature': [], 'Category': []}\n    for i in range(hours):\n        time = datetime.now() + timedelta(hours=i)\n        temp = randint(0, 100)\n        category = TEMP_CATEGORIES[0] if temp < 30 else TEMP_CATEGORIES[1] if temp < 60 else TEMP_CATEGORIES[2]\n        data['Time'].append(time)\n        data['Temperature'].append(temp)\n        data['Category'].append(category)\n\n    # Save data to CSV file\n    df = pd.DataFrame(data)\n    df.to_csv(file_path, index=False)\n\n    # Plot data\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Temperature'], color='blue')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Temperature')\n    ax.set_title('Temperature over Time')\n\n    return file_path, ax"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates sensor data for the specified number of hours and saves it in a CSV file.\n    The function outputs with:\n        hours (int): Number of hours to generate data for.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate data\n    data = []\n    current_time = datetime.now()\n    for _ in range(hours):\n        row = {}\n        row['Time'] = current_time.strftime('%Y-%m-%d %H:%M:%S')\n        for sensor in SENSORS:\n            row[sensor] = randint(0, 100)\n        data.append(row)\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV file\n    file_path = os.path.join(output_dir, 'sensor_data.csv')\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Temperature', 'Humidity', 'Pressure'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    return file_path", "raw_solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates sensor data for the specified number of hours and saves it in a CSV file.\n    The function outputs with:\n        hours (int): Number of hours to generate data for.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate data\n    data = []\n    current_time = datetime.now()\n    for _ in range(hours):\n        row = {}\n        row['Time'] = current_time.strftime('%Y-%m-%d %H:%M:%S')\n        for sensor in SENSORS:\n            row[sensor] = randint(0, 100)\n        data.append(row)\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV file\n    file_path = os.path.join(output_dir, 'sensor_data.csv')\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Temperature', 'Humidity', 'Pressure'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    return file_path"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate traffic data\n    data = []\n    for i in range(hours):\n        row = {'Time': datetime.now() + timedelta(hours=i)}\n        for vehicle in VEHICLE_TYPES:\n            row[vehicle] = randint(0, 100)\n        data.append(row)\n\n    # Save data to CSV file\n    csv_file = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Car', 'Bus', 'Truck', 'Bike'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    # Load data from CSV file\n    df = pd.read_csv(csv_file)\n    df['Time'] = pd.to_datetime(df['Time'])\n\n    # Plot data\n    fig, ax = plt.subplots()\n    for vehicle in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle], label=vehicle)\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n\n    return csv_file, ax", "raw_solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate traffic data\n    data = []\n    for i in range(hours):\n        row = {'Time': datetime.now() + timedelta(hours=i)}\n        for vehicle in VEHICLE_TYPES:\n            row[vehicle] = randint(0, 100)\n        data.append(row)\n\n    # Save data to CSV file\n    csv_file = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Car', 'Bus', 'Truck', 'Bike'])\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    # Load data from CSV file\n    df = pd.read_csv(csv_file)\n    df['Time'] = pd.to_datetime(df['Time'])\n\n    # Plot data\n    fig, ax = plt.subplots()\n    for vehicle in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle], label=vehicle)\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n\n    return csv_file, ax"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate weather data\n    weather_data = []\n    current_time = datetime.now()\n    for i in range(hours):\n        time = (current_time + timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        weather_data.append([time, condition])\n\n    # Save weather data to CSV file\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file to a backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    shutil.copy2(csv_file_path, backup_dir)\n\n    return csv_file_path", "raw_solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate weather data\n    weather_data = []\n    current_time = datetime.now()\n    for i in range(hours):\n        time = (current_time + timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        weather_data.append([time, condition])\n\n    # Save weather data to CSV file\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file to a backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    shutil.copy2(csv_file_path, backup_dir)\n\n    return csv_file_path"}
{"task_id": "BigCodeBench/595", "solution": "import random\nimport bisect\nfrom array import array\ndef task_func(n=10, total=100):\n    \"\"\"\n    Generates 'n' random integer numbers such that their sum equals 'total',\n    sorts these numbers, and determines the position where a new random number\n    can be inserted to maintain the sorted order. The function uses a retry\n    mechanism to ensure the generated numbers sum up to 'total'.\n    \"\"\"\n    while True:\n        numbers = [random.randint(1, total) for _ in range(n)]\n        if sum(numbers) == total:\n            break\n\n    numbers.sort()\n    new_number = random.randint(1, total)\n    insertion_position = bisect.bisect_left(numbers, new_number)\n\n    return (numbers, insertion_position)", "raw_solution": "import random\nimport bisect\nfrom array import array\n\ndef task_func(n=10, total=100):\n    \"\"\"\n    Generates 'n' random integer numbers such that their sum equals 'total',\n    sorts these numbers, and determines the position where a new random number\n    can be inserted to maintain the sorted order. The function uses a retry\n    mechanism to ensure the generated numbers sum up to 'total'.\n    \"\"\"\n    while True:\n        numbers = [random.randint(1, total) for _ in range(n)]\n        if sum(numbers) == total:\n            break\n\n    numbers.sort()\n    new_number = random.randint(1, total)\n    insertion_position = bisect.bisect_left(numbers, new_number)\n\n    return (numbers, insertion_position)\n\n# Test the function"}
{"task_id": "BigCodeBench/596", "solution": "import time\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\ndef task_func(duration):\n    start_time = time.time()\n    timestamps = []\n    values = []\n\n    while time.time() - start_time < duration:\n        timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]\n        value = randint(1, 100)\n        timestamps.append(timestamp)\n        values.append(value)\n        time.sleep(0.1)  # Delay for 0.1 seconds\n\n    return (timestamps, values)", "raw_solution": "import time\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\ndef task_func(duration):\n    start_time = time.time()\n    timestamps = []\n    values = []\n\n    while time.time() - start_time < duration:\n        timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]\n        value = randint(1, 100)\n        timestamps.append(timestamp)\n        values.append(value)\n        time.sleep(0.1)  # Delay for 0.1 seconds\n\n    return (timestamps, values)\n\n# Test the function\ntimestamps, values = task_func(5)"}
{"task_id": "BigCodeBench/597", "solution": "import pandas as pd\nimport time\ndef task_func(data, letter):\n    \"\"\"\n    Filters rows in a dictionary where the 'Name' column values start with a specified letter.\n    First, convert the dict to a DataFrame and then filter rows in this DataFrame.\n    \"\"\"\n    # Convert the dict to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Filter rows where the 'Name' column values start with the specified letter\n    filtered_df = df[df['Name'].str.startswith(letter)]\n\n    # Return the 'Name' column of the filtered DataFrame as a Series\n    return filtered_df['Name']\ndata = [{'Name': 'Alice', 'Age': 25}, {'Name': 'Bob', 'Age': 30}, {'Name': 'Charlie', 'Age': 35}, {'Name': 'David', 'Age': 40}]", "raw_solution": "import pandas as pd\nimport time\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef task_func(data, letter):\n    \"\"\"\n    Filters rows in a dictionary where the 'Name' column values start with a specified letter.\n    First, convert the dict to a DataFrame and then filter rows in this DataFrame.\n    \"\"\"\n    # Convert the dict to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Filter rows where the 'Name' column values start with the specified letter\n    filtered_df = df[df['Name'].str.startswith(letter)]\n\n    # Return the 'Name' column of the filtered DataFrame as a Series\n    return filtered_df['Name']\n\n# Test the function\ndata = [{'Name': 'Alice', 'Age': 25}, {'Name': 'Bob', 'Age': 30}, {'Name': 'Charlie', 'Age': 35}, {'Name': 'David', 'Age': 40}]"}
{"task_id": "BigCodeBench/598", "solution": "import pandas as pd\ndef task_func(df, letter):\n    # Filter rows where the 'Word' column starts with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n\n    # Calculate the length of the words in the filtered column\n    word_lengths = filtered_df['Word'].str.len()\n\n    # Count the occurrences of each word length\n    word_length_counts = word_lengths.value_counts().to_dict()\n\n    return word_length_counts\ndf = pd.DataFrame({'Word': ['apple', 'banana', 'cherry', 'date', 'apple', 'banana', 'cherry', 'date']})", "raw_solution": "import pandas as pd\n\ndef task_func(df, letter):\n    # Filter rows where the 'Word' column starts with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n\n    # Calculate the length of the words in the filtered column\n    word_lengths = filtered_df['Word'].str.len()\n\n    # Count the occurrences of each word length\n    word_length_counts = word_lengths.value_counts().to_dict()\n\n    return word_length_counts\n\n# Test the function\ndf = pd.DataFrame({'Word': ['apple', 'banana', 'cherry', 'date', 'apple', 'banana', 'cherry', 'date']})"}
{"task_id": "BigCodeBench/599", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df, letter):\n    # Filter rows where 'Word' column values start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n\n    # Calculate the lengths of these words\n    filtered_df['Word_Length'] = filtered_df['Word'].str.len()\n\n    # Plot a histogram of the word lengths\n    plt.figure(figsize=(10, 6))\n    sns.histplot(filtered_df['Word_Length'], bins=20, color='blue')\n    plt.title(f'Histogram of Word Lengths for Words Starting with {letter}')\n    plt.xlabel('Word Length')\n    plt.ylabel('Frequency')\n    plt.show()\ndf = pd.DataFrame({'Word': ['apple', 'banana', 'cherry', 'date', 'elderberry']})", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df, letter):\n    # Filter rows where 'Word' column values start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n\n    # Calculate the lengths of these words\n    filtered_df['Word_Length'] = filtered_df['Word'].str.len()\n\n    # Plot a histogram of the word lengths\n    plt.figure(figsize=(10, 6))\n    sns.histplot(filtered_df['Word_Length'], bins=20, color='blue')\n    plt.title(f'Histogram of Word Lengths for Words Starting with {letter}')\n    plt.xlabel('Word Length')\n    plt.ylabel('Frequency')\n    plt.show()\n\n# Test the function\ndf = pd.DataFrame({'Word': ['apple', 'banana', 'cherry', 'date', 'elderberry']})\ntask_func(df, 'a')"}
{"task_id": "BigCodeBench/600", "solution": "import numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom statistics import mode, median\ndef task_func(df, letter):\n    # Filter rows where 'Word' column values start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the lengths of the words\n    lengths = filtered_df['Word'].str.len()\n    \n    # Calculate mean, median, and mode of the word lengths\n    mean_length = np.mean(lengths)\n    median_length = median(lengths)\n    mode_length = mode(lengths)\n    \n    # Return a dictionary with the calculated statistics\n    return {'Mean': mean_length, 'Median': median_length, 'Mode': mode_length}", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom statistics import mode, median\n\ndef task_func(df, letter):\n    # Filter rows where 'Word' column values start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the lengths of the words\n    lengths = filtered_df['Word'].str.len()\n    \n    # Calculate mean, median, and mode of the word lengths\n    mean_length = np.mean(lengths)\n    median_length = median(lengths)\n    mode_length = mode(lengths)\n    \n    # Return a dictionary with the calculated statistics\n    return {'Mean': mean_length, 'Median': median_length, 'Mode': mode_length}"}
{"task_id": "BigCodeBench/601", "solution": "import seaborn as sns\nimport pandas as pd\ndef task_func(df, letter):\n    if df is None or 'Word' not in df.columns:\n        return None\n\n    # Filter rows based on the starting letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n\n    # Calculate the lengths of the words\n    filtered_df['Word_Length'] = filtered_df['Word'].str.len()\n\n    # Plot a box plot of the word lengths\n    box_plot = sns.boxplot(x=filtered_df['Word_Length'])\n\n    return box_plot", "raw_solution": "import seaborn as sns\nimport pandas as pd\n\ndef task_func(df, letter):\n    if df is None or 'Word' not in df.columns:\n        return None\n\n    # Filter rows based on the starting letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n\n    # Calculate the lengths of the words\n    filtered_df['Word_Length'] = filtered_df['Word'].str.len()\n\n    # Plot a box plot of the word lengths\n    box_plot = sns.boxplot(x=filtered_df['Word_Length'])\n\n    return box_plot"}
{"task_id": "BigCodeBench/602", "solution": "import numpy as np\nimport pandas as pd\nimport string\nimport os\nLETTERS = list(string.ascii_lowercase)\nOUTPUT_DIR = './output'\ndef task_func(file_path, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates a CSV file containing a 2D matrix populated with random lowercase letters.\n    The function writes a CSV file to the specified path.\n    \"\"\"\n    # Create a 2D matrix with random lowercase letters\n    matrix = np.random.choice(LETTERS, size=(10, 10))\n\n    # Convert the matrix to a DataFrame\n    df = pd.DataFrame(matrix)\n\n    # Write the DataFrame to a CSV file\n    df.to_csv(os.path.join(output_dir, file_path), index=False)", "raw_solution": "import numpy as np\nimport pandas as pd\nimport string\nimport os\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\nOUTPUT_DIR = './output'\n\ndef task_func(file_path, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates a CSV file containing a 2D matrix populated with random lowercase letters.\n    The function writes a CSV file to the specified path.\n    \"\"\"\n    # Create a 2D matrix with random lowercase letters\n    matrix = np.random.choice(LETTERS, size=(10, 10))\n\n    # Convert the matrix to a DataFrame\n    df = pd.DataFrame(matrix)\n\n    # Write the DataFrame to a CSV file\n    df.to_csv(os.path.join(output_dir, file_path), index=False)\n\n# Test the function\ntask_func('test.csv')"}
{"task_id": "BigCodeBench/603", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(matrix1, matrix2):\n    # Concatenate the two matrices along the second axis\n    result = np.concatenate((matrix1, matrix2), axis=1)\n    \n    # Convert the result into a Pandas DataFrame\n    df = pd.DataFrame(result)\n    \n    # Return the string representation of the DataFrame without the index and header\n    return df.to_string(header=False, index=False)\nmatrix1 = np.array([[1, 2, 3], [4, 5, 6]])\nmatrix2 = np.array([[7, 8, 9], [10, 11, 12]])", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(matrix1, matrix2):\n    # Concatenate the two matrices along the second axis\n    result = np.concatenate((matrix1, matrix2), axis=1)\n    \n    # Convert the result into a Pandas DataFrame\n    df = pd.DataFrame(result)\n    \n    # Return the string representation of the DataFrame without the index and header\n    return df.to_string(header=False, index=False)\n\n# Test the function\nmatrix1 = np.array([[1, 2, 3], [4, 5, 6]])\nmatrix2 = np.array([[7, 8, 9], [10, 11, 12]])"}
{"task_id": "BigCodeBench/604", "solution": "import subprocess\nimport logging\nimport os\ndef task_func(filepath):\n    \"\"\"\n    Attempts to compile a C++ file specified by 'filepath'. The output of the compilation process is logged, indicating whether the compilation was successful or not.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n\n    try:\n        # Use g++ as the compiler\n        subprocess.run(['g++', filepath, '-o', 'output'], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        logging.info(f\"Successfully compiled {filepath}\")\n    except subprocess.CalledProcessError as e:\n        logging.error(f\"Compilation of {filepath} failed with error: {e.output}\")\n        raise\n    except FileNotFoundError:\n        logging.error(f\"The compiler g++ is not found.\")\n        raise\n\n    return None", "raw_solution": "import subprocess\nimport logging\nimport os\n\ndef task_func(filepath):\n    \"\"\"\n    Attempts to compile a C++ file specified by 'filepath'. The output of the compilation process is logged, indicating whether the compilation was successful or not.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n\n    try:\n        # Use g++ as the compiler\n        subprocess.run(['g++', filepath, '-o', 'output'], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        logging.info(f\"Successfully compiled {filepath}\")\n    except subprocess.CalledProcessError as e:\n        logging.error(f\"Compilation of {filepath} failed with error: {e.output}\")\n        raise\n    except FileNotFoundError:\n        logging.error(f\"The compiler g++ is not found.\")\n        raise\n\n    return None"}
{"task_id": "BigCodeBench/605", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(matrix):\n    \"\"\"\n    Visualizes a 2D numeric array (matrix) as a heatmap using matplotlib, specifying a cmap for the color mapping and interpolation to control the pixel rendering.\n    \"\"\"\n    # Convert the matrix to a DataFrame\n    df = pd.DataFrame(matrix)\n\n    # Create a heatmap\n    ax = df.plot(kind='heatmap', cmap='hot', interpolation='nearest')\n\n    return ax\nmatrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(matrix):\n    \"\"\"\n    Visualizes a 2D numeric array (matrix) as a heatmap using matplotlib, specifying a cmap for the color mapping and interpolation to control the pixel rendering.\n    \"\"\"\n    # Convert the matrix to a DataFrame\n    df = pd.DataFrame(matrix)\n\n    # Create a heatmap\n    ax = df.plot(kind='heatmap', cmap='hot', interpolation='nearest')\n\n    return ax\n\n# Test the function\nmatrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nax = task_func(matrix)\nplt.show()"}
{"task_id": "BigCodeBench/606", "solution": "import pandas as pd\nfrom scipy import stats\ndef task_func(matrix):\n    \"\"\"\n    Normalizes a 2D numeric array (matrix) using the Z score.\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    df_zscore = pd.DataFrame(stats.zscore(df, axis=1), columns=df.columns)\n    return df_zscore\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]", "raw_solution": "import pandas as pd\nfrom scipy import stats\n\ndef task_func(matrix):\n    \"\"\"\n    Normalizes a 2D numeric array (matrix) using the Z score.\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    df_zscore = pd.DataFrame(stats.zscore(df, axis=1), columns=df.columns)\n    return df_zscore\n\n# Test the function\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/607", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    \"\"\"\n    Removes rows from a DataFrame based on column values and generates random scatter plots.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to process.\n        tuples (list): A list of tuples, where each tuple contains a column name and a value.\n        n_plots (int): The number of scatter plots to generate.\n\n    Returns:\n        pd.DataFrame: The DataFrame after removal of specified rows.\n        list: A list containing matplotlib Axes objects of the generated plots.\n    \"\"\"\n    # Remove rows based on column values\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Generate random scatter plots\n    axes = []\n    for _ in range(n_plots):\n        # Select random columns\n        cols = sample(COLUMNS, 2)\n\n        # Create scatter plot\n        ax = df.plot(kind='scatter', x=cols[0], y=cols[1], figsize=(6, 6))\n        axes.append(ax)\n\n    return df, axes", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants for column names to use in plots\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    \"\"\"\n    Removes rows from a DataFrame based on column values and generates random scatter plots.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to process.\n        tuples (list): A list of tuples, where each tuple contains a column name and a value.\n        n_plots (int): The number of scatter plots to generate.\n\n    Returns:\n        pd.DataFrame: The DataFrame after removal of specified rows.\n        list: A list containing matplotlib Axes objects of the generated plots.\n    \"\"\"\n    # Remove rows based on column values\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Generate random scatter plots\n    axes = []\n    for _ in range(n_plots):\n        # Select random columns\n        cols = sample(COLUMNS, 2)\n\n        # Create scatter plot\n        ax = df.plot(kind='scatter', x=cols[0], y=cols[1], figsize=(6, 6))\n        axes.append(ax)\n\n    return df, axes"}
{"task_id": "BigCodeBench/608", "solution": "import seaborn as sns\nimport pandas as pd\nfrom random import sample\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df, tuples, n_plots):\n    \"\"\"\n    Removes rows from a dataframe based on values of multiple columns, and then\n    creates n random pairs of two columns against each other to generate pairplots.\n\n    Args:\n        df (pandas.DataFrame): The input DataFrame.\n        tuples (list of tuples): A list of tuples, each containing a column name\n            and a value. Rows with values in these columns that match the specified\n            values will be removed.\n        n_plots (int): The number of pairplots to generate.\n\n    Returns:\n        tuple: A tuple containing:\n            DataFrame: The modified DataFrame after removing specified rows.\n            list of Axes: A list containing the generated pairplots.\n    \"\"\"\n    # Remove rows based on specified conditions\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Generate pairplots\n    pairplot_axes = []\n    for _ in range(n_plots):\n        pairplot_axes.append(sns.pairplot(df, vars=sample(COLUMNS, 2)))\n\n    return df, pairplot_axes", "raw_solution": "import seaborn as sns\nimport pandas as pd\nfrom random import sample\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df, tuples, n_plots):\n    \"\"\"\n    Removes rows from a dataframe based on values of multiple columns, and then\n    creates n random pairs of two columns against each other to generate pairplots.\n\n    Args:\n        df (pandas.DataFrame): The input DataFrame.\n        tuples (list of tuples): A list of tuples, each containing a column name\n            and a value. Rows with values in these columns that match the specified\n            values will be removed.\n        n_plots (int): The number of pairplots to generate.\n\n    Returns:\n        tuple: A tuple containing:\n            DataFrame: The modified DataFrame after removing specified rows.\n            list of Axes: A list containing the generated pairplots.\n    \"\"\"\n    # Remove rows based on specified conditions\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Generate pairplots\n    pairplot_axes = []\n    for _ in range(n_plots):\n        pairplot_axes.append(sns.pairplot(df, vars=sample(COLUMNS, 2)))\n\n    return df, pairplot_axes"}
{"task_id": "BigCodeBench/609", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\nfrom random import sample\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on tuples\n    for tup in tuples:\n        df = df[~df[tup].isin([None])]\n\n    # Generate scatter plots\n    plots = []\n    if n_plots > len(df.columns)**2:\n        n_plots = len(df.columns)**2\n    for cols in sample(list(combinations(df.columns, 2)), n_plots):\n        plt.figure()\n        plt.scatter(df[cols[0]], df[cols[1]])\n        plt.xlabel(cols[0])\n        plt.ylabel(cols[1])\n        plots.append((cols, plt.gcf()))\n\n    return df, plots", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\nfrom random import sample\n\ndef task_func(df, tuples, n_plots):\n    # Remove rows based on tuples\n    for tup in tuples:\n        df = df[~df[tup].isin([None])]\n\n    # Generate scatter plots\n    plots = []\n    if n_plots > len(df.columns)**2:\n        n_plots = len(df.columns)**2\n    for cols in sample(list(combinations(df.columns, 2)), n_plots):\n        plt.figure()\n        plt.scatter(df[cols[0]], df[cols[1]])\n        plt.xlabel(cols[0])\n        plt.ylabel(cols[1])\n        plots.append((cols, plt.gcf()))\n\n    return df, plots"}
{"task_id": "BigCodeBench/610", "solution": "from random import sample\nimport seaborn as sns\nimport pandas as pd\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    \"\"\"\n    Removes rows from a DataFrame based on values of multiple columns and then creates n random joint plots of two columns against each other.\n    \"\"\"\n    # Remove rows based on values of multiple columns\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Create joint plots\n    joint_plots = []\n    if not df.empty:\n        for _ in range(n_plots):\n            col1, col2 = sample(COLUMNS, 2)\n            joint_plot = sns.jointplot(x=df[col1], y=df[col2], kind='reg')\n            joint_plots.append(joint_plot)\n\n    return df, joint_plots", "raw_solution": "from random import sample\nimport seaborn as sns\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    \"\"\"\n    Removes rows from a DataFrame based on values of multiple columns and then creates n random joint plots of two columns against each other.\n    \"\"\"\n    # Remove rows based on values of multiple columns\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Create joint plots\n    joint_plots = []\n    if not df.empty:\n        for _ in range(n_plots):\n            col1, col2 = sample(COLUMNS, 2)\n            joint_plot = sns.jointplot(x=df[col1], y=df[col2], kind='reg')\n            joint_plots.append(joint_plot)\n\n    return df, joint_plots"}
{"task_id": "BigCodeBench/611", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df, tuples, n_plots):\n    \"\"\"\n    Removes rows from a DataFrame based on values of multiple columns, and then create n random line plots of two columns against each other.\n    \"\"\"\n    # Remove rows based on tuples\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Create n random line plots\n    plot_details = []\n    for _ in range(n_plots):\n        col1, col2 = sample(COLUMNS, 2)\n        plt.figure()\n        plt.plot(df[col1], df[col2])\n        plt.xlabel(col1)\n        plt.ylabel(col2)\n        plt.title(f'{col1} vs {col2}')\n        plot_details.append((col1, col2))\n\n    return df, plot_details", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df, tuples, n_plots):\n    \"\"\"\n    Removes rows from a DataFrame based on values of multiple columns, and then create n random line plots of two columns against each other.\n    \"\"\"\n    # Remove rows based on tuples\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Create n random line plots\n    plot_details = []\n    for _ in range(n_plots):\n        col1, col2 = sample(COLUMNS, 2)\n        plt.figure()\n        plt.plot(df[col1], df[col2])\n        plt.xlabel(col1)\n        plt.ylabel(col2)\n        plt.title(f'{col1} vs {col2}')\n        plot_details.append((col1, col2))\n\n    return df, plot_details"}
{"task_id": "BigCodeBench/612", "solution": "from random import choice\nimport numpy as np\nimport pandas as pd\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTIES_COSTS = [100, 200, 300, 400, 500]\ndef task_func(goals, penalties, teams=TEAMS, penalties_costs=PENALTIES_COSTS):\n    \"\"\"\n    Generates a performance report DataFrame for teams, detailing goals and penalties.\n    For each team, the function fetches goal and penalty counts, calculates 'Penalties Cost'\n    using a random multiplier from a predefined list, and computes a 'Performance Score'\n    as the non-negative difference between goals and penalties.\n    Returns a Dataframe with columns 'Team', 'Goals', 'Penalties', 'Penalties Cost' and 'Performance Score'.\n    \"\"\"\n    # Create a DataFrame from the input data\n    df = pd.DataFrame({\n        'Team': teams,\n        'Goals': goals,\n        'Penalties': penalties\n    })\n\n    # Calculate 'Penalties Cost' and 'Performance Score'\n    df['Penalties Cost'] = df['Penalties'].apply(lambda x: choice(penalties_costs) * x)\n    df['Performance Score'] = df['Goals'] - df['Penalties']\n\n    # Ensure 'Performance Score' is non-negative\n    df['Performance Score'] = df['Performance Score'].apply(lambda x: max(0, x))\n\n    return df", "raw_solution": "from random import choice\nimport numpy as np\nimport pandas as pd\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTIES_COSTS = [100, 200, 300, 400, 500]\n\ndef task_func(goals, penalties, teams=TEAMS, penalties_costs=PENALTIES_COSTS):\n    \"\"\"\n    Generates a performance report DataFrame for teams, detailing goals and penalties.\n    For each team, the function fetches goal and penalty counts, calculates 'Penalties Cost'\n    using a random multiplier from a predefined list, and computes a 'Performance Score'\n    as the non-negative difference between goals and penalties.\n    Returns a Dataframe with columns 'Team', 'Goals', 'Penalties', 'Penalties Cost' and 'Performance Score'.\n    \"\"\"\n    # Create a DataFrame from the input data\n    df = pd.DataFrame({\n        'Team': teams,\n        'Goals': goals,\n        'Penalties': penalties\n    })\n\n    # Calculate 'Penalties Cost' and 'Performance Score'\n    df['Penalties Cost'] = df['Penalties'].apply(lambda x: choice(penalties_costs) * x)\n    df['Performance Score'] = df['Goals'] - df['Penalties']\n\n    # Ensure 'Performance Score' is non-negative\n    df['Performance Score'] = df['Performance Score'].apply(lambda x: max(0, x))\n\n    return df"}
{"task_id": "BigCodeBench/613", "solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\ndef task_func(goals, penalties, clips):\n    \"\"\"\n    Calculates net scores for teams ('Team A' to 'Team E') by subtracting penalties from goals and clips scores to stay within -10 to 10.\n    Visualizes results with a bar chart showing each team's adjusted scores 'Team' on the x-axis and score values 'Score' on the y-axis.\n    \"\"\"\n    # Calculate net scores\n    scores = [goals[i] - penalties[i] - clips[i] for i in range(len(TEAMS))]\n\n    # Clip scores to range\n    scores = [max(min(score, 10), -10) for score in scores]\n\n    # Create DataFrame\n    df = pd.DataFrame({'Team': TEAMS, 'Score': scores})\n\n    # Visualize results\n    plt.bar(df['Team'], df['Score'])\n    plt.xlabel('Team')\n    plt.ylabel('Score')\n    plt.title('Net Scores for Teams')\n    plt.show()\n\n    return df", "raw_solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nGOALS_RANGE = (-10, 10)\n\ndef task_func(goals, penalties, clips):\n    \"\"\"\n    Calculates net scores for teams ('Team A' to 'Team E') by subtracting penalties from goals and clips scores to stay within -10 to 10.\n    Visualizes results with a bar chart showing each team's adjusted scores 'Team' on the x-axis and score values 'Score' on the y-axis.\n    \"\"\"\n    # Calculate net scores\n    scores = [goals[i] - penalties[i] - clips[i] for i in range(len(TEAMS))]\n\n    # Clip scores to range\n    scores = [max(min(score, 10), -10) for score in scores]\n\n    # Create DataFrame\n    df = pd.DataFrame({'Team': TEAMS, 'Score': scores})\n\n    # Visualize results\n    plt.bar(df['Team'], df['Score'])\n    plt.xlabel('Team')\n    plt.ylabel('Score')\n    plt.title('Net Scores for Teams')\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/614", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(goals, penalties):\n    \"\"\"\n    Function to visualize the distribution of goals and penalties for a number of teams.\n    Args:\n        goals (list): A list of goals for each team.\n        penalties (list): A list of penalties for each team.\n    Returns:\n        tuple: A tuple containing:\n            DataFrame: A pandas DataFrame with the goals and penalties for the teams.\n            Axes: A seaborn pairplot visualization of goals and penalties distribution for the teams.\n    \"\"\"\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Team': ['Team ' + str(i+1) for i in range(len(goals))],\n        'Goals': goals,\n        'Penalties': penalties\n    })\n\n    # Create a pairplot\n    pairplot = sns.pairplot(df, hue='Team')\n\n    return df, pairplot\ngoals = [10, 15, 20, 25, 30]\npenalties = [5, 10, 15, 20, 25]", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(goals, penalties):\n    \"\"\"\n    Function to visualize the distribution of goals and penalties for a number of teams.\n    Args:\n        goals (list): A list of goals for each team.\n        penalties (list): A list of penalties for each team.\n    Returns:\n        tuple: A tuple containing:\n            DataFrame: A pandas DataFrame with the goals and penalties for the teams.\n            Axes: A seaborn pairplot visualization of goals and penalties distribution for the teams.\n    \"\"\"\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Team': ['Team ' + str(i+1) for i in range(len(goals))],\n        'Goals': goals,\n        'Penalties': penalties\n    })\n\n    # Create a pairplot\n    pairplot = sns.pairplot(df, hue='Team')\n\n    return df, pairplot\n\n# Test the function\ngoals = [10, 15, 20, 25, 30]\npenalties = [5, 10, 15, 20, 25]\ndf, pairplot = task_func(goals, penalties)"}
{"task_id": "BigCodeBench/615", "solution": "from random import randint, seed\nimport pandas as pd\ndef task_func(teams, goals, penalties, fine_cost, rng_seed=None):\n    \"\"\"\n    Generates a Pandas DataFrame with columns 'Team' and 'Match Result' of the results of football matches for multiple teams,\n    incorporating random goals and penalties. Penalties are converted into fines using a predefined cost.\n\n    Parameters:\n    teams (list): A list of team names.\n    goals (tuple): A tuple representing the range of goals scored in a match.\n    penalties (tuple): A tuple representing the range of penalties scored in a match.\n    fine_cost (int): The cost of a penalty fine.\n    rng_seed (int, optional): A seed for the random number generator.\n\n    Returns:\n    pd.DataFrame: A pandas DataFrame with columns ['Team', 'Match Result'], detailing each team's goals and accumulated fines.\n    \"\"\"\n    # Set the random seed if provided\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Initialize an empty DataFrame\n    df = pd.DataFrame(columns=['Team', 'Match Result'])\n\n    # Generate random match results for each team\n    for team in teams:\n        # Generate random number of goals and penalties\n        goal_count = randint(*goals)\n        penalty_count = randint(*penalties)\n\n        # Calculate total fines\n        fine_count = penalty_count * fine_cost\n\n        # Generate match result string\n        match_result = f\"{goal_count} goals, {penalty_count} penalties, {fine_count} fines\"\n\n        # Append team and match result to DataFrame\n        df = df.append({'Team': team, 'Match Result': match_result}, ignore_index=True)\n\n    return df", "raw_solution": "from random import randint, seed\nimport pandas as pd\n\n# Method\ndef task_func(teams, goals, penalties, fine_cost, rng_seed=None):\n    \"\"\"\n    Generates a Pandas DataFrame with columns 'Team' and 'Match Result' of the results of football matches for multiple teams,\n    incorporating random goals and penalties. Penalties are converted into fines using a predefined cost.\n\n    Parameters:\n    teams (list): A list of team names.\n    goals (tuple): A tuple representing the range of goals scored in a match.\n    penalties (tuple): A tuple representing the range of penalties scored in a match.\n    fine_cost (int): The cost of a penalty fine.\n    rng_seed (int, optional): A seed for the random number generator.\n\n    Returns:\n    pd.DataFrame: A pandas DataFrame with columns ['Team', 'Match Result'], detailing each team's goals and accumulated fines.\n    \"\"\"\n    # Set the random seed if provided\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Initialize an empty DataFrame\n    df = pd.DataFrame(columns=['Team', 'Match Result'])\n\n    # Generate random match results for each team\n    for team in teams:\n        # Generate random number of goals and penalties\n        goal_count = randint(*goals)\n        penalty_count = randint(*penalties)\n\n        # Calculate total fines\n        fine_count = penalty_count * fine_cost\n\n        # Generate match result string\n        match_result = f\"{goal_count} goals, {penalty_count} penalties, {fine_count} fines\"\n\n        # Append team and match result to DataFrame\n        df = df.append({'Team': team, 'Match Result': match_result}, ignore_index=True)\n\n    return df"}
{"task_id": "BigCodeBench/616", "solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, teams=TEAMS, penalty_cost=PENALTY_COST, rng_seed=None):\n    \"\"\"\n    Generates a DataFrame with teams, their goals, and penalty costs, and creates a bar plot of the results.\n    Penalties are converted into fines according to the penalty costs.\n    \"\"\"\n    # Set the random seed for reproducibility\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Generate random goals and penalties for each team\n    data = {\n        'Team': teams,\n        'Goals': [randint(0, goals) for _ in teams],\n        'Penalty Cost': [randint(0, penalties) for _ in teams],\n    }\n\n    # Convert penalties into fines\n    data['Fines'] = [penalty * penalty_cost for penalty in data['Penalty Cost']]\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    # Create a bar plot of the results\n    fig, ax = plt.subplots()\n    ax.bar(df['Team'], df['Goals'], label='Goals')\n    ax.bar(df['Team'], df['Fines'], bottom=df['Goals'], label='Fines')\n    ax.set_xlabel('Team')\n    ax.set_ylabel('Score')\n    ax.legend()\n\n    return df, ax", "raw_solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants (they can be overridden with default parameters)\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, teams=TEAMS, penalty_cost=PENALTY_COST, rng_seed=None):\n    \"\"\"\n    Generates a DataFrame with teams, their goals, and penalty costs, and creates a bar plot of the results.\n    Penalties are converted into fines according to the penalty costs.\n    \"\"\"\n    # Set the random seed for reproducibility\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Generate random goals and penalties for each team\n    data = {\n        'Team': teams,\n        'Goals': [randint(0, goals) for _ in teams],\n        'Penalty Cost': [randint(0, penalties) for _ in teams],\n    }\n\n    # Convert penalties into fines\n    data['Fines'] = [penalty * penalty_cost for penalty in data['Penalty Cost']]\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    # Create a bar plot of the results\n    fig, ax = plt.subplots()\n    ax.bar(df['Team'], df['Goals'], label='Goals')\n    ax.bar(df['Team'], df['Fines'], bottom=df['Goals'], label='Fines')\n    ax.set_xlabel('Team')\n    ax.set_ylabel('Score')\n    ax.legend()\n\n    return df, ax"}
{"task_id": "BigCodeBench/617", "solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, rng_seed=None, teams=TEAMS):\n    \"\"\"\n    Generates a pandas DataFrame of football match results for multiple teams,\n    incorporating random goals and penalties, then visualizes the analyzed data.\n\n    Parameters:\n    goals (int): The number of goals per team.\n    penalties (int): The number of penalties per team.\n    rng_seed (int): The seed for the random number generator.\n    teams (list): The list of team names.\n\n    Returns:\n    pandas.DataFrame: A pandas DataFrame containing teams, their goals, and penalty costs.\n    \"\"\"\n    # Set the random number generator seed\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Generate random goals and penalties\n    team_goals = {team: randint(0, goals) for team in teams}\n    team_penalties = {team: randint(0, penalties) for team in teams}\n\n    # Calculate penalty costs\n    team_penalty_costs = {team: team_penalties[team] * PENALTY_COST for team in teams}\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(list(zip(teams, team_goals.values(), team_penalty_costs.values())),\n                      columns=['Team', 'Goals', 'Penalty Cost'])\n\n    # Visualize the data\n    df.plot(x='Team', y=['Goals', 'Penalty Cost'], kind='bar', figsize=(10, 5))\n    plt.ylabel('Value')\n    plt.title('Football Match Results')\n    plt.show()\n\n    return df", "raw_solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, rng_seed=None, teams=TEAMS):\n    \"\"\"\n    Generates a pandas DataFrame of football match results for multiple teams,\n    incorporating random goals and penalties, then visualizes the analyzed data.\n\n    Parameters:\n    goals (int): The number of goals per team.\n    penalties (int): The number of penalties per team.\n    rng_seed (int): The seed for the random number generator.\n    teams (list): The list of team names.\n\n    Returns:\n    pandas.DataFrame: A pandas DataFrame containing teams, their goals, and penalty costs.\n    \"\"\"\n    # Set the random number generator seed\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Generate random goals and penalties\n    team_goals = {team: randint(0, goals) for team in teams}\n    team_penalties = {team: randint(0, penalties) for team in teams}\n\n    # Calculate penalty costs\n    team_penalty_costs = {team: team_penalties[team] * PENALTY_COST for team in teams}\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(list(zip(teams, team_goals.values(), team_penalty_costs.values())),\n                      columns=['Team', 'Goals', 'Penalty Cost'])\n\n    # Visualize the data\n    df.plot(x='Team', y=['Goals', 'Penalty Cost'], kind='bar', figsize=(10, 5))\n    plt.ylabel('Value')\n    plt.title('Football Match Results')\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    \"\"\"\n    Generates a DataFrame of football match results for multiple teams, with random goals and penalties.\n    The function also visualizes the goals and penalty costs using seaborn.\n\n    Args:\n    goals (int): The number of goals per team.\n    penalties (int): The number of penalties per team.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing match results.\n    list: A list containing two seaborn plot objects (Axes) for goals and penalty costs.\n    \"\"\"\n    # Generate random goals and penalties\n    data = {\n        'Team': TEAMS,\n        'Goals': [randint(0, goals) for _ in range(len(TEAMS))],\n        'Penalty Cost': [randint(0, penalties) * PENALTY_COST for _ in range(len(TEAMS))]\n    }\n    df = pd.DataFrame(data)\n\n    # Create plots\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n    # Plot goals\n    sns.barplot(x='Team', y='Goals', data=df, ax=axes[0])\n    axes[0].set_title('Goals per Team')\n\n    # Plot penalty costs\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=axes[1])\n    axes[1].set_title('Penalty Costs per Team')\n\n    plt.tight_layout()\n\n    return df, axes", "raw_solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    \"\"\"\n    Generates a DataFrame of football match results for multiple teams, with random goals and penalties.\n    The function also visualizes the goals and penalty costs using seaborn.\n\n    Args:\n    goals (int): The number of goals per team.\n    penalties (int): The number of penalties per team.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing match results.\n    list: A list containing two seaborn plot objects (Axes) for goals and penalty costs.\n    \"\"\"\n    # Generate random goals and penalties\n    data = {\n        'Team': TEAMS,\n        'Goals': [randint(0, goals) for _ in range(len(TEAMS))],\n        'Penalty Cost': [randint(0, penalties) * PENALTY_COST for _ in range(len(TEAMS))]\n    }\n    df = pd.DataFrame(data)\n\n    # Create plots\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n    # Plot goals\n    sns.barplot(x='Team', y='Goals', data=df, ax=axes[0])\n    axes[0].set_title('Goals per Team')\n\n    # Plot penalty costs\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=axes[1])\n    axes[1].set_title('Penalty Costs per Team')\n\n    plt.tight_layout()\n\n    return df, axes"}
{"task_id": "BigCodeBench/619", "solution": "from random import randint, seed\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, rng_seed=None):\n    \"\"\"\n    Simulates football match results with random goals and penalties for multiple teams,\n    and trains a linear regression model to predict penalty costs from goals.\n\n    Args:\n        goals (int): Number of goals to simulate.\n        penalties (int): Number of penalties to simulate.\n        rng_seed (int, optional): Seed for the random number generator. Defaults to None.\n\n    Returns:\n        tuple:\n            pd.DataFrame: Contains 'Team', 'Goals', and 'Penalty Cost' columns.\n            LinearRegression: Trained model to predict 'Penalty Cost' based on 'Goals'.\n    \"\"\"\n    # Set the random number generator seed\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Simulate match results\n    data = []\n    for _ in range(goals + penalties):\n        team = TEAMS[randint(0, len(TEAMS) - 1)]\n        goal_or_penalty = randint(0, 1)\n        penalty_cost = PENALTY_COST if goal_or_penalty else 0\n        data.append([team, goal_or_penalty, penalty_cost])\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    # Train a linear regression model\n    X = df[['Goals']]\n    y = df['Penalty Cost']\n    model = LinearRegression().fit(X, y)\n\n    return df, model", "raw_solution": "from random import randint, seed\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, rng_seed=None):\n    \"\"\"\n    Simulates football match results with random goals and penalties for multiple teams,\n    and trains a linear regression model to predict penalty costs from goals.\n\n    Args:\n        goals (int): Number of goals to simulate.\n        penalties (int): Number of penalties to simulate.\n        rng_seed (int, optional): Seed for the random number generator. Defaults to None.\n\n    Returns:\n        tuple:\n            pd.DataFrame: Contains 'Team', 'Goals', and 'Penalty Cost' columns.\n            LinearRegression: Trained model to predict 'Penalty Cost' based on 'Goals'.\n    \"\"\"\n    # Set the random number generator seed\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Simulate match results\n    data = []\n    for _ in range(goals + penalties):\n        team = TEAMS[randint(0, len(TEAMS) - 1)]\n        goal_or_penalty = randint(0, 1)\n        penalty_cost = PENALTY_COST if goal_or_penalty else 0\n        data.append([team, goal_or_penalty, penalty_cost])\n\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['Team', 'Goals', 'Penalty Cost'])\n\n    # Train a linear regression model\n    X = df[['Goals']]\n    y = df['Penalty Cost']\n    model = LinearRegression().fit(X, y)\n\n    return df, model"}
{"task_id": "BigCodeBench/620", "solution": "import numpy as np\nimport pandas as pd\nRANGE = (1, 100)\ndef task_func(L):\n    \"\"\"\n    Generates a DataFrame filled with random integers. The dimensions of the DataFrame (number of rows and columns) are determined by multiplying pairs of integers from nested lists within the input list of lists 'L'.\n    \"\"\"\n    # Calculate the number of rows and columns\n    rows, cols = np.prod(L, axis=0)\n\n    # Generate a DataFrame filled with random integers\n    df = pd.DataFrame(np.random.randint(*RANGE, size=(rows, cols)))\n\n    return df\nL = [[2, 3], [4, 5]]", "raw_solution": "import numpy as np\nimport pandas as pd\n\n# Constants\nRANGE = (1, 100)\n\ndef task_func(L):\n    \"\"\"\n    Generates a DataFrame filled with random integers. The dimensions of the DataFrame (number of rows and columns) are determined by multiplying pairs of integers from nested lists within the input list of lists 'L'.\n    \"\"\"\n    # Calculate the number of rows and columns\n    rows, cols = np.prod(L, axis=0)\n\n    # Generate a DataFrame filled with random integers\n    df = pd.DataFrame(np.random.randint(*RANGE, size=(rows, cols)))\n\n    return df\n\n# Test the function\nL = [[2, 3], [4, 5]]"}
{"task_id": "BigCodeBench/621", "solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    # Flatten the list of lists\n    flat_list = list(chain(*L))\n\n    # Convert the list to a numpy array\n    arr = np.array(flat_list).reshape(-1, 1)\n\n    # Standardize the values\n    scaler = StandardScaler()\n    standardized_values = scaler.fit_transform(arr)\n\n    # Plot the standardized values\n    plt.plot(standardized_values)\n    plt.title('Standardized Values')\n    plt.xlabel('Index')\n    plt.ylabel('Standardized Value')\n    return plt.gca()", "raw_solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    # Flatten the list of lists\n    flat_list = list(chain(*L))\n\n    # Convert the list to a numpy array\n    arr = np.array(flat_list).reshape(-1, 1)\n\n    # Standardize the values\n    scaler = StandardScaler()\n    standardized_values = scaler.fit_transform(arr)\n\n    # Plot the standardized values\n    plt.plot(standardized_values)\n    plt.title('Standardized Values')\n    plt.xlabel('Index')\n    plt.ylabel('Standardized Value')\n    return plt.gca()"}
{"task_id": "BigCodeBench/622", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import chain\ndef task_func(L):\n    # Flatten the list of lists\n    flat_list = list(chain(*L))\n\n    # Fit a normal distribution to the data\n    mu, sigma = norm.fit(flat_list)\n\n    # Create a range of values for the normal distribution\n    x = np.linspace(min(flat_list), max(flat_list), 100)\n    p = norm.pdf(x, mu, sigma)\n\n    # Plot the histogram and the normal distribution\n    fig, ax = plt.subplots()\n    ax.hist(flat_list, bins=20, density=True, alpha=0.6, color='g')\n    ax.plot(x, p, 'k', linewidth=2)\n    ax.set_title('Histogram with fitted normal distribution')\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import chain\n\ndef task_func(L):\n    # Flatten the list of lists\n    flat_list = list(chain(*L))\n\n    # Fit a normal distribution to the data\n    mu, sigma = norm.fit(flat_list)\n\n    # Create a range of values for the normal distribution\n    x = np.linspace(min(flat_list), max(flat_list), 100)\n    p = norm.pdf(x, mu, sigma)\n\n    # Plot the histogram and the normal distribution\n    fig, ax = plt.subplots()\n    ax.hist(flat_list, bins=20, density=True, alpha=0.6, color='g')\n    ax.plot(x, p, 'k', linewidth=2)\n    ax.set_title('Histogram with fitted normal distribution')\n\n    return ax"}
{"task_id": "BigCodeBench/623", "solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    # Flatten the list of lists\n    flat_list = list(chain(*L))\n\n    # Convert the list to a numpy array\n    data = np.array(flat_list).reshape(-1, 1)\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=0).fit(data)\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(data)), data, c=kmeans.labels_, cmap='viridis')\n\n    return ax", "raw_solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    # Flatten the list of lists\n    flat_list = list(chain(*L))\n\n    # Convert the list to a numpy array\n    data = np.array(flat_list).reshape(-1, 1)\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=0).fit(data)\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(data)), data, c=kmeans.labels_, cmap='viridis')\n\n    return ax"}
{"task_id": "BigCodeBench/624", "solution": "from sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\nN_COMPONENTS = 2\ndef task_func(L):\n    \"\"\"\n    Converts a list of lists 'L' into a 2D numeric array, applies PCA to it and returns the PCA result and scatter plot.\n    \"\"\"\n    # Convert list of lists to numpy array\n    arr = np.array(L)\n\n    # Apply PCA\n    pca = PCA(n_components=N_COMPONENTS)\n    pca_result = pca.fit_transform(arr)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n\n    return pca_result, ax", "raw_solution": "from sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nN_COMPONENTS = 2\n\ndef task_func(L):\n    \"\"\"\n    Converts a list of lists 'L' into a 2D numeric array, applies PCA to it and returns the PCA result and scatter plot.\n    \"\"\"\n    # Convert list of lists to numpy array\n    arr = np.array(L)\n\n    # Apply PCA\n    pca = PCA(n_components=N_COMPONENTS)\n    pca_result = pca.fit_transform(arr)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n\n    return pca_result, ax"}
{"task_id": "BigCodeBench/625", "solution": "import math\nfrom random import randint\nimport pandas as pd", "raw_solution": "import math\nfrom random import randint\nimport pandas as pd\n\ndef generate_population_data(cities_list):\n    \"\"\"\n    Generates a DataFrame with population data for a list of cities.\n    The population is generated randomly and rounded up to the next thousand.\n    \"\"\"\n    population_data = []\n    for city in cities_list:\n        population = randint(1, 1000000)\n        population = math.ceil(population / 1000) * 1000\n        population_data.append({'City': city, 'Population': population})\n\n    df = pd.DataFrame(population_data)\n    return df\n\n# Test the function\ncities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\ndf = generate_population_data(cities)"}
{"task_id": "BigCodeBench/626", "solution": "from random import choice\nimport pytz\nfrom dateutil.parser import parse\nfrom dateutil.tz import tzutc\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\ndef task_func(date_str, from_tz):\n    \"\"\"\n    Converts a datetime string from a given timezone to a datetime string in a randomly chosen timezone.\n    \"\"\"\n    # Parse the input date string into a datetime object\n    dt = parse(date_str)\n\n    # Set the timezone of the datetime object to the given timezone\n    dt = dt.replace(tzinfo=pytz.timezone(from_tz))\n\n    # Randomly choose a timezone from the list of timezones\n    to_tz = choice(TIMEZONES)\n\n    # Convert the datetime object to the chosen timezone\n    dt = dt.astimezone(pytz.timezone(to_tz))\n\n    # Return the converted datetime string and the chosen timezone\n    return dt.strftime('%Y-%m-%d %H:%M:%S %Z%z'), to_tz\ndate_str = '2022-01-01 12:00:00'\nfrom_tz = 'America/New_York'", "raw_solution": "from random import choice\nimport pytz\nfrom dateutil.parser import parse\nfrom dateutil.tz import tzutc\n\n# Constants\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef task_func(date_str, from_tz):\n    \"\"\"\n    Converts a datetime string from a given timezone to a datetime string in a randomly chosen timezone.\n    \"\"\"\n    # Parse the input date string into a datetime object\n    dt = parse(date_str)\n\n    # Set the timezone of the datetime object to the given timezone\n    dt = dt.replace(tzinfo=pytz.timezone(from_tz))\n\n    # Randomly choose a timezone from the list of timezones\n    to_tz = choice(TIMEZONES)\n\n    # Convert the datetime object to the chosen timezone\n    dt = dt.astimezone(pytz.timezone(to_tz))\n\n    # Return the converted datetime string and the chosen timezone\n    return dt.strftime('%Y-%m-%d %H:%M:%S %Z%z'), to_tz\n\n# Test the function\ndate_str = '2022-01-01 12:00:00'\nfrom_tz = 'America/New_York'"}
{"task_id": "BigCodeBench/627", "solution": "from random import randint\nfrom statistics import mean\nimport pandas as pd", "raw_solution": "from random import randint\nfrom statistics import mean\nimport pandas as pd\n\ndef generate_sales_data(products_list):\n    sales_data = []\n    for product in products_list:\n        sales = [randint(1, 100) for _ in range(12)]\n        avg_sales = mean(sales)\n        sales_data.append({'Product': product, 'Month 1': sales[0], 'Month 2': sales[1], 'Month 3': sales[2],\n                           'Month 4': sales[3], 'Month 5': sales[4], 'Month 6': sales[5], 'Month 7': sales[6],\n                           'Month 8': sales[7], 'Month 9': sales[8], 'Month 10': sales[9], 'Month 11': sales[10],\n                           'Month 12': sales[11], 'Average Sales': avg_sales})\n    return pd.DataFrame(sales_data)\n\n# Test the function\nproducts = ['Product 1', 'Product 2', 'Product 3']\ndf = generate_sales_data(products)"}
{"task_id": "BigCodeBench/628", "solution": "import math\nfrom random import randint\nimport matplotlib.pyplot as plt", "raw_solution": "import math\nfrom random import randint\nimport matplotlib.pyplot as plt\n\ndef create_sine_wave(frequency=1, amplitude=1, phase_shift=0):\n    \"\"\"\n    Creates and draws a sine wave with a random frequency, amplitude and phase shift.\n    The function returns the ax object with 'Random Sine Wave' title, 'Time' on the x axis and 'Amplitude' on the y axis.\n    \"\"\"\n    # Generate random time values\n    time = [i/100 for i in range(100)]\n\n    # Calculate corresponding sine wave values\n    sine_wave = [amplitude * math.sin(2 * math.pi * frequency * t + phase_shift) for t in time]\n\n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(time, sine_wave)\n\n    # Set the title and labels\n    ax.set_title('Random Sine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n\n    return ax"}
{"task_id": "BigCodeBench/629", "solution": "import os\nimport pandas as pd\nOUTPUT_DIR = './output'\ndef task_func(dataset, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Writes multiple Pandas DataFrames to a single CSV file, separating each DataFrame by a line of hyphens (\"------\").\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filepath = os.path.join(output_dir, filename)\n\n    with open(filepath, 'w') as f:\n        for i, df in enumerate(dataset):\n            df.to_csv(f, index=False)\n            if i < len(dataset) - 1:\n                f.write('------\\n')\n\n    return None", "raw_solution": "import os\nimport pandas as pd\n\nOUTPUT_DIR = './output'\n\ndef task_func(dataset, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Writes multiple Pandas DataFrames to a single CSV file, separating each DataFrame by a line of hyphens (\"------\").\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filepath = os.path.join(output_dir, filename)\n\n    with open(filepath, 'w') as f:\n        for i, df in enumerate(dataset):\n            df.to_csv(f, index=False)\n            if i < len(dataset) - 1:\n                f.write('------\\n')\n\n    return None"}
{"task_id": "BigCodeBench/630", "solution": "import pandas as pd\nimport os\nOUTPUT_DIR = './output'\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Saves a Pandas DataFrame to a JSON file in a specified directory.\n    Args:\n        df (pd.DataFrame): The DataFrame to be saved.\n        filename (str): The name of the JSON file.\n        output_dir (str): The directory where the JSON file will be saved. Defaults to OUTPUT_DIR.\n    Returns:\n        str: The full file path where the DataFrame is saved.\n    \"\"\"\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Save the DataFrame to a JSON file\n    file_path = os.path.join(output_dir, filename)\n    df.to_json(file_path, orient='records')\n\n    return file_path", "raw_solution": "import pandas as pd\nimport os\n\nOUTPUT_DIR = './output'\n\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Saves a Pandas DataFrame to a JSON file in a specified directory.\n    Args:\n        df (pd.DataFrame): The DataFrame to be saved.\n        filename (str): The name of the JSON file.\n        output_dir (str): The directory where the JSON file will be saved. Defaults to OUTPUT_DIR.\n    Returns:\n        str: The full file path where the DataFrame is saved.\n    \"\"\"\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Save the DataFrame to a JSON file\n    file_path = os.path.join(output_dir, filename)\n    df.to_json(file_path, orient='records')\n\n    return file_path"}
{"task_id": "BigCodeBench/631", "solution": "import pandas as pd\nimport os", "raw_solution": "import pandas as pd\nimport os\n\nOUTPUT_DIR = './output'\n\ndef save_dataframe_to_csv(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Saves a Pandas DataFrame to a CSV file in a specified directory.\n    The CSV file will be saved in the 'data' directory relative to the parent directory of this script.\n    \"\"\"\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(os.path.join(output_dir, filename), index=False)\n\n    # Return the absolute path of the saved CSV file\n    return os.path.abspath(os.path.join(output_dir, filename))\n\n# Test the function\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6],\n    'C': [7, 8, 9]\n})"}
{"task_id": "BigCodeBench/632", "solution": "import pandas as pd\nimport json\nimport os\nOUTPUT_DIR = './output'\ndef task_func(df: pd.DataFrame, filename: str) -> str:\n    \"\"\"\n    Writes a Pandas DataFrame into a JSON Lines file and saves it in a specified directory.\n    The function returns the full path where the JSON Lines file was saved.\n    \"\"\"\n    # Ensure the output directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    # Construct the full path for the JSON Lines file\n    file_path = os.path.join(OUTPUT_DIR, filename)\n\n    # Write the DataFrame to a JSON Lines file\n    df.to_json(file_path, orient='records', lines=True)\n\n    return file_path", "raw_solution": "import pandas as pd\nimport json\nimport os\n\nOUTPUT_DIR = './output'\n\ndef task_func(df: pd.DataFrame, filename: str) -> str:\n    \"\"\"\n    Writes a Pandas DataFrame into a JSON Lines file and saves it in a specified directory.\n    The function returns the full path where the JSON Lines file was saved.\n    \"\"\"\n    # Ensure the output directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    # Construct the full path for the JSON Lines file\n    file_path = os.path.join(OUTPUT_DIR, filename)\n\n    # Write the DataFrame to a JSON Lines file\n    df.to_json(file_path, orient='records', lines=True)\n\n    return file_path"}
{"task_id": "BigCodeBench/633", "solution": "import re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\ndef task_func(text: str) -> dict:\n    \"\"\"\n    Analyzes a given text string by removing duplicate words and stopwords defined by nltk.corpus,\n    and then returns a frequency distribution of the remaining words.\n    \"\"\"\n    # Tokenize the text\n    words = word_tokenize(text)\n\n    # Remove punctuation\n    words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n\n    # Convert to lowercase\n    words = [word.lower() for word in words]\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in words if word not in stop_words]\n\n    # Remove duplicate words\n    words = list(set(words))\n\n    # Calculate frequency distribution\n    freq_dist = dict(Counter(words))\n\n    return freq_dist", "raw_solution": "import re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\n\ndef task_func(text: str) -> dict:\n    \"\"\"\n    Analyzes a given text string by removing duplicate words and stopwords defined by nltk.corpus,\n    and then returns a frequency distribution of the remaining words.\n    \"\"\"\n    # Tokenize the text\n    words = word_tokenize(text)\n\n    # Remove punctuation\n    words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n\n    # Convert to lowercase\n    words = [word.lower() for word in words]\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in words if word not in stop_words]\n\n    # Remove duplicate words\n    words = list(set(words))\n\n    # Calculate frequency distribution\n    freq_dist = dict(Counter(words))\n\n    return freq_dist"}
{"task_id": "BigCodeBench/634", "solution": "import itertools\nfrom typing import Any\nfrom scipy import stats\ndef task_func(input_list: list, repetitions: int) -> Any:\n    \"\"\"\n    Takes a list and a repetition count as input.\n    Flattens the list with multiple repetitions.\n    Calculates the mode of the flattened list.\n    \"\"\"\n    # Flatten the list with multiple repetitions\n    flattened_list = list(itertools.chain.from_iterable([input_list]*repetitions))\n\n    # Calculate the mode of the flattened list\n    mode_result = stats.mode(flattened_list)\n\n    return mode_result", "raw_solution": "import itertools\nfrom typing import Any\nfrom scipy import stats\n\ndef task_func(input_list: list, repetitions: int) -> Any:\n    \"\"\"\n    Takes a list and a repetition count as input.\n    Flattens the list with multiple repetitions.\n    Calculates the mode of the flattened list.\n    \"\"\"\n    # Flatten the list with multiple repetitions\n    flattened_list = list(itertools.chain.from_iterable([input_list]*repetitions))\n\n    # Calculate the mode of the flattened list\n    mode_result = stats.mode(flattened_list)\n\n    return mode_result"}
{"task_id": "BigCodeBench/635", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\ndef task_func(text, n=2):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in text.split() if word not in stop_words]\n\n    # Remove duplicate consecutive words\n    words = [words[i] for i in range(len(words)) if i == 0 or words[i] != words[i-1]]\n\n    # Generate a square co-occurrence matrix of words\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    matrix = vectorizer.fit_transform([' '.join(words)])\n    matrix = matrix.toarray()\n\n    # Create a DataFrame from the matrix\n    df = pd.DataFrame(matrix, columns=vectorizer.get_feature_names())\n\n    # Plot the co-occurrence matrix\n    plt.imshow(matrix, cmap='hot', interpolation='nearest')\n    plt.xticks(range(len(df.columns)), df.columns, rotation=90)\n    plt.yticks(range(len(df.index)), df.index)\n    plt.xlabel('Words')\n    plt.ylabel('Words')\n    plt.title('Co-occurrence Matrix')\n    plt.colorbar()\n    ax = plt.gca()\n\n    return (df, ax)", "raw_solution": "# Importing the required libraries\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\n\ndef task_func(text, n=2):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in text.split() if word not in stop_words]\n\n    # Remove duplicate consecutive words\n    words = [words[i] for i in range(len(words)) if i == 0 or words[i] != words[i-1]]\n\n    # Generate a square co-occurrence matrix of words\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    matrix = vectorizer.fit_transform([' '.join(words)])\n    matrix = matrix.toarray()\n\n    # Create a DataFrame from the matrix\n    df = pd.DataFrame(matrix, columns=vectorizer.get_feature_names())\n\n    # Plot the co-occurrence matrix\n    plt.imshow(matrix, cmap='hot', interpolation='nearest')\n    plt.xticks(range(len(df.columns)), df.columns, rotation=90)\n    plt.yticks(range(len(df.index)), df.index)\n    plt.xlabel('Words')\n    plt.ylabel('Words')\n    plt.title('Co-occurrence Matrix')\n    plt.colorbar()\n    ax = plt.gca()\n\n    return (df, ax)"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    \"\"\"\n    Generates a DataFrame with random integer values between 0 and 9 for a given number of rows.\n    Counts the non-zero values in each column and visualizes this information using a bar plot.\n    \"\"\"\n    # Generate DataFrame with random integer values\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.ne(0).sum()\n\n    # Create bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Non-zero count')\n    ax.set_title('Count of non-zero values in each column')\n\n    return (df, ax)", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    \"\"\"\n    Generates a DataFrame with random integer values between 0 and 9 for a given number of rows.\n    Counts the non-zero values in each column and visualizes this information using a bar plot.\n    \"\"\"\n    # Generate DataFrame with random integer values\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.ne(0).sum()\n\n    # Create bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Non-zero count')\n    ax.set_title('Count of non-zero values in each column')\n\n    return (df, ax)"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students):\n    np.random.seed(0)\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    grades = np.random.randint(50, 100, (num_students, len(courses)))\n    students = [f'Student_{i}' for i in range(1, num_students + 1)]\n\n    df = pd.DataFrame(grades, index=students, columns=courses)\n\n    avg_grades = df.mean()\n    passing_grades = (df >= 60).sum()\n\n    df_stats = pd.DataFrame({\n        'Average Grade': avg_grades,\n        'Passing Grades': passing_grades\n    })\n\n    fig, ax = plt.subplots()\n    df_stats.plot(kind='bar', ax=ax)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.legend(loc='upper left')\n\n    return df_stats, ax", "raw_solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students):\n    np.random.seed(0)\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    grades = np.random.randint(50, 100, (num_students, len(courses)))\n    students = [f'Student_{i}' for i in range(1, num_students + 1)]\n\n    df = pd.DataFrame(grades, index=students, columns=courses)\n\n    avg_grades = df.mean()\n    passing_grades = (df >= 60).sum()\n\n    df_stats = pd.DataFrame({\n        'Average Grade': avg_grades,\n        'Passing Grades': passing_grades\n    })\n\n    fig, ax = plt.subplots()\n    df_stats.plot(kind='bar', ax=ax)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.legend(loc='upper left')\n\n    return df_stats, ax"}
{"task_id": "BigCodeBench/638", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(num_teams=5, num_games=100):\n    teams = ['Team' + str(i) for i in range(1, num_teams + 1)]\n    games = ['Game' + str(i) for i in range(1, num_games + 1)]\n    scores = np.random.randint(0, 100, size=(num_teams, num_games))\n    df = pd.DataFrame(scores, index=teams, columns=games)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(num_teams=5, num_games=100):\n    teams = ['Team' + str(i) for i in range(1, num_teams + 1)]\n    games = ['Game' + str(i) for i in range(1, num_games + 1)]\n    scores = np.random.randint(0, 100, size=(num_teams, num_games))\n    df = pd.DataFrame(scores, index=teams, columns=games)\n    return df\n\n# Test the function\ndf = task_func(5, 10)"}
{"task_id": "BigCodeBench/639", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(num_samples=100, num_features=5):\n    # Generate a DataFrame with random values\n    np.random.seed(0)\n    data = np.random.rand(num_samples, num_features)\n    df = pd.DataFrame(data, columns=[f'Feature_{i}' for i in range(1, num_features + 1)])\n\n    # Calculate the correlation matrix\n    corr_matrix = df.corr()\n\n    # Visualize the correlation matrix using a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Heatmap')\n    plt.show()\n\n    return df, corr_matrix", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(num_samples=100, num_features=5):\n    # Generate a DataFrame with random values\n    np.random.seed(0)\n    data = np.random.rand(num_samples, num_features)\n    df = pd.DataFrame(data, columns=[f'Feature_{i}' for i in range(1, num_features + 1)])\n\n    # Calculate the correlation matrix\n    corr_matrix = df.corr()\n\n    # Visualize the correlation matrix using a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Heatmap')\n    plt.show()\n\n    return df, corr_matrix"}
{"task_id": "BigCodeBench/640", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nPRODUCTS = ['Product' + str(i) for i in range(1, 6)]\nMONTHS = ['Month' + str(i) for i in range(1, 13)]\ndef task_func():\n    # Generate random sales figures\n    np.random.seed(0)\n    sales_data = np.random.randint(100, 500, size=(12, 5))\n\n    # Create DataFrame\n    df = pd.DataFrame(sales_data, columns=PRODUCTS, index=MONTHS)\n\n    # Plot total sales per product\n    df.sum().plot(kind='line', title='Total Sales per Product')\n    plt.ylabel('Sales')\n    plt.show()\n\n    # Plot heatmap\n    sns.heatmap(df, annot=True, fmt=\"d\", cmap='coolwarm', cbar=True, title='Sales Heatmap')\n    plt.show()\n\n    return df", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nPRODUCTS = ['Product' + str(i) for i in range(1, 6)]\nMONTHS = ['Month' + str(i) for i in range(1, 13)]\n\ndef task_func():\n    # Generate random sales figures\n    np.random.seed(0)\n    sales_data = np.random.randint(100, 500, size=(12, 5))\n\n    # Create DataFrame\n    df = pd.DataFrame(sales_data, columns=PRODUCTS, index=MONTHS)\n\n    # Plot total sales per product\n    df.sum().plot(kind='line', title='Total Sales per Product')\n    plt.ylabel('Sales')\n    plt.show()\n\n    # Plot heatmap\n    sns.heatmap(df, annot=True, fmt=\"d\", cmap='coolwarm', cbar=True, title='Sales Heatmap')\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/641", "solution": "import os\nimport re\nimport pandas as pd\ndef task_func(pattern: str, directory: str, output_csv: str) -> pd.DataFrame:\n    \"\"\"\n    Searches for files in the specified directory that match a given regex pattern.\n    This function walks through the directory, matches filenames against the pattern,\n    and saves the matched file paths to a CSV file. It returns a DataFrame of these\n    paths with colomn 'File Path'.\n\n    :param pattern: The regex pattern to match against the filenames.\n    :param directory: The directory to search for files.\n    :param output_csv: The path to the CSV file to save the matched file paths.\n    :return: A DataFrame with a single column 'File Path' of matched paths.\n    \"\"\"\n    matched_paths = []\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(pattern, file):\n                matched_paths.append(os.path.join(root, file))\n\n    df = pd.DataFrame(matched_paths, columns=['File Path'])\n    df.to_csv(output_csv, index=False)\n\n    return df", "raw_solution": "import os\nimport re\nimport pandas as pd\n\ndef task_func(pattern: str, directory: str, output_csv: str) -> pd.DataFrame:\n    \"\"\"\n    Searches for files in the specified directory that match a given regex pattern.\n    This function walks through the directory, matches filenames against the pattern,\n    and saves the matched file paths to a CSV file. It returns a DataFrame of these\n    paths with colomn 'File Path'.\n\n    :param pattern: The regex pattern to match against the filenames.\n    :param directory: The directory to search for files.\n    :param output_csv: The path to the CSV file to save the matched file paths.\n    :return: A DataFrame with a single column 'File Path' of matched paths.\n    \"\"\"\n    matched_paths = []\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(pattern, file):\n                matched_paths.append(os.path.join(root, file))\n\n    df = pd.DataFrame(matched_paths, columns=['File Path'])\n    df.to_csv(output_csv, index=False)\n\n    return df"}
{"task_id": "BigCodeBench/642", "solution": "import os\nimport re\nimport hashlib\ndef task_func(directory: str, pattern: str = r\"(?<!Distillr)\\\\AcroTray\\.exe\") -> dict:\n    \"\"\"\n    Searches for files within the specified directory matching a given regex pattern and computes a SHA256 hash of each file's content.\n    The function returns a dictionary with file paths as keys and their SHA256 hashes as values.\n    \"\"\"\n    # Initialize an empty dictionary to store the file paths and their SHA256 hashes\n    file_hashes = {}\n\n    # Iterate over all files in the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file name matches the regex pattern\n            if re.search(pattern, file):\n                # Compute the SHA256 hash of the file's content\n                with open(os.path.join(root, file), 'rb') as f:\n                    file_hash = hashlib.sha256(f.read()).hexdigest()\n\n                # Add the file path and its SHA256 hash to the dictionary\n                file_hashes[os.path.join(root, file)] = file_hash\n\n    return file_hashes", "raw_solution": "import os\nimport re\nimport hashlib\n\ndef task_func(directory: str, pattern: str = r\"(?<!Distillr)\\\\AcroTray\\.exe\") -> dict:\n    \"\"\"\n    Searches for files within the specified directory matching a given regex pattern and computes a SHA256 hash of each file's content.\n    The function returns a dictionary with file paths as keys and their SHA256 hashes as values.\n    \"\"\"\n    # Initialize an empty dictionary to store the file paths and their SHA256 hashes\n    file_hashes = {}\n\n    # Iterate over all files in the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file name matches the regex pattern\n            if re.search(pattern, file):\n                # Compute the SHA256 hash of the file's content\n                with open(os.path.join(root, file), 'rb') as f:\n                    file_hash = hashlib.sha256(f.read()).hexdigest()\n\n                # Add the file path and its SHA256 hash to the dictionary\n                file_hashes[os.path.join(root, file)] = file_hash\n\n    return file_hashes"}
{"task_id": "BigCodeBench/643", "solution": "import re\nimport pandas as pd\nimport numpy as np\nDATA_PATTERN = r'>\\d+\\.\\d+<'\ndef task_func(dataframe, data_pattern=DATA_PATTERN):\n    \"\"\"\n    Extracts numeric data from a Pandas DataFrame based on a specific pattern.\n    The function searches each cell for occurrences of the regex pattern '>number<number>'\n    and replaces the cell content with the extracted numeric value. If no match is found,\n    the cell is replaced with NaN.\n    \"\"\"\n    # Iterate over each cell in the DataFrame\n    for col in dataframe.columns:\n        for i in range(len(dataframe[col])):\n            cell_value = dataframe.at[i, col]\n            # If the cell value matches the pattern, extract the numeric value\n            match = re.search(data_pattern, cell_value)\n            if match:\n                numeric_value = float(match.group(0)[1:-1])\n                dataframe.at[i, col] = numeric_value\n            else:\n                dataframe.at[i, col] = np.nan\n    return dataframe", "raw_solution": "import re\nimport pandas as pd\nimport numpy as np\n\n# Constants\nDATA_PATTERN = r'>\\d+\\.\\d+<'\n\ndef task_func(dataframe, data_pattern=DATA_PATTERN):\n    \"\"\"\n    Extracts numeric data from a Pandas DataFrame based on a specific pattern.\n    The function searches each cell for occurrences of the regex pattern '>number<number>'\n    and replaces the cell content with the extracted numeric value. If no match is found,\n    the cell is replaced with NaN.\n    \"\"\"\n    # Iterate over each cell in the DataFrame\n    for col in dataframe.columns:\n        for i in range(len(dataframe[col])):\n            cell_value = dataframe.at[i, col]\n            # If the cell value matches the pattern, extract the numeric value\n            match = re.search(data_pattern, cell_value)\n            if match:\n                numeric_value = float(match.group(0)[1:-1])\n                dataframe.at[i, col] = numeric_value\n            else:\n                dataframe.at[i, col] = np.nan\n    return dataframe"}
{"task_id": "BigCodeBench/644", "solution": "import os\nimport hashlib\nimport base64\nfrom Crypto.Cipher import AES\nfrom Crypto import Random\ndef task_func(filename, data, password):\n    def encrypt(raw, password):\n        private_key = hashlib.sha256(password.encode(\"utf-8\")).digest()\n        raw = pad(raw)\n        iv = Random.new().read(AES.block_size)\n        cipher = AES.new(private_key, AES.MODE_CBC, iv)\n        return base64.b64encode(iv + cipher.encrypt(raw))\n\n    def pad(s):\n        return s + b\"\\0\" * (AES.block_size - len(s) % AES.block_size)\n\n    if not os.path.exists(os.path.dirname(filename)):\n        os.makedirs(os.path.dirname(filename))\n\n    with open(filename, \"wb\") as file:\n        file.write(encrypt(data.encode(\"utf-8\"), password))\n\n    return \"Encrypted data written to file\"", "raw_solution": "import os\nimport hashlib\nimport base64\nfrom Crypto.Cipher import AES\nfrom Crypto import Random\n\ndef task_func(filename, data, password):\n    def encrypt(raw, password):\n        private_key = hashlib.sha256(password.encode(\"utf-8\")).digest()\n        raw = pad(raw)\n        iv = Random.new().read(AES.block_size)\n        cipher = AES.new(private_key, AES.MODE_CBC, iv)\n        return base64.b64encode(iv + cipher.encrypt(raw))\n\n    def pad(s):\n        return s + b\"\\0\" * (AES.block_size - len(s) % AES.block_size)\n\n    if not os.path.exists(os.path.dirname(filename)):\n        os.makedirs(os.path.dirname(filename))\n\n    with open(filename, \"wb\") as file:\n        file.write(encrypt(data.encode(\"utf-8\"), password))\n\n    return \"Encrypted data written to file\""}
{"task_id": "BigCodeBench/645", "solution": "import os\nimport pandas as pd\ndef task_func(filename: str) -> pd.DataFrame:\n    \"\"\"\n    Reads a CSV file into a pandas DataFrame and then deletes the entire contents of the original file.\n    If the CSV file does not exist, it raises a FileNotFoundError.\n    \"\"\"\n    if not os.path.isfile(filename):\n        raise FileNotFoundError(f\"The file {filename} does not exist.\")\n\n    df = pd.read_csv(filename)\n    os.remove(filename)\n\n    return df", "raw_solution": "import os\nimport pandas as pd\n\ndef task_func(filename: str) -> pd.DataFrame:\n    \"\"\"\n    Reads a CSV file into a pandas DataFrame and then deletes the entire contents of the original file.\n    If the CSV file does not exist, it raises a FileNotFoundError.\n    \"\"\"\n    if not os.path.isfile(filename):\n        raise FileNotFoundError(f\"The file {filename} does not exist.\")\n\n    df = pd.read_csv(filename)\n    os.remove(filename)\n\n    return df"}
{"task_id": "BigCodeBench/646", "solution": "import os\nimport pandas as pd\nfrom dateutil.parser import parse\nimport matplotlib.pyplot as plt\nOUTPUT_DIR = './output'\ndef task_func(csv_path=os.path.join(OUTPUT_DIR, 'data.csv'), date_column='date'):\n    # Read the CSV file\n    df = pd.read_csv(csv_path)\n\n    # Convert the date column to datetime objects\n    df[date_column] = df[date_column].apply(lambda x: parse(x))\n\n    # Extract the year from the datetime objects\n    df['year'] = df[date_column].apply(lambda x: x.year)\n\n    # Draw a histogram of the year distribution\n    plt.hist(df['year'], bins=10, edgecolor='black')\n    plt.xlabel('Year')\n    plt.ylabel('Frequency')\n    plt.title('Year Distribution')\n\n    return plt.gca()", "raw_solution": "import os\nimport pandas as pd\nfrom dateutil.parser import parse\nimport matplotlib.pyplot as plt\n\nOUTPUT_DIR = './output'\n\ndef task_func(csv_path=os.path.join(OUTPUT_DIR, 'data.csv'), date_column='date'):\n    # Read the CSV file\n    df = pd.read_csv(csv_path)\n\n    # Convert the date column to datetime objects\n    df[date_column] = df[date_column].apply(lambda x: parse(x))\n\n    # Extract the year from the datetime objects\n    df['year'] = df[date_column].apply(lambda x: x.year)\n\n    # Draw a histogram of the year distribution\n    plt.hist(df['year'], bins=10, edgecolor='black')\n    plt.xlabel('Year')\n    plt.ylabel('Frequency')\n    plt.title('Year Distribution')\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/647", "solution": "from datetime import datetime\nfrom dateutil.parser import parse\nfrom dateutil.tz import tzutc, tzoffset\nimport pytz\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the date string into a datetime object\n    dt = parse(date_str)\n\n    # Convert the datetime object to the from_tz timezone\n    dt = dt.replace(tzinfo=pytz.timezone(from_tz))\n\n    # Convert the datetime object to the to_tz timezone\n    dt = dt.astimezone(pytz.timezone(to_tz))\n\n    # Calculate the time difference in seconds to the current time in the to_tz timezone\n    diff_seconds = (dt - datetime.now(pytz.timezone(to_tz))).total_seconds()\n\n    return int(diff_seconds)", "raw_solution": "from datetime import datetime\nfrom dateutil.parser import parse\nfrom dateutil.tz import tzutc, tzoffset\nimport pytz\n\ndef task_func(date_str, from_tz, to_tz):\n    # Parse the date string into a datetime object\n    dt = parse(date_str)\n\n    # Convert the datetime object to the from_tz timezone\n    dt = dt.replace(tzinfo=pytz.timezone(from_tz))\n\n    # Convert the datetime object to the to_tz timezone\n    dt = dt.astimezone(pytz.timezone(to_tz))\n\n    # Calculate the time difference in seconds to the current time in the to_tz timezone\n    diff_seconds = (dt - datetime.now(pytz.timezone(to_tz))).total_seconds()\n\n    return int(diff_seconds)\n\n# Test the function"}
{"task_id": "BigCodeBench/648", "solution": "from dateutil.parser import parse\nfrom datetime import timedelta", "raw_solution": "from dateutil.parser import parse\nfrom datetime import timedelta\n\ndef next_business_day(date_str):\n    \"\"\"\n    Calculates the next business day (Mon-Fri) after a certain date string.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Calculate the next day\n    next_day = date + timedelta(days=1)\n\n    # If the next day is a weekend, add 2 days to get the next business day\n    if next_day.weekday() > 4:  # 0-Monday, 1-Tuesday, ..., 4-Friday, 5-Saturday, 6-Sunday\n        next_day = next_day + timedelta(days=2)\n\n    return next_day\n\n# Test the function"}
{"task_id": "BigCodeBench/649", "solution": "import numpy as np\nimport pandas as pd\nfrom dateutil.parser import parse\ndef task_func(dates_str_list):\n    # Convert the date strings to datetime objects\n    dates = [parse(date_str) for date_str in dates_str_list]\n\n    # Calculate the weekday for each date\n    weekdays = [date.weekday() for date in dates]\n\n    # Create a pandas Series from the weekday counts\n    series = pd.Series(weekdays).value_counts().sort_index()\n\n    # Map the index to weekday names\n    series.index = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n    return series", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom dateutil.parser import parse\n\ndef task_func(dates_str_list):\n    # Convert the date strings to datetime objects\n    dates = [parse(date_str) for date_str in dates_str_list]\n\n    # Calculate the weekday for each date\n    weekdays = [date.weekday() for date in dates]\n\n    # Create a pandas Series from the weekday counts\n    series = pd.Series(weekdays).value_counts().sort_index()\n\n    # Map the index to weekday names\n    series.index = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n    return series"}
{"task_id": "BigCodeBench/650", "solution": "from datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\ndef task_func(date_str, tz_str):\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Set the timezone of the date\n    date = date.replace(tzinfo=pytz.timezone(tz_str))\n\n    # Calculate the time until the next New Year in the specified timezone\n    next_year = date.year + 1\n    next_year_date = datetime(next_year, 1, 1, 0, 0, 0, tzinfo=pytz.timezone(tz_str))\n    time_until_next_year = (next_year_date - date).total_seconds()\n\n    return int(time_until_next_year)", "raw_solution": "from datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\n\ndef task_func(date_str, tz_str):\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Set the timezone of the date\n    date = date.replace(tzinfo=pytz.timezone(tz_str))\n\n    # Calculate the time until the next New Year in the specified timezone\n    next_year = date.year + 1\n    next_year_date = datetime(next_year, 1, 1, 0, 0, 0, tzinfo=pytz.timezone(tz_str))\n    time_until_next_year = (next_year_date - date).total_seconds()\n\n    return int(time_until_next_year)\n\n# Test the function"}
{"task_id": "BigCodeBench/651", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, target_value):\n    # Convert the input dictionary of lists to a DataFrame\n    df = pd.DataFrame(df)\n\n    # Search for rows with cells equal to the target_value\n    target_rows = df[df == target_value]\n\n    # Count the number of such rows per column\n    counts = target_rows.count()\n\n    # Plot the count of such rows per column\n    if not df.empty:\n        counts.plot(kind='bar')\n        plt.ylabel('Count')\n        plt.title('Count of Rows with Target Value per Column')\n        ax = plt.gca()\n    else:\n        ax = None\n\n    return counts, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_value):\n    # Convert the input dictionary of lists to a DataFrame\n    df = pd.DataFrame(df)\n\n    # Search for rows with cells equal to the target_value\n    target_rows = df[df == target_value]\n\n    # Count the number of such rows per column\n    counts = target_rows.count()\n\n    # Plot the count of such rows per column\n    if not df.empty:\n        counts.plot(kind='bar')\n        plt.ylabel('Count')\n        plt.title('Count of Rows with Target Value per Column')\n        ax = plt.gca()\n    else:\n        ax = None\n\n    return counts, ax"}
{"task_id": "BigCodeBench/652", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nTARGET_VALUE = '332'\nARRAY = np.array([['0', '1', '2'], ['a', 'bb', 'ccc'], ['332', '33', '2'], ['33', '22', '332']])\ndef task_func(target_value=TARGET_VALUE, array=ARRAY):\n    # Find the row indices where the first cell matches target_value\n    indices = np.where(array[:, 0] == target_value)[0]\n\n    if indices.size == 0:\n        # If no target value found, return 'N/A' for all stats\n        return 'N/A', 'N/A', 'N/A', 'N/A'\n    else:\n        # Perform statistical analysis on the indices\n        mean = np.mean(indices)\n        variance = np.var(indices)\n        skewness = stats.skew(indices)\n        kurtosis = stats.kurtosis(indices)\n\n        # Plot the distribution of the indices\n        plt.hist(indices, bins='auto', color='#007acc')\n        plt.title(f'Distribution of Indices for \"{target_value}\"')\n        plt.xlabel('Index')\n        plt.ylabel('Frequency')\n        plt.show()\n\n        return mean, variance, skewness, kurtosis", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Constants\nTARGET_VALUE = '332'\nARRAY = np.array([['0', '1', '2'], ['a', 'bb', 'ccc'], ['332', '33', '2'], ['33', '22', '332']])\n\ndef task_func(target_value=TARGET_VALUE, array=ARRAY):\n    # Find the row indices where the first cell matches target_value\n    indices = np.where(array[:, 0] == target_value)[0]\n\n    if indices.size == 0:\n        # If no target value found, return 'N/A' for all stats\n        return 'N/A', 'N/A', 'N/A', 'N/A'\n    else:\n        # Perform statistical analysis on the indices\n        mean = np.mean(indices)\n        variance = np.var(indices)\n        skewness = stats.skew(indices)\n        kurtosis = stats.kurtosis(indices)\n\n        # Plot the distribution of the indices\n        plt.hist(indices, bins='auto', color='#007acc')\n        plt.title(f'Distribution of Indices for \"{target_value}\"')\n        plt.xlabel('Index')\n        plt.ylabel('Frequency')\n        plt.show()\n\n        return mean, variance, skewness, kurtosis\n\n# Test the function"}
{"task_id": "BigCodeBench/653", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(dataframe, target_value='332'):\n    \"\"\"\n    Searches a given DataFrame for occurrences of a specified target value and visualizes these occurrences using a heatmap.\n    \"\"\"\n    # Create a Boolean DataFrame indicating the presence of the target value\n    target_df = dataframe == target_value\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(target_df, cmap='YlGnBu', cbar=False)\n    plt.title(f'Heatmap of occurrences of \"{target_value}\"')\n    plt.xlabel('Columns')\n    plt.ylabel('Rows')\n\n    return target_df, plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(dataframe, target_value='332'):\n    \"\"\"\n    Searches a given DataFrame for occurrences of a specified target value and visualizes these occurrences using a heatmap.\n    \"\"\"\n    # Create a Boolean DataFrame indicating the presence of the target value\n    target_df = dataframe == target_value\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(target_df, cmap='YlGnBu', cbar=False)\n    plt.title(f'Heatmap of occurrences of \"{target_value}\"')\n    plt.xlabel('Columns')\n    plt.ylabel('Rows')\n\n    return target_df, plt.gca()"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    \"\"\"\n    Fits an exponential decay function to the indices in the array where the first column matches the target value.\n    \"\"\"\n    # Extract the indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n\n    # Define the exponential decay function\n    def exponential_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Perform the curve fitting\n    popt, _ = optimize.curve_fit(exponential_decay, indices, array[indices, 1])\n\n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.plot(indices, array[indices, 1], 'o', label='data')\n    ax.plot(indices, exponential_decay(indices, *popt), '-', label='fit')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return (popt, ax)", "raw_solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    \"\"\"\n    Fits an exponential decay function to the indices in the array where the first column matches the target value.\n    \"\"\"\n    # Extract the indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n\n    # Define the exponential decay function\n    def exponential_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Perform the curve fitting\n    popt, _ = optimize.curve_fit(exponential_decay, indices, array[indices, 1])\n\n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.plot(indices, array[indices, 1], 'o', label='data')\n    ax.plot(indices, exponential_decay(indices, *popt), '-', label='fit')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return (popt, ax)"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords", "raw_solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    # Remove non-alphanumeric characters (excluding spaces)\n    text = ALPHANUMERIC.sub(' ', text)\n    # Convert to lowercase\n    text = text.lower()\n    # Remove stopwords\n    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n    return text\n\ndef topic_extraction(texts, num_topics):\n    # Preprocess texts\n    texts = [preprocess_text(text) for text in texts]\n    # Vectorize texts using TF-IDF\n    vectorizer = TfidfVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(texts)\n    # Apply NMF to extract topics\n    nmf = NMF(n_components=num_topics, random_state=42, alpha=.1, l1_ratio=.5)\n    nmf.fit(X)\n    # Extract most significant words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_features_ind = topic.argsort()[:-num_topics - 1:-1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        topics.append(top_features)\n    return topics"}
{"task_id": "BigCodeBench/656", "solution": "import re\nimport string\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nALPHANUMERIC = re.compile('[\\W_]+')\nPUNCTUATIONS = string.punctuation\ndef task_func(text: str, sia: SentimentIntensityAnalyzer) -> dict:\n    # Clean the text\n    text = text.lower()\n    text = ALPHANUMERIC.sub(' ', text)\n    text = ''.join(c for c in text if c not in PUNCTUATIONS)\n\n    # Analyze the sentiment\n    sentiment = sia.polarity_scores(text)\n\n    return sentiment", "raw_solution": "import re\nimport string\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nnltk.download('vader_lexicon')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nPUNCTUATIONS = string.punctuation\n\ndef task_func(text: str, sia: SentimentIntensityAnalyzer) -> dict:\n    # Clean the text\n    text = text.lower()\n    text = ALPHANUMERIC.sub(' ', text)\n    text = ''.join(c for c in text if c not in PUNCTUATIONS)\n\n    # Analyze the sentiment\n    sentiment = sia.polarity_scores(text)\n\n    return sentiment"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generates word vectors from a list of texts using the gensim Word2Vec model and nltk.corpus.stopwords.\n    The texts are first cleaned by removing all non-alphanumeric characters except space, lowercased, and stop words are removed.\n    \"\"\"\n    # If no stopwords are provided, use nltk's default stopwords\n    if stopwords is None:\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n\n    # Clean the texts\n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space\n        text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase and split into words\n        words = text.lower().split()\n        # Remove stopwords\n        words = [word for word in words if word not in stopwords]\n        cleaned_texts.append(words)\n\n    # Train the Word2Vec model\n    model = Word2Vec(cleaned_texts, min_count=1)\n\n    return model", "raw_solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generates word vectors from a list of texts using the gensim Word2Vec model and nltk.corpus.stopwords.\n    The texts are first cleaned by removing all non-alphanumeric characters except space, lowercased, and stop words are removed.\n    \"\"\"\n    # If no stopwords are provided, use nltk's default stopwords\n    if stopwords is None:\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n\n    # Clean the texts\n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space\n        text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase and split into words\n        words = text.lower().split()\n        # Remove stopwords\n        words = [word for word in words if word not in stopwords]\n        cleaned_texts.append(words)\n\n    # Train the Word2Vec model\n    model = Word2Vec(cleaned_texts, min_count=1)\n\n    return model"}
{"task_id": "BigCodeBench/658", "solution": "import re\nimport nltk\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(texts):\n    \"\"\"\n    Creates a document-term matrix (DTM) from a list of text documents using CountVectorizer from Scikit-learn.\n    Texts are preprocessed by removing non-alphanumeric characters (excluding spaces), converting to lowercase,\n    and excluding English stop words defined in NLTK.\n    \"\"\"\n    # Preprocess the texts\n    texts = [ALPHANUMERIC.sub(' ', text.lower()) for text in texts]\n    texts = [' '.join([word for word in text.split() if word not in STOPWORDS]) for text in texts]\n\n    # Create a CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the texts\n    dtm = vectorizer.fit_transform(texts)\n\n    # Convert the sparse matrix to a DataFrame\n    dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return dtm_df", "raw_solution": "import re\nimport nltk\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\n\n# Make sure to download NLTK stopwords\nnltk.download('stopwords')\n\n# Define a regex pattern for matching all non-alphanumeric characters\nALPHANUMERIC = re.compile('[\\W_]+')\n\n# Load NLTK's list of English stop words\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(texts):\n    \"\"\"\n    Creates a document-term matrix (DTM) from a list of text documents using CountVectorizer from Scikit-learn.\n    Texts are preprocessed by removing non-alphanumeric characters (excluding spaces), converting to lowercase,\n    and excluding English stop words defined in NLTK.\n    \"\"\"\n    # Preprocess the texts\n    texts = [ALPHANUMERIC.sub(' ', text.lower()) for text in texts]\n    texts = [' '.join([word for word in text.split() if word not in STOPWORDS]) for text in texts]\n\n    # Create a CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the texts\n    dtm = vectorizer.fit_transform(texts)\n\n    # Convert the sparse matrix to a DataFrame\n    dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return dtm_df"}
{"task_id": "BigCodeBench/659", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\ndef task_func(x, y, labels):\n    fig, ax = plt.subplots(figsize=(10, 7))\n\n    for i in range(len(x)):\n        mu, sigma = x[i], y[i]\n        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n        plt.plot(x, stats.norm.pdf(x, mu, sigma), label=labels[i])\n\n    plt.xlabel('x')\n    plt.ylabel('Probability Density')\n    plt.title('Normal Distribution for Multiple Chemical Compounds')\n    plt.legend()\n\n    return fig", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\ndef task_func(x, y, labels):\n    fig, ax = plt.subplots(figsize=(10, 7))\n\n    for i in range(len(x)):\n        mu, sigma = x[i], y[i]\n        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n        plt.plot(x, stats.norm.pdf(x, mu, sigma), label=labels[i])\n\n    plt.xlabel('x')\n    plt.ylabel('Probability Density')\n    plt.title('Normal Distribution for Multiple Chemical Compounds')\n    plt.legend()\n\n    return fig"}
{"task_id": "BigCodeBench/660", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(x, y, labels):\n    \"\"\"\n    Scales the \"x\" and \"y\" arrays using the standard scaler of sklearn and plots them with given labels.\n    Each pair of x and y arrays are scaled independently and plotted as a separate series with a label.\n    \"\"\"\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Scale x and y arrays\n    x_scaled = scaler.fit_transform(x)\n    y_scaled = scaler.fit_transform(y)\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot scaled x and y arrays with corresponding labels\n    for i in range(len(x_scaled)):\n        ax.plot(x_scaled[i], y_scaled[i], label=labels[i])\n\n    # Add legend\n    ax.legend()\n\n    return fig", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(x, y, labels):\n    \"\"\"\n    Scales the \"x\" and \"y\" arrays using the standard scaler of sklearn and plots them with given labels.\n    Each pair of x and y arrays are scaled independently and plotted as a separate series with a label.\n    \"\"\"\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Scale x and y arrays\n    x_scaled = scaler.fit_transform(x)\n    y_scaled = scaler.fit_transform(y)\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot scaled x and y arrays with corresponding labels\n    for i in range(len(x_scaled)):\n        ax.plot(x_scaled[i], y_scaled[i], label=labels[i])\n\n    # Add legend\n    ax.legend()\n\n    return fig"}
{"task_id": "BigCodeBench/661", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nLABELS = ['H\\u2082O', 'O\\u2082', 'CO\\u2082', 'N\\u2082', 'Ar']\ndef task_func(x, y, labels):\n    \"\"\"\n    Creates a heatmap using the seaborn library for \"x\" as x-values and \"y\" as y-values with labels.\n    The function returns a seaborn heatmap object and the dataframe used to create the heatmap.\n    \"\"\"\n    # Create a DataFrame from the input data\n    df = pd.DataFrame({'x': x, 'y': y}, columns=['x', 'y'])\n\n    # Create a heatmap\n    ax = sns.heatmap(df, annot=True, fmt=\".2f\", cmap='viridis', xticklabels=labels, yticklabels=labels)\n\n    return ax, df\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([1, 2, 3, 4, 5])\nlabels = LABELS", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nLABELS = ['H\\u2082O', 'O\\u2082', 'CO\\u2082', 'N\\u2082', 'Ar']\n\ndef task_func(x, y, labels):\n    \"\"\"\n    Creates a heatmap using the seaborn library for \"x\" as x-values and \"y\" as y-values with labels.\n    The function returns a seaborn heatmap object and the dataframe used to create the heatmap.\n    \"\"\"\n    # Create a DataFrame from the input data\n    df = pd.DataFrame({'x': x, 'y': y}, columns=['x', 'y'])\n\n    # Create a heatmap\n    ax = sns.heatmap(df, annot=True, fmt=\".2f\", cmap='viridis', xticklabels=labels, yticklabels=labels)\n\n    return ax, df\n\n# Test the function\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([1, 2, 3, 4, 5])\nlabels = LABELS\nax, df = task_func(x, y, labels)"}
{"task_id": "BigCodeBench/662", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(x, y, labels):\n    # Combine x and y into a single array\n    data = np.column_stack((x, y))\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    pca.fit(data)\n\n    # Transform the data\n    transformed_data = pca.transform(data)\n\n    # Create a scatter plot\n    plt.scatter(transformed_data[:, 0], transformed_data[:, 1], c=labels)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.title('PCA on x and y')\n\n    # Return the figure\n    fig = plt.gcf()\n    return fig", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(x, y, labels):\n    # Combine x and y into a single array\n    data = np.column_stack((x, y))\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    pca.fit(data)\n\n    # Transform the data\n    transformed_data = pca.transform(data)\n\n    # Create a scatter plot\n    plt.scatter(transformed_data[:, 0], transformed_data[:, 1], c=labels)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.title('PCA on x and y')\n\n    # Return the figure\n    fig = plt.gcf()\n    return fig"}
{"task_id": "BigCodeBench/663", "solution": "import numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\ndef exponential_func(x, a, b, c):\n    return a * np.exp(-b * x) + c\ndef task_func(x, y, labels):\n    fig, ax = plt.subplots()\n\n    for i in range(len(x)):\n        popt, _ = curve_fit(exponential_func, x[i], y[i])\n        ax.plot(x[i], y[i], 'o', label=labels[i])\n        ax.plot(x[i], exponential_func(x[i], *popt), '-', label=f'{labels[i]}: f(x) = {popt[0]:.2f} * exp(-{popt[1]:.2f} * x) + {popt[2]:.2f}')\n\n    ax.legend()\n    plt.show()\n    return fig", "raw_solution": "import numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\ndef exponential_func(x, a, b, c):\n    return a * np.exp(-b * x) + c\n\ndef task_func(x, y, labels):\n    fig, ax = plt.subplots()\n\n    for i in range(len(x)):\n        popt, _ = curve_fit(exponential_func, x[i], y[i])\n        ax.plot(x[i], y[i], 'o', label=labels[i])\n        ax.plot(x[i], exponential_func(x[i], *popt), '-', label=f'{labels[i]}: f(x) = {popt[0]:.2f} * exp(-{popt[1]:.2f} * x) + {popt[2]:.2f}')\n\n    ax.legend()\n    plt.show()\n    return fig"}
{"task_id": "BigCodeBench/664", "solution": "import statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(sales_data):\n    \"\"\"\n    Plots sales trends for five products over a year, highlighting variability with standard deviation shading.\n    \"\"\"\n    # Extract product names and sales data\n    product_names = list(sales_data.keys())\n    sales_values = list(sales_data.values())\n\n    # Calculate mean and standard deviation for each product\n    means = [statistics.mean(sales) for sales in sales_values]\n    std_devs = [statistics.stdev(sales) for sales in sales_values]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot sales data\n    for i in range(len(product_names)):\n        ax.plot('Month', sales_values[i], data=sales_data, label=product_names[i])\n\n    # Add shading for standard deviation\n    for i in range(len(product_names)):\n        ax.fill_between('Month', np.array(means[i]) - np.array(std_devs[i]), np.array(means[i]) + np.array(std_devs[i]), alpha=0.2, label=f'{product_names[i]} Std. Dev.')\n\n    # Set labels and title\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Trends for Five Products Over a Year')\n    ax.legend()\n\n    return ax", "raw_solution": "import statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(sales_data):\n    \"\"\"\n    Plots sales trends for five products over a year, highlighting variability with standard deviation shading.\n    \"\"\"\n    # Extract product names and sales data\n    product_names = list(sales_data.keys())\n    sales_values = list(sales_data.values())\n\n    # Calculate mean and standard deviation for each product\n    means = [statistics.mean(sales) for sales in sales_values]\n    std_devs = [statistics.stdev(sales) for sales in sales_values]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot sales data\n    for i in range(len(product_names)):\n        ax.plot('Month', sales_values[i], data=sales_data, label=product_names[i])\n\n    # Add shading for standard deviation\n    for i in range(len(product_names)):\n        ax.fill_between('Month', np.array(means[i]) - np.array(std_devs[i]), np.array(means[i]) + np.array(std_devs[i]), alpha=0.2, label=f'{product_names[i]} Std. Dev.')\n\n    # Set labels and title\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Trends for Five Products Over a Year')\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/665", "solution": "import shutil\nimport os\nimport fnmatch\nimport itertools\ndef task_func(src_dir, dst_dir):\n    \"\"\"\n    Copies all files from 'src_dir' to 'dst_dir' that match any pattern in ['*.txt', '*.docx'].\n    \"\"\"\n    # Define the file patterns\n    patterns = ['*.txt', '*.docx']\n\n    # Iterate over all patterns\n    for pattern in patterns:\n        # Find all files that match the pattern in the source directory\n        for file_name in fnmatch.filter(os.listdir(src_dir), pattern):\n            # Construct the full file path\n            src_file = os.path.join(src_dir, file_name)\n            # Copy the file to the destination directory\n            shutil.copy2(src_file, dst_dir)\n\n    # Return the destination directory\n    return dst_dir", "raw_solution": "import shutil\nimport os\nimport fnmatch\nimport itertools\n\ndef task_func(src_dir, dst_dir):\n    \"\"\"\n    Copies all files from 'src_dir' to 'dst_dir' that match any pattern in ['*.txt', '*.docx'].\n    \"\"\"\n    # Define the file patterns\n    patterns = ['*.txt', '*.docx']\n\n    # Iterate over all patterns\n    for pattern in patterns:\n        # Find all files that match the pattern in the source directory\n        for file_name in fnmatch.filter(os.listdir(src_dir), pattern):\n            # Construct the full file path\n            src_file = os.path.join(src_dir, file_name)\n            # Copy the file to the destination directory\n            shutil.copy2(src_file, dst_dir)\n\n    # Return the destination directory\n    return dst_dir"}
{"task_id": "BigCodeBench/666", "solution": "from itertools import combinations\nimport math\ndef task_func(seq, letter_weight_dict):\n    max_weight = 0\n    max_subseq = \"\"\n    for L in range(0, len(seq)+1):\n        for subset in combinations(seq, L):\n            subset_weight = sum(letter_weight_dict[char] for char in subset)\n            if subset_weight > max_weight:\n                max_weight = subset_weight\n                max_subseq = \"\".join(subset)\n    return max_subseq\nletter_weight_dict = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\nseq = \"abcde\"", "raw_solution": "from itertools import combinations\nimport math\n\ndef task_func(seq, letter_weight_dict):\n    max_weight = 0\n    max_subseq = \"\"\n    for L in range(0, len(seq)+1):\n        for subset in combinations(seq, L):\n            subset_weight = sum(letter_weight_dict[char] for char in subset)\n            if subset_weight > max_weight:\n                max_weight = subset_weight\n                max_subseq = \"\".join(subset)\n    return max_subseq\n\n# Test the function\nletter_weight_dict = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\nseq = \"abcde\""}
{"task_id": "BigCodeBench/667", "solution": "import heapq\nimport collections\ndef task_func(x, n):\n    \"\"\"\n    Finds the n most common letters in a dictionary, x, where the key letters and the values are their frequencies.\n    \"\"\"\n    # Count the frequencies of each letter in the dictionary\n    freq_count = collections.Counter(x)\n\n    # Create a max heap with the frequencies\n    max_heap = [(-freq, letter) for letter, freq in freq_count.items()]\n    heapq.heapify(max_heap)\n\n    # Pop the n most common letters from the heap\n    most_common_letters = [heapq.heappop(max_heap)[1] for _ in range(n)]\n\n    return most_common_letters", "raw_solution": "import heapq\nimport collections\n\ndef task_func(x, n):\n    \"\"\"\n    Finds the n most common letters in a dictionary, x, where the key letters and the values are their frequencies.\n    \"\"\"\n    # Count the frequencies of each letter in the dictionary\n    freq_count = collections.Counter(x)\n\n    # Create a max heap with the frequencies\n    max_heap = [(-freq, letter) for letter, freq in freq_count.items()]\n    heapq.heapify(max_heap)\n\n    # Pop the n most common letters from the heap\n    most_common_letters = [heapq.heappop(max_heap)[1] for _ in range(n)]\n\n    return most_common_letters"}
{"task_id": "BigCodeBench/668", "solution": "import itertools\nimport math\ndef task_func(x):\n    \"\"\"\n    Finds the sub-sequence of a dictionary, x, with the minimum total length, where the keys are letters and the values are their lengths.\n    \"\"\"\n    # Sort the dictionary by values\n    sorted_x = dict(sorted(x.items(), key=lambda item: item[1]))\n\n    # Initialize variables\n    min_length = math.inf\n    min_subsequence = []\n\n    # Iterate over all possible subsequences\n    for L in range(0, len(sorted_x)+1):\n        for subset in itertools.combinations(sorted_x, L):\n            # Calculate the total length of the current subsequence\n            total_length = sum(sorted_x[i] for i in subset)\n\n            # If the total length is less than the current minimum, update the minimum and the subsequence\n            if total_length < min_length:\n                min_length = total_length\n                min_subsequence = [i for i in subset]\n\n    # Return the subsequence with the minimum total length\n    return min_subsequence", "raw_solution": "import itertools\nimport math\n\ndef task_func(x):\n    \"\"\"\n    Finds the sub-sequence of a dictionary, x, with the minimum total length, where the keys are letters and the values are their lengths.\n    \"\"\"\n    # Sort the dictionary by values\n    sorted_x = dict(sorted(x.items(), key=lambda item: item[1]))\n\n    # Initialize variables\n    min_length = math.inf\n    min_subsequence = []\n\n    # Iterate over all possible subsequences\n    for L in range(0, len(sorted_x)+1):\n        for subset in itertools.combinations(sorted_x, L):\n            # Calculate the total length of the current subsequence\n            total_length = sum(sorted_x[i] for i in subset)\n\n            # If the total length is less than the current minimum, update the minimum and the subsequence\n            if total_length < min_length:\n                min_length = total_length\n                min_subsequence = [i for i in subset]\n\n    # Return the subsequence with the minimum total length\n    return min_subsequence"}
{"task_id": "BigCodeBench/669", "solution": "import itertools\nimport math\ndef task_func(x):\n    max_cos_sum = -1\n    max_pair = None\n    for pair in itertools.combinations(x.keys(), 2):\n        cos_sum = math.cos(x[pair[0]]) + math.cos(x[pair[1]])\n        if cos_sum > max_cos_sum:\n            max_cos_sum = cos_sum\n            max_pair = pair\n    return max_pair", "raw_solution": "import itertools\nimport math\n\ndef task_func(x):\n    max_cos_sum = -1\n    max_pair = None\n    for pair in itertools.combinations(x.keys(), 2):\n        cos_sum = math.cos(x[pair[0]]) + math.cos(x[pair[1]])\n        if cos_sum > max_cos_sum:\n            max_cos_sum = cos_sum\n            max_pair = pair\n    return max_pair"}
{"task_id": "BigCodeBench/670", "solution": "from itertools import combinations\nimport math\ndef task_func(x, w):\n    max_weight = -math.inf\n    max_substr = ''\n    for i in range(len(x), 0, -1):\n        for substr in combinations(x, i):\n            substr = ''.join(substr)\n            weight = sum(w[char] for char in substr)\n            if weight > max_weight:\n                max_weight = weight\n                max_substr = substr\n    return max_substr\nx = 'abcdef'\nw = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6}", "raw_solution": "from itertools import combinations\nimport math\n\ndef task_func(x, w):\n    max_weight = -math.inf\n    max_substr = ''\n    for i in range(len(x), 0, -1):\n        for substr in combinations(x, i):\n            substr = ''.join(substr)\n            weight = sum(w[char] for char in substr)\n            if weight > max_weight:\n                max_weight = weight\n                max_substr = substr\n    return max_substr\n\n# Test the function\nx = 'abcdef'\nw = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6}\nassert task_func(x, w) == 'abc'\n\nx = 'abcdefgh'\nw = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8}\nassert task_func(x, w) == 'abcdefgh'\n\nx = 'abcdefghi'\nw = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9}\nassert task_func(x, w) == 'abcdefghi'\n"}
{"task_id": "BigCodeBench/671", "solution": "import os\nimport random\nimport json\ndef task_func(directory, n):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n):\n        file_name = f\"{directory}/file_{i}.json\"\n        with open(file_name, 'w') as file:\n            data = {'number': random.randint(1, 100)}\n            json.dump(data, file)\n\n    return directory", "raw_solution": "import os\nimport random\nimport json\n\ndef task_func(directory, n):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n):\n        file_name = f\"{directory}/file_{i}.json\"\n        with open(file_name, 'w') as file:\n            data = {'number': random.randint(1, 100)}\n            json.dump(data, file)\n\n    return directory"}
{"task_id": "BigCodeBench/672", "solution": "import csv\nimport sys\ndef task_func(filename):\n    \"\"\"\n    Reads a CSV file, inverses the order of the lines and writes the inverted lines back into the file.\n    Then resets the cursor to the beginning of the file.\n    \"\"\"\n    try:\n        with open(filename, 'r') as file:\n            lines = file.readlines()\n\n        with open(filename, 'w') as file:\n            for line in reversed(lines):\n                file.write(line)\n\n        with open(filename, 'r+') as file:\n            file.seek(0)\n\n    except FileNotFoundError:\n        print(f\"File {filename} not found.\")\n        sys.exit(1)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        sys.exit(1)\n\n    return filename", "raw_solution": "import csv\nimport sys\n\ndef task_func(filename):\n    \"\"\"\n    Reads a CSV file, inverses the order of the lines and writes the inverted lines back into the file.\n    Then resets the cursor to the beginning of the file.\n    \"\"\"\n    try:\n        with open(filename, 'r') as file:\n            lines = file.readlines()\n\n        with open(filename, 'w') as file:\n            for line in reversed(lines):\n                file.write(line)\n\n        with open(filename, 'r+') as file:\n            file.seek(0)\n\n    except FileNotFoundError:\n        print(f\"File {filename} not found.\")\n        sys.exit(1)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        sys.exit(1)\n\n    return filename"}
{"task_id": "BigCodeBench/673", "solution": "import os\nimport random\ndef task_func(directory, n_files):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        file_name = os.path.join(directory, f'file_{i+1}.txt')\n        with open(file_name, 'w') as f:\n            f.write(str(random.randint(0, 9)))\n        f.seek(0)\n\n    return n_files", "raw_solution": "import os\nimport random\n\ndef task_func(directory, n_files):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        file_name = os.path.join(directory, f'file_{i+1}.txt')\n        with open(file_name, 'w') as f:\n            f.write(str(random.randint(0, 9)))\n        f.seek(0)\n\n    return n_files"}
{"task_id": "BigCodeBench/674", "solution": "import pandas as pd\nimport os\ndef task_func(filename):\n    \"\"\"\n    Reads a CSV file, reverses the order of the lines, and writes the inverted lines back into the file.\n    The header is not inverted and the file may be empty.\n    \"\"\"\n    if os.path.exists(filename):\n        df = pd.read_csv(filename)\n        df = df.iloc[::-1]\n        df.to_csv(filename, index=False)\n\n        with open(filename, 'r+') as f:\n            lines = f.readlines()\n            f.seek(0)\n            f.write(lines[0].strip() + '\\n')\n            for line in lines[1:]:\n                f.write(line.strip() + '\\n')\n            f.truncate()\n    else:\n        print(f\"File {filename} does not exist.\")\n\n    return filename", "raw_solution": "import pandas as pd\nimport os\n\ndef task_func(filename):\n    \"\"\"\n    Reads a CSV file, reverses the order of the lines, and writes the inverted lines back into the file.\n    The header is not inverted and the file may be empty.\n    \"\"\"\n    if os.path.exists(filename):\n        df = pd.read_csv(filename)\n        df = df.iloc[::-1]\n        df.to_csv(filename, index=False)\n\n        with open(filename, 'r+') as f:\n            lines = f.readlines()\n            f.seek(0)\n            f.write(lines[0].strip() + '\\n')\n            for line in lines[1:]:\n                f.write(line.strip() + '\\n')\n            f.truncate()\n    else:\n        print(f\"File {filename} does not exist.\")\n\n    return filename"}
{"task_id": "BigCodeBench/675", "solution": "import os\nimport random\nimport string\ndef task_func(directory, n_files):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        file_name = f\"{directory}/file_{i}.txt\"\n        with open(file_name, 'w') as f:\n            f.write(''.join(random.choices(string.ascii_letters + string.digits, k=100)))\n\n    return directory", "raw_solution": "import os\nimport random\nimport string\n\ndef task_func(directory, n_files):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        file_name = f\"{directory}/file_{i}.txt\"\n        with open(file_name, 'w') as f:\n            f.write(''.join(random.choices(string.ascii_letters + string.digits, k=100)))\n\n    return directory"}
{"task_id": "BigCodeBench/676", "solution": "import pandas as pd\nimport random\ndef task_func(df):\n    \"\"\"\n    Adds a 'winner' column to the DataFrame, which is the team with the highest score in each match.\n    If the scores are equal, the winner is randomly decided.\n    \"\"\"\n    df['winner'] = df.apply(lambda row: random.choice([row['team1'], row['team2']]) if row['score1'] == row['score2'] else (row['team1'] if row['score1'] > row['score2'] else row['team2']), axis=1)\n    return df\ndf = pd.DataFrame({\n    'team1': ['Team A', 'Team B', 'Team C', 'Team D'],\n    'score1': [100, 120, 110, 130],\n    'team2': ['Team E', 'Team F', 'Team G', 'Team H'],\n    'score2': [90, 110, 120, 100]\n})", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(df):\n    \"\"\"\n    Adds a 'winner' column to the DataFrame, which is the team with the highest score in each match.\n    If the scores are equal, the winner is randomly decided.\n    \"\"\"\n    df['winner'] = df.apply(lambda row: random.choice([row['team1'], row['team2']]) if row['score1'] == row['score2'] else (row['team1'] if row['score1'] > row['score2'] else row['team2']), axis=1)\n    return df\n\n# Test the function\ndf = pd.DataFrame({\n    'team1': ['Team A', 'Team B', 'Team C', 'Team D'],\n    'score1': [100, 120, 110, 130],\n    'team2': ['Team E', 'Team F', 'Team G', 'Team H'],\n    'score2': [90, 110, 120, 100]\n})\n"}
{"task_id": "BigCodeBench/677", "solution": "import numpy as np\nimport pandas as pd\nfrom scipy.stats import linregress\ndef task_func(df):\n    \"\"\"\n    Performs a linear regression on two variables in a DataFrame and adds a 'predicted' column.\n    \"\"\"\n    # Check if the DataFrame has at least two columns\n    if len(df.columns) < 2:\n        raise ValueError(\"The DataFrame must have at least two columns.\")\n\n    # Get the names of the two variables\n    var1, var2 = df.columns[0], df.columns[1]\n\n    # Perform the linear regression\n    slope, intercept, r_value, p_value, std_err = linregress(df[var1], df[var2])\n\n    # Add the 'predicted' column\n    df['predicted'] = intercept + slope * df[var1]\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom scipy.stats import linregress\n\ndef task_func(df):\n    \"\"\"\n    Performs a linear regression on two variables in a DataFrame and adds a 'predicted' column.\n    \"\"\"\n    # Check if the DataFrame has at least two columns\n    if len(df.columns) < 2:\n        raise ValueError(\"The DataFrame must have at least two columns.\")\n\n    # Get the names of the two variables\n    var1, var2 = df.columns[0], df.columns[1]\n\n    # Perform the linear regression\n    slope, intercept, r_value, p_value, std_err = linregress(df[var1], df[var2])\n\n    # Add the 'predicted' column\n    df['predicted'] = intercept + slope * df[var1]\n\n    return df"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in a directory. The function reads each JSON file alphabetically into a DataFrame and inserts a \"Source\" column that specifies the filename. The processed files are then moved to a \"processed\" subdirectory. The path may not exist initially.\n    \"\"\"\n    # Create the \"processed\" subdirectory if it doesn't exist\n    if not os.path.exists(os.path.join(path, \"processed\")):\n        os.makedirs(os.path.join(path, \"processed\"))\n\n    # Get a list of all JSON files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n\n    # Sort the files alphabetically\n    json_files.sort()\n\n    # Initialize an empty DataFrame\n    df = pd.DataFrame()\n\n    # Loop over each JSON file\n    for file in json_files:\n        # Read the JSON file into a DataFrame\n        temp_df = pd.read_json(os.path.join(path, file))\n\n        # Add a \"Source\" column that specifies the filename\n        temp_df['Source'] = file\n\n        # Append the DataFrame to the main DataFrame\n        df = df.append(temp_df, ignore_index=True)\n\n        # Move the processed file to the \"processed\" subdirectory\n        shutil.move(os.path.join(path, file), os.path.join(path, \"processed\", file))\n\n    return df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in a directory. The function reads each JSON file alphabetically into a DataFrame and inserts a \"Source\" column that specifies the filename. The processed files are then moved to a \"processed\" subdirectory. The path may not exist initially.\n    \"\"\"\n    # Create the \"processed\" subdirectory if it doesn't exist\n    if not os.path.exists(os.path.join(path, \"processed\")):\n        os.makedirs(os.path.join(path, \"processed\"))\n\n    # Get a list of all JSON files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n\n    # Sort the files alphabetically\n    json_files.sort()\n\n    # Initialize an empty DataFrame\n    df = pd.DataFrame()\n\n    # Loop over each JSON file\n    for file in json_files:\n        # Read the JSON file into a DataFrame\n        temp_df = pd.read_json(os.path.join(path, file))\n\n        # Add a \"Source\" column that specifies the filename\n        temp_df['Source'] = file\n\n        # Append the DataFrame to the main DataFrame\n        df = df.append(temp_df, ignore_index=True)\n\n        # Move the processed file to the \"processed\" subdirectory\n        shutil.move(os.path.join(path, file), os.path.join(path, \"processed\", file))\n\n    return df"}
{"task_id": "BigCodeBench/679", "solution": "import pandas as pd\nfrom collections import Counter\nfrom itertools import combinations\ndef task_func(df):\n    # Add a 'combination' column to the DataFrame\n    df['combination'] = df.apply(lambda row: '-'.join(sorted(row.astype(str))), axis=1)\n\n    # Calculate the frequency of each combination\n    combination_freq = Counter(df['combination'])\n\n    return dict(combination_freq)\ndf = pd.DataFrame({\n    'A': ['a', 'b', 'c'],\n    'B': ['d', 'e', 'f'],\n    'C': ['g', 'h', 'i']\n})", "raw_solution": "import pandas as pd\nfrom collections import Counter\nfrom itertools import combinations\n\ndef task_func(df):\n    # Add a 'combination' column to the DataFrame\n    df['combination'] = df.apply(lambda row: '-'.join(sorted(row.astype(str))), axis=1)\n\n    # Calculate the frequency of each combination\n    combination_freq = Counter(df['combination'])\n\n    return dict(combination_freq)\n\n# Test the function\ndf = pd.DataFrame({\n    'A': ['a', 'b', 'c'],\n    'B': ['d', 'e', 'f'],\n    'C': ['g', 'h', 'i']\n})\n"}
{"task_id": "BigCodeBench/680", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, features):\n    \"\"\"\n    Standardize the features in a DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to standardize.\n    features (list): The features to standardize.\n\n    Returns:\n    pandas.DataFrame: The DataFrame with the standardized features.\n    \"\"\"\n    scaler = StandardScaler()\n    df[features] = scaler.fit_transform(df[features])\n    return df\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})\nfeatures = ['A', 'B', 'C']", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, features):\n    \"\"\"\n    Standardize the features in a DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to standardize.\n    features (list): The features to standardize.\n\n    Returns:\n    pandas.DataFrame: The DataFrame with the standardized features.\n    \"\"\"\n    scaler = StandardScaler()\n    df[features] = scaler.fit_transform(df[features])\n    return df\n\n# Test the function\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})\n\nfeatures = ['A', 'B', 'C']\n"}
{"task_id": "BigCodeBench/681", "solution": "import pandas as pd\nimport json\ndef task_func(file_path, key):\n    # Load JSON file into a DataFrame\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    df = pd.DataFrame(data)\n\n    # Remove specific key from each object\n    df = df.drop(key, axis=1)\n\n    # Write the processed DataFrame back into a JSON file\n    df.to_json(file_path, orient='records')\n\n    return df", "raw_solution": "import pandas as pd\nimport json\n\ndef task_func(file_path, key):\n    # Load JSON file into a DataFrame\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    df = pd.DataFrame(data)\n\n    # Remove specific key from each object\n    df = df.drop(key, axis=1)\n\n    # Write the processed DataFrame back into a JSON file\n    df.to_json(file_path, orient='records')\n\n    return df"}
{"task_id": "BigCodeBench/682", "solution": "from collections import Counter\nimport math\ndef task_func(nested_dict):\n    # Flatten the dictionary and aggregate the values of the same keys\n    flat_dict = flatten_dict(nested_dict)\n    aggregated_dict = dict(Counter(flat_dict))\n\n    # Remove the \"ele\" key and take the sine of the remaining keys\n    result_dict = {key: math.sin(int(val)) for key, val in aggregated_dict.items() if key != \"ele\"}\n\n    return result_dict\ndef flatten_dict(dd, separator='_', prefix=''):\n    return { prefix + separator + k if prefix else k : v\n             for kk, vv in dd.items()\n             for k, v in flatten_dict(vv, separator, kk).items()\n             } if isinstance(dd, dict) else { prefix : dd }\nnested_dict = {\n    \"a\": {\"ele\": 1, \"b\": 2},\n    \"b\": {\"ele\": 2, \"c\": 3},\n    \"c\": {\"ele\": 3, \"a\": 1}\n}", "raw_solution": "from collections import Counter\nimport math\n\ndef task_func(nested_dict):\n    # Flatten the dictionary and aggregate the values of the same keys\n    flat_dict = flatten_dict(nested_dict)\n    aggregated_dict = dict(Counter(flat_dict))\n\n    # Remove the \"ele\" key and take the sine of the remaining keys\n    result_dict = {key: math.sin(int(val)) for key, val in aggregated_dict.items() if key != \"ele\"}\n\n    return result_dict\n\ndef flatten_dict(dd, separator='_', prefix=''):\n    return { prefix + separator + k if prefix else k : v\n             for kk, vv in dd.items()\n             for k, v in flatten_dict(vv, separator, kk).items()\n             } if isinstance(dd, dict) else { prefix : dd }\n\n# Test the function\nnested_dict = {\n    \"a\": {\"ele\": 1, \"b\": 2},\n    \"b\": {\"ele\": 2, \"c\": 3},\n    \"c\": {\"ele\": 3, \"a\": 1}\n}\n"}
{"task_id": "BigCodeBench/683", "solution": "import math\nimport yaml\ndef task_func(yaml_path, key):\n    with open(yaml_path, 'r') as file:\n        data = yaml.safe_load(file)\n\n    if key in data:\n        data[key] = math.cos(data[key])\n\n    with open(yaml_path, 'w') as file:\n        yaml.safe_dump(data, file)\n\n    return data", "raw_solution": "import math\nimport yaml\n\ndef task_func(yaml_path, key):\n    with open(yaml_path, 'r') as file:\n        data = yaml.safe_load(file)\n\n    if key in data:\n        data[key] = math.cos(data[key])\n\n    with open(yaml_path, 'w') as file:\n        yaml.safe_dump(data, file)\n\n    return data"}
{"task_id": "BigCodeBench/684", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(df, col):\n    # Remove the specified column\n    df = df.drop(col, axis=1)\n\n    # Add a 'IsEvenIndex' column\n    df['IsEvenIndex'] = df.index % 2 == 0\n\n    return df\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(df, col):\n    # Remove the specified column\n    df = df.drop(col, axis=1)\n\n    # Add a 'IsEvenIndex' column\n    df['IsEvenIndex'] = df.index % 2 == 0\n\n    return df\n\n# Test the function\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})\n\ndf = task_func(df, 'B')"}
{"task_id": "BigCodeBench/685", "solution": "from collections import Counter\nfrom itertools import chain\ndef task_func(list_of_lists):\n    \"\"\"\n    Merges all sublists from a list of lists into a list and returns a count of the elements.\n    \"\"\"\n    flattened_list = list(chain(*list_of_lists))\n    count_dict = Counter(flattened_list)\n    return count_dict", "raw_solution": "from collections import Counter\nfrom itertools import chain\n\ndef task_func(list_of_lists):\n    \"\"\"\n    Merges all sublists from a list of lists into a list and returns a count of the elements.\n    \"\"\"\n    flattened_list = list(chain(*list_of_lists))\n    count_dict = Counter(flattened_list)\n    return count_dict"}
{"task_id": "BigCodeBench/686", "solution": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\ndef task_func(list_of_lists):\n    # Merge the lists\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n    \n    # Convert the merged list to a numpy array\n    merged_array = np.array(merged_list).reshape(-1, 1)\n    \n    # One-hot encode the array\n    enc = OneHotEncoder(sparse=False)\n    one_hot = enc.fit_transform(merged_array)\n    \n    return one_hot", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef task_func(list_of_lists):\n    # Merge the lists\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n    \n    # Convert the merged list to a numpy array\n    merged_array = np.array(merged_list).reshape(-1, 1)\n    \n    # One-hot encode the array\n    enc = OneHotEncoder(sparse=False)\n    one_hot = enc.fit_transform(merged_array)\n    \n    return one_hot"}
{"task_id": "BigCodeBench/687", "solution": "import numpy as np\nfrom scipy.stats import mode\nfrom collections import Counter\ndef task_func(list_of_lists):\n    # Merge the lists\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n\n    # Find the mode and count of the elements in the merged list\n    mode_value, mode_count = mode(merged_list)\n    mode_value = mode_value[0]\n\n    # Find the frequency count of the mode_value within the merged array\n    count_dict = Counter(merged_list)\n    mode_count_in_array = count_dict[mode_value]\n\n    return (mode_value, mode_count_in_array), mode_value, mode_count_in_array\nlist_of_lists = [[1, 2, 2, 3], [2, 2, 4], [3, 4, 4, 4, 5]]", "raw_solution": "import numpy as np\nfrom scipy.stats import mode\nfrom collections import Counter\n\ndef task_func(list_of_lists):\n    # Merge the lists\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n\n    # Find the mode and count of the elements in the merged list\n    mode_value, mode_count = mode(merged_list)\n    mode_value = mode_value[0]\n\n    # Find the frequency count of the mode_value within the merged array\n    count_dict = Counter(merged_list)\n    mode_count_in_array = count_dict[mode_value]\n\n    return (mode_value, mode_count_in_array), mode_value, mode_count_in_array\n\n# Test the function\nlist_of_lists = [[1, 2, 2, 3], [2, 2, 4], [3, 4, 4, 4, 5]]"}
{"task_id": "BigCodeBench/688", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    \"\"\"\n    Standardizes a Pandas DataFrame with random numeric values using the StandardScaler from sklearn.\n    \"\"\"\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    return df_standardized", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    \"\"\"\n    Standardizes a Pandas DataFrame with random numeric values using the StandardScaler from sklearn.\n    \"\"\"\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    return df_standardized"}
{"task_id": "BigCodeBench/689", "solution": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\ndef task_func(df):\n    \"\"\"\n    Given a Pandas DataFrame with random numeric values, test if the data in each column is normally distributed using the Shapiro-Wilk test.\n    The function returns a dictionary with p-values from the Shapiro-Wilk test for each column.\n    \"\"\"\n    p_values = {}\n    for col in df.columns:\n        _, p_value = stats.shapiro(df[col])\n        p_values[col] = p_value\n    return p_values", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndef task_func(df):\n    \"\"\"\n    Given a Pandas DataFrame with random numeric values, test if the data in each column is normally distributed using the Shapiro-Wilk test.\n    The function returns a dictionary with p-values from the Shapiro-Wilk test for each column.\n    \"\"\"\n    p_values = {}\n    for col in df.columns:\n        _, p_value = stats.shapiro(df[col])\n        p_values[col] = p_value\n    return p_values"}
{"task_id": "BigCodeBench/690", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(df):\n    \"\"\"\n    Fits a linear regression model to the data in `df` using columns 'X' and 'Y'.\n    Returns the fitted model.\n    \"\"\"\n    X = df['X'].values.reshape(-1, 1)\n    Y = df['Y'].values.reshape(-1, 1)\n\n    model = LinearRegression()\n    model.fit(X, Y)\n\n    return model\ndf = pd.DataFrame(np.random.rand(100, 2), columns=['X', 'Y'])", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(df):\n    \"\"\"\n    Fits a linear regression model to the data in `df` using columns 'X' and 'Y'.\n    Returns the fitted model.\n    \"\"\"\n    X = df['X'].values.reshape(-1, 1)\n    Y = df['Y'].values.reshape(-1, 1)\n\n    model = LinearRegression()\n    model.fit(X, Y)\n\n    return model\n\n# Test the function\nnp.random.seed(0)\ndf = pd.DataFrame(np.random.rand(100, 2), columns=['X', 'Y'])\nmodel = task_func(df)"}
{"task_id": "BigCodeBench/691", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    \"\"\"\n    Given a pandas DataFrame with random numeric values, run KMeans clusters on the data and return the labels.\n    \"\"\"\n    # Standardize the data\n    scaler = StandardScaler()\n    df_std = scaler.fit_transform(df)\n\n    # Run KMeans clustering\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(df_std)\n\n    # Return the labels\n    return kmeans.labels_", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    \"\"\"\n    Given a pandas DataFrame with random numeric values, run KMeans clusters on the data and return the labels.\n    \"\"\"\n    # Standardize the data\n    scaler = StandardScaler()\n    df_std = scaler.fit_transform(df)\n\n    # Run KMeans clustering\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(df_std)\n\n    # Return the labels\n    return kmeans.labels_"}
{"task_id": "BigCodeBench/692", "solution": "import math\nimport pandas as pd\ndef task_func(tuples_list):\n    \"\"\"\n    Converts a list of tuples into a pandas DataFrame, applying math.sin to each number.\n    \"\"\"\n    # Convert the list of tuples into a DataFrame\n    df = pd.DataFrame(tuples_list, columns=['x', 'y'])\n\n    # Apply math.sin to each number\n    df['x'] = df['x'].apply(math.sin)\n    df['y'] = df['y'].apply(math.sin)\n\n    return df", "raw_solution": "import math\nimport pandas as pd\n\ndef task_func(tuples_list):\n    \"\"\"\n    Converts a list of tuples into a pandas DataFrame, applying math.sin to each number.\n    \"\"\"\n    # Convert the list of tuples into a DataFrame\n    df = pd.DataFrame(tuples_list, columns=['x', 'y'])\n\n    # Apply math.sin to each number\n    df['x'] = df['x'].apply(math.sin)\n    df['y'] = df['y'].apply(math.sin)\n\n    return df"}
{"task_id": "BigCodeBench/693", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(tuples_list, columns):\n    \"\"\"\n    Converts a list of tuples into a Pandas DataFrame, performs a default scaling in each column, and returns the transformed DataFrame.\n    \"\"\"\n    # Convert the list of tuples into a DataFrame\n    df = pd.DataFrame(tuples_list, columns=columns)\n\n    # Initialize a StandardScaler\n    scaler = StandardScaler()\n\n    # Scale each column in the DataFrame\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=columns)\n\n    return df_scaled", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(tuples_list, columns):\n    \"\"\"\n    Converts a list of tuples into a Pandas DataFrame, performs a default scaling in each column, and returns the transformed DataFrame.\n    \"\"\"\n    # Convert the list of tuples into a DataFrame\n    df = pd.DataFrame(tuples_list, columns=columns)\n\n    # Initialize a StandardScaler\n    scaler = StandardScaler()\n\n    # Scale each column in the DataFrame\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=columns)\n\n    return df_scaled"}
{"task_id": "BigCodeBench/694", "solution": "import itertools\nimport random\ndef task_func(t, n):\n    \"\"\"\n    Generates all combinations from a tuple with length n and returns a random combination of length n.\n    \"\"\"\n    # Generate all combinations\n    all_combinations = list(itertools.combinations(t, n))\n\n    # Return a random combination\n    return random.choice(all_combinations)\nt = (1, 2, 3, 4, 5)\nn = 3", "raw_solution": "import itertools\nimport random\n\ndef task_func(t, n):\n    \"\"\"\n    Generates all combinations from a tuple with length n and returns a random combination of length n.\n    \"\"\"\n    # Generate all combinations\n    all_combinations = list(itertools.combinations(t, n))\n\n    # Return a random combination\n    return random.choice(all_combinations)\n\n# Test the function\nt = (1, 2, 3, 4, 5)\nn = 3"}
{"task_id": "BigCodeBench/695", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\ndef task_func(tuples_list, n_components):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on a list of tuples.\n\n    Parameters:\n    tuples_list (list): A list of tuples.\n    n_components (int): The number of components to keep.\n\n    Returns:\n    transformed_data (ndarray): The transformed data.\n    \"\"\"\n    # Convert the list of tuples to a numpy array\n    data = np.array(tuples_list)\n\n    # Initialize PCA with the desired number of components\n    pca = PCA(n_components=n_components)\n\n    # Fit and transform the data\n    transformed_data = pca.fit_transform(data)\n\n    return transformed_data", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\n\ndef task_func(tuples_list, n_components):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on a list of tuples.\n\n    Parameters:\n    tuples_list (list): A list of tuples.\n    n_components (int): The number of components to keep.\n\n    Returns:\n    transformed_data (ndarray): The transformed data.\n    \"\"\"\n    # Convert the list of tuples to a numpy array\n    data = np.array(tuples_list)\n\n    # Initialize PCA with the desired number of components\n    pca = PCA(n_components=n_components)\n\n    # Fit and transform the data\n    transformed_data = pca.fit_transform(data)\n\n    return transformed_data"}
{"task_id": "BigCodeBench/696", "solution": "import numpy as np\nimport math\nimport random\nfrom random import uniform\ndef task_func(radius, num_points):\n    \"\"\"\n    Generates a list of random points within a circle of a given radius.\n\n    Args:\n        radius (float): The radius of the circle.\n        num_points (int): The number of points to generate.\n\n    Returns:\n        list: A list of points within the circle.\n    \"\"\"\n    points = []\n    for _ in range(num_points):\n        angle = 2 * math.pi * random.random()\n        dist = radius * math.sqrt(random.uniform(0, 1))\n        x = dist * math.cos(angle)\n        y = dist * math.sin(angle)\n        points.append((x, y))\n    return points\nradius = 5\nnum_points = 10", "raw_solution": "import numpy as np\nimport math\nimport random\nfrom random import uniform\n\ndef task_func(radius, num_points):\n    \"\"\"\n    Generates a list of random points within a circle of a given radius.\n\n    Args:\n        radius (float): The radius of the circle.\n        num_points (int): The number of points to generate.\n\n    Returns:\n        list: A list of points within the circle.\n    \"\"\"\n    points = []\n    for _ in range(num_points):\n        angle = 2 * math.pi * random.random()\n        dist = radius * math.sqrt(random.uniform(0, 1))\n        x = dist * math.cos(angle)\n        y = dist * math.sin(angle)\n        points.append((x, y))\n    return points\n\n# Test the function\nradius = 5\nnum_points = 10\npoints = task_func(radius, num_points)\nfor point in points:\n    dist = math.sqrt(point[0]**2 + point[1]**2)\n    assert dist <= radius, f\"Point {point} is outside the circle\""}
{"task_id": "BigCodeBench/697", "solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    X = df['feature'].values.reshape(-1, 1)\n    y = df['value'].values.reshape(-1, 1)\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    coef = model.coef_[0][0]\n    intercept = model.intercept_[0]\n\n    result = {\n        'coefficient': coef,\n        'intercept': intercept\n    }\n\n    return result", "raw_solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    X = df['feature'].values.reshape(-1, 1)\n    y = df['value'].values.reshape(-1, 1)\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    coef = model.coef_[0][0]\n    intercept = model.intercept_[0]\n\n    result = {\n        'coefficient': coef,\n        'intercept': intercept\n    }\n\n    return result"}
{"task_id": "BigCodeBench/698", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef task_func(df):\n    # Split the DataFrame into a training set and a test set\n    train, test = train_test_split(df, test_size=0.3, random_state=42)\n\n    # Separate the \"target\" column from the rest of the DataFrame\n    X_train = train.drop('target', axis=1)\n    y_train = train['target']\n    X_test = test.drop('target', axis=1)\n    y_test = test['target']\n\n    return (X_train, X_test, y_train, y_test)", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(df):\n    # Split the DataFrame into a training set and a test set\n    train, test = train_test_split(df, test_size=0.3, random_state=42)\n\n    # Separate the \"target\" column from the rest of the DataFrame\n    X_train = train.drop('target', axis=1)\n    y_train = train['target']\n    X_test = test.drop('target', axis=1)\n    y_test = test['target']\n\n    return (X_train, X_test, y_train, y_test)"}
{"task_id": "BigCodeBench/699", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport numpy as np\ndef task_func(x_list, y_list, n_clusters=2, random_state=0):\n    \"\"\"\n    Performs K-Means clustering on the given data.\n    The data is represented as two lists: x_list and y_list.\n    The function returns the labels and centroids as numpy arrays.\n    \"\"\"\n    # Create a DataFrame from the input lists\n    data = pd.DataFrame({\n        'x': x_list,\n        'y': y_list\n    })\n\n    # Perform K-Means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(data)\n\n    # Return the labels and centroids as numpy arrays\n    return kmeans.labels_, kmeans.cluster_centers_\nx_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef task_func(x_list, y_list, n_clusters=2, random_state=0):\n    \"\"\"\n    Performs K-Means clustering on the given data.\n    The data is represented as two lists: x_list and y_list.\n    The function returns the labels and centroids as numpy arrays.\n    \"\"\"\n    # Create a DataFrame from the input lists\n    data = pd.DataFrame({\n        'x': x_list,\n        'y': y_list\n    })\n\n    # Perform K-Means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(data)\n\n    # Return the labels and centroids as numpy arrays\n    return kmeans.labels_, kmeans.cluster_centers_\n\n# Test the function\nx_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nlabels, centroids = task_func(x_list, y_list, n_clusters=2)"}
{"task_id": "BigCodeBench/700", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data, cols):\n    \"\"\"\n    Converts the provided data into a DataFrame and then calculates the correlation matrix of numeric columns.\n    \"\"\"\n    # Convert the data into a DataFrame\n    df = pd.DataFrame(data, columns=cols)\n\n    # Calculate the correlation matrix\n    correlation_matrix = df.corr()\n\n    return correlation_matrix", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data, cols):\n    \"\"\"\n    Converts the provided data into a DataFrame and then calculates the correlation matrix of numeric columns.\n    \"\"\"\n    # Convert the data into a DataFrame\n    df = pd.DataFrame(data, columns=cols)\n\n    # Calculate the correlation matrix\n    correlation_matrix = df.corr()\n\n    return correlation_matrix"}
{"task_id": "BigCodeBench/701", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\ndef task_func(df, target):\n    # Split the data into features and target\n    X = df.drop(target, axis=1)\n    y = df[target]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Fit the model to the training data\n    model.fit(X_train, y_train)\n\n    # Make predictions on the testing set\n    y_pred = model.predict(X_test)\n\n    # Calculate the R-squared score of the model\n    score = r2_score(y_test, y_pred)\n\n    return score", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\ndef task_func(df, target):\n    # Split the data into features and target\n    X = df.drop(target, axis=1)\n    y = df[target]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Fit the model to the training data\n    model.fit(X_train, y_train)\n\n    # Make predictions on the testing set\n    y_pred = model.predict(X_test)\n\n    # Calculate the R-squared score of the model\n    score = r2_score(y_test, y_pred)\n\n    return score"}
{"task_id": "BigCodeBench/702", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\ndef task_func(df):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the DataFrame and records the first two main components.\n    \"\"\"\n    pca = PCA(n_components=2)\n    df_pca = pca.fit_transform(df)\n    df_pca = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\n    return df_pca", "raw_solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\n\ndef task_func(df):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the DataFrame and records the first two main components.\n    \"\"\"\n    pca = PCA(n_components=2)\n    df_pca = pca.fit_transform(df)\n    df_pca = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\n    return df_pca"}
{"task_id": "BigCodeBench/703", "solution": "import pandas as pd\nfrom sklearn.cluster import DBSCAN\ndef task_func(data, cols):\n    \"\"\"\n    Performs DBSCAN clustering on the data by transforming it into a DataFrame and recording the clusters in a new column named 'Cluster'.\n    The function takes in a DataFrame and a list of column names, and returns the DataFrame with a new 'Cluster' column.\n    \"\"\"\n    df = pd.DataFrame(data, columns=cols)\n    dbscan = DBSCAN(eps=3, min_samples=2)\n    df['Cluster'] = dbscan.fit_predict(df[cols])\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import DBSCAN\n\ndef task_func(data, cols):\n    \"\"\"\n    Performs DBSCAN clustering on the data by transforming it into a DataFrame and recording the clusters in a new column named 'Cluster'.\n    The function takes in a DataFrame and a list of column names, and returns the DataFrame with a new 'Cluster' column.\n    \"\"\"\n    df = pd.DataFrame(data, columns=cols)\n    dbscan = DBSCAN(eps=3, min_samples=2)\n    df['Cluster'] = dbscan.fit_predict(df[cols])\n    return df"}
{"task_id": "BigCodeBench/704", "solution": "import pandas as pd\nfrom itertools import combinations\nMIN_PERCENTAGE = 0.75\ndef task_func(data, cols, percentage):\n    \"\"\"\n    Find all combinations of columns from a given DataFrame so that the absolute correlation between them is greater than a certain threshold.\n    \"\"\"\n    corr_combinations = []\n    for i in range(2, len(cols) + 1):\n        for combination in combinations(cols, i):\n            corr = data[list(combination)].corr().abs()\n            if (corr.values >= percentage).all().all():\n                corr_combinations.append(combination)\n    return corr_combinations\ndata = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [2, 3, 4, 5, 6],\n    'C': [3, 4, 5, 6, 7],\n    'D': [4, 5, 6, 7, 8],\n    'E': [5, 6, 7, 8, 9]\n})\ncols = ['A', 'B', 'C', 'D', 'E']\npercentage = MIN_PERCENTAGE", "raw_solution": "import pandas as pd\nfrom itertools import combinations\n\n# Constants\nMIN_PERCENTAGE = 0.75\n\ndef task_func(data, cols, percentage):\n    \"\"\"\n    Find all combinations of columns from a given DataFrame so that the absolute correlation between them is greater than a certain threshold.\n    \"\"\"\n    corr_combinations = []\n    for i in range(2, len(cols) + 1):\n        for combination in combinations(cols, i):\n            corr = data[list(combination)].corr().abs()\n            if (corr.values >= percentage).all().all():\n                corr_combinations.append(combination)\n    return corr_combinations\n\n# Test the function\ndata = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [2, 3, 4, 5, 6],\n    'C': [3, 4, 5, 6, 7],\n    'D': [4, 5, 6, 7, 8],\n    'E': [5, 6, 7, 8, 9]\n})\n\ncols = ['A', 'B', 'C', 'D', 'E']\npercentage = MIN_PERCENTAGE\n"}
{"task_id": "BigCodeBench/705", "solution": "import numpy as np\nfrom scipy import stats\ndef task_func(df, column, alpha):\n    \"\"\"\n    Tests the normality of a numeric column from a DataFrame using the Shapiro-Wilk test.\n    The function returns True if the column passes the normality test, False otherwise.\n    \"\"\"\n    # Extract the column data\n    data = df[column]\n\n    # Perform the Shapiro-Wilk test\n    stat, p = stats.shapiro(data)\n\n    # If p-value is greater than alpha, we fail to reject the null hypothesis\n    # and conclude that the data is normally distributed\n    return p > alpha", "raw_solution": "import numpy as np\nfrom scipy import stats\n\ndef task_func(df, column, alpha):\n    \"\"\"\n    Tests the normality of a numeric column from a DataFrame using the Shapiro-Wilk test.\n    The function returns True if the column passes the normality test, False otherwise.\n    \"\"\"\n    # Extract the column data\n    data = df[column]\n\n    # Perform the Shapiro-Wilk test\n    stat, p = stats.shapiro(data)\n\n    # If p-value is greater than alpha, we fail to reject the null hypothesis\n    # and conclude that the data is normally distributed\n    return p > alpha"}
{"task_id": "BigCodeBench/706", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\ndef task_func(data, columns, target_column):\n    # Split the data into features and target\n    X = data[columns]\n    y = data[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Initialize the logistic regression model\n    model = LogisticRegression()\n\n    # Fit the model to the training data\n    model.fit(X_train, y_train)\n\n    # Make predictions on the testing set\n    y_pred = model.predict(X_test)\n\n    # Calculate the accuracy of the model\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef task_func(data, columns, target_column):\n    # Split the data into features and target\n    X = data[columns]\n    y = data[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Initialize the logistic regression model\n    model = LogisticRegression()\n\n    # Fit the model to the training data\n    model.fit(X_train, y_train)\n\n    # Make predictions on the testing set\n    y_pred = model.predict(X_test)\n\n    # Calculate the accuracy of the model\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy"}
{"task_id": "BigCodeBench/707", "solution": "import json\nimport numpy as np\nimport pandas as pd\ndef task_func(df):\n    df['IntCol'] = np.log10(df['IntCol'])\n    df['IntCol'] = df['IntCol'].apply(lambda x: int(x))\n    with open('IntCol.json', 'w') as f:\n        json.dump(df['IntCol'].tolist(), f)\n    return df", "raw_solution": "import json\nimport numpy as np\nimport pandas as pd\n\ndef task_func(df):\n    df['IntCol'] = np.log10(df['IntCol'])\n    df['IntCol'] = df['IntCol'].apply(lambda x: int(x))\n    with open('IntCol.json', 'w') as f:\n        json.dump(df['IntCol'].tolist(), f)\n    return df"}
{"task_id": "BigCodeBench/708", "solution": "import json\nimport csv\nimport os\nimport base64\ndef task_func(raw_string, filename, output_dir):\n    \"\"\"\n    Processes a base64-encoded JSON string, stores the data in a CSV file, and returns the path of the file.\n    \"\"\"\n    # Decode the base64 string\n    decoded_string = base64.b64decode(raw_string).decode('utf-8')\n\n    # Convert the decoded string to a JSON object\n    json_data = json.loads(decoded_string)\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create the file path\n    file_path = os.path.join(output_dir, filename)\n\n    # Write the JSON data to a CSV file\n    with open(file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        for key in json_data:\n            writer.writerow([key, json_data[key]])\n\n    return file_path", "raw_solution": "import json\nimport csv\nimport os\nimport base64\n\ndef task_func(raw_string, filename, output_dir):\n    \"\"\"\n    Processes a base64-encoded JSON string, stores the data in a CSV file, and returns the path of the file.\n    \"\"\"\n    # Decode the base64 string\n    decoded_string = base64.b64decode(raw_string).decode('utf-8')\n\n    # Convert the decoded string to a JSON object\n    json_data = json.loads(decoded_string)\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create the file path\n    file_path = os.path.join(output_dir, filename)\n\n    # Write the JSON data to a CSV file\n    with open(file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        for key in json_data:\n            writer.writerow([key, json_data[key]])\n\n    return file_path"}
{"task_id": "BigCodeBench/709", "solution": "import base64\nimport re\nfrom html import unescape\nimport textwrap\ndef task_func(raw_string, line_length):\n    # Decode base64\n    decoded_string = base64.b64decode(raw_string).decode()\n\n    # Decouple HTML entities\n    decoupled_string = unescape(decoded_string)\n\n    # Replace multiple spaces with a single space\n    single_space_string = re.sub(' +', ' ', decoupled_string)\n\n    # Strip leading and subsequent spaces\n    stripped_string = single_space_string.strip()\n\n    # Wrap text to a certain line length\n    wrapped_text = textwrap.fill(stripped_string, line_length)\n\n    return wrapped_text", "raw_solution": "import base64\nimport re\nfrom html import unescape\nimport textwrap\n\ndef task_func(raw_string, line_length):\n    # Decode base64\n    decoded_string = base64.b64decode(raw_string).decode()\n\n    # Decouple HTML entities\n    decoupled_string = unescape(decoded_string)\n\n    # Replace multiple spaces with a single space\n    single_space_string = re.sub(' +', ' ', decoupled_string)\n\n    # Strip leading and subsequent spaces\n    stripped_string = single_space_string.strip()\n\n    # Wrap text to a certain line length\n    wrapped_text = textwrap.fill(stripped_string, line_length)\n\n    return wrapped_text"}
{"task_id": "BigCodeBench/710", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data_path):\n    \"\"\"\n    Normalizes a dataset from a .csv file.\n    \"\"\"\n    # Load the dataset\n    df = pd.read_csv(data_path)\n\n    # Initialize a scaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    df[df.columns] = scaler.fit_transform(df[df.columns])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data_path):\n    \"\"\"\n    Normalizes a dataset from a .csv file.\n    \"\"\"\n    # Load the dataset\n    df = pd.read_csv(data_path)\n\n    # Initialize a scaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    df[df.columns] = scaler.fit_transform(df[df.columns])\n\n    return df"}
{"task_id": "BigCodeBench/711", "solution": "import json\nimport csv\nimport os\ndef task_func(json_file, csv_file):\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(data[0].keys())\n        for row in data:\n            writer.writerow(row.values())\n\n    return csv_file", "raw_solution": "import json\nimport csv\nimport os\n\ndef task_func(json_file, csv_file):\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(data[0].keys())\n        for row in data:\n            writer.writerow(row.values())\n\n    return csv_file"}
{"task_id": "BigCodeBench/712", "solution": "import os\nimport shutil\nimport glob\ndef task_func(source_dir, dest_dir, extension):\n    \"\"\"\n    Moves all files with a particular extension from one directory to another.\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(source_dir):\n        return \"Source directory does not exist.\"\n\n    # Check if destination directory exists, if not, create it\n    if not os.path.isdir(dest_dir):\n        os.makedirs(dest_dir)\n\n    # Get a list of all files with the specified extension in the source directory\n    files = glob.glob(os.path.join(source_dir, f'*.{extension}'))\n\n    # Move each file to the destination directory\n    for file in files:\n        shutil.move(file, dest_dir)\n\n    # Count the number of files moved\n    count = len(glob.glob(os.path.join(dest_dir, f'*.{extension}')))\n\n    return count", "raw_solution": "import os\nimport shutil\nimport glob\n\ndef task_func(source_dir, dest_dir, extension):\n    \"\"\"\n    Moves all files with a particular extension from one directory to another.\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(source_dir):\n        return \"Source directory does not exist.\"\n\n    # Check if destination directory exists, if not, create it\n    if not os.path.isdir(dest_dir):\n        os.makedirs(dest_dir)\n\n    # Get a list of all files with the specified extension in the source directory\n    files = glob.glob(os.path.join(source_dir, f'*.{extension}'))\n\n    # Move each file to the destination directory\n    for file in files:\n        shutil.move(file, dest_dir)\n\n    # Count the number of files moved\n    count = len(glob.glob(os.path.join(dest_dir, f'*.{extension}')))\n\n    return count"}
{"task_id": "BigCodeBench/713", "solution": "import os\nimport re\ndef task_func(log_file_path: str, keywords: list):\n    \"\"\"\n    This function reads a log file and formats the lines that contain certain keywords.\n    It searches for lines containing any of the keywords provided in the list and formats each line to display the keyword, the timestamp, and the message separated by 20 spaces.\n    \"\"\"\n    if not os.path.exists(log_file_path):\n        return \"Log file does not exist.\"\n\n    formatted_lines = []\n    with open(log_file_path, 'r') as log_file:\n        for line in log_file:\n            for keyword in keywords:\n                if keyword in line:\n                    timestamp = line[:23]\n                    message = line[23:].strip()\n                    formatted_line = f\"{keyword} {timestamp} {' '*20} {message}\"\n                    formatted_lines.append(formatted_line)\n                    break\n\n    return formatted_lines", "raw_solution": "import os\nimport re\n\ndef task_func(log_file_path: str, keywords: list):\n    \"\"\"\n    This function reads a log file and formats the lines that contain certain keywords.\n    It searches for lines containing any of the keywords provided in the list and formats each line to display the keyword, the timestamp, and the message separated by 20 spaces.\n    \"\"\"\n    if not os.path.exists(log_file_path):\n        return \"Log file does not exist.\"\n\n    formatted_lines = []\n    with open(log_file_path, 'r') as log_file:\n        for line in log_file:\n            for keyword in keywords:\n                if keyword in line:\n                    timestamp = line[:23]\n                    message = line[23:].strip()\n                    formatted_line = f\"{keyword} {timestamp} {' '*20} {message}\"\n                    formatted_lines.append(formatted_line)\n                    break\n\n    return formatted_lines"}
{"task_id": "BigCodeBench/714", "solution": "import sys\nfrom pathlib import Path\nPATH_TO_APPEND = '/path/to/whatever'\ndef task_func(path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Adds a specific path to sys.path and creates a directory in that path if it does not exist.\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Create a directory in the path if it does not exist\n    Path(path_to_append).mkdir(parents=True, exist_ok=True)\n\n    return path_to_append", "raw_solution": "import sys\nfrom pathlib import Path\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\n\ndef task_func(path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Adds a specific path to sys.path and creates a directory in that path if it does not exist.\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Create a directory in the path if it does not exist\n    Path(path_to_append).mkdir(parents=True, exist_ok=True)\n\n    return path_to_append\n\n# Test the function"}
{"task_id": "BigCodeBench/715", "solution": "import sys\nimport subprocess\nPYTHON_VERSION = '3.8'\nPATH_TO_APPEND = '/path/to/whatever'\ndef task_func(python_version=PYTHON_VERSION, path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Switches to a specific version of Python and adds a specific path to sys.path.\n    \"\"\"\n    # Switch to the specified Python version\n    subprocess.run(['pyenv', 'local', python_version], check=True)\n\n    # Add the specified path to sys.path\n    sys.path.append(path_to_append)\n\n    # Return the Python version that was switched to\n    return python_version", "raw_solution": "import sys\nimport subprocess\n\n# Constants\nPYTHON_VERSION = '3.8'\nPATH_TO_APPEND = '/path/to/whatever'\n\ndef task_func(python_version=PYTHON_VERSION, path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Switches to a specific version of Python and adds a specific path to sys.path.\n    \"\"\"\n    # Switch to the specified Python version\n    subprocess.run(['pyenv', 'local', python_version], check=True)\n\n    # Add the specified path to sys.path\n    sys.path.append(path_to_append)\n\n    # Return the Python version that was switched to\n    return python_version\n\n# Test the function\nassert task_func() == PYTHON_VERSION\nassert PATH_TO_APPEND in sys.path"}
{"task_id": "BigCodeBench/716", "solution": "import sys\nimport json\nfrom datetime import datetime\nPATH_TO_APPEND = '/path/to/whatever'\nJSON_FILE = '/path/to/json_file.json'\ndef task_func(path_to_append=PATH_TO_APPEND, json_file=JSON_FILE):\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Load the JSON file\n    with open(json_file, 'r') as file:\n        json_data = json.load(file)\n\n    # Update the 'last_updated' key with the current date and time\n    json_data['last_updated'] = str(datetime.now())\n\n    # Save the updated JSON data back to the file\n    with open(json_file, 'w') as file:\n        json.dump(json_data, file)\n\n    return json_data", "raw_solution": "import sys\nimport json\nfrom datetime import datetime\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\nJSON_FILE = '/path/to/json_file.json'\n\ndef task_func(path_to_append=PATH_TO_APPEND, json_file=JSON_FILE):\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Load the JSON file\n    with open(json_file, 'r') as file:\n        json_data = json.load(file)\n\n    # Update the 'last_updated' key with the current date and time\n    json_data['last_updated'] = str(datetime.now())\n\n    # Save the updated JSON data back to the file\n    with open(json_file, 'w') as file:\n        json.dump(json_data, file)\n\n    return json_data"}
{"task_id": "BigCodeBench/717", "solution": "import sys\nfrom configparser import ConfigParser\nPATH_TO_APPEND = '/path/to/whatever'\nCONFIG_FILE = '/path/to/config.ini'\ndef task_func(path_to_append=PATH_TO_APPEND, config_file=CONFIG_FILE):\n    \"\"\"\n    Adds a specific path to sys.path and updates a configuration file with this path.\n    \"\"\"\n    # Add the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Parse the configuration file\n    config = ConfigParser()\n    config.read(config_file)\n\n    # Update the configuration file with the new path\n    config.set('DEFAULT', 'path', path_to_append)\n\n    with open(config_file, 'w') as file:\n        config.write(file)\n\n    return config, config_file", "raw_solution": "import sys\nfrom configparser import ConfigParser\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\nCONFIG_FILE = '/path/to/config.ini'\n\ndef task_func(path_to_append=PATH_TO_APPEND, config_file=CONFIG_FILE):\n    \"\"\"\n    Adds a specific path to sys.path and updates a configuration file with this path.\n    \"\"\"\n    # Add the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Parse the configuration file\n    config = ConfigParser()\n    config.read(config_file)\n\n    # Update the configuration file with the new path\n    config.set('DEFAULT', 'path', path_to_append)\n\n    with open(config_file, 'w') as file:\n        config.write(file)\n\n    return config, config_file"}
{"task_id": "BigCodeBench/718", "solution": "import re\nimport numpy as np\nfrom scipy.stats import ttest_rel\ndef task_func(text1, text2):\n    # Split the texts into words\n    words1 = re.findall(r'\\b\\w+\\b', text1)\n    words2 = re.findall(r'\\b\\w+\\b', text2)\n\n    # Check if the lengths of the words lists are equal\n    if len(words1) != len(words2):\n        # If not, return NaN for t_statistic and p_value\n        return np.nan, np.nan\n\n    # Convert the word lists to numpy arrays\n    words1 = np.array(words1)\n    words2 = np.array(words2)\n\n    # Perform the paired t-test\n    t_statistic, p_value = ttest_rel(words1, words2)\n\n    return t_statistic, p_value", "raw_solution": "import re\nimport numpy as np\nfrom scipy.stats import ttest_rel\n\ndef task_func(text1, text2):\n    # Split the texts into words\n    words1 = re.findall(r'\\b\\w+\\b', text1)\n    words2 = re.findall(r'\\b\\w+\\b', text2)\n\n    # Check if the lengths of the words lists are equal\n    if len(words1) != len(words2):\n        # If not, return NaN for t_statistic and p_value\n        return np.nan, np.nan\n\n    # Convert the word lists to numpy arrays\n    words1 = np.array(words1)\n    words2 = np.array(words2)\n\n    # Perform the paired t-test\n    t_statistic, p_value = ttest_rel(words1, words2)\n\n    return t_statistic, p_value"}
{"task_id": "BigCodeBench/719", "solution": "import re\nimport os\nimport glob\ndef task_func(directory, word):\n    count = 0\n    for filename in glob.glob(os.path.join(directory, '*')):\n        if os.path.isfile(filename):\n            with open(filename, 'r') as file:\n                if re.search(r'\\b' + word + r'\\b', file.read()):\n                    count += 1\n    return count", "raw_solution": "import re\nimport os\nimport glob\n\ndef task_func(directory, word):\n    count = 0\n    for filename in glob.glob(os.path.join(directory, '*')):\n        if os.path.isfile(filename):\n            with open(filename, 'r') as file:\n                if re.search(r'\\b' + word + r'\\b', file.read()):\n                    count += 1\n    return count\n\n# Test the function"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Create the directory if it doesn't exist\n    if not os.path.exists('task_func_data'):\n        os.makedirs('task_func_data')\n\n    # Generate random temperature and humidity data\n    temperature = [random.uniform(20, 30) for _ in range(10)]\n    humidity = [random.uniform(40, 80) for _ in range(10)]\n\n    # Create a CSV file in append mode\n    with open('task_func_data/Output.txt', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Timestamp', 'Temperature', 'Humidity'])\n        for i in range(10):\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            writer.writerow([timestamp, temperature[i], humidity[i]])\n\n    # Return the path to the CSV file\n    return os.path.abspath('task_func_data/Output.txt')", "raw_solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Create the directory if it doesn't exist\n    if not os.path.exists('task_func_data'):\n        os.makedirs('task_func_data')\n\n    # Generate random temperature and humidity data\n    temperature = [random.uniform(20, 30) for _ in range(10)]\n    humidity = [random.uniform(40, 80) for _ in range(10)]\n\n    # Create a CSV file in append mode\n    with open('task_func_data/Output.txt', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Timestamp', 'Temperature', 'Humidity'])\n        for i in range(10):\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            writer.writerow([timestamp, temperature[i], humidity[i]])\n\n    # Return the path to the CSV file\n    return os.path.abspath('task_func_data/Output.txt')\n\n# Test the function"}
{"task_id": "BigCodeBench/721", "solution": "import os\nimport csv\nfrom collections import Counter\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return None\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        words = [word for row in reader for word in row]\n\n    if not words:\n        return None\n\n    word_counts = Counter(words)\n    most_common_word = word_counts.most_common(1)[0]\n\n    return most_common_word", "raw_solution": "import os\nimport csv\nfrom collections import Counter\n\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return None\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        words = [word for row in reader for word in row]\n\n    if not words:\n        return None\n\n    word_counts = Counter(words)\n    most_common_word = word_counts.most_common(1)[0]\n\n    return most_common_word"}
{"task_id": "BigCodeBench/722", "solution": "import urllib.request\nimport os\nimport re\nTARGET_FILE = 'downloaded_file.txt'\nSEARCH_PATTERN = r'\\bERROR\\b'\ndef task_func(url):\n    # Download the file\n    urllib.request.urlretrieve(url, TARGET_FILE)\n\n    # Check if the file exists\n    if not os.path.isfile(TARGET_FILE):\n        return \"File not found\"\n\n    # Open the file and read its content\n    with open(TARGET_FILE, 'r') as file:\n        content = file.read()\n\n    # Use regular expression to find all occurrences of 'ERROR'\n    occurrences = len(re.findall(SEARCH_PATTERN, content))\n\n    # Remove the downloaded file\n    os.remove(TARGET_FILE)\n\n    return occurrences", "raw_solution": "import urllib.request\nimport os\nimport re\n\n# Constants\nTARGET_FILE = 'downloaded_file.txt'\nSEARCH_PATTERN = r'\\bERROR\\b'\n\ndef task_func(url):\n    # Download the file\n    urllib.request.urlretrieve(url, TARGET_FILE)\n\n    # Check if the file exists\n    if not os.path.isfile(TARGET_FILE):\n        return \"File not found\"\n\n    # Open the file and read its content\n    with open(TARGET_FILE, 'r') as file:\n        content = file.read()\n\n    # Use regular expression to find all occurrences of 'ERROR'\n    occurrences = len(re.findall(SEARCH_PATTERN, content))\n\n    # Remove the downloaded file\n    os.remove(TARGET_FILE)\n\n    return occurrences"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Scrapes data from a given URL and saves the scraped data to a CSV file.\n    \"\"\"\n    # Send a request to the URL\n    response = urllib.request.urlopen(url)\n\n    # Parse the HTML content\n    soup = BeautifulSoup(response, 'html.parser')\n\n    # Find the data you want to scrape\n    # This is a placeholder, you need to replace it with the actual HTML tags and attributes\n    data = soup.find_all('div', {'class': 'data'})\n\n    # Write the data to a CSV file\n    with open(CSV_FILE_PATH, 'w', newline='') as file:\n        writer = csv.writer(file)\n        for item in data:\n            writer.writerow([item.text])\n\n    return CSV_FILE_PATH", "raw_solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    Scrapes data from a given URL and saves the scraped data to a CSV file.\n    \"\"\"\n    # Send a request to the URL\n    response = urllib.request.urlopen(url)\n\n    # Parse the HTML content\n    soup = BeautifulSoup(response, 'html.parser')\n\n    # Find the data you want to scrape\n    # This is a placeholder, you need to replace it with the actual HTML tags and attributes\n    data = soup.find_all('div', {'class': 'data'})\n\n    # Write the data to a CSV file\n    with open(CSV_FILE_PATH, 'w', newline='') as file:\n        writer = csv.writer(file)\n        for item in data:\n            writer.writerow([item.text])\n\n    return CSV_FILE_PATH"}
{"task_id": "BigCodeBench/724", "solution": "import os\nimport json\ndef task_func(config_path: str) -> dict:\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\"The configuration file {config_path} does not exist.\")\n\n    with open(config_path, 'r') as file:\n        config = json.load(file)\n\n    return config", "raw_solution": "import os\nimport json\n\ndef task_func(config_path: str) -> dict:\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\"The configuration file {config_path} does not exist.\")\n\n    with open(config_path, 'r') as file:\n        config = json.load(file)\n\n    return config"}
{"task_id": "BigCodeBench/725", "solution": "import codecs\nimport os\nimport glob\nDIRECTORY_PATH = './files/'\ndef task_func(directory=DIRECTORY_PATH, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    Converts the encoding of all text files in a specified directory from one encoding to another.\n    The function modifies the files in-place.\n    \"\"\"\n    # Get all text files in the directory\n    files = glob.glob(os.path.join(directory, '*.txt'))\n\n    # Iterate over each file\n    for file in files:\n        # Read the file in the original encoding\n        with codecs.open(file, 'r', from_encoding) as f:\n            content = f.read()\n\n        # Write the file in the new encoding\n        with codecs.open(file, 'w', to_encoding) as f:\n            f.write(content)\n\n    # Return None\n    return None", "raw_solution": "import codecs\nimport os\nimport glob\n\n# Constants\nDIRECTORY_PATH = './files/'\n\ndef task_func(directory=DIRECTORY_PATH, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    Converts the encoding of all text files in a specified directory from one encoding to another.\n    The function modifies the files in-place.\n    \"\"\"\n    # Get all text files in the directory\n    files = glob.glob(os.path.join(directory, '*.txt'))\n\n    # Iterate over each file\n    for file in files:\n        # Read the file in the original encoding\n        with codecs.open(file, 'r', from_encoding) as f:\n            content = f.read()\n\n        # Write the file in the new encoding\n        with codecs.open(file, 'w', to_encoding) as f:\n            f.write(content)\n\n    # Return None\n    return None"}
{"task_id": "BigCodeBench/726", "solution": "import re\nimport random\nfrom nltk.corpus import words\nfrom random import sample\nimport nltk\nSAMPLE_ENGLISH_WORDS = set(words.words())\ndef task_func(s, n):\n    # Convert the string to lowercase and split it into words\n    words = re.findall(r'\\b\\w+\\b', s.lower())\n    # Find the distinct English words in the list\n    english_words = [word for word in words if word in SAMPLE_ENGLISH_WORDS]\n    # Return up to n different English words\n    return list(set(english_words))[:n]", "raw_solution": "import re\nimport random\nfrom nltk.corpus import words\nfrom random import sample\n# Ensure the words corpus is downloaded\nimport nltk\nnltk.download('words')\n# Constants\nSAMPLE_ENGLISH_WORDS = set(words.words())  # Correct initialization\n\ndef task_func(s, n):\n    # Convert the string to lowercase and split it into words\n    words = re.findall(r'\\b\\w+\\b', s.lower())\n    # Find the distinct English words in the list\n    english_words = [word for word in words if word in SAMPLE_ENGLISH_WORDS]\n    # Return up to n different English words\n    return list(set(english_words))[:n]\n\n# Test the function"}
{"task_id": "BigCodeBench/727", "solution": "import re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\ndef task_func(s: str) -> np.ndarray:\n    \"\"\"\n    Vectorizes a string using the Bag-of-Words model.\n    \"\"\"\n    # Split the string into words\n    words = re.findall(r'\\b\\w+\\b', s)\n\n    # Create a CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the words\n    X = vectorizer.fit_transform(words)\n\n    # Return the transformed data as a numpy array\n    return X.toarray()", "raw_solution": "import re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Constants\nSENTENCES = ['This is a sentence', 'Another sentence here', 'More sentences']\n\ndef task_func(s: str) -> np.ndarray:\n    \"\"\"\n    Vectorizes a string using the Bag-of-Words model.\n    \"\"\"\n    # Split the string into words\n    words = re.findall(r'\\b\\w+\\b', s)\n\n    # Create a CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the words\n    X = vectorizer.fit_transform(words)\n\n    # Return the transformed data as a numpy array\n    return X.toarray()\n\n# Test the function\nfor sentence in SENTENCES:\n    print(f\"Vectorizing '{sentence}': {task_func(sentence)}\")"}
{"task_id": "BigCodeBench/728", "solution": "import csv\nimport io\ndef task_func(filename='sample.csv', from_encoding='cp1251', to_encoding='utf8', delimiter=','):\n    with open(filename, 'r', encoding=from_encoding) as file:\n        csv_data = file.read()\n\n    csv_data_converted = csv_data.encode(to_encoding).decode(to_encoding)\n\n    csv_string = io.StringIO(csv_data_converted)\n    csv_reader = csv.reader(csv_string, delimiter=delimiter)\n    headers = next(csv_reader)\n\n    csv_list = []\n    for row in csv_reader:\n        csv_list.append(dict(zip(headers, row)))\n\n    return csv_list, csv_data_converted", "raw_solution": "import csv\nimport io\n\ndef task_func(filename='sample.csv', from_encoding='cp1251', to_encoding='utf8', delimiter=','):\n    with open(filename, 'r', encoding=from_encoding) as file:\n        csv_data = file.read()\n\n    csv_data_converted = csv_data.encode(to_encoding).decode(to_encoding)\n\n    csv_string = io.StringIO(csv_data_converted)\n    csv_reader = csv.reader(csv_string, delimiter=delimiter)\n    headers = next(csv_reader)\n\n    csv_list = []\n    for row in csv_reader:\n        csv_list.append(dict(zip(headers, row)))\n\n    return csv_list, csv_data_converted"}
{"task_id": "BigCodeBench/729", "solution": "import pickle\nimport os\nimport random\nimport string\ndef task_func(strings, filename=None):\n    if filename is None:\n        filename = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)) + '.pkl'\n\n    with open(filename, 'wb') as f:\n        pickle.dump(strings, f)\n\n    with open(filename, 'rb') as f:\n        loaded_strings = pickle.load(f)\n\n    os.remove(filename)\n\n    return loaded_strings", "raw_solution": "import pickle\nimport os\nimport random\nimport string\n\ndef task_func(strings, filename=None):\n    if filename is None:\n        filename = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)) + '.pkl'\n\n    with open(filename, 'wb') as f:\n        pickle.dump(strings, f)\n\n    with open(filename, 'rb') as f:\n        loaded_strings = pickle.load(f)\n\n    os.remove(filename)\n\n    return loaded_strings"}
{"task_id": "BigCodeBench/730", "solution": "import pickle\nimport os\nfrom datetime import datetime\nFILE_NAME = 'save.pkl'\ndef task_func(dt):\n    # Save the datetime object to a pickle file\n    with open(FILE_NAME, 'wb') as f:\n        pickle.dump(dt, f)\n\n    # Load the datetime object from the pickle file\n    with open(FILE_NAME, 'rb') as f:\n        loaded_dt = pickle.load(f)\n\n    # Remove the pickle file\n    os.remove(FILE_NAME)\n\n    return loaded_dt\ndt = datetime.now()", "raw_solution": "import pickle\nimport os\nfrom datetime import datetime\n\n# Constants\nFILE_NAME = 'save.pkl'\n\ndef task_func(dt):\n    # Save the datetime object to a pickle file\n    with open(FILE_NAME, 'wb') as f:\n        pickle.dump(dt, f)\n\n    # Load the datetime object from the pickle file\n    with open(FILE_NAME, 'rb') as f:\n        loaded_dt = pickle.load(f)\n\n    # Remove the pickle file\n    os.remove(FILE_NAME)\n\n    return loaded_dt\n\n# Test the function\ndt = datetime.now()\nloaded_dt = task_func(dt)\nassert loaded_dt == dt, \"The loaded datetime object is not the same as the original one\""}
{"task_id": "BigCodeBench/731", "solution": "import pickle\nimport os\nfrom sklearn.datasets import make_classification\nFILE_NAME = 'save.pkl'\ndef task_func(data, target):\n    # Save the data and target to a pickle file\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump((data, target), file)\n\n    # Load the data and target from the pickle file\n    with open(FILE_NAME, 'rb') as file:\n        loaded_data, loaded_target = pickle.load(file)\n\n    # Remove the pickle file\n    os.remove(FILE_NAME)\n\n    # Return the loaded data and target\n    return (loaded_data, loaded_target)", "raw_solution": "import pickle\nimport os\nfrom sklearn.datasets import make_classification\n\n# Constants\nFILE_NAME = 'save.pkl'\nDATA, TARGET = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=1)\n\ndef task_func(data, target):\n    # Save the data and target to a pickle file\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump((data, target), file)\n\n    # Load the data and target from the pickle file\n    with open(FILE_NAME, 'rb') as file:\n        loaded_data, loaded_target = pickle.load(file)\n\n    # Remove the pickle file\n    os.remove(FILE_NAME)\n\n    # Return the loaded data and target\n    return (loaded_data, loaded_target)\n\n# Test the function\ndata, target = task_func(DATA, TARGET)"}
{"task_id": "BigCodeBench/732", "solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nSTEMMER = PorterStemmer()\ndef task_func(content):\n    # Remove punctuation\n    content = re.sub('['+string.punctuation+']', '', content)\n    # Split the content into words\n    words = content.split()\n    # Stem every word except the last one\n    stems = [STEMMER.stem(word) if i != len(words) - 1 else word for i, word in enumerate(words)]\n    # Count the frequency of each stem\n    freq = Counter(stems)\n    return freq", "raw_solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nSTEMMER = PorterStemmer()\n\ndef task_func(content):\n    # Remove punctuation\n    content = re.sub('['+string.punctuation+']', '', content)\n    # Split the content into words\n    words = content.split()\n    # Stem every word except the last one\n    stems = [STEMMER.stem(word) if i != len(words) - 1 else word for i, word in enumerate(words)]\n    # Count the frequency of each stem\n    freq = Counter(stems)\n    return freq"}
{"task_id": "BigCodeBench/733", "solution": "import re\nimport string\ndef task_func(content):\n    # Define stop words\n    stop_words = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"to\", \"with\", \"by\", \"for\", \"about\", \"of\"]\n\n    # Remove punctuation\n    content = re.sub(r'[^\\w\\s]', '', content)\n\n    # Split the sentence into words\n    words = content.split()\n\n    # Count non-stop words\n    count = sum(1 for word in words[:-1] if word.lower() not in stop_words)\n\n    return count", "raw_solution": "import re\nimport string\n\ndef task_func(content):\n    # Define stop words\n    stop_words = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"to\", \"with\", \"by\", \"for\", \"about\", \"of\"]\n\n    # Remove punctuation\n    content = re.sub(r'[^\\w\\s]', '', content)\n\n    # Split the sentence into words\n    words = content.split()\n\n    # Count non-stop words\n    count = sum(1 for word in words[:-1] if word.lower() not in stop_words)\n\n    return count\n\n# Test the function\nassert task_func(\"This is a test sentence.\") == 3\nassert task_func(\"Count the non-stop words in a sentence without the last word.\") == 7\nassert task_func(\"A quick brown fox jumps over the lazy dog.\") == 7"}
{"task_id": "BigCodeBench/734", "solution": "import nltk\nfrom collections import Counter\ndef task_func(content):\n    # Tokenize the sentence\n    tokens = nltk.word_tokenize(content)\n\n    # Tag the POS\n    tagged = nltk.pos_tag(tokens)\n\n    # Count the POS tags\n    pos_counts = Counter(tag for word, tag in tagged)\n\n    # Remove the last word from the count\n    pos_counts.pop(tagged[-1][1])\n\n    return pos_counts", "raw_solution": "import nltk\nfrom collections import Counter\n\n# Download necessary NLTK data (if not already present)\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\ndef task_func(content):\n    # Tokenize the sentence\n    tokens = nltk.word_tokenize(content)\n\n    # Tag the POS\n    tagged = nltk.pos_tag(tokens)\n\n    # Count the POS tags\n    pos_counts = Counter(tag for word, tag in tagged)\n\n    # Remove the last word from the count\n    pos_counts.pop(tagged[-1][1])\n\n    return pos_counts"}
{"task_id": "BigCodeBench/735", "solution": "import numpy as np\nfrom itertools import chain\ndef task_func(L):\n    flat_list = list(chain(*L))\n    mean = np.mean(flat_list)\n    variance = np.var(flat_list)\n    return {'mean': mean, 'variance': variance}\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]", "raw_solution": "import numpy as np\nfrom itertools import chain\n\ndef task_func(L):\n    flat_list = list(chain(*L))\n    mean = np.mean(flat_list)\n    variance = np.var(flat_list)\n    return {'mean': mean, 'variance': variance}\n\n# Test the function\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/736", "solution": "import numpy as np\nfrom scipy import stats\ndef task_func(L):\n    \"\"\"\n    Calculates the mode of all elements in a nested list 'L'.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n\n    # Calculate the mode\n    mode = stats.mode(flat_list)\n\n    return mode.mode[0]\nL = [[1, 2, 2, 3], [4, 5, 5, 5], [6, 7, 7, 8]]", "raw_solution": "import numpy as np\nfrom scipy import stats\n\ndef task_func(L):\n    \"\"\"\n    Calculates the mode of all elements in a nested list 'L'.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n\n    # Calculate the mode\n    mode = stats.mode(flat_list)\n\n    return mode.mode[0]\n\n# Test the function\nL = [[1, 2, 2, 3], [4, 5, 5, 5], [6, 7, 7, 8]]"}
{"task_id": "BigCodeBench/737", "solution": "import numpy as np\nimport math\ndef task_func(L):\n    \"\"\"\n    Calculates the median of all elements in a nested list 'L'.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n\n    # Calculate the median\n    median = np.median(flat_list)\n\n    return median", "raw_solution": "import numpy as np\nimport math\n\ndef task_func(L):\n    \"\"\"\n    Calculates the median of all elements in a nested list 'L'.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n\n    # Calculate the median\n    median = np.median(flat_list)\n\n    return median"}
{"task_id": "BigCodeBench/738", "solution": "import numpy as np\nfrom scipy.stats import iqr\ndef task_func(L):\n    \"\"\"\n    Calculates the interquartile range of all elements in a nested list 'L'.\n    \"\"\"\n    # Flatten the nested list\n    flat_list = [item for sublist in L for item in sublist]\n\n    # Calculate the interquartile range\n    iqr_value = iqr(flat_list)\n\n    return iqr_value\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]", "raw_solution": "import numpy as np\nfrom scipy.stats import iqr\n\ndef task_func(L):\n    \"\"\"\n    Calculates the interquartile range of all elements in a nested list 'L'.\n    \"\"\"\n    # Flatten the nested list\n    flat_list = [item for sublist in L for item in sublist]\n\n    # Calculate the interquartile range\n    iqr_value = iqr(flat_list)\n\n    return iqr_value\n\n# Test the function\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/739", "solution": "import struct\nimport random\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_key=None):\n    if hex_key is None:\n        hex_key = random.choice(KEYS)\n    float_value = struct.unpack('f', bytes.fromhex(hex_key))[0]\n    rounded_float = round(float_value, 2)\n    return rounded_float", "raw_solution": "import struct\nimport random\n\n# Constants\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef task_func(hex_key=None):\n    if hex_key is None:\n        hex_key = random.choice(KEYS)\n    float_value = struct.unpack('f', bytes.fromhex(hex_key))[0]\n    rounded_float = round(float_value, 2)\n    return rounded_float\n\n# Test the function\nfor _ in range(5):\n    print(task_func())"}
{"task_id": "BigCodeBench/740", "solution": "from collections import Counter\nimport heapq\nimport random\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\ndef task_func(my_dict):\n    # Generate a dictionary with random integers\n    if not my_dict:\n        my_dict = {ch: random.randint(1, 100) for ch in LETTERS}\n\n    # Count the frequency of each letter\n    counter = Counter(my_dict)\n\n    # Find the 3 most common letters\n    most_common_letters = heapq.nlargest(3, counter.items(), key=lambda x: x[1])\n    most_common_letters = [letter for letter, count in most_common_letters]\n\n    return most_common_letters\nmy_dict = {'a': 5, 'b': 10, 'c': 15, 'd': 20, 'e': 25, 'f': 30, 'g': 35}", "raw_solution": "from collections import Counter\nimport heapq\nimport random\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef task_func(my_dict):\n    # Generate a dictionary with random integers\n    if not my_dict:\n        my_dict = {ch: random.randint(1, 100) for ch in LETTERS}\n\n    # Count the frequency of each letter\n    counter = Counter(my_dict)\n\n    # Find the 3 most common letters\n    most_common_letters = heapq.nlargest(3, counter.items(), key=lambda x: x[1])\n    most_common_letters = [letter for letter, count in most_common_letters]\n\n    return most_common_letters\n\n# Test the function\nmy_dict = {'a': 5, 'b': 10, 'c': 15, 'd': 20, 'e': 25, 'f': 30, 'g': 35}"}
{"task_id": "BigCodeBench/741", "solution": "from itertools import groupby\nfrom operator import itemgetter\nKEY_FUNC = itemgetter(0)\ndef task_func(my_dict):\n    \"\"\"\n    Groups the dictionary entries after the first character of the key and adds the values for each group.\n    \"\"\"\n    # Sort the dictionary by the first character of the key\n    sorted_dict = dict(sorted(my_dict.items(), key=KEY_FUNC))\n\n    # Group the sorted dictionary by the first character of the key\n    grouped_dict = {key: sum(val for _, val in group) for key, group in groupby(sorted_dict.items(), KEY_FUNC)}\n\n    return grouped_dict", "raw_solution": "from itertools import groupby\nfrom operator import itemgetter\n\n# Constants\nKEY_FUNC = itemgetter(0)\n\ndef task_func(my_dict):\n    \"\"\"\n    Groups the dictionary entries after the first character of the key and adds the values for each group.\n    \"\"\"\n    # Sort the dictionary by the first character of the key\n    sorted_dict = dict(sorted(my_dict.items(), key=KEY_FUNC))\n\n    # Group the sorted dictionary by the first character of the key\n    grouped_dict = {key: sum(val for _, val in group) for key, group in groupby(sorted_dict.items(), KEY_FUNC)}\n\n    return grouped_dict\n\n# Test the function\nassert task_func({'apple': 5, 'banana': 3, 'cherry': 2, 'date': 4, 'elderberry': 1}) == {'a': 12, 'b': 3, 'c': 2, 'd': 4, 'e': 1}\nassert task_func({'apple': 1, 'banana': 2, 'cherry': 3, 'date': 4, 'elderberry': 5}) == {'a': 15, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\nassert task_func({'apple': 10, 'banana': 20, 'cherry': 30, 'date': 40, 'elderberry': 50}) == {'a': 150, 'b': 20, 'c': 30, 'd': 40, 'e': 50}"}
{"task_id": "BigCodeBench/742", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(list_of_pairs):\n    if not list_of_pairs:\n        raise Exception(\"Input array is empty\")\n    for pair in list_of_pairs:\n        if not isinstance(pair, tuple) or not len(pair) == 2 or not isinstance(pair[0], str) or not isinstance(pair[1], (int, float)):\n            raise ValueError(\"Values are not numeric\")\n    data = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n    scaler = MinMaxScaler()\n    data['Value'] = scaler.fit_transform(data[['Value']])\n    return data", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(list_of_pairs):\n    if not list_of_pairs:\n        raise Exception(\"Input array is empty\")\n    for pair in list_of_pairs:\n        if not isinstance(pair, tuple) or not len(pair) == 2 or not isinstance(pair[0], str) or not isinstance(pair[1], (int, float)):\n            raise ValueError(\"Values are not numeric\")\n    data = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n    scaler = MinMaxScaler()\n    data['Value'] = scaler.fit_transform(data[['Value']])\n    return data"}
{"task_id": "BigCodeBench/743", "solution": "import json\nimport os\nPREFIXES = [\"is_\", \"has_\", \"can_\", \"should_\"]\ndef task_func(directory):\n    \"\"\"\n    Counts the occurrence of keys starting with certain prefixes in all JSON files in a directory.\n    \"\"\"\n    stats = {prefix: 0 for prefix in PREFIXES}\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                data = json.load(file)\n\n                for key in data.keys():\n                    for prefix in PREFIXES:\n                        if key.startswith(prefix):\n                            stats[prefix] += 1\n\n    return stats", "raw_solution": "import json\nimport os\n\n# Constants\nPREFIXES = [\"is_\", \"has_\", \"can_\", \"should_\"]\n\ndef task_func(directory):\n    \"\"\"\n    Counts the occurrence of keys starting with certain prefixes in all JSON files in a directory.\n    \"\"\"\n    stats = {prefix: 0 for prefix in PREFIXES}\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                data = json.load(file)\n\n                for key in data.keys():\n                    for prefix in PREFIXES:\n                        if key.startswith(prefix):\n                            stats[prefix] += 1\n\n    return stats"}
{"task_id": "BigCodeBench/744", "solution": "import pandas as pd\nfrom string import punctuation\ndef task_func(text):\n    if not isinstance(text, str):\n        raise ValueError(\"Input should be a string\")\n\n    # Remove punctuation from the text\n    for p in punctuation:\n        text = text.replace(p, '')\n\n    # Split the text into words\n    words = text.split()\n\n    # Find words starting with '$'\n    dollar_words = [word for word in words if word.startswith('$')]\n\n    # Compute the frequency of each word\n    freq = {}\n    for word in dollar_words:\n        if word in freq:\n            freq[word] += 1\n        else:\n            freq[word] = 1\n\n    # Create a DataFrame from the frequency dictionary\n    df = pd.DataFrame(list(freq.items()), columns=['Word', 'Frequency'])\n\n    return df", "raw_solution": "import pandas as pd\nfrom string import punctuation\n\ndef task_func(text):\n    if not isinstance(text, str):\n        raise ValueError(\"Input should be a string\")\n\n    # Remove punctuation from the text\n    for p in punctuation:\n        text = text.replace(p, '')\n\n    # Split the text into words\n    words = text.split()\n\n    # Find words starting with '$'\n    dollar_words = [word for word in words if word.startswith('$')]\n\n    # Compute the frequency of each word\n    freq = {}\n    for word in dollar_words:\n        if word in freq:\n            freq[word] += 1\n        else:\n            freq[word] = 1\n\n    # Create a DataFrame from the frequency dictionary\n    df = pd.DataFrame(list(freq.items()), columns=['Word', 'Frequency'])\n\n    return df"}
{"task_id": "BigCodeBench/745", "solution": "import subprocess\nimport random\nimport os\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\nSCRIPTS_DIR = '/path/to/scripts'\ndef task_func():\n    # Randomly select a script from the list\n    script = random.choice(SCRIPTS)\n    script_path = os.path.join(SCRIPTS_DIR, script)\n\n    # Execute the script\n    subprocess.run(['bash', script_path])\n\n    return script_path", "raw_solution": "import subprocess\nimport random\nimport os\n\n# Constants\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\nSCRIPTS_DIR = '/path/to/scripts'  \n\ndef task_func():\n    # Randomly select a script from the list\n    script = random.choice(SCRIPTS)\n    script_path = os.path.join(SCRIPTS_DIR, script)\n\n    # Execute the script\n    subprocess.run(['bash', script_path])\n\n    return script_path"}
{"task_id": "BigCodeBench/746", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, target_column, target_values=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df should be a pandas DataFrame\")\n    if not isinstance(target_column, str):\n        raise ValueError(\"target_column should be a string\")\n    if target_values is not None and not hasattr(target_values, '__iter__'):\n        raise ValueError(\"target_values should be an array-like object\")\n\n    if target_values is not None:\n        df = df[df[target_column].isin(target_values)]\n    else:\n        target_values = df[target_column].unique()\n\n    df.loc[:, ~df.columns.isin([target_column])] = df.loc[:, ~df.columns.isin([target_column])].replace(np.nan, 0)\n\n    X = df.loc[:, ~df.columns.isin([target_column])]\n    y = df[target_column]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, target_column, target_values=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df should be a pandas DataFrame\")\n    if not isinstance(target_column, str):\n        raise ValueError(\"target_column should be a string\")\n    if target_values is not None and not hasattr(target_values, '__iter__'):\n        raise ValueError(\"target_values should be an array-like object\")\n\n    if target_values is not None:\n        df = df[df[target_column].isin(target_values)]\n    else:\n        target_values = df[target_column].unique()\n\n    df.loc[:, ~df.columns.isin([target_column])] = df.loc[:, ~df.columns.isin([target_column])].replace(np.nan, 0)\n\n    X = df.loc[:, ~df.columns.isin([target_column])]\n    y = df[target_column]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model"}
{"task_id": "BigCodeBench/747", "solution": "import re\nimport math\ndef task_func(s):\n    # Find all numbers in the string\n    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|[-+]?\\d+\", s)\n\n    # Initialize count and sum\n    count = 0\n    sqrt_sum = 0\n\n    # Iterate over each number\n    for num in numbers:\n        # If the number is a float, add it to the sum of square roots\n        if '.' in num:\n            sqrt_sum += math.sqrt(float(num))\n        # If the number is an integer, add it to the sum of square roots\n        else:\n            sqrt_sum += math.sqrt(int(num))\n        # Increment the count\n        count += 1\n\n    return count, sqrt_sum", "raw_solution": "import re\nimport math\n\ndef task_func(s):\n    # Find all numbers in the string\n    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|[-+]?\\d+\", s)\n\n    # Initialize count and sum\n    count = 0\n    sqrt_sum = 0\n\n    # Iterate over each number\n    for num in numbers:\n        # If the number is a float, add it to the sum of square roots\n        if '.' in num:\n            sqrt_sum += math.sqrt(float(num))\n        # If the number is an integer, add it to the sum of square roots\n        else:\n            sqrt_sum += math.sqrt(int(num))\n        # Increment the count\n        count += 1\n\n    return count, sqrt_sum\n\n# Test the function\nassert task_func(\"1,2,3.0,4.0,-5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1.0,2.0,3.0,4.0,5.0\") == (5, 11.401427243679034)\nassert task_func(\"1.0,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2.0,3.0,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4.0,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5.0\") == (5, 11.401427243679034)\nassert task_func(\"1.0,2.0,3.0,4.0,5.0\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.401427243679034)\nassert task_func(\"1,2,3,4,5\") == (5, 11.4014"}
{"task_id": "BigCodeBench/748", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, age, weight):\n    \"\"\"\n    Filters and standardizes a given DataFrame based on specified age and weight criteria.\n    \"\"\"\n    # Check if the DataFrame contains the required columns\n    if 'Age' not in df.columns or 'Weight' not in df.columns:\n        raise KeyError(\"The input DataFrame does not contain the required columns 'Age' and 'Weight'.\")\n\n    # Filter the DataFrame based on the age and weight criteria\n    filtered_df = df[(df['Age'] < age) & (df['Weight'] > weight)]\n\n    # If the filtered DataFrame is empty, return an empty DataFrame\n    if filtered_df.empty:\n        return filtered_df\n\n    # Standardize the numerical values in the filtered DataFrame\n    scaler = StandardScaler()\n    standardized_df = pd.DataFrame(scaler.fit_transform(filtered_df[['Age', 'Weight']]), columns=['Age', 'Weight'])\n\n    # Concatenate the standardized DataFrame with the non-standardized DataFrame\n    standardized_df = pd.concat([standardized_df, filtered_df[['shoe_size', 'banana_consumption']]], axis=1)\n\n    return standardized_df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, age, weight):\n    \"\"\"\n    Filters and standardizes a given DataFrame based on specified age and weight criteria.\n    \"\"\"\n    # Check if the DataFrame contains the required columns\n    if 'Age' not in df.columns or 'Weight' not in df.columns:\n        raise KeyError(\"The input DataFrame does not contain the required columns 'Age' and 'Weight'.\")\n\n    # Filter the DataFrame based on the age and weight criteria\n    filtered_df = df[(df['Age'] < age) & (df['Weight'] > weight)]\n\n    # If the filtered DataFrame is empty, return an empty DataFrame\n    if filtered_df.empty:\n        return filtered_df\n\n    # Standardize the numerical values in the filtered DataFrame\n    scaler = StandardScaler()\n    standardized_df = pd.DataFrame(scaler.fit_transform(filtered_df[['Age', 'Weight']]), columns=['Age', 'Weight'])\n\n    # Concatenate the standardized DataFrame with the non-standardized DataFrame\n    standardized_df = pd.concat([standardized_df, filtered_df[['shoe_size', 'banana_consumption']]], axis=1)\n\n    return standardized_df"}
{"task_id": "BigCodeBench/749", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\ndef task_func(myList):\n    \"\"\"\n    Normalizes a list of numeric values to the range [0, 1] using min-max scaling.\n    \"\"\"\n    # Convert the list to a numpy array\n    myArray = np.array(myList).reshape(-1, 1)\n\n    # Initialize a MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the array\n    normalized_array = scaler.fit_transform(myArray)\n\n    # Return the reshaped array\n    return normalized_array.reshape(-1)", "raw_solution": "from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\ndef task_func(myList):\n    \"\"\"\n    Normalizes a list of numeric values to the range [0, 1] using min-max scaling.\n    \"\"\"\n    # Convert the list to a numpy array\n    myArray = np.array(myList).reshape(-1, 1)\n\n    # Initialize a MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the array\n    normalized_array = scaler.fit_transform(myArray)\n\n    # Return the reshaped array\n    return normalized_array.reshape(-1)"}
{"task_id": "BigCodeBench/750", "solution": "import pandas as pd\nimport statsmodels.api as sm\ndef task_func(df: pd.DataFrame, height: int, weight: int, columns: list) -> sm.regression.linear_model.RegressionResultsWrapper:\n    if df.empty or df.shape[0] == 0:\n        return None\n\n    filtered_df = df[(df[columns[1]] > height) & (df[columns[2]] < weight)]\n\n    if filtered_df.empty or filtered_df.shape[0] == 0:\n        return None\n\n    X = filtered_df[columns[1:]]\n    y = filtered_df[columns[0]]\n\n    X = sm.add_constant(X)\n    model = sm.OLS(y, X)\n    results = model.fit()\n\n    return results", "raw_solution": "import pandas as pd\nimport statsmodels.api as sm\n\ndef task_func(df: pd.DataFrame, height: int, weight: int, columns: list) -> sm.regression.linear_model.RegressionResultsWrapper:\n    if df.empty or df.shape[0] == 0:\n        return None\n\n    filtered_df = df[(df[columns[1]] > height) & (df[columns[2]] < weight)]\n\n    if filtered_df.empty or filtered_df.shape[0] == 0:\n        return None\n\n    X = filtered_df[columns[1:]]\n    y = filtered_df[columns[0]]\n\n    X = sm.add_constant(X)\n    model = sm.OLS(y, X)\n    results = model.fit()\n\n    return results"}
{"task_id": "BigCodeBench/751", "solution": "import random\nfrom collections import Counter\ndef task_func(values, weights, n_samples):\n    \"\"\"\n    Generates a histogram of `n_samples` random numbers based on a given weighted distribution.\n\n    Args:\n        values (list): A list of unique values representing the possible outcomes.\n        weights (list): A list of weights corresponding to the values. The length of weights should be the same as the length of values.\n        n_samples (int): The number of samples to generate.\n\n    Returns:\n        dict: A histogram as a dictionary with the values as keys and counts as values.\n    \"\"\"\n    random.seed(0)  # For reproducibility\n    samples = random.choices(values, weights, k=n_samples)\n    histogram = dict(Counter(samples))\n    return histogram\nvalues = [1, 2, 3, 4, 5]\nweights = [0.1, 0.2, 0.3, 0.2, 0.2]\nn_samples = 1000", "raw_solution": "import random\nfrom collections import Counter\n\ndef task_func(values, weights, n_samples):\n    \"\"\"\n    Generates a histogram of `n_samples` random numbers based on a given weighted distribution.\n\n    Args:\n        values (list): A list of unique values representing the possible outcomes.\n        weights (list): A list of weights corresponding to the values. The length of weights should be the same as the length of values.\n        n_samples (int): The number of samples to generate.\n\n    Returns:\n        dict: A histogram as a dictionary with the values as keys and counts as values.\n    \"\"\"\n    random.seed(0)  # For reproducibility\n    samples = random.choices(values, weights, k=n_samples)\n    histogram = dict(Counter(samples))\n    return histogram\n\n# Test the function\nvalues = [1, 2, 3, 4, 5]\nweights = [0.1, 0.2, 0.3, 0.2, 0.2]\nn_samples = 1000\n\nhistogram = task_func(values, weights, n_samples)"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data is not a DataFrame.\")\n    if data.empty:\n        raise ValueError(\"Data is empty.\")\n    if target_column not in data.columns:\n        raise ValueError(f\"{target_column} is not a column of data.\")\n    if not all(data.dtypes == 'float64') and not all(data.dtypes == 'int64'):\n        raise ValueError(\"Data contains values that are not numeric.\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random_state is not an integer.\")\n    if not 0 <= test_size <= 1:\n        raise ValueError(\"Test_size is not between 0 and 1.\")\n\n    X = data.drop([target_column], axis=1)\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.score(X_test, y_test)", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data is not a DataFrame.\")\n    if data.empty:\n        raise ValueError(\"Data is empty.\")\n    if target_column not in data.columns:\n        raise ValueError(f\"{target_column} is not a column of data.\")\n    if not all(data.dtypes == 'float64') and not all(data.dtypes == 'int64'):\n        raise ValueError(\"Data contains values that are not numeric.\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random_state is not an integer.\")\n    if not 0 <= test_size <= 1:\n        raise ValueError(\"Test_size is not between 0 and 1.\")\n\n    X = data.drop([target_column], axis=1)\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/753", "solution": "import math\nimport random\nimport statistics\nRADIUS = 5\ndef task_func(n):\n    \"\"\"\n    Generates n random points within a circle of radius RADIUS (default value is 5) and returns their average distance from the center.\n    \"\"\"\n    # Generate n random points within a circle\n    points = [(random.uniform(-RADIUS, RADIUS), random.uniform(-RADIUS, RADIUS)) for _ in range(n)]\n\n    # Calculate the distance from the center for each point\n    distances = [math.sqrt(x**2 + y**2) for x, y in points]\n\n    # Return the average distance from the center\n    return statistics.mean(distances)", "raw_solution": "import math\nimport random\nimport statistics\n\n# Constants\nRADIUS = 5\n\ndef task_func(n):\n    \"\"\"\n    Generates n random points within a circle of radius RADIUS (default value is 5) and returns their average distance from the center.\n    \"\"\"\n    # Generate n random points within a circle\n    points = [(random.uniform(-RADIUS, RADIUS), random.uniform(-RADIUS, RADIUS)) for _ in range(n)]\n\n    # Calculate the distance from the center for each point\n    distances = [math.sqrt(x**2 + y**2) for x, y in points]\n\n    # Return the average distance from the center\n    return statistics.mean(distances)\n\n# Test the function"}
{"task_id": "BigCodeBench/754", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(result):\n    \"\"\"\n    Calculates the mean, median, min, max, and standard deviation of the \"from_user\" values in \"result\" and adds the current date and time in the format YYYY-mm-dd HH:MM:SS to the summary.\n    \"\"\"\n    # Check if \"from_user\" values are numeric\n    if not all(isinstance(i, (int, float)) for i in result.get(\"from_user\", [])):\n        raise ValueError(\"All 'from_user' values must be numeric.\")\n\n    # Calculate statistics\n    values = np.array(result.get(\"from_user\", []))\n    if values.size == 0:\n        stats = pd.Series([np.nan]*5, index=['mean', 'median', 'min', 'max', 'std'])\n    else:\n        stats = pd.Series([np.mean(values), np.median(values), np.min(values), np.max(values), np.std(values)], index=['mean', 'median', 'min', 'max', 'std'])\n\n    # Add current date and time\n    current_time = datetime.now().strftime(DATE_FORMAT)\n    stats['current_time'] = current_time\n\n    return stats", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(result):\n    \"\"\"\n    Calculates the mean, median, min, max, and standard deviation of the \"from_user\" values in \"result\" and adds the current date and time in the format YYYY-mm-dd HH:MM:SS to the summary.\n    \"\"\"\n    # Check if \"from_user\" values are numeric\n    if not all(isinstance(i, (int, float)) for i in result.get(\"from_user\", [])):\n        raise ValueError(\"All 'from_user' values must be numeric.\")\n\n    # Calculate statistics\n    values = np.array(result.get(\"from_user\", []))\n    if values.size == 0:\n        stats = pd.Series([np.nan]*5, index=['mean', 'median', 'min', 'max', 'std'])\n    else:\n        stats = pd.Series([np.mean(values), np.median(values), np.min(values), np.max(values), np.std(values)], index=['mean', 'median', 'min', 'max', 'std'])\n\n    # Add current date and time\n    current_time = datetime.now().strftime(DATE_FORMAT)\n    stats['current_time'] = current_time\n\n    return stats"}
{"task_id": "BigCodeBench/755", "solution": "import os\nimport glob\ndef task_func(directory_path):\n    \"\"\"\n    Reverse the order of words in all the filenames of a directory, where words are separated by periods.\n    \"\"\"\n    # Get all file names in the directory\n    filenames = glob.glob(os.path.join(directory_path, '*'))\n\n    # Initialize an empty list to store the new filenames\n    new_filenames = []\n\n    # Loop through each filename\n    for filename in filenames:\n        # Split the filename by periods and reverse the order\n        reversed_words = '.'.join(filename.split('.')[::-1])\n\n        # Append the reversed filename to the new_filenames list\n        new_filenames.append(reversed_words)\n\n    return new_filenames", "raw_solution": "import os\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    Reverse the order of words in all the filenames of a directory, where words are separated by periods.\n    \"\"\"\n    # Get all file names in the directory\n    filenames = glob.glob(os.path.join(directory_path, '*'))\n\n    # Initialize an empty list to store the new filenames\n    new_filenames = []\n\n    # Loop through each filename\n    for filename in filenames:\n        # Split the filename by periods and reverse the order\n        reversed_words = '.'.join(filename.split('.')[::-1])\n\n        # Append the reversed filename to the new_filenames list\n        new_filenames.append(reversed_words)\n\n    return new_filenames"}
{"task_id": "BigCodeBench/756", "solution": "import shutil\nfrom pathlib import Path\nfrom typing import List\ndef task_func(source_dir: str, target_dir: str, extensions: List[str]) -> int:\n    \"\"\"\n    Moves all files with certain extensions from one directory to another.\n    Raises ValueError if source_dir or target_dir does not exist.\n    Returns the number of moved files.\n    \"\"\"\n    source_dir = Path(source_dir)\n    target_dir = Path(target_dir)\n\n    if not source_dir.exists():\n        raise ValueError(f\"Source directory {source_dir} does not exist.\")\n    if not target_dir.exists():\n        raise ValueError(f\"Target directory {target_dir} does not exist.\")\n\n    moved_files = 0\n    for ext in extensions:\n        for file in source_dir.glob(f\"*{ext}\"):\n            shutil.move(str(file), target_dir)\n            moved_files += 1\n\n    return moved_files", "raw_solution": "import shutil\nfrom pathlib import Path\nfrom typing import List\n\ndef task_func(source_dir: str, target_dir: str, extensions: List[str]) -> int:\n    \"\"\"\n    Moves all files with certain extensions from one directory to another.\n    Raises ValueError if source_dir or target_dir does not exist.\n    Returns the number of moved files.\n    \"\"\"\n    source_dir = Path(source_dir)\n    target_dir = Path(target_dir)\n\n    if not source_dir.exists():\n        raise ValueError(f\"Source directory {source_dir} does not exist.\")\n    if not target_dir.exists():\n        raise ValueError(f\"Target directory {target_dir} does not exist.\")\n\n    moved_files = 0\n    for ext in extensions:\n        for file in source_dir.glob(f\"*{ext}\"):\n            shutil.move(str(file), target_dir)\n            moved_files += 1\n\n    return moved_files"}
{"task_id": "BigCodeBench/757", "solution": "import numpy as np\ndef task_func(arr):\n    \"\"\"\n    This function takes a numpy array of strings and reverses the order of words separated by '.' in all strings.\n    \"\"\"\n    # Convert the numpy array to a list\n    arr_list = arr.tolist()\n\n    # Reverse the order of words separated by '.' in each string\n    for i in range(len(arr_list)):\n        arr_list[i] = '. '.join(arr_list[i].split('. ')[::-1])\n\n    # Convert the list back to a numpy array\n    arr = np.array(arr_list)\n\n    return arr", "raw_solution": "import numpy as np\n\ndef task_func(arr):\n    \"\"\"\n    This function takes a numpy array of strings and reverses the order of words separated by '.' in all strings.\n    \"\"\"\n    # Convert the numpy array to a list\n    arr_list = arr.tolist()\n\n    # Reverse the order of words separated by '.' in each string\n    for i in range(len(arr_list)):\n        arr_list[i] = '. '.join(arr_list[i].split('. ')[::-1])\n\n    # Convert the list back to a numpy array\n    arr = np.array(arr_list)\n\n    return arr"}
{"task_id": "BigCodeBench/758", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(num_samples, countries=['Russia', 'China', 'USA', 'India', 'Brazil'], \n           ages=np.arange(18, 60), genders=['Male', 'Female'], rng_seed=None):\n    if not isinstance(num_samples, int):\n        raise ValueError(\"num_samples should be an integer\")\n\n    rng = np.random.default_rng(rng_seed)\n\n    country = rng.choice(countries, num_samples)\n    age = rng.choice(ages, num_samples)\n    gender = rng.choice(genders, num_samples)\n\n    le = LabelEncoder()\n    gender = le.fit_transform(gender)\n\n    data = {'Country': country, 'Age': age, 'Gender': gender}\n    df = pd.DataFrame(data)\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(num_samples, countries=['Russia', 'China', 'USA', 'India', 'Brazil'], \n           ages=np.arange(18, 60), genders=['Male', 'Female'], rng_seed=None):\n    if not isinstance(num_samples, int):\n        raise ValueError(\"num_samples should be an integer\")\n\n    rng = np.random.default_rng(rng_seed)\n\n    country = rng.choice(countries, num_samples)\n    age = rng.choice(ages, num_samples)\n    gender = rng.choice(genders, num_samples)\n\n    le = LabelEncoder()\n    gender = le.fit_transform(gender)\n\n    data = {'Country': country, 'Age': age, 'Gender': gender}\n    df = pd.DataFrame(data)\n\n    return df"}
{"task_id": "BigCodeBench/759", "solution": "import os\nimport shutil\nimport fnmatch\ndef task_func(source_directory, destination_directory, file_pattern):\n    \"\"\"\n    Moves all files that match a particular pattern from one directory to another.\n    \"\"\"\n    # Create destination directory if it doesn't exist\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    # Get a list of files in the source directory that match the file pattern\n    files_to_move = fnmatch.filter(os.listdir(source_directory), file_pattern)\n\n    # Move each file to the destination directory\n    moved_files = []\n    for file in files_to_move:\n        shutil.move(os.path.join(source_directory, file), destination_directory)\n        moved_files.append(file)\n\n    return moved_files", "raw_solution": "import os\nimport shutil\nimport fnmatch\n\ndef task_func(source_directory, destination_directory, file_pattern):\n    \"\"\"\n    Moves all files that match a particular pattern from one directory to another.\n    \"\"\"\n    # Create destination directory if it doesn't exist\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    # Get a list of files in the source directory that match the file pattern\n    files_to_move = fnmatch.filter(os.listdir(source_directory), file_pattern)\n\n    # Move each file to the destination directory\n    moved_files = []\n    for file in files_to_move:\n        shutil.move(os.path.join(source_directory, file), destination_directory)\n        moved_files.append(file)\n\n    return moved_files"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport random\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    random.seed(rng_seed)\n    np.random.seed(rng_seed)\n\n    # Generate random data\n    ids = list(range(1, 101))\n    names = latin_names + other_names\n    random.shuffle(names)\n    dobs = [datetime.datetime(random.randint(start_year, end_year), random.randint(1, 12), random.randint(1, 28)) for _ in range(100)]\n    emails = [f\"{name}{dob.year}@{email_domain}\" for name, dob in zip(names, dobs)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dobs,\n        'Email': emails\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport random\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    random.seed(rng_seed)\n    np.random.seed(rng_seed)\n\n    # Generate random data\n    ids = list(range(1, 101))\n    names = latin_names + other_names\n    random.shuffle(names)\n    dobs = [datetime.datetime(random.randint(start_year, end_year), random.randint(1, 12), random.randint(1, 28)) for _ in range(100)]\n    emails = [f\"{name}{dob.year}@{email_domain}\" for name, dob in zip(names, dobs)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dobs,\n        'Email': emails\n    })\n\n    return df\n\n# Test the function\ndf = task_func(start_year=1980, end_year=2000, email_domain='test.at', rng_seed=3)"}
{"task_id": "BigCodeBench/761", "solution": "import json\nimport re\nfrom collections import Counter\nREPLACE_NONE = \"None\"\nEMAIL_REGEX = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\ndef task_func(json_str):\n    # Remove None values\n    json_data = json.loads(json_str)\n    json_data = remove_none_values(json_data)\n\n    # Count the frequency of each unique value\n    value_counts = count_unique_values(json_data)\n\n    # Replace all email addresses with the placeholder \"None\"\n    json_data = replace_email_addresses(json_data)\n\n    return {\n        \"data\": json_data,\n        \"value_counts\": value_counts\n    }\ndef remove_none_values(data):\n    if isinstance(data, dict):\n        return {k: remove_none_values(v) for k, v in data.items() if v is not None}\n    elif isinstance(data, list):\n        return [remove_none_values(v) for v in data if v is not None]\n    else:\n        return data\ndef count_unique_values(data):\n    if isinstance(data, dict):\n        return Counter(v for v in data.values())\n    elif isinstance(data, list):\n        return Counter(v for v in data)\n    else:\n        return Counter([data])\ndef replace_email_addresses(data):\n    if isinstance(data, dict):\n        return {k: replace_email_addresses(v) for k, v in data.items()}\n    elif isinstance(data, list):\n        return [replace_email_addresses(v) for v in data]\n    elif isinstance(data, str):\n        if re.match(EMAIL_REGEX, data):\n            return REPLACE_NONE\n        else:\n            return data\n    else:\n        return data", "raw_solution": "import json\nimport re\nfrom collections import Counter\n\n# Constants\nREPLACE_NONE = \"None\"\nEMAIL_REGEX = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n\ndef task_func(json_str):\n    # Remove None values\n    json_data = json.loads(json_str)\n    json_data = remove_none_values(json_data)\n\n    # Count the frequency of each unique value\n    value_counts = count_unique_values(json_data)\n\n    # Replace all email addresses with the placeholder \"None\"\n    json_data = replace_email_addresses(json_data)\n\n    return {\n        \"data\": json_data,\n        \"value_counts\": value_counts\n    }\n\ndef remove_none_values(data):\n    if isinstance(data, dict):\n        return {k: remove_none_values(v) for k, v in data.items() if v is not None}\n    elif isinstance(data, list):\n        return [remove_none_values(v) for v in data if v is not None]\n    else:\n        return data\n\ndef count_unique_values(data):\n    if isinstance(data, dict):\n        return Counter(v for v in data.values())\n    elif isinstance(data, list):\n        return Counter(v for v in data)\n    else:\n        return Counter([data])\n\ndef replace_email_addresses(data):\n    if isinstance(data, dict):\n        return {k: replace_email_addresses(v) for k, v in data.items()}\n    elif isinstance(data, list):\n        return [replace_email_addresses(v) for v in data]\n    elif isinstance(data, str):\n        if re.match(EMAIL_REGEX, data):\n            return REPLACE_NONE\n        else:\n            return data\n    else:\n        return data"}
{"task_id": "BigCodeBench/762", "solution": "import os\nimport zipfile\ndef task_func(directory_name=\"latin_files\",\n              content='Sopet\u00f3n',\n              file_names=['file1.txt', 'file2.txt', 'file3.txt'],\n              encoding=\"latin-1\"):\n    \"\"\"\n    Creates a directory with the given name, creates specified .txt files,\n    encodes the content using the specified encoding and writes it into all .txt files,\n    then zips the directory.\n    \"\"\"\n    # Create directory\n    if not os.path.exists(directory_name):\n        os.makedirs(directory_name)\n\n    # Create .txt files\n    for file_name in file_names:\n        with open(os.path.join(directory_name, file_name), 'w', encoding=encoding) as f:\n            f.write(content)\n\n    # Zip the directory\n    zip_file_name = f\"{directory_name}.zip\"\n    with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n        for file_name in file_names:\n            zipf.write(os.path.join(directory_name, file_name), arcname=file_name)\n\n    return zip_file_name", "raw_solution": "import os\nimport zipfile\n\ndef task_func(directory_name=\"latin_files\",\n              content='Sopet\u00f3n',\n              file_names=['file1.txt', 'file2.txt', 'file3.txt'],\n              encoding=\"latin-1\"):\n    \"\"\"\n    Creates a directory with the given name, creates specified .txt files,\n    encodes the content using the specified encoding and writes it into all .txt files,\n    then zips the directory.\n    \"\"\"\n    # Create directory\n    if not os.path.exists(directory_name):\n        os.makedirs(directory_name)\n\n    # Create .txt files\n    for file_name in file_names:\n        with open(os.path.join(directory_name, file_name), 'w', encoding=encoding) as f:\n            f.write(content)\n\n    # Zip the directory\n    zip_file_name = f\"{directory_name}.zip\"\n    with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n        for file_name in file_names:\n            zipf.write(os.path.join(directory_name, file_name), arcname=file_name)\n\n    return zip_file_name\n\n# Test the function\nzipped_file = task_func(directory_name=\"directorio\", content='hi', file_names=[\"custom1.txt\", \"custom2.txt\"], encoding='utf-8')"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Load data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize defaultdict to store mean and median for each key\n    stats = defaultdict(lambda: {'mean': None, 'median': None})\n\n    # Calculate mean and median for each key\n    for item in data:\n        for key, value in item.items():\n            if isinstance(value, list) and len(value) > 0:\n                values = np.array(value)\n                stats[key]['mean'] = np.mean(values)\n                stats[key]['median'] = np.median(values)\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for field, stat in stats.items():\n            writer.writerow([field, stat['mean'], stat['median']])\n\n    return dict(stats)", "raw_solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\ndef task_func(input_file, output_file):\n    # Load data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize defaultdict to store mean and median for each key\n    stats = defaultdict(lambda: {'mean': None, 'median': None})\n\n    # Calculate mean and median for each key\n    for item in data:\n        for key, value in item.items():\n            if isinstance(value, list) and len(value) > 0:\n                values = np.array(value)\n                stats[key]['mean'] = np.mean(values)\n                stats[key]['median'] = np.median(values)\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for field, stat in stats.items():\n            writer.writerow([field, stat['mean'], stat['median']])\n\n    return dict(stats)"}
{"task_id": "BigCodeBench/764", "solution": "", "raw_solution": ""}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(file_dict, target_dir=\"non_none_files\"):\n    \"\"\"\n    Processes files from a dictionary by checking if the file exists and if it has content,\n    then copies it to a target directory.\n\n    Args:\n        file_dict (dict): A dictionary where keys are file names and values are file paths.\n        target_dir (str): The target directory where the files will be copied. Default is \"non_none_files\".\n\n    Returns:\n        copied_files (list): A list of full file paths that were copied.\n    \"\"\"\n    # Create target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    copied_files = []\n\n    for file_name, file_path in file_dict.items():\n        # Check if file exists and has content\n        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n            # Construct full target file path\n            target_file_path = os.path.join(target_dir, file_name)\n\n            # Copy file to target directory\n            shutil.copy2(file_path, target_file_path)\n\n            # Add full file path to list of copied files\n            copied_files.append(target_file_path)\n\n    return copied_files", "raw_solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(file_dict, target_dir=\"non_none_files\"):\n    \"\"\"\n    Processes files from a dictionary by checking if the file exists and if it has content,\n    then copies it to a target directory.\n\n    Args:\n        file_dict (dict): A dictionary where keys are file names and values are file paths.\n        target_dir (str): The target directory where the files will be copied. Default is \"non_none_files\".\n\n    Returns:\n        copied_files (list): A list of full file paths that were copied.\n    \"\"\"\n    # Create target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    copied_files = []\n\n    for file_name, file_path in file_dict.items():\n        # Check if file exists and has content\n        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n            # Construct full target file path\n            target_file_path = os.path.join(target_dir, file_name)\n\n            # Copy file to target directory\n            shutil.copy2(file_path, target_file_path)\n\n            # Add full file path to list of copied files\n            copied_files.append(target_file_path)\n\n    return copied_files"}
{"task_id": "BigCodeBench/766", "solution": "import re\nimport collections\ndef task_func(string, patterns=['nnn', 'aaa', 'sss', 'ddd', 'fff']):\n    if not isinstance(string, str):\n        raise TypeError(\"string must be a str\")\n    if not all(isinstance(pattern, str) for pattern in patterns):\n        raise TypeError(\"patterns must be a list of str\")\n\n    pattern_counts = collections.defaultdict(int)\n    for pattern in patterns:\n        pattern_counts[pattern] = len(re.findall(pattern, string))\n\n    return pattern_counts", "raw_solution": "import re\nimport collections\n\ndef task_func(string, patterns=['nnn', 'aaa', 'sss', 'ddd', 'fff']):\n    if not isinstance(string, str):\n        raise TypeError(\"string must be a str\")\n    if not all(isinstance(pattern, str) for pattern in patterns):\n        raise TypeError(\"patterns must be a list of str\")\n\n    pattern_counts = collections.defaultdict(int)\n    for pattern in patterns:\n        pattern_counts[pattern] = len(re.findall(pattern, string))\n\n    return pattern_counts"}
{"task_id": "BigCodeBench/767", "solution": "from collections import Counter\nimport random\nimport string\nLETTERS = string.ascii_letters\ndef task_func(list_of_lists):\n    # Replace each sublist with a random letter\n    replaced_list = [random.choice(LETTERS) for _ in list_of_lists]\n\n    # Count the occurrences of each letter\n    count_dict = dict(Counter(replaced_list))\n\n    return count_dict", "raw_solution": "from collections import Counter\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef task_func(list_of_lists):\n    # Replace each sublist with a random letter\n    replaced_list = [random.choice(LETTERS) for _ in list_of_lists]\n\n    # Count the occurrences of each letter\n    count_dict = dict(Counter(replaced_list))\n\n    return count_dict"}
{"task_id": "BigCodeBench/768", "solution": "import re\nimport os\nimport glob\ndef task_func(dir_path):\n    if not os.path.exists(dir_path):\n        raise ValueError(\"Directory does not exist\")\n\n    file_dict = {}\n    for filename in glob.iglob(dir_path + '/**/*.txt', recursive=True):\n        with open(filename, 'r') as file:\n            file_content = file.read()\n            count = len(re.findall(r'\\berror\\b', file_content, re.IGNORECASE))\n            file_dict[filename] = count\n\n    return file_dict", "raw_solution": "import re\nimport os\nimport glob\n\ndef task_func(dir_path):\n    if not os.path.exists(dir_path):\n        raise ValueError(\"Directory does not exist\")\n\n    file_dict = {}\n    for filename in glob.iglob(dir_path + '/**/*.txt', recursive=True):\n        with open(filename, 'r') as file:\n            file_content = file.read()\n            count = len(re.findall(r'\\berror\\b', file_content, re.IGNORECASE))\n            file_dict[filename] = count\n\n    return file_dict"}
{"task_id": "BigCodeBench/769", "solution": "from collections import Counter\nimport itertools\nimport operator\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Flatten the list of menu items and return the most common menu item.\n    \"\"\"\n    # Flatten the list of menu items\n    flat_list = list(itertools.chain.from_iterable(list_of_menuitems))\n\n    # Count the frequency of each menu item\n    counter = Counter(flat_list)\n\n    # Return the most common menu item\n    return max(counter.items(), key=operator.itemgetter(1))[0]", "raw_solution": "from collections import Counter\nimport itertools\nimport operator\n\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Flatten the list of menu items and return the most common menu item.\n    \"\"\"\n    # Flatten the list of menu items\n    flat_list = list(itertools.chain.from_iterable(list_of_menuitems))\n\n    # Count the frequency of each menu item\n    counter = Counter(flat_list)\n\n    # Return the most common menu item\n    return max(counter.items(), key=operator.itemgetter(1))[0]\n\n# Test the function\nmenu_items = [[\"Pasta\", \"Pizza\", \"Pasta\"], [\"Soup\", \"Soup\", \"Salad\"], [\"Pizza\", \"Pasta\", \"Pasta\"]]"}
{"task_id": "BigCodeBench/770", "solution": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\ndef task_func(num_samples=500, noise_strength=1, random_seed=None, test_size=0.2):\n    if test_size < 0 or test_size > 1:\n        raise ValueError(\"Test set size must be between 0 and 1\")\n    if num_samples < 2:\n        raise ValueError(\"Number of samples must be greater than 1\")\n    if test_size * num_samples < 2:\n        raise ValueError(\"Test set size is smaller than 2\")\n\n    np.random.seed(random_seed)\n\n    X = np.random.rand(num_samples, 1)\n    y = 3 * X + np.random.randn(num_samples, 1) * noise_strength\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n\n    return r2_score(y_test, y_pred), model", "raw_solution": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ndef task_func(num_samples=500, noise_strength=1, random_seed=None, test_size=0.2):\n    if test_size < 0 or test_size > 1:\n        raise ValueError(\"Test set size must be between 0 and 1\")\n    if num_samples < 2:\n        raise ValueError(\"Number of samples must be greater than 1\")\n    if test_size * num_samples < 2:\n        raise ValueError(\"Test set size is smaller than 2\")\n\n    np.random.seed(random_seed)\n\n    X = np.random.rand(num_samples, 1)\n    y = 3 * X + np.random.randn(num_samples, 1) * noise_strength\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n\n    return r2_score(y_test, y_pred), model"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in a directory based on a specified pattern and creates new files with altered names while preserving the content.\n    \"\"\"\n    # Initialize an empty list to store the names of the new files\n    new_files = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        if re.match(pattern, filename):\n            # If it does, create a new filename by replacing the number with a different one\n            new_filename = re.sub(r'-\\d+\\.csv$', '-999.csv', filename)\n            # Add the new filename to the list\n            new_files.append(new_filename)\n            # Rename the file\n            os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n\n    # Return the list of new filenames\n    return new_files", "raw_solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in a directory based on a specified pattern and creates new files with altered names while preserving the content.\n    \"\"\"\n    # Initialize an empty list to store the names of the new files\n    new_files = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        if re.match(pattern, filename):\n            # If it does, create a new filename by replacing the number with a different one\n            new_filename = re.sub(r'-\\d+\\.csv$', '-999.csv', filename)\n            # Add the new filename to the list\n            new_files.append(new_filename)\n            # Rename the file\n            os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n\n    # Return the list of new filenames\n    return new_files"}
{"task_id": "BigCodeBench/772", "solution": "import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\ndef task_func(num_samples=1000, k=5, d=2, random_seed=None):\n    \"\"\"\n    Generates a dataset of `num_samples` random numbers sampled from a Gaussian\n    normal distribution, applies a linear transformation `k*x + d`, and standardizes\n    the result using `sklearn.preprocessing.StandardScaler`. Calculates and returns\n    the mean squared error between the original and standardized datasets.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate a dataset of `num_samples` random numbers sampled from a Gaussian\n    # normal distribution\n    original_data = np.random.normal(size=num_samples)\n\n    # Apply a linear transformation `k*x + d` to the original dataset\n    transformed_data = k * original_data + d\n\n    # Standardize the transformed dataset using `sklearn.preprocessing.StandardScaler`\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(transformed_data.reshape(-1, 1))\n\n    # Calculate and return the mean squared error between the original and\n    # standardized datasets\n    mse = mean_squared_error(original_data, standardized_data)\n\n    return mse", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\ndef task_func(num_samples=1000, k=5, d=2, random_seed=None):\n    \"\"\"\n    Generates a dataset of `num_samples` random numbers sampled from a Gaussian\n    normal distribution, applies a linear transformation `k*x + d`, and standardizes\n    the result using `sklearn.preprocessing.StandardScaler`. Calculates and returns\n    the mean squared error between the original and standardized datasets.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate a dataset of `num_samples` random numbers sampled from a Gaussian\n    # normal distribution\n    original_data = np.random.normal(size=num_samples)\n\n    # Apply a linear transformation `k*x + d` to the original dataset\n    transformed_data = k * original_data + d\n\n    # Standardize the transformed dataset using `sklearn.preprocessing.StandardScaler`\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(transformed_data.reshape(-1, 1))\n\n    # Calculate and return the mean squared error between the original and\n    # standardized datasets\n    mse = mean_squared_error(original_data, standardized_data)\n\n    return mse\n\n# Test the function with the provided examples\nassert abs(task_func() - 19.03543917135251) < 1e-6\nassert abs(task_func(k=1, d=0) - 0.001113785307245742) < 1e-6"}
{"task_id": "BigCodeBench/773", "solution": "import os\nimport re\nimport shutil\nSOURCE_DIR = '/source/dir'\nTARGET_DIR = '/target/dir'\nFILE_PATTERN = re.compile(r'^(.*?)-\\d+\\.json$')\ndef task_func():\n    for filename in os.listdir(SOURCE_DIR):\n        match = FILE_PATTERN.match(filename)\n        if match:\n            prefix = match.group(1)\n            new_filename = f'{prefix}.json'\n            shutil.move(os.path.join(SOURCE_DIR, filename), os.path.join(TARGET_DIR, new_filename))", "raw_solution": "import os\nimport re\nimport shutil\n\n# Constants\nSOURCE_DIR = '/source/dir'\nTARGET_DIR = '/target/dir'\nFILE_PATTERN = re.compile(r'^(.*?)-\\d+\\.json$')\n\ndef task_func():\n    for filename in os.listdir(SOURCE_DIR):\n        match = FILE_PATTERN.match(filename)\n        if match:\n            prefix = match.group(1)\n            new_filename = f'{prefix}.json'\n            shutil.move(os.path.join(SOURCE_DIR, filename), os.path.join(TARGET_DIR, new_filename))\n\n# Test the function\ntask_func()"}
{"task_id": "BigCodeBench/774", "solution": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\ndef task_func(num_samples=100, n_estimators=100, random_seed=None, cv=5):\n    if num_samples / cv < 2:\n        raise ValueError(\"num_samples / cv should be greater than 2\")\n\n    np.random.seed(random_seed)\n    X = np.random.normal(size=(num_samples, 5))\n    y = np.sum(X, axis=1) + np.random.normal(size=num_samples)\n\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_seed)\n    scores = cross_val_score(model, X, y, cv=cv)\n\n    return np.mean(scores), model", "raw_solution": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef task_func(num_samples=100, n_estimators=100, random_seed=None, cv=5):\n    if num_samples / cv < 2:\n        raise ValueError(\"num_samples / cv should be greater than 2\")\n\n    np.random.seed(random_seed)\n    X = np.random.normal(size=(num_samples, 5))\n    y = np.sum(X, axis=1) + np.random.normal(size=num_samples)\n\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_seed)\n    scores = cross_val_score(model, X, y, cv=cv)\n\n    return np.mean(scores), model"}
{"task_id": "BigCodeBench/775", "solution": "from string import ascii_lowercase\nimport re\nfrom collections import Counter\nLETTERS_PATTERN = re.compile(r'^(.*?)-[a-z]$')\nLETTERS = ascii_lowercase\ndef task_func(string):\n    \"\"\"\n    If a string occurs, divide it the last time \"-\" occurs and count the frequency of each lowercase letter in the prefix of the string.\n    \"\"\"\n    match = LETTERS_PATTERN.match(string)\n    if match:\n        prefix = match.group(1)\n        letters = Counter(prefix.lower())\n        return {letter: letters[letter] for letter in LETTERS if letter in letters}\n    else:\n        return {}", "raw_solution": "from string import ascii_lowercase\nimport re\nfrom collections import Counter\n\n# Constants\nLETTERS_PATTERN = re.compile(r'^(.*?)-[a-z]$')\nLETTERS = ascii_lowercase\n\ndef task_func(string):\n    \"\"\"\n    If a string occurs, divide it the last time \"-\" occurs and count the frequency of each lowercase letter in the prefix of the string.\n    \"\"\"\n    match = LETTERS_PATTERN.match(string)\n    if match:\n        prefix = match.group(1)\n        letters = Counter(prefix.lower())\n        return {letter: letters[letter] for letter in LETTERS if letter in letters}\n    else:\n        return {}\n\n# Test cases\nassert task_func(\"hello-world\") == {'h': 1, 'e': 1, 'l': 3, 'o': 2, 'w': 1, 'r': 1, 'd': 1}\nassert task_func(\"python-programming\") == {'p': 1, 'y': 1, 't': 1, 'h': 1, 'o': 1, 'n': 2, 'g': 1, 'r': 1, 'a': 1, 'm': 2}\nassert task_func(\"no-match\") == {}"}
{"task_id": "BigCodeBench/776", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(file_path, output_path=None, sort_key='title', linear_regression=False, x_column=None, y_column=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        raise Exception(f\"Error in reading the file: {e}\")\n\n    # Sort data by the specified key\n    try:\n        df = df.sort_values(by=sort_key)\n    except Exception as e:\n        raise Exception(f\"Error in sorting the data: {e}\")\n\n    if output_path is not None:\n        # Write sorted data to another CSV file\n        try:\n            df.to_csv(output_path, index=False)\n        except Exception as e:\n            raise Exception(f\"Error in writing the file: {e}\")\n        return output_path\n\n    if linear_regression:\n        # Check if specified columns for linear regression exist in the dataframe\n        if x_column not in df.columns or y_column not in df.columns:\n            raise ValueError(\"Specified columns for linear regression do not exist in the dataframe\")\n\n        # Fit a linear regression model\n        try:\n            model = LinearRegression()\n            model.fit(df[[x_column]], df[y_column])\n        except Exception as e:\n            raise Exception(f\"Error in fitting the model: {e}\")\n        return model\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(file_path, output_path=None, sort_key='title', linear_regression=False, x_column=None, y_column=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        raise Exception(f\"Error in reading the file: {e}\")\n\n    # Sort data by the specified key\n    try:\n        df = df.sort_values(by=sort_key)\n    except Exception as e:\n        raise Exception(f\"Error in sorting the data: {e}\")\n\n    if output_path is not None:\n        # Write sorted data to another CSV file\n        try:\n            df.to_csv(output_path, index=False)\n        except Exception as e:\n            raise Exception(f\"Error in writing the file: {e}\")\n        return output_path\n\n    if linear_regression:\n        # Check if specified columns for linear regression exist in the dataframe\n        if x_column not in df.columns or y_column not in df.columns:\n            raise ValueError(\"Specified columns for linear regression do not exist in the dataframe\")\n\n        # Fit a linear regression model\n        try:\n            model = LinearRegression()\n            model.fit(df[[x_column]], df[y_column])\n        except Exception as e:\n            raise Exception(f\"Error in fitting the model: {e}\")\n        return model\n\n    return df"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    for filename in os.listdir(directory):\n        match = re.match(pattern, filename)\n        if match:\n            prefix = match.group(1)\n            if not os.path.exists(os.path.join(directory, prefix)):\n                os.mkdir(os.path.join(directory, prefix))\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(os.path.join(directory, prefix))\n            extracted_dirs.append(prefix)\n    return extracted_dirs", "raw_solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    for filename in os.listdir(directory):\n        match = re.match(pattern, filename)\n        if match:\n            prefix = match.group(1)\n            if not os.path.exists(os.path.join(directory, prefix)):\n                os.mkdir(os.path.join(directory, prefix))\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(os.path.join(directory, prefix))\n            extracted_dirs.append(prefix)\n    return extracted_dirs"}
{"task_id": "BigCodeBench/778", "solution": "from collections import defaultdict\nfrom operator import itemgetter\nfrom itertools import groupby\ndef task_func(news_articles):\n    \"\"\"\n    Sorts a list of news articles by 'category' and 'title'. The news articles are then grouped by 'category'.\n    \"\"\"\n    if not all(isinstance(article, dict) and set(article.keys()) == {'title', 'title_url', 'id', 'category'} for article in news_articles):\n        raise ValueError(\"All articles must be dictionaries with keys 'title', 'title_url', 'id', and 'category'\")\n\n    sorted_articles = sorted(news_articles, key=itemgetter('category', 'title'))\n    grouped_articles = defaultdict(list)\n\n    for category, articles in groupby(sorted_articles, key=itemgetter('category')):\n        grouped_articles[category].extend(articles)\n\n    return grouped_articles", "raw_solution": "from collections import defaultdict\nfrom operator import itemgetter\nfrom itertools import groupby\n\ndef task_func(news_articles):\n    \"\"\"\n    Sorts a list of news articles by 'category' and 'title'. The news articles are then grouped by 'category'.\n    \"\"\"\n    if not all(isinstance(article, dict) and set(article.keys()) == {'title', 'title_url', 'id', 'category'} for article in news_articles):\n        raise ValueError(\"All articles must be dictionaries with keys 'title', 'title_url', 'id', and 'category'\")\n\n    sorted_articles = sorted(news_articles, key=itemgetter('category', 'title'))\n    grouped_articles = defaultdict(list)\n\n    for category, articles in groupby(sorted_articles, key=itemgetter('category')):\n        grouped_articles[category].extend(articles)\n\n    return grouped_articles"}
{"task_id": "BigCodeBench/779", "solution": "import os\nimport shutil\nimport time\ndef get_unique_backup_dir():\n    return \"/fake/backup/path\"\ndef task_func(directory):\n    \"\"\"\n    Creates a backup of a directory and cleans the directory afterwards.\n    \"\"\"\n    backup_dir = get_unique_backup_dir()\n    errors = []\n\n    # Create backup\n    try:\n        shutil.copytree(directory, backup_dir)\n    except Exception as e:\n        errors.append(f\"Error creating backup: {str(e)}\")\n\n    # Clean directory\n    try:\n        for filename in os.listdir(directory):\n            file_path = os.path.join(directory, filename)\n            try:\n                if os.path.isfile(file_path) or os.path.islink(file_path):\n                    os.unlink(file_path)\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n            except Exception as e:\n                errors.append(f\"Error cleaning directory: {str(e)}\")\n    except Exception as e:\n        errors.append(f\"Error cleaning directory: {str(e)}\")\n\n    return backup_dir, errors", "raw_solution": "import os\nimport shutil\nimport time\n\n# Constants\nBACKUP_DIR = '/tmp/backup'\n\ndef get_unique_backup_dir():\n    return \"/fake/backup/path\"\n\ndef task_func(directory):\n    \"\"\"\n    Creates a backup of a directory and cleans the directory afterwards.\n    \"\"\"\n    backup_dir = get_unique_backup_dir()\n    errors = []\n\n    # Create backup\n    try:\n        shutil.copytree(directory, backup_dir)\n    except Exception as e:\n        errors.append(f\"Error creating backup: {str(e)}\")\n\n    # Clean directory\n    try:\n        for filename in os.listdir(directory):\n            file_path = os.path.join(directory, filename)\n            try:\n                if os.path.isfile(file_path) or os.path.islink(file_path):\n                    os.unlink(file_path)\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n            except Exception as e:\n                errors.append(f\"Error cleaning directory: {str(e)}\")\n    except Exception as e:\n        errors.append(f\"Error cleaning directory: {str(e)}\")\n\n    return backup_dir, errors"}
{"task_id": "BigCodeBench/780", "solution": "import pandas as pd\nimport pytz\nfrom typing import List, Dict\ndef task_func(articles: List[Dict[str, str]], timezone: str) -> pd.DataFrame:\n    \"\"\"\n    Analyzes the publication times of a list of articles.\n    Converts 'published_time' to a specified timezone, groups articles by 'category',\n    and calculates the count, mean, min, max publication times only considering the hour.\n    \"\"\"\n    if not isinstance(articles, list):\n        raise TypeError(\"articles should be a list of dictionaries\")\n    if not articles:\n        raise ValueError(\"articles list should not be empty\")\n    for article in articles:\n        if not isinstance(article, dict):\n            raise TypeError(\"articles should be a list of dictionaries\")\n        if not set(article.keys()).issuperset({'category', 'published_time'}):\n            raise ValueError(\"dictionary keys do not match the requirements\")\n\n    # Convert 'published_time' to a specified timezone\n    for article in articles:\n        published_time = pd.to_datetime(article['published_time'])\n        article['published_time'] = published_time.tz_localize(pytz.utc).tz_convert(timezone)\n\n    # Group articles by 'category'\n    df = pd.DataFrame(articles)\n    df['published_hour'] = df['published_time'].dt.hour\n    result = df.groupby('category').agg({'published_hour': ['count', 'mean', 'min', 'max']})\n\n    return result", "raw_solution": "import pandas as pd\nimport pytz\nfrom typing import List, Dict\n\ndef task_func(articles: List[Dict[str, str]], timezone: str) -> pd.DataFrame:\n    \"\"\"\n    Analyzes the publication times of a list of articles.\n    Converts 'published_time' to a specified timezone, groups articles by 'category',\n    and calculates the count, mean, min, max publication times only considering the hour.\n    \"\"\"\n    if not isinstance(articles, list):\n        raise TypeError(\"articles should be a list of dictionaries\")\n    if not articles:\n        raise ValueError(\"articles list should not be empty\")\n    for article in articles:\n        if not isinstance(article, dict):\n            raise TypeError(\"articles should be a list of dictionaries\")\n        if not set(article.keys()).issuperset({'category', 'published_time'}):\n            raise ValueError(\"dictionary keys do not match the requirements\")\n\n    # Convert 'published_time' to a specified timezone\n    for article in articles:\n        published_time = pd.to_datetime(article['published_time'])\n        article['published_time'] = published_time.tz_localize(pytz.utc).tz_convert(timezone)\n\n    # Group articles by 'category'\n    df = pd.DataFrame(articles)\n    df['published_hour'] = df['published_time'].dt.hour\n    result = df.groupby('category').agg({'published_hour': ['count', 'mean', 'min', 'max']})\n\n    return result"}
{"task_id": "BigCodeBench/781", "solution": "import os\nfrom datetime import datetime\ndef task_func(filepath: str) -> dict:\n    \"\"\"\n    Determines the size and date of the last modification of a file.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n\n    file_size = os.path.getsize(filepath)\n    modification_time = os.path.getmtime(filepath)\n    modification_date = datetime.fromtimestamp(modification_time).strftime('%Y-%m-%d %H:%M:%S')\n\n    return {\"size\": file_size, \"last_modification\": modification_date}", "raw_solution": "import os\nfrom datetime import datetime\n\ndef task_func(filepath: str) -> dict:\n    \"\"\"\n    Determines the size and date of the last modification of a file.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n\n    file_size = os.path.getsize(filepath)\n    modification_time = os.path.getmtime(filepath)\n    modification_date = datetime.fromtimestamp(modification_time).strftime('%Y-%m-%d %H:%M:%S')\n\n    return {\"size\": file_size, \"last_modification\": modification_date}"}
{"task_id": "BigCodeBench/782", "solution": "import random\nimport pandas as pd\nimport numpy as np\ndef task_func(n,\n              domain=\"samplewebsite.com\",\n              categories=['Sports', 'Technology', 'Health', 'Science', 'Business'],\n              random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for i in range(n):\n        title = f\"Article {i}\"\n        title_url = f\"{domain}/{title}\"\n        id = i\n        category = random.choice(categories)\n        views = np.random.poisson(1000)\n        data.append([title, title_url, id, category, views])\n\n    df = pd.DataFrame(data, columns=['title', 'title_url', 'id', 'category', 'views'])\n    return df", "raw_solution": "import random\nimport pandas as pd\nimport numpy as np\n\ndef task_func(n,\n              domain=\"samplewebsite.com\",\n              categories=['Sports', 'Technology', 'Health', 'Science', 'Business'],\n              random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for i in range(n):\n        title = f\"Article {i}\"\n        title_url = f\"{domain}/{title}\"\n        id = i\n        category = random.choice(categories)\n        views = np.random.poisson(1000)\n        data.append([title, title_url, id, category, views])\n\n    df = pd.DataFrame(data, columns=['title', 'title_url', 'id', 'category', 'views'])\n    return df\n\n# Test the function\ndf = task_func(3, categories=['A', 'B'], domain='test.de', random_seed=12)"}
{"task_id": "BigCodeBench/783", "solution": "import os\nimport shutil\ndef task_func(src_dir, dest_dir, extension):\n    \"\"\"\n    Moves all files with a particular extension from one directory to another.\n    :param src_dir: The source directory.\n    :param dest_dir: The destination directory.\n    :param extension: The file extension to move.\n    :return: The number of files moved.\n    \"\"\"\n    if not os.path.exists(src_dir) or not os.path.exists(dest_dir):\n        raise ValueError(\"Source or destination directory does not exist.\")\n\n    files_moved = 0\n    for file_name in os.listdir(src_dir):\n        if file_name.endswith(extension):\n            shutil.move(os.path.join(src_dir, file_name), dest_dir)\n            files_moved += 1\n\n    return files_moved", "raw_solution": "import os\nimport shutil\n\ndef task_func(src_dir, dest_dir, extension):\n    \"\"\"\n    Moves all files with a particular extension from one directory to another.\n    :param src_dir: The source directory.\n    :param dest_dir: The destination directory.\n    :param extension: The file extension to move.\n    :return: The number of files moved.\n    \"\"\"\n    if not os.path.exists(src_dir) or not os.path.exists(dest_dir):\n        raise ValueError(\"Source or destination directory does not exist.\")\n\n    files_moved = 0\n    for file_name in os.listdir(src_dir):\n        if file_name.endswith(extension):\n            shutil.move(os.path.join(src_dir, file_name), dest_dir)\n            files_moved += 1\n\n    return files_moved"}
{"task_id": "BigCodeBench/784", "solution": "import pandas as pd\nimport random\nimport csv\ndef task_func(n, \n           categories=['Sports', 'Technology', 'Business', 'Politics', 'Entertainment'],\n           news_sites=['New York Times', 'USA Today', 'Apple News', 'CNN', 'BBC'],\n           likert_scale=['Strongly Disagree', 'Disagree', 'Neither Agree nor Disagree', 'Agree', 'Strongly Agree'],\n           file_path='news_survey_data.csv',\n           random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        site = random.choice(news_sites)\n        category = random.choice(categories)\n        response = random.choice(likert_scale)\n        value = likert_scale.index(response) + 1\n        data.append([site, category, response, value])\n\n    df = pd.DataFrame(data, columns=['Site', 'Category', 'Response', 'Value'])\n\n    df.to_csv(file_path, index=False)\n\n    df = pd.read_csv(file_path)\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport csv\n\ndef task_func(n, \n           categories=['Sports', 'Technology', 'Business', 'Politics', 'Entertainment'],\n           news_sites=['New York Times', 'USA Today', 'Apple News', 'CNN', 'BBC'],\n           likert_scale=['Strongly Disagree', 'Disagree', 'Neither Agree nor Disagree', 'Agree', 'Strongly Agree'],\n           file_path='news_survey_data.csv',\n           random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        site = random.choice(news_sites)\n        category = random.choice(categories)\n        response = random.choice(likert_scale)\n        value = likert_scale.index(response) + 1\n        data.append([site, category, response, value])\n\n    df = pd.DataFrame(data, columns=['Site', 'Category', 'Response', 'Value'])\n\n    df.to_csv(file_path, index=False)\n\n    df = pd.read_csv(file_path)\n\n    return df\n\n# Test the function\ndf = task_func(8, ['test', 'fun'], likert_scale=['true', 'false'], news_sites=['cat', 'dog'], random_seed=12)"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nimport shutil\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    \"\"\"\n    Archives all files that match a particular pattern and then deletes the original files.\n    \"\"\"\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find files that match the pattern\n    files = glob.glob(pattern)\n\n    # Archive and delete each file\n    for file in files:\n        # Get the base name of the file\n        base_name = os.path.basename(file)\n\n        # Create the archive file path\n        archive_file = os.path.join(ARCHIVE_DIR, base_name)\n\n        # Archive the file\n        shutil.move(file, archive_file)\n\n    # Return the archive file path\n    return archive_file", "raw_solution": "import subprocess\nimport os\nimport glob\nimport shutil\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    Archives all files that match a particular pattern and then deletes the original files.\n    \"\"\"\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find files that match the pattern\n    files = glob.glob(pattern)\n\n    # Archive and delete each file\n    for file in files:\n        # Get the base name of the file\n        base_name = os.path.basename(file)\n\n        # Create the archive file path\n        archive_file = os.path.join(ARCHIVE_DIR, base_name)\n\n        # Archive the file\n        shutil.move(file, archive_file)\n\n    # Return the archive file path\n    return archive_file"}
{"task_id": "BigCodeBench/786", "solution": "import pandas as pd\nimport random\ndef task_func(\n    n, \n    countries=['USA', 'UK', 'China', 'India', 'Germany'], \n    products=['Product A', 'Product B', 'Product C', 'Product D', 'Product E'], \n    output_path=None,\n    random_seed=None):\n    \n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = {\n        'Country': [random.choice(countries) for _ in range(n)],\n        'Product': [random.choice(products) for _ in range(n)],\n        'Sales': [random.randint(1, 100) for _ in range(n)]\n    }\n\n    df = pd.DataFrame(data)\n\n    if output_path is not None:\n        df.to_csv(output_path, index=False)\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(\n    n, \n    countries=['USA', 'UK', 'China', 'India', 'Germany'], \n    products=['Product A', 'Product B', 'Product C', 'Product D', 'Product E'], \n    output_path=None,\n    random_seed=None):\n    \n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = {\n        'Country': [random.choice(countries) for _ in range(n)],\n        'Product': [random.choice(products) for _ in range(n)],\n        'Sales': [random.randint(1, 100) for _ in range(n)]\n    }\n\n    df = pd.DataFrame(data)\n\n    if output_path is not None:\n        df.to_csv(output_path, index=False)\n\n    return df"}
{"task_id": "BigCodeBench/787", "solution": "import numpy as np\nfrom itertools import combinations\ndef task_func(array1, array2):\n    if len(array1) != len(array2):\n        raise ValueError(\"Input arrays must have the same length\")\n\n    if not array1 or not array2:\n        return 0\n\n    points = [(x, y) for x in array1 for y in array2]\n    distances = [np.linalg.norm(np.array(p1) - np.array(p2)) for p1, p2 in combinations(points, 2)]\n    return max(distances)", "raw_solution": "import numpy as np\nfrom itertools import combinations\n\ndef task_func(array1, array2):\n    if len(array1) != len(array2):\n        raise ValueError(\"Input arrays must have the same length\")\n\n    if not array1 or not array2:\n        return 0\n\n    points = [(x, y) for x in array1 for y in array2]\n    distances = [np.linalg.norm(np.array(p1) - np.array(p2)) for p1, p2 in combinations(points, 2)]\n    return max(distances)"}
{"task_id": "BigCodeBench/788", "solution": "import pandas as pd\nimport heapq\nfrom scipy import stats\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame\")\n    if N <= 1:\n        raise ValueError(\"N should be greater than 1\")\n\n    df['diff'] = abs(df[col1] - df[col2])\n    largest_diffs = heapq.nlargest(N, df['diff'])\n    t_stat, p_value = stats.ttest_1samp(largest_diffs, 0)\n\n    return p_value", "raw_solution": "import pandas as pd\nimport heapq\nfrom scipy import stats\n\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame\")\n    if N <= 1:\n        raise ValueError(\"N should be greater than 1\")\n\n    df['diff'] = abs(df[col1] - df[col2])\n    largest_diffs = heapq.nlargest(N, df['diff'])\n    t_stat, p_value = stats.ttest_1samp(largest_diffs, 0)\n\n    return p_value"}
{"task_id": "BigCodeBench/789", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nARRAY_LENGTH = 10\ndef task_func():\n    # Generate a random array\n    array = np.random.rand(ARRAY_LENGTH)\n\n    # Apply min-max normalization\n    scaler = MinMaxScaler()\n    scaled_array = scaler.fit_transform(array.reshape(-1, 1))\n\n    return scaled_array", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nARRAY_LENGTH = 10\n\ndef task_func():\n    # Generate a random array\n    array = np.random.rand(ARRAY_LENGTH)\n\n    # Apply min-max normalization\n    scaler = MinMaxScaler()\n    scaled_array = scaler.fit_transform(array.reshape(-1, 1))\n\n    return scaled_array"}
{"task_id": "BigCodeBench/790", "solution": "import pandas as pd\nimport heapq\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame\")\n\n    scaler = StandardScaler()\n    df[[col1, col2]] = scaler.fit_transform(df[[col1, col2]])\n\n    df['diff'] = df[col1] - df[col2]\n    df['diff'] = df['diff'].abs()\n\n    indices = df.nlargest(N, 'diff').index.tolist()\n\n    return indices", "raw_solution": "import pandas as pd\nimport heapq\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame\")\n\n    scaler = StandardScaler()\n    df[[col1, col2]] = scaler.fit_transform(df[[col1, col2]])\n\n    df['diff'] = df[col1] - df[col2]\n    df['diff'] = df['diff'].abs()\n\n    indices = df.nlargest(N, 'diff').index.tolist()\n\n    return indices"}
{"task_id": "BigCodeBench/791", "solution": "from collections import Counter\nimport random\nfrom itertools import cycle\ndef task_func(l):\n    # Shuffle the list and create a cycled iterator\n    random.shuffle(l)\n    cycled_l = cycle(l)\n\n    # Create a list with the first 30 elements from the cycled iterator\n    first_30_elements = [next(cycled_l) for _ in range(30)]\n\n    # Move the first 3 elements to the end of the list\n    first_30_elements = first_30_elements[3:] + first_30_elements[:3]\n\n    # Create a counter from the list and return it\n    return Counter(first_30_elements)\nl = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']", "raw_solution": "from collections import Counter\nimport random\nfrom itertools import cycle\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\ndef task_func(l):\n    # Shuffle the list and create a cycled iterator\n    random.shuffle(l)\n    cycled_l = cycle(l)\n\n    # Create a list with the first 30 elements from the cycled iterator\n    first_30_elements = [next(cycled_l) for _ in range(30)]\n\n    # Move the first 3 elements to the end of the list\n    first_30_elements = first_30_elements[3:] + first_30_elements[:3]\n\n    # Create a counter from the list and return it\n    return Counter(first_30_elements)\n\n# Test the function\nl = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']"}
{"task_id": "BigCodeBench/792", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, feature, target, n=10):\n    if feature not in df.columns or target not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame\")\n\n    X = df[[feature]]\n    y = df[target]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    residuals = np.abs(y - model.predict(X))\n    indices = np.argsort(residuals)[-n:]\n\n    return list(indices), model", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, feature, target, n=10):\n    if feature not in df.columns or target not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame\")\n\n    X = df[[feature]]\n    y = df[target]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    residuals = np.abs(y - model.predict(X))\n    indices = np.argsort(residuals)[-n:]\n\n    return list(indices), model"}
{"task_id": "BigCodeBench/793", "solution": "import numpy as np\nimport random\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\ndef task_func(l=None):\n    if l is None:\n        l = random.choices(ELEMENTS, k=10)\n    arr = np.array(l)\n    arr = np.roll(arr, -3)\n    return arr", "raw_solution": "import numpy as np\nimport random\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\ndef task_func(l=None):\n    if l is None:\n        l = random.choices(ELEMENTS, k=10)\n    arr = np.array(l)\n    arr = np.roll(arr, -3)\n    return arr\n\n# Test the function"}
{"task_id": "BigCodeBench/794", "solution": "import string\nimport random\ndef task_func(length, random_seed=None):\n    BRACKETS = \"(){}[]\"\n    CHARACTERS = string.ascii_lowercase\n\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    result = [random.choice(BRACKETS + CHARACTERS) for _ in range(length)]\n    return ''.join(result)", "raw_solution": "import string\nimport random\n\ndef task_func(length, random_seed=None):\n    BRACKETS = \"(){}[]\"\n    CHARACTERS = string.ascii_lowercase\n\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    result = [random.choice(BRACKETS + CHARACTERS) for _ in range(length)]\n    return ''.join(result)\n\n# Testing the function"}
{"task_id": "BigCodeBench/795", "solution": "from collections import deque\nimport math\ndef task_func(l):\n    dq = deque(l)\n    dq.rotate(3)\n    sum_of_elements = sum(i for i in dq if isinstance(i, (int, float)))\n    if sum_of_elements:\n        print(f\"The square root of the sum of numeric elements: {math.sqrt(sum_of_elements)}\")\n    return dq", "raw_solution": "from collections import deque\nimport math\n\ndef task_func(l):\n    dq = deque(l)\n    dq.rotate(3)\n    sum_of_elements = sum(i for i in dq if isinstance(i, (int, float)))\n    if sum_of_elements:\n        print(f\"The square root of the sum of numeric elements: {math.sqrt(sum_of_elements)}\")\n    return dq"}
{"task_id": "BigCodeBench/796", "solution": "import os\nimport re\nBRACKET_PATTERN = '[(){}\\[\\]]'\ndef task_func(directory):\n    \"\"\"\n    Finds all files in the specified directory whose names contain any type of bracket (round, curly, or square).\n    Uses an internal constant BRACKET_PATTERN = '[(){}\\\\[\\\\]]', which specifies the brackets that are looked for.\n    \"\"\"\n    bracket_files = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(BRACKET_PATTERN, file):\n                bracket_files.append(os.path.join(root, file))\n    return bracket_files", "raw_solution": "import os\nimport re\n\nBRACKET_PATTERN = '[(){}\\[\\]]'\n\ndef task_func(directory):\n    \"\"\"\n    Finds all files in the specified directory whose names contain any type of bracket (round, curly, or square).\n    Uses an internal constant BRACKET_PATTERN = '[(){}\\\\[\\\\]]', which specifies the brackets that are looked for.\n    \"\"\"\n    bracket_files = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(BRACKET_PATTERN, file):\n                bracket_files.append(os.path.join(root, file))\n    return bracket_files"}
{"task_id": "BigCodeBench/797", "solution": "import re\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> int:\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input should be a pandas DataFrame\")\n\n    pattern = r'[\\(\\)\\{\\}\\[\\]]{1}'\n    df['Test'] = df['Test'].astype(str)\n    df['Count'] = df['Test'].apply(lambda x: len(re.findall(pattern, x)))\n    total_count = df['Count'].sum()\n    return total_count\ndf = pd.DataFrame({'Test': ['(a)', 'b', '[[[[))c']})", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> int:\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input should be a pandas DataFrame\")\n\n    pattern = r'[\\(\\)\\{\\}\\[\\]]{1}'\n    df['Test'] = df['Test'].astype(str)\n    df['Count'] = df['Test'].apply(lambda x: len(re.findall(pattern, x)))\n    total_count = df['Count'].sum()\n    return total_count\n\n# Test the function\ndf = pd.DataFrame({'Test': ['(a)', 'b', '[[[[))c']})"}
{"task_id": "BigCodeBench/798", "solution": "import os\nimport shutil\nBACKUP_DIR = '/tmp/backup'\ndef task_func(directory):\n    \"\"\"\n    Restores a directory from a backup.\n    \"\"\"\n    backup_path = os.path.join(BACKUP_DIR, directory)\n\n    if not os.path.exists(backup_path):\n        return f\"Backup directory {BACKUP_DIR} does not exist. Cannot rollback update.\"\n\n    if os.path.exists(directory):\n        shutil.rmtree(directory)\n\n    shutil.copytree(backup_path, directory)\n\n    return directory", "raw_solution": "import os\nimport shutil\n\n# Constants\nBACKUP_DIR = '/tmp/backup'\n\ndef task_func(directory):\n    \"\"\"\n    Restores a directory from a backup.\n    \"\"\"\n    backup_path = os.path.join(BACKUP_DIR, directory)\n\n    if not os.path.exists(backup_path):\n        return f\"Backup directory {BACKUP_DIR} does not exist. Cannot rollback update.\"\n\n    if os.path.exists(directory):\n        shutil.rmtree(directory)\n\n    shutil.copytree(backup_path, directory)\n\n    return directory"}
{"task_id": "BigCodeBench/799", "solution": "import pandas as pd\nfrom random import seed, choices\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if not L:\n        return pd.DataFrame(), []\n\n    column_names = [''.join(choices('abcdefghijklmnopqrstuvwxyz', k=3)) for _ in range(3)]\n    df_list = []\n\n    for _ in range(num_dataframes):\n        df = pd.DataFrame(columns=column_names)\n        for _ in range(3):\n            df = df.append(pd.Series(choices(L, k=len(column_names))), ignore_index=True)\n        df_list.append(df)\n\n    common_rows = df_list[0]\n    for df in df_list[1:]:\n        common_rows = pd.merge(common_rows, df, how='inner')\n\n    return common_rows, df_list", "raw_solution": "import pandas as pd\nfrom random import seed, choices\n\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if not L:\n        return pd.DataFrame(), []\n\n    column_names = [''.join(choices('abcdefghijklmnopqrstuvwxyz', k=3)) for _ in range(3)]\n    df_list = []\n\n    for _ in range(num_dataframes):\n        df = pd.DataFrame(columns=column_names)\n        for _ in range(3):\n            df = df.append(pd.Series(choices(L, k=len(column_names))), ignore_index=True)\n        df_list.append(df)\n\n    common_rows = df_list[0]\n    for df in df_list[1:]:\n        common_rows = pd.merge(common_rows, df, how='inner')\n\n    return common_rows, df_list"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize counters\n    total_goals = 0\n    total_penalties = 0\n\n    # Read CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.reader(file)\n        next(reader)  # Skip header\n        for row in reader:\n            total_goals += int(row[1])\n            total_penalties += int(row[2])\n\n    # Update with given goals and penalties\n    total_goals += goals\n    total_penalties += penalties\n\n    # Return Counter object\n    return Counter({'goals': total_goals, 'penalties': total_penalties})", "raw_solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize counters\n    total_goals = 0\n    total_penalties = 0\n\n    # Read CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.reader(file)\n        next(reader)  # Skip header\n        for row in reader:\n            total_goals += int(row[1])\n            total_penalties += int(row[2])\n\n    # Update with given goals and penalties\n    total_goals += goals\n    total_penalties += penalties\n\n    # Return Counter object\n    return Counter({'goals': total_goals, 'penalties': total_penalties})\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\n# Test setup\nsetup_csv()\n\n# Test task_func"}
{"task_id": "BigCodeBench/801", "solution": "import collections\nimport csv\nimport numpy as np\ndef task_func(file_name):\n    \"\"\"\n    Finds the most common value in each column of a csv file with column names.\n    If some values occur the same number of times, the values are sorted alphabetically\n    and the first is considered most common. If an empty csv is passed, an empty\n    dictionary is returned.\n\n    Args:\n        file_name (str): The name of the csv file.\n\n    Returns:\n        dict: A dictionary with column names as keys and most common values as values.\n    \"\"\"\n    try:\n        with open(file_name, 'r') as file:\n            reader = csv.reader(file)\n            data = list(reader)\n\n        if not data:\n            return {}\n\n        # Transpose the data to get columns\n        data_t = np.transpose(data)\n\n        # Find the most common value in each column\n        result = {}\n        for i, col in enumerate(data_t):\n            counter = collections.Counter(col)\n            most_common = counter.most_common(1)[0][0]\n            result[data[0][i]] = most_common\n\n        return result\n\n    except FileNotFoundError:\n        print(f\"File {file_name} not found.\")\n        return {}", "raw_solution": "import collections\nimport csv\nimport numpy as np\n\ndef task_func(file_name):\n    \"\"\"\n    Finds the most common value in each column of a csv file with column names.\n    If some values occur the same number of times, the values are sorted alphabetically\n    and the first is considered most common. If an empty csv is passed, an empty\n    dictionary is returned.\n\n    Args:\n        file_name (str): The name of the csv file.\n\n    Returns:\n        dict: A dictionary with column names as keys and most common values as values.\n    \"\"\"\n    try:\n        with open(file_name, 'r') as file:\n            reader = csv.reader(file)\n            data = list(reader)\n\n        if not data:\n            return {}\n\n        # Transpose the data to get columns\n        data_t = np.transpose(data)\n\n        # Find the most common value in each column\n        result = {}\n        for i, col in enumerate(data_t):\n            counter = collections.Counter(col)\n            most_common = counter.most_common(1)[0][0]\n            result[data[0][i]] = most_common\n\n        return result\n\n    except FileNotFoundError:\n        print(f\"File {file_name} not found.\")\n        return {}\n\n# Test the function"}
{"task_id": "BigCodeBench/802", "solution": "import numpy as np\ndef task_func(dimension, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.randint(1, 101, dimension)\n    flat_list = list(itertools.chain.from_iterable(matrix))\n    return matrix, flat_list", "raw_solution": "import numpy as np\n\ndef task_func(dimension, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.randint(1, 101, dimension)\n    flat_list = list(itertools.chain.from_iterable(matrix))\n    return matrix, flat_list"}
{"task_id": "BigCodeBench/803", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(file_name: str) -> pd.DataFrame:\n    \"\"\"\n    Normalizes data in a csv file using MinMaxScaler from sklearn. Only numeric columns are normalized.\n    Columns with other dtypes are left as they are.\n    \"\"\"\n    # Read the csv file\n    df = pd.read_csv(file_name)\n\n    # Check if there are any numeric columns\n    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n    if not numeric_cols.any():\n        raise ValueError(\"Input does not have numeric columns\")\n\n    # Normalize the numeric columns\n    scaler = MinMaxScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(file_name: str) -> pd.DataFrame:\n    \"\"\"\n    Normalizes data in a csv file using MinMaxScaler from sklearn. Only numeric columns are normalized.\n    Columns with other dtypes are left as they are.\n    \"\"\"\n    # Read the csv file\n    df = pd.read_csv(file_name)\n\n    # Check if there are any numeric columns\n    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n    if not numeric_cols.any():\n        raise ValueError(\"Input does not have numeric columns\")\n\n    # Normalize the numeric columns\n    scaler = MinMaxScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    return df"}
{"task_id": "BigCodeBench/804", "solution": "import os\nfrom datetime import datetime\nLOG_DIR = './logs'\ndef task_func(metrics, filename, log_dir=LOG_DIR):\n    \"\"\"\n    Writes a dictionary of metrics to a specified log file, appending a timestamp to each entry.\n    \"\"\"\n    # Create the log directory if it doesn't exist\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    # Construct the full path to the log file\n    log_file = os.path.join(log_dir, filename)\n\n    # Append the metrics to the log file\n    try:\n        with open(log_file, 'a') as f:\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            f.write(f'{timestamp}: {str(metrics)}\\n')\n    except IOError:\n        return False\n\n    return True", "raw_solution": "import os\nfrom datetime import datetime\n\n# Constants\nLOG_DIR = './logs'\n\ndef task_func(metrics, filename, log_dir=LOG_DIR):\n    \"\"\"\n    Writes a dictionary of metrics to a specified log file, appending a timestamp to each entry.\n    \"\"\"\n    # Create the log directory if it doesn't exist\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    # Construct the full path to the log file\n    log_file = os.path.join(log_dir, filename)\n\n    # Append the metrics to the log file\n    try:\n        with open(log_file, 'a') as f:\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            f.write(f'{timestamp}: {str(metrics)}\\n')\n    except IOError:\n        return False\n\n    return True"}
{"task_id": "BigCodeBench/805", "solution": "import pandas as pd\nimport random\ndef task_func(dictionary, item, seed):\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(dictionary)\n\n    # Find locations of the item\n    locations = [(i, col) for col in df.columns for i in range(df.shape[0]) if df.loc[i, col] == item]\n\n    # Count the number of occurences\n    count = len(locations)\n\n    # Add a random integer x, where 0 <=x < 10, to it\n    random.seed(seed)\n    for loc in locations:\n        df.loc[loc[0], loc[1]] += random.randint(0, 9)\n\n    return locations, count, df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(dictionary, item, seed):\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(dictionary)\n\n    # Find locations of the item\n    locations = [(i, col) for col in df.columns for i in range(df.shape[0]) if df.loc[i, col] == item]\n\n    # Count the number of occurences\n    count = len(locations)\n\n    # Add a random integer x, where 0 <=x < 10, to it\n    random.seed(seed)\n    for loc in locations:\n        df.loc[loc[0], loc[1]] += random.randint(0, 9)\n\n    return locations, count, df"}
{"task_id": "BigCodeBench/806", "solution": "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk.util import ngrams\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(text, n=2):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Remove stopwords\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Generate n-grams\n    n_grams = ngrams(words, n)\n\n    # Count n-grams\n    n_grams_count = Counter(n_grams)\n\n    return n_grams_count", "raw_solution": "import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk.util import ngrams\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(text, n=2):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Remove stopwords\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Generate n-grams\n    n_grams = ngrams(words, n)\n\n    # Count n-grams\n    n_grams_count = Counter(n_grams)\n\n    return n_grams_count"}
{"task_id": "BigCodeBench/807", "solution": "import numpy as np\nfrom scipy.stats import norm\ndef task_func(data: np.ndarray, threshold: float = 2.0) -> list:\n    \"\"\"\n    Determines the outlier indices in a 1D numpy array based on the Z score.\n    The function first fits a normal distribution to the data, the mean and standard deviation\n    is used to calculate the z scores of each datapoint. If the absolute z score of a datapoint\n    is larger than threshold it is considered an outlier and its index is recorded. If the\n    standard deviation is 0, an empty list is returned as outliers.\n    \"\"\"\n    # Calculate mean and standard deviation\n    mean = np.mean(data)\n    std_dev = np.std(data)\n\n    # If standard deviation is 0, return empty list as outliers\n    if std_dev == 0:\n        return [], mean, std_dev**2\n\n    # Calculate Z scores\n    z_scores = (data - mean) / std_dev\n\n    # Find outliers\n    outliers = [i for i, z in enumerate(z_scores) if np.abs(z) > threshold]\n\n    return outliers, mean, std_dev**2\ndata = np.array([-10, 3, 5, 5, 5, 5, 5, 7, 20])", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\n\ndef task_func(data: np.ndarray, threshold: float = 2.0) -> list:\n    \"\"\"\n    Determines the outlier indices in a 1D numpy array based on the Z score.\n    The function first fits a normal distribution to the data, the mean and standard deviation\n    is used to calculate the z scores of each datapoint. If the absolute z score of a datapoint\n    is larger than threshold it is considered an outlier and its index is recorded. If the\n    standard deviation is 0, an empty list is returned as outliers.\n    \"\"\"\n    # Calculate mean and standard deviation\n    mean = np.mean(data)\n    std_dev = np.std(data)\n\n    # If standard deviation is 0, return empty list as outliers\n    if std_dev == 0:\n        return [], mean, std_dev**2\n\n    # Calculate Z scores\n    z_scores = (data - mean) / std_dev\n\n    # Find outliers\n    outliers = [i for i, z in enumerate(z_scores) if np.abs(z) > threshold]\n\n    return outliers, mean, std_dev**2\n\n# Test the function\ndata = np.array([-10, 3, 5, 5, 5, 5, 5, 7, 20])\noutliers, mean, var = task_func(data, threshold=4)"}
{"task_id": "BigCodeBench/808", "solution": "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Remove stopwords\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n    text = ' '.join(words)\n\n    # Analyze sentiment\n    sentiment = TextBlob(text).sentiment.polarity\n\n    return f\"Sentiment: {sentiment}\"", "raw_solution": "import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Remove stopwords\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n    text = ' '.join(words)\n\n    # Analyze sentiment\n    sentiment = TextBlob(text).sentiment.polarity\n\n    return f\"Sentiment: {sentiment}\""}
{"task_id": "BigCodeBench/809", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(data)\n    labels = kmeans.labels_\n    result = {}\n    for i, label in enumerate(labels):\n        if label not in result:\n            result[label] = []\n        result[label].append(i)\n    return result\ndata = np.array([[1, 1], [2, 2]])", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef task_func(data, n_clusters):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(data)\n    labels = kmeans.labels_\n    result = {}\n    for i, label in enumerate(labels):\n        if label not in result:\n            result[label] = []\n        result[label].append(i)\n    return result\n\n# Test the function\ndata = np.array([[1, 1], [2, 2]])\ncluster = task_func(data, 2)\ncluster_list = list(cluster.values())\ncluster_list.sort(key=lambda x: x[0])"}
{"task_id": "BigCodeBench/810", "solution": "import re\nimport os\nimport subprocess\ndef task_func(dir_path, exe_pattern, execute_files=True):\n    \"\"\"\n    Searches for executable files in a specified directory that match a given regular expression pattern.\n    Optionally executes any matching files and returns a list of standard outputs from the executed files or the paths of the found files.\n\n    Args:\n        dir_path (str): The directory path to search for files.\n        exe_pattern (str): The regular expression pattern to match file names.\n        execute_files (bool, optional): If True, execute the matched files and return their standard outputs. If False, return the paths of the matched files. Defaults to True.\n\n    Returns:\n        list: A list of standard outputs from the executed files or the paths of the found files.\n    \"\"\"\n    results = []\n    for root, dirs, files in os.walk(dir_path):\n        for file in files:\n            if re.search(exe_pattern, file):\n                file_path = os.path.join(root, file)\n                if execute_files:\n                    try:\n                        output = subprocess.check_output(file_path, stderr=subprocess.STDOUT, shell=True)\n                        results.append(output.decode('utf-8'))\n                    except subprocess.CalledProcessError as e:\n                        print(f\"Error executing {file_path}: {e}\")\n                else:\n                    results.append(file_path)\n    return results", "raw_solution": "import re\nimport os\nimport subprocess\n\ndef task_func(dir_path, exe_pattern, execute_files=True):\n    \"\"\"\n    Searches for executable files in a specified directory that match a given regular expression pattern.\n    Optionally executes any matching files and returns a list of standard outputs from the executed files or the paths of the found files.\n\n    Args:\n        dir_path (str): The directory path to search for files.\n        exe_pattern (str): The regular expression pattern to match file names.\n        execute_files (bool, optional): If True, execute the matched files and return their standard outputs. If False, return the paths of the matched files. Defaults to True.\n\n    Returns:\n        list: A list of standard outputs from the executed files or the paths of the found files.\n    \"\"\"\n    results = []\n    for root, dirs, files in os.walk(dir_path):\n        for file in files:\n            if re.search(exe_pattern, file):\n                file_path = os.path.join(root, file)\n                if execute_files:\n                    try:\n                        output = subprocess.check_output(file_path, stderr=subprocess.STDOUT, shell=True)\n                        results.append(output.decode('utf-8'))\n                    except subprocess.CalledProcessError as e:\n                        print(f\"Error executing {file_path}: {e}\")\n                else:\n                    results.append(file_path)\n    return results"}
{"task_id": "BigCodeBench/811", "solution": "import pandas as pd\nfrom random import randint, seed\ndef task_func(dictionary, item, sample_size=None, random_seed=None):\n    df = pd.DataFrame(dictionary)\n    positions = [(i, col) for i, col in enumerate(df.columns) if df[col].str.contains(item).any()]\n    if random_seed is not None:\n        seed(random_seed)\n    if sample_size is not None:\n        positions = [positions[randint(0, len(positions) - 1)] for _ in range(sample_size)]\n    return positions, df\ndictionary = {\n    1: ['road', 'car', 'traffic'],\n    2: ['car', 'light', 'candle']\n}", "raw_solution": "import pandas as pd\nfrom random import randint, seed\n\ndef task_func(dictionary, item, sample_size=None, random_seed=None):\n    df = pd.DataFrame(dictionary)\n    positions = [(i, col) for i, col in enumerate(df.columns) if df[col].str.contains(item).any()]\n    if random_seed is not None:\n        seed(random_seed)\n    if sample_size is not None:\n        positions = [positions[randint(0, len(positions) - 1)] for _ in range(sample_size)]\n    return positions, df\n\n# Test the function\ndictionary = {\n    1: ['road', 'car', 'traffic'],\n    2: ['car', 'light', 'candle']\n}\npositions, df = task_func(dictionary, 'car', sample_size=2, random_seed=42)"}
{"task_id": "BigCodeBench/812", "solution": "import re\nfrom pathlib import Path\nimport tarfile\nPATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIRECTORY = r\"C:\\\\SomeDir\\\\\"\ndef task_func(directory=DIRECTORY, file_pattern=PATTERN):\n    \"\"\"\n    Finds files in the given directory that match the given file pattern,\n    and archives them in a tar file.\n    \"\"\"\n    # Create a regex object\n    regex = re.compile(file_pattern)\n\n    # Find all files in the directory that match the pattern\n    files = [str(f) for f in Path(directory).rglob('*') if f.is_file() and regex.search(str(f))]\n\n    # If no files found, return None\n    if not files:\n        return None\n\n    # Create a tar file\n    tar_name = 'archive.tar'\n    with tarfile.open(tar_name, 'w') as tar:\n        for file in files:\n            tar.add(file)\n\n    # Return the path to the created tar file\n    return Path(tar_name).resolve()", "raw_solution": "import re\nfrom pathlib import Path\nimport tarfile\n\n# Constants\nPATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIRECTORY = r\"C:\\\\SomeDir\\\\\"\n\ndef task_func(directory=DIRECTORY, file_pattern=PATTERN):\n    \"\"\"\n    Finds files in the given directory that match the given file pattern,\n    and archives them in a tar file.\n    \"\"\"\n    # Create a regex object\n    regex = re.compile(file_pattern)\n\n    # Find all files in the directory that match the pattern\n    files = [str(f) for f in Path(directory).rglob('*') if f.is_file() and regex.search(str(f))]\n\n    # If no files found, return None\n    if not files:\n        return None\n\n    # Create a tar file\n    tar_name = 'archive.tar'\n    with tarfile.open(tar_name, 'w') as tar:\n        for file in files:\n            tar.add(file)\n\n    # Return the path to the created tar file\n    return Path(tar_name).resolve()"}
{"task_id": "BigCodeBench/813", "solution": "from itertools import combinations\nimport pandas as pd\ndef task_func(number_list, element):\n    if not number_list:\n        return pd.DataFrame(columns=['Combinations'])\n\n    result = list(combinations(number_list, 3))\n    result = [x for x in result if sum(x) == element]\n\n    df = pd.DataFrame(result, columns=['Combinations'])\n    df.index = range(1, len(df) + 1)\n\n    return df", "raw_solution": "from itertools import combinations\nimport pandas as pd\n\ndef task_func(number_list, element):\n    if not number_list:\n        return pd.DataFrame(columns=['Combinations'])\n\n    result = list(combinations(number_list, 3))\n    result = [x for x in result if sum(x) == element]\n\n    df = pd.DataFrame(result, columns=['Combinations'])\n    df.index = range(1, len(df) + 1)\n\n    return df"}
{"task_id": "BigCodeBench/814", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Finds files that match the given pattern in the source directory and writes their paths to a configuration file in the target directory.\n    \"\"\"\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Create the configuration file path\n    config_file_path = os.path.join(target_dir, 'config.txt')\n\n    # Open the configuration file for writing\n    with open(config_file_path, 'w') as config_file:\n        # Iterate over all files in the source directory\n        for filename in os.listdir(source_dir):\n            # Check if the file matches the pattern\n            if re.match(file_pattern, filename):\n                # If it does, write the file path to the configuration file\n                file_path = os.path.join(source_dir, filename)\n                config_file.write(f'{file_path}\\n')\n\n    # Return the path to the configuration file\n    return config_file_path", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Finds files that match the given pattern in the source directory and writes their paths to a configuration file in the target directory.\n    \"\"\"\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Create the configuration file path\n    config_file_path = os.path.join(target_dir, 'config.txt')\n\n    # Open the configuration file for writing\n    with open(config_file_path, 'w') as config_file:\n        # Iterate over all files in the source directory\n        for filename in os.listdir(source_dir):\n            # Check if the file matches the pattern\n            if re.match(file_pattern, filename):\n                # If it does, write the file path to the configuration file\n                file_path = os.path.join(source_dir, filename)\n                config_file.write(f'{file_path}\\n')\n\n    # Return the path to the configuration file\n    return config_file_path"}
{"task_id": "BigCodeBench/815", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(test_scores, student):\n    df = pd.DataFrame(test_scores)\n    if student not in df['Student'].values:\n        raise ValueError('student is not present in the test_scores dataframe')\n    else:\n        student_scores = df[df['Student'] == student]['Score'].values\n        return np.array([np.mean(student_scores), np.std(student_scores)]), df", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(test_scores, student):\n    df = pd.DataFrame(test_scores)\n    if student not in df['Student'].values:\n        raise ValueError('student is not present in the test_scores dataframe')\n    else:\n        student_scores = df[df['Student'] == student]['Score'].values\n        return np.array([np.mean(student_scores), np.std(student_scores)]), df"}
{"task_id": "BigCodeBench/816", "solution": "from collections import Counter\nimport random", "raw_solution": "from collections import Counter\nimport random\n\n# Constants\nHAND_RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\nSUITS = ['H', 'D', 'C', 'S']\n\ndef generate_poker_hand():\n    hand = [random.choice(HAND_RANKS) + random.choice(SUITS) for _ in range(5)]\n    rank_count = Counter(card[0] for card in hand)\n    return (hand, rank_count)\n\ndef test_generate_poker_hand():\n    hand, rank_count = generate_poker_hand()\n    assert len(hand) == 5\n    assert all(card[0] in HAND_RANKS for card in hand)\n    assert all(card[1] in SUITS for card in hand)\n    assert len(rank_count) == 5\n    assert all(count >= 1 for count in rank_count.values())\n\ntest_generate_poker_hand()"}
{"task_id": "BigCodeBench/817", "solution": "from collections import Counter\nimport logging\nimport os\ndef task_func(letter_list, element, log_path='./'):\n    # Set up logging\n    logging.basicConfig(filename=os.path.join(log_path, 'task_func.log'),\n                        filemode='w',\n                        format='%(levelname)s:%(message)s',\n                        level=logging.DEBUG,\n                        encoding='utf-8')\n\n    # Log function call\n    logging.info(f\"Function called with list: {letter_list} and element: {element}\")\n\n    # Check if element is in letter_list\n    if element not in letter_list:\n        logging.error(f\"The element '{element}' is not in the letter list.\")\n        raise ValueError(f\"The element '{element}' is not in the letter list.\")\n\n    # Count frequency of element\n    frequency = Counter(letter_list)[element]\n\n    # Log frequency\n    logging.info(f\"Frequency of '{element}' is {frequency}\")\n\n    # Shutdown logging\n    logging.shutdown()\n\n    return frequency", "raw_solution": "from collections import Counter\nimport logging\nimport os\n\ndef task_func(letter_list, element, log_path='./'):\n    # Set up logging\n    logging.basicConfig(filename=os.path.join(log_path, 'task_func.log'),\n                        filemode='w',\n                        format='%(levelname)s:%(message)s',\n                        level=logging.DEBUG,\n                        encoding='utf-8')\n\n    # Log function call\n    logging.info(f\"Function called with list: {letter_list} and element: {element}\")\n\n    # Check if element is in letter_list\n    if element not in letter_list:\n        logging.error(f\"The element '{element}' is not in the letter list.\")\n        raise ValueError(f\"The element '{element}' is not in the letter list.\")\n\n    # Count frequency of element\n    frequency = Counter(letter_list)[element]\n\n    # Log frequency\n    logging.info(f\"Frequency of '{element}' is {frequency}\")\n\n    # Shutdown logging\n    logging.shutdown()\n\n    return frequency"}
{"task_id": "BigCodeBench/818", "solution": "import re\nimport string\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    cleaned_text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Split into words\n    cleaned_words = cleaned_text.split()\n\n    return cleaned_words", "raw_solution": "import re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    cleaned_text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Split into words\n    cleaned_words = cleaned_text.split()\n\n    return cleaned_words"}
{"task_id": "BigCodeBench/819", "solution": "import time\nimport random\ndef task_func(iterations=5, min_delay=1.0, max_delay=2.0, seed=None):\n    if not isinstance(iterations, int) or iterations <= 0:\n        raise ValueError(\"Iterations must be a positive integer\")\n    if not isinstance(min_delay, (int, float)) or min_delay <= 0:\n        raise ValueError(\"Min_delay must be a positive number\")\n    if not isinstance(max_delay, (int, float)) or max_delay <= 0:\n        raise ValueError(\"Max_delay must be a positive number\")\n    if seed is not None:\n        random.seed(seed)\n    messages = []\n    total_delay = 0\n    for i in range(iterations):\n        delay = random.uniform(min_delay, max_delay)\n        time.sleep(delay)\n        messages.append(f\"{delay:.2f} seconds have passed\")\n        total_delay += delay\n    return messages, total_delay", "raw_solution": "import time\nimport random\n\ndef task_func(iterations=5, min_delay=1.0, max_delay=2.0, seed=None):\n    if not isinstance(iterations, int) or iterations <= 0:\n        raise ValueError(\"Iterations must be a positive integer\")\n    if not isinstance(min_delay, (int, float)) or min_delay <= 0:\n        raise ValueError(\"Min_delay must be a positive number\")\n    if not isinstance(max_delay, (int, float)) or max_delay <= 0:\n        raise ValueError(\"Max_delay must be a positive number\")\n    if seed is not None:\n        random.seed(seed)\n    messages = []\n    total_delay = 0\n    for i in range(iterations):\n        delay = random.uniform(min_delay, max_delay)\n        time.sleep(delay)\n        messages.append(f\"{delay:.2f} seconds have passed\")\n        total_delay += delay\n    return messages, total_delay"}
{"task_id": "BigCodeBench/820", "solution": "import random\nimport string\nLETTERS = string.ascii_letters\ndef task_func(num_words, word_length):\n    if num_words < 0 or word_length < 0:\n        raise ValueError(\"num_words and word_length should be non-negative\")\n\n    words = []\n    for _ in range(num_words):\n        word = ''.join(random.choice(LETTERS) for _ in range(word_length))\n        words.append(word)\n\n    return words", "raw_solution": "import random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef task_func(num_words, word_length):\n    if num_words < 0 or word_length < 0:\n        raise ValueError(\"num_words and word_length should be non-negative\")\n\n    words = []\n    for _ in range(num_words):\n        word = ''.join(random.choice(LETTERS) for _ in range(word_length))\n        words.append(word)\n\n    return words\n\n# Test cases\nassert len(task_func(5, 3)) == 5\nassert all(len(word) == 3 for word in task_func(5, 3))\nassert all(word.isalpha() for word in task_func(5, 3))\n\ntry:\n    task_func(-1, 3)\nexcept ValueError as e:\n    assert str(e) == \"num_words and word_length should be non-negative\"\n\ntry:\n    task_func(3, -1)\nexcept ValueError as e:\n    assert str(e) == \"num_words and word_length should be non-negative\""}
{"task_id": "BigCodeBench/821", "solution": "import time\nimport threading\ndef task_func(delay_time: float = 1.0, num_threads: int = 5):\n    def thread_func(i):\n        time.sleep(delay_time)\n        return f'Delay in thread {i} completed'\n\n    threads = []\n    for i in range(num_threads):\n        thread = threading.Thread(target=thread_func, args=(i,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return [thread.get_message() for thread in threads]", "raw_solution": "import time\nimport threading\n\ndef task_func(delay_time: float = 1.0, num_threads: int = 5):\n    def thread_func(i):\n        time.sleep(delay_time)\n        return f'Delay in thread {i} completed'\n\n    threads = []\n    for i in range(num_threads):\n        thread = threading.Thread(target=thread_func, args=(i,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return [thread.get_message() for thread in threads]\n\nassert task_func(1, 10) == ['Delay in thread 0 completed', 'Delay in thread 1 completed', 'Delay in thread 2 completed', 'Delay in thread 3 completed', 'Delay in thread 4 completed', 'Delay in thread 5 completed', 'Delay in thread 6 completed', 'Delay in thread 7 completed', 'Delay in thread 8 completed', 'Delay in thread 9 completed']"}
{"task_id": "BigCodeBench/822", "solution": "import random\nimport string\nLETTERS = string.ascii_letters\nDIGITS = string.digits\ndef task_func(length, num_digits):\n    if num_digits > length:\n        raise ValueError(\"Number of digits cannot be greater than the length of the password\")\n\n    # Generate a list of random letters and digits\n    password_chars = [random.choice(LETTERS) for _ in range(length - num_digits)] + \\\n                     [random.choice(DIGITS) for _ in range(num_digits)]\n\n    # Shuffle the list to ensure randomness\n    random.shuffle(password_chars)\n\n    # Join the list into a string and return\n    return ''.join(password_chars)", "raw_solution": "import random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\nDIGITS = string.digits\n\ndef task_func(length, num_digits):\n    if num_digits > length:\n        raise ValueError(\"Number of digits cannot be greater than the length of the password\")\n\n    # Generate a list of random letters and digits\n    password_chars = [random.choice(LETTERS) for _ in range(length - num_digits)] + \\\n                     [random.choice(DIGITS) for _ in range(num_digits)]\n\n    # Shuffle the list to ensure randomness\n    random.shuffle(password_chars)\n\n    # Join the list into a string and return\n    return ''.join(password_chars)\n\n# Test the function"}
{"task_id": "BigCodeBench/823", "solution": "import time\nimport numpy as np\ndef task_func(samples=10, delay=0.1):\n    \"\"\"\n    This function simulates a task that takes a certain amount of time to complete.\n    It measures the actual time taken for each sample and calculates the mean and standard deviation of the delay times.\n    \"\"\"\n    delays = []\n    for i in range(samples):\n        start_time = time.time()\n        time.sleep(delay)\n        end_time = time.time()\n        delay_time = end_time - start_time\n        delays.append(delay_time)\n\n    mean_delay = np.mean(delays)\n    std_dev_delay = np.std(delays)\n\n    return (mean_delay, std_dev_delay)", "raw_solution": "import time\nimport numpy as np\n\ndef task_func(samples=10, delay=0.1):\n    \"\"\"\n    This function simulates a task that takes a certain amount of time to complete.\n    It measures the actual time taken for each sample and calculates the mean and standard deviation of the delay times.\n    \"\"\"\n    delays = []\n    for i in range(samples):\n        start_time = time.time()\n        time.sleep(delay)\n        end_time = time.time()\n        delay_time = end_time - start_time\n        delays.append(delay_time)\n\n    mean_delay = np.mean(delays)\n    std_dev_delay = np.std(delays)\n\n    return (mean_delay, std_dev_delay)\n\n# Test the function\nmean, std_dev = task_func(samples=5, delay=0.2)"}
{"task_id": "BigCodeBench/824", "solution": "import re\nimport string\nPUNCTUATION = string.punctuation\ndef task_func(text):\n    words = re.findall(r'\\b\\w+\\b', text)\n    punctuation_marks = sum(text.count(ch) for ch in PUNCTUATION)\n    return (len(words), punctuation_marks)", "raw_solution": "import re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef task_func(text):\n    words = re.findall(r'\\b\\w+\\b', text)\n    punctuation_marks = sum(text.count(ch) for ch in PUNCTUATION)\n    return (len(words), punctuation_marks)\n\n# Test cases\nassert task_func(\"Hello, world! How are you?\") == (3, 3)\nassert task_func(\"This is a test.\") == (4, 1)\nassert task_func(\"\") == (0, 0)\nassert task_func(\"Testing punctuation!!!\") == (1, 3)\nassert task_func(\"Multiple   spaces\") == (2, 0)\nassert task_func(\"No. punctuation\") == (2, 0)\nassert task_func(\"One, two, three, four, five\") == (5, 3)"}
{"task_id": "BigCodeBench/825", "solution": "import numpy as np\nfrom itertools import product\nimport string\ndef task_func(length, seed=None, alphabets=list(string.ascii_lowercase)):\n    np.random.seed(seed)\n    return [''.join(np.random.choice(alphabets, size=length)) for _ in range(10)]", "raw_solution": "import numpy as np\nfrom itertools import product\nimport string\n\ndef task_func(length, seed=None, alphabets=list(string.ascii_lowercase)):\n    np.random.seed(seed)\n    return [''.join(np.random.choice(alphabets, size=length)) for _ in range(10)]\n\n# Test the function"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(source_dir) or not os.path.exists(target_dir):\n        return \"Source or target directory does not exist.\"\n\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(source_dir) or not os.path.exists(target_dir):\n        return \"Source or target directory does not exist.\"\n\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count"}
{"task_id": "BigCodeBench/827", "solution": "import math\nfrom sympy import isprime\ndef task_func(input_list):\n    \"\"\"\n    Filters the prime numbers from the specified list, sort the prime numbers ascending based on their radian value converted to degrees, and return the sorted list.\n    \"\"\"\n    # Filter prime numbers\n    prime_list = [num for num in input_list if isprime(num)]\n\n    # Convert radian to degree and sort\n    prime_list.sort(key=lambda x: math.degrees(x))\n\n    return prime_list", "raw_solution": "import math\nfrom sympy import isprime\n\ndef task_func(input_list):\n    \"\"\"\n    Filters the prime numbers from the specified list, sort the prime numbers ascending based on their radian value converted to degrees, and return the sorted list.\n    \"\"\"\n    # Filter prime numbers\n    prime_list = [num for num in input_list if isprime(num)]\n\n    # Convert radian to degree and sort\n    prime_list.sort(key=lambda x: math.degrees(x))\n\n    return prime_list\n\n# Test the function"}
{"task_id": "BigCodeBench/828", "solution": "import os\nimport errno\nimport shutil\ndef task_func(filename, dest_dir):\n    \"\"\"\n    Copies a file to a specified destination directory and clears its contents.\n    If the destination directory does not exist, it will be created.\n    If a file with the same name already exists in the destination directory, it will be overwritten.\n    If the file cannot be copied, an OSError will be raised.\n    \"\"\"\n    # Check if the destination directory exists, if not, create it\n    if not os.path.exists(dest_dir):\n        try:\n            os.makedirs(dest_dir)\n        except OSError as exc:\n            if exc.errno != errno.EEXIST:\n                raise OSError(\"Could not create directory: {}\".format(dest_dir))\n\n    # Construct the full path to the destination file\n    dest_file = os.path.join(dest_dir, os.path.basename(filename))\n\n    # Copy the file to the destination directory\n    try:\n        shutil.copy2(filename, dest_dir)\n    except OSError as exc:\n        if exc.errno != errno.EEXIST:\n            raise OSError(\"Could not copy file: {}\".format(filename))\n\n    # Clear the content of the original file\n    with open(dest_file, 'w') as f:\n        pass\n\n    return os.path.abspath(dest_file)", "raw_solution": "import os\nimport errno\nimport shutil\n\ndef task_func(filename, dest_dir):\n    \"\"\"\n    Copies a file to a specified destination directory and clears its contents.\n    If the destination directory does not exist, it will be created.\n    If a file with the same name already exists in the destination directory, it will be overwritten.\n    If the file cannot be copied, an OSError will be raised.\n    \"\"\"\n    # Check if the destination directory exists, if not, create it\n    if not os.path.exists(dest_dir):\n        try:\n            os.makedirs(dest_dir)\n        except OSError as exc:\n            if exc.errno != errno.EEXIST:\n                raise OSError(\"Could not create directory: {}\".format(dest_dir))\n\n    # Construct the full path to the destination file\n    dest_file = os.path.join(dest_dir, os.path.basename(filename))\n\n    # Copy the file to the destination directory\n    try:\n        shutil.copy2(filename, dest_dir)\n    except OSError as exc:\n        if exc.errno != errno.EEXIST:\n            raise OSError(\"Could not copy file: {}\".format(filename))\n\n    # Clear the content of the original file\n    with open(dest_file, 'w') as f:\n        pass\n\n    return os.path.abspath(dest_file)"}
{"task_id": "BigCodeBench/829", "solution": "import pandas as pd\nfrom statistics import mean\nfrom typing import Dict, Generator\ndef task_func(df: pd.DataFrame) -> Dict[str, Generator[tuple, None, None]]:\n    if 'Name' not in df.columns or 'Score' not in df.columns:\n        raise ValueError(\"The DataFrame must have 'Name' and 'Score' columns.\")\n\n    df_grouped = df.groupby('Name')['Score'].apply(list).reset_index()\n    gen_dict = {name: (name, mean(scores)) for name, scores in zip(df_grouped['Name'], df_grouped['Score'])}\n\n    return gen_dict", "raw_solution": "import pandas as pd\nfrom statistics import mean\nfrom typing import Dict, Generator\n\ndef task_func(df: pd.DataFrame) -> Dict[str, Generator[tuple, None, None]]:\n    if 'Name' not in df.columns or 'Score' not in df.columns:\n        raise ValueError(\"The DataFrame must have 'Name' and 'Score' columns.\")\n\n    df_grouped = df.groupby('Name')['Score'].apply(list).reset_index()\n    gen_dict = {name: (name, mean(scores)) for name, scores in zip(df_grouped['Name'], df_grouped['Score'])}\n\n    return gen_dict"}
{"task_id": "BigCodeBench/830", "solution": "import json\nimport os\ndef task_func(filename, data):\n    # Check if file exists\n    if os.path.exists(filename):\n        return (False, \"File already exists.\")\n\n    # Write dictionary to file\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n\n    # Read back the written data\n    with open(filename, 'r') as f:\n        written_data = json.load(f)\n\n    # Verify the written data\n    if written_data == data:\n        return (True, \"Success\", written_data)\n    else:\n        return (False, \"Verification failed.\", written_data)\nfilename = 'test.json'\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}", "raw_solution": "import json\nimport os\n\ndef task_func(filename, data):\n    # Check if file exists\n    if os.path.exists(filename):\n        return (False, \"File already exists.\")\n\n    # Write dictionary to file\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n\n    # Read back the written data\n    with open(filename, 'r') as f:\n        written_data = json.load(f)\n\n    # Verify the written data\n    if written_data == data:\n        return (True, \"Success\", written_data)\n    else:\n        return (False, \"Verification failed.\", written_data)\n\n# Test the function\nfilename = 'test.json'\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}"}
{"task_id": "BigCodeBench/831", "solution": "import random\nimport math\ndef task_func(range_start=1, range_end=100, pairs_count=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    for _ in range(pairs_count):\n        num1 = random.randint(range_start, range_end)\n        num2 = random.randint(range_start, range_end)\n        yield (num1, num2, math.sqrt(abs(num1 - num2)))", "raw_solution": "import random\nimport math\n\ndef task_func(range_start=1, range_end=100, pairs_count=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    for _ in range(pairs_count):\n        num1 = random.randint(range_start, range_end)\n        num2 = random.randint(range_start, range_end)\n        yield (num1, num2, math.sqrt(abs(num1 - num2)))\n\n# Test the function\npairs = task_func(1, 3, pairs_count=25, random_seed=14)\nfor _ in range(25):\n    print(next(pairs))"}
{"task_id": "BigCodeBench/832", "solution": "import pickle\nimport os\ndef task_func(filename, data):\n    \"\"\"\n    Serializes an object using pickle and overwrites the specified file with this serialized data.\n    Before writing, checks if the directory exists, creating it if necessary.\n    \"\"\"\n    try:\n        # Check if the directory exists, create it if necessary\n        directory = os.path.dirname(filename)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        # Serialize the data\n        serialized_data = pickle.dumps(data)\n\n        # Write the serialized data to the file\n        with open(filename, 'wb') as file:\n            file.write(serialized_data)\n\n        return True\n    except Exception:\n        return False", "raw_solution": "import pickle\nimport os\n\ndef task_func(filename, data):\n    \"\"\"\n    Serializes an object using pickle and overwrites the specified file with this serialized data.\n    Before writing, checks if the directory exists, creating it if necessary.\n    \"\"\"\n    try:\n        # Check if the directory exists, create it if necessary\n        directory = os.path.dirname(filename)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        # Serialize the data\n        serialized_data = pickle.dumps(data)\n\n        # Write the serialized data to the file\n        with open(filename, 'wb') as file:\n            file.write(serialized_data)\n\n        return True\n    except Exception:\n        return False"}
{"task_id": "BigCodeBench/833", "solution": "import random\nfrom collections import Counter\nfrom statistics import mode\ndef task_func(list_length=1000, range_start=1, range_end=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n    random_list = [random.randint(range_start, range_end) for _ in range(list_length)]\n    counter = Counter(random_list)\n    gen = ((num, freq) for num, freq in counter.items())\n    return mode(random_list), gen", "raw_solution": "import random\nfrom collections import Counter\nfrom statistics import mode\n\ndef task_func(list_length=1000, range_start=1, range_end=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n    random_list = [random.randint(range_start, range_end) for _ in range(list_length)]\n    counter = Counter(random_list)\n    gen = ((num, freq) for num, freq in counter.items())\n    return mode(random_list), gen"}
{"task_id": "BigCodeBench/834", "solution": "import binascii\nimport io\nimport gzip\nfrom Crypto.Cipher import AES\nfrom Crypto.Util.Padding import unpad\ndef task_func(compressed_hex, key):\n    try:\n        # Convert hexadecimal string to bytes\n        compressed_bytes = binascii.unhexlify(compressed_hex)\n\n        # Decompress the gzip-compressed bytes\n        decompressed_bytes = gzip.decompress(compressed_bytes)\n\n        # Decrypt the AES-encrypted bytes\n        cipher = AES.new(key, AES.MODE_ECB)\n        decrypted_bytes = unpad(cipher.decrypt(decompressed_bytes), AES.block_size)\n\n        # Convert the decrypted bytes to UTF-8 string\n        decoded_string = decrypted_bytes.decode('utf-8')\n\n    except (binascii.Error, gzip.BadGzipFile, UnicodeDecodeError) as e:\n        decoded_string = f\"Error: {str(e)}\"\n\n    return decoded_string", "raw_solution": "import binascii\nimport io\nimport gzip\nfrom Crypto.Cipher import AES\nfrom Crypto.Util.Padding import unpad\n\ndef task_func(compressed_hex, key):\n    try:\n        # Convert hexadecimal string to bytes\n        compressed_bytes = binascii.unhexlify(compressed_hex)\n\n        # Decompress the gzip-compressed bytes\n        decompressed_bytes = gzip.decompress(compressed_bytes)\n\n        # Decrypt the AES-encrypted bytes\n        cipher = AES.new(key, AES.MODE_ECB)\n        decrypted_bytes = unpad(cipher.decrypt(decompressed_bytes), AES.block_size)\n\n        # Convert the decrypted bytes to UTF-8 string\n        decoded_string = decrypted_bytes.decode('utf-8')\n\n    except (binascii.Error, gzip.BadGzipFile, UnicodeDecodeError) as e:\n        decoded_string = f\"Error: {str(e)}\"\n\n    return decoded_string"}
{"task_id": "BigCodeBench/835", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(n_rows, remove_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    df = pd.DataFrame(np.random.randint(0, 100, size=(n_rows, len(columns))), columns=columns)\n    df = df.drop(df.columns[remove_cols], axis=1)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(n_rows, remove_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    df = pd.DataFrame(np.random.randint(0, 100, size=(n_rows, len(columns))), columns=columns)\n    df = df.drop(df.columns[remove_cols], axis=1)\n    return df\n\n# Test the function\ndf = task_func(3, [1, 3], columns=['test', 'rem1', 'apple', 'remove'], random_seed=12)"}
{"task_id": "BigCodeBench/836", "solution": "import os\nimport shutil\nimport csv\ndef task_func(target_value='332', csv_dir='./csv_files/', processed_dir='./processed_files/', simulate=False):\n    result = {}\n    for filename in os.listdir(csv_dir):\n        if filename.endswith('.csv'):\n            with open(os.path.join(csv_dir, filename), 'r') as file:\n                reader = csv.reader(file)\n                for i, row in enumerate(reader):\n                    if row[0] == target_value:\n                        result[filename] = i\n                        if not simulate:\n                            shutil.move(os.path.join(csv_dir, filename), os.path.join(processed_dir, filename))\n                        break\n    return result", "raw_solution": "import os\nimport shutil\nimport csv\n\ndef task_func(target_value='332', csv_dir='./csv_files/', processed_dir='./processed_files/', simulate=False):\n    result = {}\n    for filename in os.listdir(csv_dir):\n        if filename.endswith('.csv'):\n            with open(os.path.join(csv_dir, filename), 'r') as file:\n                reader = csv.reader(file)\n                for i, row in enumerate(reader):\n                    if row[0] == target_value:\n                        result[filename] = i\n                        if not simulate:\n                            shutil.move(os.path.join(csv_dir, filename), os.path.join(processed_dir, filename))\n                        break\n    return result"}
{"task_id": "BigCodeBench/837", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(n_rows, scale_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    df = pd.DataFrame(np.random.randint(1, 100, size=(n_rows, len(columns))), columns=columns)\n\n    scaler = StandardScaler()\n    df.iloc[:, scale_cols] = scaler.fit_transform(df.iloc[:, scale_cols])\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(n_rows, scale_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    df = pd.DataFrame(np.random.randint(1, 100, size=(n_rows, len(columns))), columns=columns)\n\n    scaler = StandardScaler()\n    df.iloc[:, scale_cols] = scaler.fit_transform(df.iloc[:, scale_cols])\n\n    return df\n\n# Test the function\ndf = task_func(5, [1, 2, 3], random_seed=12)"}
{"task_id": "BigCodeBench/838", "solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom pandas import Series\ndef task_func(text_series: Series) -> Series:\n    \"\"\"\n    Process a pandas Series of text data by lowercasing all letters, removing non-alphanumeric characters (except spaces),\n    removing punctuation, and stemming each word to its root form. Stemming is done using the NLTK's PorterStemmer,\n    which applies a series of rules to find the stem of each word.\n    \"\"\"\n    # Initialize PorterStemmer\n    ps = PorterStemmer()\n\n    # Lowercase all letters\n    text_series = text_series.str.lower()\n\n    # Remove non-alphanumeric characters (except spaces)\n    text_series = text_series.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n\n    # Remove punctuation\n    text_series = text_series.str.translate(str.maketrans('', '', string.punctuation))\n\n    # Stem each word\n    text_series = text_series.apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n\n    return text_series", "raw_solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom pandas import Series\n\ndef task_func(text_series: Series) -> Series:\n    \"\"\"\n    Process a pandas Series of text data by lowercasing all letters, removing non-alphanumeric characters (except spaces),\n    removing punctuation, and stemming each word to its root form. Stemming is done using the NLTK's PorterStemmer,\n    which applies a series of rules to find the stem of each word.\n    \"\"\"\n    # Initialize PorterStemmer\n    ps = PorterStemmer()\n\n    # Lowercase all letters\n    text_series = text_series.str.lower()\n\n    # Remove non-alphanumeric characters (except spaces)\n    text_series = text_series.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n\n    # Remove punctuation\n    text_series = text_series.str.translate(str.maketrans('', '', string.punctuation))\n\n    # Stem each word\n    text_series = text_series.apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n\n    return text_series"}
{"task_id": "BigCodeBench/839", "solution": "import csv\nimport random\nimport string\ndef task_func(file_path,\n              num_rows,\n              gender=['Male', 'Female', 'Non-Binary'],\n              countries=['USA', 'UK', 'Canada', 'Australia', 'India'],\n              seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Name', 'Age', 'Gender', 'Country'])\n\n        if num_rows > 0:\n            for _ in range(num_rows):\n                name = ''.join(random.choices(string.ascii_uppercase, k=5))\n                age = random.randint(20, 60)\n                gender_choice = random.choice(gender)\n                country_choice = random.choice(countries)\n                writer.writerow([name, age, gender_choice, country_choice])\n\n    return file_path", "raw_solution": "import csv\nimport random\nimport string\n\ndef task_func(file_path,\n              num_rows,\n              gender=['Male', 'Female', 'Non-Binary'],\n              countries=['USA', 'UK', 'Canada', 'Australia', 'India'],\n              seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Name', 'Age', 'Gender', 'Country'])\n\n        if num_rows > 0:\n            for _ in range(num_rows):\n                name = ''.join(random.choices(string.ascii_uppercase, k=5))\n                age = random.randint(20, 60)\n                gender_choice = random.choice(gender)\n                country_choice = random.choice(countries)\n                writer.writerow([name, age, gender_choice, country_choice])\n\n    return file_path"}
{"task_id": "BigCodeBench/840", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(file_path, num_rows, data_dimensions=5, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    data = np.random.rand(num_rows, data_dimensions)\n\n    df = pd.DataFrame(data, columns=[f'Feature_{i+1}' for i in range(data_dimensions)])\n    df.to_csv(file_path, index=False)\n\n    return file_path", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(file_path, num_rows, data_dimensions=5, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    data = np.random.rand(num_rows, data_dimensions)\n\n    df = pd.DataFrame(data, columns=[f'Feature_{i+1}' for i in range(data_dimensions)])\n    df.to_csv(file_path, index=False)\n\n    return file_path"}
{"task_id": "BigCodeBench/841", "solution": "import re\nimport json\nfrom collections import defaultdict\nimport string\ndef task_func(json_string):\n    try:\n        data = json.loads(json_string)\n        if \"text\" not in data:\n            return {}\n        text = data[\"text\"]\n        text = text.lower()\n        text = re.sub(r'[^\\w\\s]', '', text)\n        words = text.split()\n        word_counts = defaultdict(int)\n        for word in words:\n            word_counts[word] += 1\n        return word_counts\n    except (json.JSONDecodeError, TypeError):\n        return {}", "raw_solution": "import re\nimport json\nfrom collections import defaultdict\nimport string\n\ndef task_func(json_string):\n    try:\n        data = json.loads(json_string)\n        if \"text\" not in data:\n            return {}\n        text = data[\"text\"]\n        text = text.lower()\n        text = re.sub(r'[^\\w\\s]', '', text)\n        words = text.split()\n        word_counts = defaultdict(int)\n        for word in words:\n            word_counts[word] += 1\n        return word_counts\n    except (json.JSONDecodeError, TypeError):\n        return {}"}
{"task_id": "BigCodeBench/842", "solution": "import sqlite3\nimport random\nfrom typing import List, Optional\ndef task_func(db_path: str,\n              num_entries: int,\n              users: List[str] = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve'],\n              countries: List[str] = ['USA', 'UK', 'Canada', 'Australia', 'India'],\n              random_seed: Optional[int] = None) -> str:\n    \"\"\"\n    Generates an SQLite database with a 'users' table containing random user data.\n    The 'users' table has columns: id (integer), name (string), age (integer), and country (string).\n    The number of entries in the database is determined by num_entries.\n    The function returns the file path of the generated SQLite database.\n    \"\"\"\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    # Create 'users' table\n    c.execute('''\n        CREATE TABLE users\n        (id INTEGER PRIMARY KEY, name TEXT, age INTEGER, country TEXT)\n    ''')\n\n    # Insert random user data into 'users' table\n    for i in range(num_entries):\n        name = random.choice(users)\n        age = random.randint(20, 60)\n        country = random.choice(countries)\n        c.execute(\"INSERT INTO users VALUES (?, ?, ?, ?)\", (i+1, name, age, country))\n\n    conn.commit()\n    conn.close()\n\n    return db_path", "raw_solution": "import sqlite3\nimport random\nfrom typing import List, Optional\n\ndef task_func(db_path: str,\n              num_entries: int,\n              users: List[str] = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve'],\n              countries: List[str] = ['USA', 'UK', 'Canada', 'Australia', 'India'],\n              random_seed: Optional[int] = None) -> str:\n    \"\"\"\n    Generates an SQLite database with a 'users' table containing random user data.\n    The 'users' table has columns: id (integer), name (string), age (integer), and country (string).\n    The number of entries in the database is determined by num_entries.\n    The function returns the file path of the generated SQLite database.\n    \"\"\"\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    # Create 'users' table\n    c.execute('''\n        CREATE TABLE users\n        (id INTEGER PRIMARY KEY, name TEXT, age INTEGER, country TEXT)\n    ''')\n\n    # Insert random user data into 'users' table\n    for i in range(num_entries):\n        name = random.choice(users)\n        age = random.randint(20, 60)\n        country = random.choice(countries)\n        c.execute(\"INSERT INTO users VALUES (?, ?, ?, ?)\", (i+1, name, age, country))\n\n    conn.commit()\n    conn.close()\n\n    return db_path"}
{"task_id": "BigCodeBench/843", "solution": "import random\nimport re\nWORD_LIST = [\"sample\", \"text\", \"contains\", \"several\", \"words\", \"including\"]\ndef task_func(n_sentences):\n    \"\"\"\n    Generates a string of random sentences using a predefined word list.\n    Each sentence is guaranteed to have one period at the end, and no period within the sentence.\n    The generated sentences are concatenated into a single string, with all letters in lowercase\n    and all non-alphanumeric characters except spaces removed.\n    \"\"\"\n    # Generate sentences\n    sentences = []\n    for _ in range(n_sentences):\n        # Randomly select words from the word list\n        words = [random.choice(WORD_LIST) for _ in range(random.randint(1, 10))]\n        # Join words with spaces and add a period at the end\n        sentence = \" \".join(words) + \".\"\n        sentences.append(sentence)\n\n    # Concatenate sentences into a single string\n    text = \" \".join(sentences)\n\n    # Remove non-alphanumeric characters except spaces and periods\n    text = re.sub(r'[^a-z\\s.]', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    return text", "raw_solution": "import random\nimport re\n\n# Constants\nWORD_LIST = [\"sample\", \"text\", \"contains\", \"several\", \"words\", \"including\"]\n\ndef task_func(n_sentences):\n    \"\"\"\n    Generates a string of random sentences using a predefined word list.\n    Each sentence is guaranteed to have one period at the end, and no period within the sentence.\n    The generated sentences are concatenated into a single string, with all letters in lowercase\n    and all non-alphanumeric characters except spaces removed.\n    \"\"\"\n    # Generate sentences\n    sentences = []\n    for _ in range(n_sentences):\n        # Randomly select words from the word list\n        words = [random.choice(WORD_LIST) for _ in range(random.randint(1, 10))]\n        # Join words with spaces and add a period at the end\n        sentence = \" \".join(words) + \".\"\n        sentences.append(sentence)\n\n    # Concatenate sentences into a single string\n    text = \" \".join(sentences)\n\n    # Remove non-alphanumeric characters except spaces and periods\n    text = re.sub(r'[^a-z\\s.]', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    return text"}
{"task_id": "BigCodeBench/844", "solution": "import csv\nimport random\nfrom faker import Faker\ndef task_func(file_path, num_rows, random_seed=None):\n    if not isinstance(num_rows, int) or num_rows < 0:\n        raise ValueError(\"num_rows must be an integer >= 0\")\n\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    fake = Faker()\n\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Name', 'Age', 'Address', 'Email'])\n\n        for _ in range(num_rows):\n            name = fake.name()\n            age = random.randint(20, 60)\n            address = fake.address().replace('\\n', ', ')\n            email = fake.email()\n\n            writer.writerow([name, age, address, email])\n\n    return file_path", "raw_solution": "import csv\nimport random\nfrom faker import Faker\n\ndef task_func(file_path, num_rows, random_seed=None):\n    if not isinstance(num_rows, int) or num_rows < 0:\n        raise ValueError(\"num_rows must be an integer >= 0\")\n\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    fake = Faker()\n\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Name', 'Age', 'Address', 'Email'])\n\n        for _ in range(num_rows):\n            name = fake.name()\n            age = random.randint(20, 60)\n            address = fake.address().replace('\\n', ', ')\n            email = fake.email()\n\n            writer.writerow([name, age, address, email])\n\n    return file_path"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    # Clean the texts\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Calculate term frequencies\n    counter1 = Counter(text1.split())\n    counter2 = Counter(text2.split())\n\n    # Calculate cosine similarity\n    vec1 = np.array(list(counter1.values()))\n    vec2 = np.array(list(counter2.values()))\n    cosine_similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n\n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return (cosine_similarity, levenshtein_ratio)", "raw_solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    # Clean the texts\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Calculate term frequencies\n    counter1 = Counter(text1.split())\n    counter2 = Counter(text2.split())\n\n    # Calculate cosine similarity\n    vec1 = np.array(list(counter1.values()))\n    vec2 = np.array(list(counter2.values()))\n    cosine_similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n\n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return (cosine_similarity, levenshtein_ratio)"}
{"task_id": "BigCodeBench/846", "solution": "import collections\nimport pandas as pd\nclass ExampleObject:\n    def __init__(self, animal, shape):\n        self.animal = animal\n        self.shape = shape\ndef task_func(obj_list, attr):\n    \"\"\"\n    Counts the frequency of each value of the given attribute from a list of objects.\n    Returns a pandas Dataframe containing frequency count of the specified attribute from the objects in the list.\n    \"\"\"\n    if not obj_list:\n        return pd.DataFrame(columns=['attribute', 'count'])\n\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n    counter = collections.Counter(attr_values)\n\n    df = pd.DataFrame(list(counter.items()), columns=['attribute', 'count'])\n    df.set_index('attribute', inplace=True)\n\n    return df\nobj_list = [ExampleObject('tiger', 'Square'), ExampleObject('leopard', 'Circle'), ExampleObject('cat', 'Rectangle'), ExampleObject('elephant', 'Rectangle')]", "raw_solution": "import collections\nimport pandas as pd\n\nclass ExampleObject:\n    def __init__(self, animal, shape):\n        self.animal = animal\n        self.shape = shape\n\ndef task_func(obj_list, attr):\n    \"\"\"\n    Counts the frequency of each value of the given attribute from a list of objects.\n    Returns a pandas Dataframe containing frequency count of the specified attribute from the objects in the list.\n    \"\"\"\n    if not obj_list:\n        return pd.DataFrame(columns=['attribute', 'count'])\n\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n    counter = collections.Counter(attr_values)\n\n    df = pd.DataFrame(list(counter.items()), columns=['attribute', 'count'])\n    df.set_index('attribute', inplace=True)\n\n    return df\n\n# Test the function\nobj_list = [ExampleObject('tiger', 'Square'), ExampleObject('leopard', 'Circle'), ExampleObject('cat', 'Rectangle'), ExampleObject('elephant', 'Rectangle')]\ncount = task_func(obj_list, 'shape')"}
{"task_id": "BigCodeBench/847", "solution": "import re\nimport os\nimport string\nimport random\ndef task_func(input_string, directory='./text_files'):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Split the input string into separate strings\n    lines = input_string.split('\\n')\n\n    file_paths = []\n    for i, line in enumerate(lines):\n        # Remove special characters\n        cleaned_line = re.sub(r'[^A-Za-z0-9 ]+', '', line)\n\n        # Generate a random file name\n        file_name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)) + '.txt'\n        file_path = os.path.join(directory, file_name)\n\n        # Save the cleaned line to a text file\n        with open(file_path, 'w') as f:\n            f.write(cleaned_line)\n\n        file_paths.append(file_path)\n\n    return file_paths", "raw_solution": "import re\nimport os\nimport string\nimport random\n\ndef task_func(input_string, directory='./text_files'):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Split the input string into separate strings\n    lines = input_string.split('\\n')\n\n    file_paths = []\n    for i, line in enumerate(lines):\n        # Remove special characters\n        cleaned_line = re.sub(r'[^A-Za-z0-9 ]+', '', line)\n\n        # Generate a random file name\n        file_name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)) + '.txt'\n        file_path = os.path.join(directory, file_name)\n\n        # Save the cleaned line to a text file\n        with open(file_path, 'w') as f:\n            f.write(cleaned_line)\n\n        file_paths.append(file_path)\n\n    return file_paths"}
{"task_id": "BigCodeBench/848", "solution": "import heapq\nimport random\nclass Object:\n    def __init__(self, value):\n        self.test = value\ndef task_func(obj_list, attr, top_n=5, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    if not hasattr(obj_list[0], attr):\n        return [], None\n\n    values = [getattr(obj, attr) for obj in obj_list]\n    top_values = heapq.nlargest(top_n, values)\n    random_value = random.choice(values)\n\n    return top_values, random_value\nobj_list = [Object(random.randint(1, 12)) for _ in range(13)]", "raw_solution": "import heapq\nimport random\n\nclass Object:\n    def __init__(self, value):\n        self.test = value\n\ndef task_func(obj_list, attr, top_n=5, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    if not hasattr(obj_list[0], attr):\n        return [], None\n\n    values = [getattr(obj, attr) for obj in obj_list]\n    top_values = heapq.nlargest(top_n, values)\n    random_value = random.choice(values)\n\n    return top_values, random_value\n\nrandom.seed(2)\nobj_list = [Object(random.randint(1, 12)) for _ in range(13)]\ntop_values, random_value = task_func(obj_list, 'test', 2, 12)"}
{"task_id": "BigCodeBench/849", "solution": "import re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(input_string):\n    # Split the string into individual lines\n    lines = input_string.split('\\n')\n\n    # Remove stopwords and count the frequency of each word\n    word_count = Counter()\n    for line in lines:\n        # Remove punctuation and convert to lowercase\n        words = re.findall(r'\\b\\w+\\b', line.lower())\n\n        # Remove stopwords\n        words = [word for word in words if word not in STOPWORDS]\n\n        # Count the frequency of each word\n        word_count.update(words)\n\n    return dict(word_count)", "raw_solution": "import re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(input_string):\n    # Split the string into individual lines\n    lines = input_string.split('\\n')\n\n    # Remove stopwords and count the frequency of each word\n    word_count = Counter()\n    for line in lines:\n        # Remove punctuation and convert to lowercase\n        words = re.findall(r'\\b\\w+\\b', line.lower())\n\n        # Remove stopwords\n        words = [word for word in words if word not in STOPWORDS]\n\n        # Count the frequency of each word\n        word_count.update(words)\n\n    return dict(word_count)"}
{"task_id": "BigCodeBench/850", "solution": "import pandas as pd\nimport random\ndef task_func(students, subjects, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    data = {'Student': []}\n    for subject in subjects:\n        data[subject] = []\n\n    for student in range(1, students + 1):\n        data['Student'].append(f'Student_{student}')\n        for subject in subjects:\n            data[subject].append(random.randint(0, 100))\n\n    df = pd.DataFrame(data)\n    df['Average Grade'] = df.drop('Student', axis=1).mean(axis=1)\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(students, subjects, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    data = {'Student': []}\n    for subject in subjects:\n        data[subject] = []\n\n    for student in range(1, students + 1):\n        data['Student'].append(f'Student_{student}')\n        for subject in subjects:\n            data[subject].append(random.randint(0, 100))\n\n    df = pd.DataFrame(data)\n    df['Average Grade'] = df.drop('Student', axis=1).mean(axis=1)\n\n    return df"}
{"task_id": "BigCodeBench/851", "solution": "import textwrap\nimport re\ndef task_func(input_string, width):\n    # Split the input string into separate lines\n    lines = input_string.split('\\n')\n\n    # Wrap each line to the specified width\n    wrapped_lines = [textwrap.fill(line, width) for line in lines]\n\n    # Join the wrapped lines back into a single string\n    output_string = '\\n'.join(wrapped_lines)\n\n    return output_string\ninput_string = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec a diam lectus. Sed sit amet ipsum mauris. Maecenas congue ligula ac quam viverra nec consectetur ante hendrerit. Donec et mollis dolor. Praesent et diam eget libero egestas mattis sit amet vitae augue. Nam tincidunt congue enim, ut porta lorem lacinia consectetur.\"\nwidth = 20", "raw_solution": "import textwrap\nimport re\n\ndef task_func(input_string, width):\n    # Split the input string into separate lines\n    lines = input_string.split('\\n')\n\n    # Wrap each line to the specified width\n    wrapped_lines = [textwrap.fill(line, width) for line in lines]\n\n    # Join the wrapped lines back into a single string\n    output_string = '\\n'.join(wrapped_lines)\n\n    return output_string\n\n# Test the function\ninput_string = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec a diam lectus. Sed sit amet ipsum mauris. Maecenas congue ligula ac quam viverra nec consectetur ante hendrerit. Donec et mollis dolor. Praesent et diam eget libero egestas mattis sit amet vitae augue. Nam tincidunt congue enim, ut porta lorem lacinia consectetur.\"\nwidth = 20"}
{"task_id": "BigCodeBench/852", "solution": "import random\nimport string\ndef task_func(max_length, n_samples, seed=None):\n    if max_length < 1:\n        raise ValueError(\"max_length must be greater than 0\")\n\n    random.seed(seed)\n\n    result = []\n    for _ in range(n_samples):\n        string_length = random.randint(1, max_length)\n        random_string = ''.join(random.choices(string.ascii_lowercase, k=string_length))\n        result.append(random_string)\n\n    return result", "raw_solution": "import random\nimport string\n\ndef task_func(max_length, n_samples, seed=None):\n    if max_length < 1:\n        raise ValueError(\"max_length must be greater than 0\")\n\n    random.seed(seed)\n\n    result = []\n    for _ in range(n_samples):\n        string_length = random.randint(1, max_length)\n        random_string = ''.join(random.choices(string.ascii_lowercase, k=string_length))\n        result.append(random_string)\n\n    return result"}
{"task_id": "BigCodeBench/853", "solution": "import os\nimport shutil\nimport string\nINVALID_CHARACTERS = string.punctuation + string.whitespace\ndef task_func(directory_path):\n    \"\"\"\n    Organizes files in a directory according to their endings.\n    Files with invalid characters in the name are moved to a new directory called \"Invalid.\"\n    \"\"\"\n    # Create the \"Invalid\" directory if it doesn't exist\n    invalid_dir = os.path.join(directory_path, \"Invalid\")\n    if not os.path.exists(invalid_dir):\n        os.makedirs(invalid_dir)\n\n    # Initialize summary dictionary\n    summary = {}\n\n    # Scan the directory\n    for filename in os.listdir(directory_path):\n        # Check if the file name contains invalid characters\n        if any(char in filename for char in INVALID_CHARACTERS):\n            # Move the file to the \"Invalid\" directory\n            shutil.move(os.path.join(directory_path, filename), invalid_dir)\n            # Update the summary\n            if \"Invalid\" in summary:\n                summary[\"Invalid\"] += 1\n            else:\n                summary[\"Invalid\"] = 1\n        else:\n            # Update the summary\n            ending = os.path.splitext(filename)[1]\n            if ending in summary:\n                summary[ending] += 1\n            else:\n                summary[ending] = 1\n\n    return summary", "raw_solution": "import os\nimport shutil\nimport string\n\n# Constants\nINVALID_CHARACTERS = string.punctuation + string.whitespace\n\ndef task_func(directory_path):\n    \"\"\"\n    Organizes files in a directory according to their endings.\n    Files with invalid characters in the name are moved to a new directory called \"Invalid.\"\n    \"\"\"\n    # Create the \"Invalid\" directory if it doesn't exist\n    invalid_dir = os.path.join(directory_path, \"Invalid\")\n    if not os.path.exists(invalid_dir):\n        os.makedirs(invalid_dir)\n\n    # Initialize summary dictionary\n    summary = {}\n\n    # Scan the directory\n    for filename in os.listdir(directory_path):\n        # Check if the file name contains invalid characters\n        if any(char in filename for char in INVALID_CHARACTERS):\n            # Move the file to the \"Invalid\" directory\n            shutil.move(os.path.join(directory_path, filename), invalid_dir)\n            # Update the summary\n            if \"Invalid\" in summary:\n                summary[\"Invalid\"] += 1\n            else:\n                summary[\"Invalid\"] = 1\n        else:\n            # Update the summary\n            ending = os.path.splitext(filename)[1]\n            if ending in summary:\n                summary[ending] += 1\n            else:\n                summary[ending] = 1\n\n    return summary"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input should be a list of integers.\")\n    if not all(isinstance(i, int) for i in numbers):\n        raise TypeError(\"Input should be a list of integers.\")\n    if any(i < 0 for i in numbers):\n        raise ValueError(\"Input should not contain negative numbers.\")\n\n    fac = [math.factorial(i) for i in numbers]\n    perm = [i for i in permutations(numbers, len(numbers))]\n\n    return fac, perm", "raw_solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input should be a list of integers.\")\n    if not all(isinstance(i, int) for i in numbers):\n        raise TypeError(\"Input should be a list of integers.\")\n    if any(i < 0 for i in numbers):\n        raise ValueError(\"Input should not contain negative numbers.\")\n\n    fac = [math.factorial(i) for i in numbers]\n    perm = [i for i in permutations(numbers, len(numbers))]\n\n    return fac, perm"}
{"task_id": "BigCodeBench/855", "solution": "import random\nimport string\nimport collections\nVALID_CHARACTERS = string.ascii_letters + string.digits\ndef task_func(n_strings, string_length):\n    # Generate n random strings of specified length\n    random_strings = [''.join(random.choices(VALID_CHARACTERS, k=string_length)) for _ in range(n_strings)]\n\n    # Flatten the list of strings and count the frequency of each character\n    char_counts = collections.Counter(''.join(random_strings))\n\n    return dict(char_counts)", "raw_solution": "import random\nimport string\nimport collections\n\n# Constants\nVALID_CHARACTERS = string.ascii_letters + string.digits\n\ndef task_func(n_strings, string_length):\n    # Generate n random strings of specified length\n    random_strings = [''.join(random.choices(VALID_CHARACTERS, k=string_length)) for _ in range(n_strings)]\n\n    # Flatten the list of strings and count the frequency of each character\n    char_counts = collections.Counter(''.join(random_strings))\n\n    return dict(char_counts)\n\n# Test the function"}
{"task_id": "BigCodeBench/856", "solution": "from functools import reduce\nfrom itertools import combinations\nimport numpy as np\ndef task_func(shape=(3, 3), low=1, high=10, seed=None):\n    if high <= low:\n        raise ValueError(\"high should be greater than low\")\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    matrix = np.random.randint(low, high, shape)\n\n    pairs = list(combinations(matrix.flatten(), 2))\n    product_sum = sum(reduce(lambda x, y: x*y, pair) for pair in pairs)\n\n    return product_sum, matrix", "raw_solution": "from functools import reduce\nfrom itertools import combinations\nimport numpy as np\n\ndef task_func(shape=(3, 3), low=1, high=10, seed=None):\n    if high <= low:\n        raise ValueError(\"high should be greater than low\")\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    matrix = np.random.randint(low, high, shape)\n\n    pairs = list(combinations(matrix.flatten(), 2))\n    product_sum = sum(reduce(lambda x, y: x*y, pair) for pair in pairs)\n\n    return product_sum, matrix"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfers files from one directory to another based on the specified file extensions.\n    \"\"\"\n    transferred_files = []\n    for extension in EXTENSIONS:\n        source_files = glob.glob(os.path.join(SOURCE_DIR, f'*.{extension}'))\n        for file in source_files:\n            try:\n                shutil.move(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Failed to transfer file {file}: {str(e)}\")\n    return transferred_files", "raw_solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfers files from one directory to another based on the specified file extensions.\n    \"\"\"\n    transferred_files = []\n    for extension in EXTENSIONS:\n        source_files = glob.glob(os.path.join(SOURCE_DIR, f'*.{extension}'))\n        for file in source_files:\n            try:\n                shutil.move(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Failed to transfer file {file}: {str(e)}\")\n    return transferred_files"}
{"task_id": "BigCodeBench/858", "solution": "import string\nimport random\nfrom collections import Counter\ndef task_func(n, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    letters = [random.choice(string.ascii_lowercase) for _ in range(n)]\n    return Counter(letters)", "raw_solution": "import string\nimport random\nfrom collections import Counter\n\ndef task_func(n, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    letters = [random.choice(string.ascii_lowercase) for _ in range(n)]\n    return Counter(letters)\n\n# Test the function"}
{"task_id": "BigCodeBench/859", "solution": "import warnings\nimport sklearn.model_selection as model_selection\nimport sklearn.svm as svm\nimport sklearn.datasets as datasets\nimport sklearn.metrics as metrics\ndef task_func():\n    # Load the iris dataset\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n\n    # Create a SVM classifier\n    clf = svm.SVC(gamma='scale')\n\n    # Train the classifier\n    clf.fit(X_train, y_train)\n\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test)\n\n    # Calculate the accuracy\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n\n    # Warn if the accuracy is less than 0.9\n    if accuracy < 0.9:\n        warnings.warn(f'Accuracy is less than 0.9: {accuracy}', UserWarning, stacklevel=2)\n        warning_msg = f'Accuracy is less than 0.9: {accuracy}'\n    else:\n        warning_msg = None\n\n    return accuracy, warning_msg", "raw_solution": "import warnings\nimport sklearn.model_selection as model_selection\nimport sklearn.svm as svm\nimport sklearn.datasets as datasets\nimport sklearn.metrics as metrics\n\ndef task_func():\n    # Load the iris dataset\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n\n    # Create a SVM classifier\n    clf = svm.SVC(gamma='scale')\n\n    # Train the classifier\n    clf.fit(X_train, y_train)\n\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test)\n\n    # Calculate the accuracy\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n\n    # Warn if the accuracy is less than 0.9\n    if accuracy < 0.9:\n        warnings.warn(f'Accuracy is less than 0.9: {accuracy}', UserWarning, stacklevel=2)\n        warning_msg = f'Accuracy is less than 0.9: {accuracy}'\n    else:\n        warning_msg = None\n\n    return accuracy, warning_msg"}
{"task_id": "BigCodeBench/860", "solution": "import re\nimport random\nimport string\ndef task_func(n, pattern, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=n))\n    matches = re.findall(pattern, random_string)\n    return matches", "raw_solution": "import re\nimport random\nimport string\n\ndef task_func(n, pattern, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=n))\n    matches = re.findall(pattern, random_string)\n    return matches\n\n# Test the function"}
{"task_id": "BigCodeBench/861", "solution": "from collections import Counter\nfrom random import choice, seed\nPOSSIBLE_ITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\ndef task_func(list_of_lists):\n    baskets = []\n    for sublist in list_of_lists:\n        basket = Counter()\n        for _ in range(len(sublist)):\n            item = choice(POSSIBLE_ITEMS)\n            basket[item] += 1\n        baskets.append(basket)\n    return baskets\nlist_of_lists = [[], ['apple', 'banana'], ['cherry', 'date', 'elderberry'], ['apple', 'apple', 'apple']]", "raw_solution": "from collections import Counter\nfrom random import choice, seed\n\n# Constants\nPOSSIBLE_ITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef task_func(list_of_lists):\n    baskets = []\n    for sublist in list_of_lists:\n        basket = Counter()\n        for _ in range(len(sublist)):\n            item = choice(POSSIBLE_ITEMS)\n            basket[item] += 1\n        baskets.append(basket)\n    return baskets\n\n# Test the function\nlist_of_lists = [[], ['apple', 'banana'], ['cherry', 'date', 'elderberry'], ['apple', 'apple', 'apple']]\nbaskets = task_func(list_of_lists)\nfor i, basket in enumerate(baskets):\n    print(f\"Basket {i+1}: {basket}\")"}
{"task_id": "BigCodeBench/862", "solution": "import random\nimport string\nfrom collections import defaultdict\ndef task_func(n, seed=None):\n    random.seed(seed)\n    letters = list(string.ascii_lowercase)\n    result = defaultdict(list)\n\n    for _ in range(n):\n        letter = random.choice(letters)\n        result[letter].append(letter)\n\n    return result", "raw_solution": "import random\nimport string\nfrom collections import defaultdict\n\ndef task_func(n, seed=None):\n    random.seed(seed)\n    letters = list(string.ascii_lowercase)\n    result = defaultdict(list)\n\n    for _ in range(n):\n        letter = random.choice(letters)\n        result[letter].append(letter)\n\n    return result"}
{"task_id": "BigCodeBench/863", "solution": "import numpy as np\nimport math\nPOSSIBLE_NUMBERS = np.arange(1, 11)\ndef task_func(list_of_lists):\n    sums = []\n    for lst in list_of_lists:\n        if len(lst) <= len(POSSIBLE_NUMBERS):\n            selected_numbers = POSSIBLE_NUMBERS[:len(lst)]\n            sum_of_squares = sum(num**2 for num in selected_numbers)\n            sums.append(sum_of_squares)\n    return sums", "raw_solution": "import numpy as np\nimport math\n\n# Constants\nPOSSIBLE_NUMBERS = np.arange(1, 11)\n\ndef task_func(list_of_lists):\n    sums = []\n    for lst in list_of_lists:\n        if len(lst) <= len(POSSIBLE_NUMBERS):\n            selected_numbers = POSSIBLE_NUMBERS[:len(lst)]\n            sum_of_squares = sum(num**2 for num in selected_numbers)\n            sums.append(sum_of_squares)\n    return sums\n\n# Test cases\nassert task_func([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) == [14, 72, 162]\nassert task_func([[10, 9, 8], [7, 6, 5], [4, 3, 2]]) == [262, 171, 55]\nassert task_func([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [1, 2, 3, 4, 5]]) == [110, 385, 110]"}
{"task_id": "BigCodeBench/864", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(fruit_data):\n    if not fruit_data:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(fruit_data, columns=['Fruit', 'Count'])\n    df = df.groupby('Fruit').agg({'Count': ['sum', 'mean']})\n    df.columns = ['Total Count', 'Average Count']\n    df.reset_index(inplace=True)\n\n    return df\nfruit = [('apple', 1), ('orange', 25), ('apple', 111)]", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(fruit_data):\n    if not fruit_data:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(fruit_data, columns=['Fruit', 'Count'])\n    df = df.groupby('Fruit').agg({'Count': ['sum', 'mean']})\n    df.columns = ['Total Count', 'Average Count']\n    df.reset_index(inplace=True)\n\n    return df\n\nfruit = [('apple', 1), ('orange', 25), ('apple', 111)]\ndf = task_func(fruit)\ndf.sort_index(inplace=True)"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Convert the input data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n\n    # Normalize the counts using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n\n    # Normalize the weights using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    # Return the DataFrame with the normalized counts and weights\n    return df[['Item', 'Normalized Count', 'Normalized Weight']]", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Convert the input data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n\n    # Normalize the counts using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n\n    # Normalize the weights using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    # Return the DataFrame with the normalized counts and weights\n    return df[['Item', 'Normalized Count', 'Normalized Weight']]"}
{"task_id": "BigCodeBench/866", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters=2, random_state=0):\n    # Extract the 2D coordinates from the data\n    coordinates = [point[1:] for point in data]\n    coordinates = np.array(coordinates)\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(coordinates)\n\n    # Return the cluster labels\n    return kmeans.labels_\ndata = [('T1', 1, 1), ('T2', 1, 1.1), ('T2', 1.1, 1), ('C1', 400, 400), ('C2', 401, 401), ('B1', 35, 35)]", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef task_func(data, n_clusters=2, random_state=0):\n    # Extract the 2D coordinates from the data\n    coordinates = [point[1:] for point in data]\n    coordinates = np.array(coordinates)\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(coordinates)\n\n    # Return the cluster labels\n    return kmeans.labels_\n\n# Test the function\ndata = [('T1', 1, 1), ('T2', 1, 1.1), ('T2', 1.1, 1), ('C1', 400, 400), ('C2', 401, 401), ('B1', 35, 35)]\nlabels = task_func(data, n_clusters=3, random_state=42)"}
{"task_id": "BigCodeBench/867", "solution": "import re\nimport string\ndef task_func(text1, text2):\n    \"\"\"\n    This function takes two strings, removes any ASCII punctuation using regular expressions, and returns the cleaned strings as a tuple.\n    \"\"\"\n    # Define the set of punctuation characters\n    punctuation = set(string.punctuation)\n\n    # Remove punctuation from the first string\n    cleaned_text1 = ''.join(ch for ch in text1 if ch not in punctuation)\n\n    # Remove punctuation from the second string\n    cleaned_text2 = ''.join(ch for ch in text2 if ch not in punctuation)\n\n    # Return the cleaned strings as a tuple\n    return (cleaned_text1, cleaned_text2)", "raw_solution": "import re\nimport string\n\ndef task_func(text1, text2):\n    \"\"\"\n    This function takes two strings, removes any ASCII punctuation using regular expressions, and returns the cleaned strings as a tuple.\n    \"\"\"\n    # Define the set of punctuation characters\n    punctuation = set(string.punctuation)\n\n    # Remove punctuation from the first string\n    cleaned_text1 = ''.join(ch for ch in text1 if ch not in punctuation)\n\n    # Remove punctuation from the second string\n    cleaned_text2 = ''.join(ch for ch in text2 if ch not in punctuation)\n\n    # Return the cleaned strings as a tuple\n    return (cleaned_text1, cleaned_text2)\n\n# Test the function\ncleaned_text1, cleaned_text2 = task_func(\"test (with parenthesis []!!)\", \"And, other; stuff ^_`\")\n\n# Print the cleaned strings"}
{"task_id": "BigCodeBench/868", "solution": "from itertools import cycle\nfrom random import choice, seed\ndef task_func(n_colors, colors=['Red', 'Green', 'Blue', 'Yellow', 'Purple'], rng_seed=None):\n    if n_colors <= 0:\n        return []\n\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    color_pattern = []\n    color_cycle = cycle(colors)\n\n    for i in range(n_colors):\n        if i % 2 == 0:\n            color_pattern.append(next(color_cycle))\n        else:\n            color_pattern.append(choice(colors))\n\n    return color_pattern\ncolors = ['Brown', 'Green', 'Black']", "raw_solution": "from itertools import cycle\nfrom random import choice, seed\n\ndef task_func(n_colors, colors=['Red', 'Green', 'Blue', 'Yellow', 'Purple'], rng_seed=None):\n    if n_colors <= 0:\n        return []\n\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    color_pattern = []\n    color_cycle = cycle(colors)\n\n    for i in range(n_colors):\n        if i % 2 == 0:\n            color_pattern.append(next(color_cycle))\n        else:\n            color_pattern.append(choice(colors))\n\n    return color_pattern\n\n# Test the function\ncolors = ['Brown', 'Green', 'Black']\ncolor_pattern = task_func(12, colors=colors, rng_seed=42)"}
{"task_id": "BigCodeBench/869", "solution": "import pandas as pd\nfrom itertools import cycle\nfrom random import randint, seed\ndef task_func(\n    n_grades,\n    students=['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    grade_range=range(1, 11),\n    rng_seed=None\n):\n    if not students:\n        raise ValueError(\"List of students cannot be empty\")\n\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    student_cycle = cycle(students)\n    grades = [randint(min(grade_range), max(grade_range)) for _ in range(n_grades)]\n\n    data = {'Student': [next(student_cycle) for _ in range(n_grades)], 'Grade': grades}\n    grade_report = pd.DataFrame(data)\n\n    return grade_report", "raw_solution": "import pandas as pd\nfrom itertools import cycle\nfrom random import randint, seed\n\ndef task_func(\n    n_grades,\n    students=['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    grade_range=range(1, 11),\n    rng_seed=None\n):\n    if not students:\n        raise ValueError(\"List of students cannot be empty\")\n\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    student_cycle = cycle(students)\n    grades = [randint(min(grade_range), max(grade_range)) for _ in range(n_grades)]\n\n    data = {'Student': [next(student_cycle) for _ in range(n_grades)], 'Grade': grades}\n    grade_report = pd.DataFrame(data)\n\n    return grade_report\n\n# Test the function\ngrade_report = task_func(5, rng_seed=12)"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Filter out non-numeric values\n    filtered_data = [tuple(filter(lambda x: isinstance(x, (int, float)), t)) for t in data_list]\n    \n    # Calculate means\n    means = [np.mean(list(iter(t))) for t in zip(*filtered_data)]\n    \n    # Create DataFrame\n    df = pd.DataFrame(means, columns=['Mean Value'])\n    df.index = ['Position ' + str(i) for i in range(len(means))]\n    \n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Filter out non-numeric values\n    filtered_data = [tuple(filter(lambda x: isinstance(x, (int, float)), t)) for t in data_list]\n    \n    # Calculate means\n    means = [np.mean(list(iter(t))) for t in zip(*filtered_data)]\n    \n    # Create DataFrame\n    df = pd.DataFrame(means, columns=['Mean Value'])\n    df.index = ['Position ' + str(i) for i in range(len(means))]\n    \n    return df\n\n# Test the function\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)"}
{"task_id": "BigCodeBench/871", "solution": "import numpy as np\nimport itertools\ndef task_func(data_list, file_name):\n    means = []\n    with open(file_name, 'w') as f:\n        for i, data in enumerate(data_list, start=1):\n            numeric_data = [x for x in data[1:] if isinstance(x, (int, float))]\n            if not numeric_data:\n                mean = np.nan\n            else:\n                mean = np.mean(numeric_data)\n            means.append(mean)\n            f.write(f\"Position {i}: {mean}\\n\")\n    return means", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(data_list, file_name):\n    means = []\n    with open(file_name, 'w') as f:\n        for i, data in enumerate(data_list, start=1):\n            numeric_data = [x for x in data[1:] if isinstance(x, (int, float))]\n            if not numeric_data:\n                mean = np.nan\n            else:\n                mean = np.mean(numeric_data)\n            means.append(mean)\n            f.write(f\"Position {i}: {mean}\\n\")\n    return means"}
{"task_id": "BigCodeBench/872", "solution": "import numpy as np\nimport itertools\ndef task_func(data_list):\n    if not data_list:\n        return []\n\n    # Unzip the list of tuples\n    unzipped = list(itertools.zip_longest(*data_list, fillvalue=0))\n\n    # Convert all values to float, non-numeric values are set to np.nan\n    for i in range(len(unzipped)):\n        for j in range(len(unzipped[i])):\n            try:\n                unzipped[i][j] = float(unzipped[i][j])\n            except ValueError:\n                unzipped[i][j] = np.nan\n\n    # Calculate the mean of numeric values at each position\n    means = [np.nanmean(col) for col in unzipped]\n\n    return means", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(data_list):\n    if not data_list:\n        return []\n\n    # Unzip the list of tuples\n    unzipped = list(itertools.zip_longest(*data_list, fillvalue=0))\n\n    # Convert all values to float, non-numeric values are set to np.nan\n    for i in range(len(unzipped)):\n        for j in range(len(unzipped[i])):\n            try:\n                unzipped[i][j] = float(unzipped[i][j])\n            except ValueError:\n                unzipped[i][j] = np.nan\n\n    # Calculate the mean of numeric values at each position\n    means = [np.nanmean(col) for col in unzipped]\n\n    return means"}
{"task_id": "BigCodeBench/873", "solution": "import csv\nimport os\ndef task_func(data, file_path, headers):\n    \"\"\"\n    Writes a list of tuples to a CSV file. Each tuple in the 'data' list represents a row in the CSV file, with each element of the tuple corresponding to a cell in the row. If a tuple contains fewer elements than there are headers, the missing elements are filled with None.\n    \"\"\"\n    if file_path is None:\n        raise ValueError(\"'file_path' cannot be None\")\n\n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(headers)\n        for row in data:\n            # If a tuple contains fewer elements than there are headers, fill the missing elements with None\n            row = list(row) + [None] * (len(headers) - len(row))\n            writer.writerow(row)\n\n    return os.path.abspath(file_path)", "raw_solution": "import csv\nimport os\n\ndef task_func(data, file_path, headers):\n    \"\"\"\n    Writes a list of tuples to a CSV file. Each tuple in the 'data' list represents a row in the CSV file, with each element of the tuple corresponding to a cell in the row. If a tuple contains fewer elements than there are headers, the missing elements are filled with None.\n    \"\"\"\n    if file_path is None:\n        raise ValueError(\"'file_path' cannot be None\")\n\n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(headers)\n        for row in data:\n            # If a tuple contains fewer elements than there are headers, fill the missing elements with None\n            row = list(row) + [None] * (len(headers) - len(row))\n            writer.writerow(row)\n\n    return os.path.abspath(file_path)"}
{"task_id": "BigCodeBench/874", "solution": "from itertools import tee, islice\nfrom math import sqrt\ndef pairwise(iterable):\n    a, b = tee(iterable)\n    _ = next(b, None)\n    return zip(a, b)\ndef task_func(points):\n    if len(points) < 2:\n        return []\n\n    def point_to_coords(point):\n        if len(point) == 1:\n            return (point[0], point[0])\n        elif len(point) == 2:\n            return point\n        else:\n            raise ValueError(\"Invalid point\")\n\n    points = [point_to_coords(point) for point in points]\n\n    distances = []\n    for (x1, y1), (x2, y2) in pairwise(points):\n        dist = sqrt((x2 - x1)**2 + (y2 - y1)**2)\n        distances.append(dist)\n\n    return distances", "raw_solution": "from itertools import tee, islice\nfrom math import sqrt\n\ndef pairwise(iterable):\n    a, b = tee(iterable)\n    _ = next(b, None)\n    return zip(a, b)\n\ndef task_func(points):\n    if len(points) < 2:\n        return []\n\n    def point_to_coords(point):\n        if len(point) == 1:\n            return (point[0], point[0])\n        elif len(point) == 2:\n            return point\n        else:\n            raise ValueError(\"Invalid point\")\n\n    points = [point_to_coords(point) for point in points]\n\n    distances = []\n    for (x1, y1), (x2, y2) in pairwise(points):\n        dist = sqrt((x2 - x1)**2 + (y2 - y1)**2)\n        distances.append(dist)\n\n    return distances\n\n# Test cases\nassert task_func([(1, 2), (4), (-1.2, 4)]) == [3.605551275463989, 5.2]\nassert task_func([(1, 1), (2, 2), (3, 3)]) == [1.4142135623730951, 1.4142135623730951]\nassert task_func([(0, 0), (1, 1), (2, 2)]) == [1.4142135623730951, 1.4142135623730951]\nassert task_func([(0, 0)]) == []\nassert task_func([]) == []"}
{"task_id": "BigCodeBench/875", "solution": "import pandas as pd\nimport random\ndef task_func(data, columns=['Name', 'Age', 'Occupation'], fill_missing=False, num_range=(0, 100), seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    max_len = max(len(row) for row in data)\n    data = [row + (None,) * (max_len - len(row)) for row in data]\n\n    if fill_missing:\n        data = [row if row[1] is not None else (row[0], random.randint(num_range[0], num_range[1])) for row in data]\n\n    df = pd.DataFrame(data, columns=columns)\n    return df\ndata = [('Mango', 20), ('Apple', ), ('Banana', )]", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(data, columns=['Name', 'Age', 'Occupation'], fill_missing=False, num_range=(0, 100), seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    max_len = max(len(row) for row in data)\n    data = [row + (None,) * (max_len - len(row)) for row in data]\n\n    if fill_missing:\n        data = [row if row[1] is not None else (row[0], random.randint(num_range[0], num_range[1])) for row in data]\n\n    df = pd.DataFrame(data, columns=columns)\n    return df\n\ndata = [('Mango', 20), ('Apple', ), ('Banana', )]\ndf = task_func(data, columns=['Fruit', 'Quantity'], fill_missing=False, seed=42)"}
{"task_id": "BigCodeBench/876", "solution": "import collections\nimport operator\nimport os\nimport shutil\ndef task_func(data_dict, source_directory, backup_directory):\n    # Task 1: Update the dictionary\n    data_dict['a'] = 1\n\n    # Task 2: Sort the dictionary by frequency of values\n    value_frequencies = collections.Counter(data_dict.values()).most_common()\n    value_frequencies.sort(key=operator.itemgetter(1), reverse=True)\n\n    # Task 3: Backup files from source directory to backup directory\n    backup_status = False\n    if os.path.exists(source_directory):\n        if not os.path.exists(backup_directory):\n            os.makedirs(backup_directory)\n        for file_name in os.listdir(source_directory):\n            source_file = os.path.join(source_directory, file_name)\n            backup_file = os.path.join(backup_directory, file_name)\n            shutil.copy2(source_file, backup_file)\n        backup_status = True\n\n    return data_dict, value_frequencies, backup_status", "raw_solution": "import collections\nimport operator\nimport os\nimport shutil\n\ndef task_func(data_dict, source_directory, backup_directory):\n    # Task 1: Update the dictionary\n    data_dict['a'] = 1\n\n    # Task 2: Sort the dictionary by frequency of values\n    value_frequencies = collections.Counter(data_dict.values()).most_common()\n    value_frequencies.sort(key=operator.itemgetter(1), reverse=True)\n\n    # Task 3: Backup files from source directory to backup directory\n    backup_status = False\n    if os.path.exists(source_directory):\n        if not os.path.exists(backup_directory):\n            os.makedirs(backup_directory)\n        for file_name in os.listdir(source_directory):\n            source_file = os.path.join(source_directory, file_name)\n            backup_file = os.path.join(backup_directory, file_name)\n            shutil.copy2(source_file, backup_file)\n        backup_status = True\n\n    return data_dict, value_frequencies, backup_status"}
{"task_id": "BigCodeBench/877", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data is not a DataFrame\")\n    if data.empty:\n        raise ValueError(\"Input data is empty\")\n    if any(data.dtypes != 'float64'):\n        raise ValueError(\"Input data contains non-numeric data\")\n    if n_components > data.shape[1]:\n        raise ValueError(\"n_components is greater than the number of columns in the data\")\n\n    scaler = StandardScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    pca = PCA(n_components=n_components)\n    pca_result = pca.fit_transform(data_scaled)\n\n    return pd.DataFrame(pca_result)", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data is not a DataFrame\")\n    if data.empty:\n        raise ValueError(\"Input data is empty\")\n    if any(data.dtypes != 'float64'):\n        raise ValueError(\"Input data contains non-numeric data\")\n    if n_components > data.shape[1]:\n        raise ValueError(\"n_components is greater than the number of columns in the data\")\n\n    scaler = StandardScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    pca = PCA(n_components=n_components)\n    pca_result = pca.fit_transform(data_scaled)\n\n    return pd.DataFrame(pca_result)"}
{"task_id": "BigCodeBench/878", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\ndef task_func(data, target, test_size=0.2, random_state=None):\n    \"\"\"\n    Trains a RandomForestRegressor model and returns the mean squared error (MSE) of the predictions and the model.\n    \"\"\"\n    # Convert the input data into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the DataFrame is empty or if the target column is not in the DataFrame\n    if df.empty or target not in df.columns:\n        raise ValueError(\"The input DataFrame is empty or the target column name is not in the DataFrame.\")\n\n    # Split the DataFrame into a train and test set\n    X = df.drop(columns=[target])\n    y = df[target]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Train a RandomForestRegressor on the data\n    model = RandomForestRegressor(random_state=random_state)\n    model.fit(X_train, y_train)\n\n    # Calculate the MSE on the test set\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n\n    return mse, model, df", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndef task_func(data, target, test_size=0.2, random_state=None):\n    \"\"\"\n    Trains a RandomForestRegressor model and returns the mean squared error (MSE) of the predictions and the model.\n    \"\"\"\n    # Convert the input data into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the DataFrame is empty or if the target column is not in the DataFrame\n    if df.empty or target not in df.columns:\n        raise ValueError(\"The input DataFrame is empty or the target column name is not in the DataFrame.\")\n\n    # Split the DataFrame into a train and test set\n    X = df.drop(columns=[target])\n    y = df[target]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Train a RandomForestRegressor on the data\n    model = RandomForestRegressor(random_state=random_state)\n    model.fit(X_train, y_train)\n\n    # Calculate the MSE on the test set\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n\n    return mse, model, df"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    if data.empty:\n        raise ValueError(\"'data' is empty\")\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"'col1' or 'col2' are not in 'data'\")\n    if data[col1].nunique() == 1 or data[col2].nunique() == 1:\n        raise ValueError(\"One or both of the columns do not have multiple categories\")\n    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\n        raise ValueError(\"Some categories have less than 5 observations\")\n    if not np.issubdtype(data[col1].dtype, np.object_) or not np.issubdtype(data[col2].dtype, np.object_):\n        raise TypeError(\"One or both of the columns contain non-categorical data\")\n\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    if data.empty:\n        raise ValueError(\"'data' is empty\")\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"'col1' or 'col2' are not in 'data'\")\n    if data[col1].nunique() == 1 or data[col2].nunique() == 1:\n        raise ValueError(\"One or both of the columns do not have multiple categories\")\n    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\n        raise ValueError(\"Some categories have less than 5 observations\")\n    if not np.issubdtype(data[col1].dtype, np.object_) or not np.issubdtype(data[col2].dtype, np.object_):\n        raise TypeError(\"One or both of the columns contain non-categorical data\")\n\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p"}
{"task_id": "BigCodeBench/880", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters=3, seed=None):\n    if not all(data.dtypes == 'int64'):\n        raise ValueError(\"DataFrame contains non-numeric entries\")\n\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=seed)\n    labels = kmeans.fit_predict(data)\n\n    return labels, kmeans", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\n\ndef task_func(data, n_clusters=3, seed=None):\n    if not all(data.dtypes == 'int64'):\n        raise ValueError(\"DataFrame contains non-numeric entries\")\n\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=seed)\n    labels = kmeans.fit_predict(data)\n\n    return labels, kmeans"}
{"task_id": "BigCodeBench/881", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(csv_file, column_name='data', pattern='\\d+[xX]', sample_size=None, seed=42):\n    \"\"\"\n    Searches for matches with a specified regex pattern in a given column of a CSV file and optionally returns a random sample of these matches.\n\n    Args:\n        csv_file (str): The path to the CSV file.\n        column_name (str, optional): The name of the column to search in. Defaults to 'data'.\n        pattern (str, optional): The regex pattern to search for. Defaults to '\\d+[xX]'.\n        sample_size (int, optional): The size of the random sample to return. If None, returns all matches. Defaults to None.\n        seed (int, optional): The seed for the random number generator. Defaults to 42.\n\n    Returns:\n        pandas.DataFrame: A pandas DataFrame containing either all the rows with matches or a random sample of them.\n    \"\"\"\n    # Read the CSV file\n    df = pd.read_csv(csv_file)\n\n    # Search for matches\n    df['match'] = df[column_name].apply(lambda x: bool(re.search(pattern, str(x))))\n\n    # Filter rows with matches\n    df = df[df['match'] == True]\n\n    # If sample_size is not None, return a random sample\n    if sample_size is not None:\n        random.seed(seed)\n        indices = random.sample(range(len(df)), sample_size)\n        df = df.iloc[indices]\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nimport random\n\ndef task_func(csv_file, column_name='data', pattern='\\d+[xX]', sample_size=None, seed=42):\n    \"\"\"\n    Searches for matches with a specified regex pattern in a given column of a CSV file and optionally returns a random sample of these matches.\n\n    Args:\n        csv_file (str): The path to the CSV file.\n        column_name (str, optional): The name of the column to search in. Defaults to 'data'.\n        pattern (str, optional): The regex pattern to search for. Defaults to '\\d+[xX]'.\n        sample_size (int, optional): The size of the random sample to return. If None, returns all matches. Defaults to None.\n        seed (int, optional): The seed for the random number generator. Defaults to 42.\n\n    Returns:\n        pandas.DataFrame: A pandas DataFrame containing either all the rows with matches or a random sample of them.\n    \"\"\"\n    # Read the CSV file\n    df = pd.read_csv(csv_file)\n\n    # Search for matches\n    df['match'] = df[column_name].apply(lambda x: bool(re.search(pattern, str(x))))\n\n    # Filter rows with matches\n    df = df[df['match'] == True]\n\n    # If sample_size is not None, return a random sample\n    if sample_size is not None:\n        random.seed(seed)\n        indices = random.sample(range(len(df)), sample_size)\n        df = df.iloc[indices]\n\n    return df"}
{"task_id": "BigCodeBench/882", "solution": "import sqlite3\nimport pandas as pd\nimport os\nimport re\ndef task_func(db_file, table_name, column_name, pattern='\\d+[xX]'):\n    if not os.path.exists(db_file):\n        raise ValueError(\"db_file does not exist\")\n\n    conn = sqlite3.connect(db_file)\n    query = f\"SELECT * FROM {table_name} WHERE {column_name} LIKE ?\"\n    df = pd.read_sql_query(query, conn, params=(f\"%{pattern}%\",))\n    conn.close()\n\n    return df", "raw_solution": "import sqlite3\nimport pandas as pd\nimport os\nimport re\n\ndef task_func(db_file, table_name, column_name, pattern='\\d+[xX]'):\n    if not os.path.exists(db_file):\n        raise ValueError(\"db_file does not exist\")\n\n    conn = sqlite3.connect(db_file)\n    query = f\"SELECT * FROM {table_name} WHERE {column_name} LIKE ?\"\n    df = pd.read_sql_query(query, conn, params=(f\"%{pattern}%\",))\n    conn.close()\n\n    return df"}
{"task_id": "BigCodeBench/883", "solution": "import pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\ndef task_func(df: pd.DataFrame, column_a: str, column_b: str, column_c: str) -> bool:\n    \"\"\"\n    Determines if a specific subset of data is stationary by filtering rows where column_b bigger than 50 and column_c equal to 900.\n    Data is considered to be stationary if the p_value returned by the Augmented Dickey-Fuller test is smaller than 0.05.\n    If column_a is empty after filtering or if its values are constant, True is returned.\n    \"\"\"\n    # Filter rows where column_b > 50 and column_c == 900\n    filtered_df = df[(df[column_b] > 50) & (df[column_c] == 900)]\n\n    # Check if column_a is empty or if its values are constant\n    if filtered_df[column_a].empty or filtered_df[column_a].nunique() == 1:\n        return True\n\n    # Perform Augmented Dickey-Fuller test on column_a\n    result = adfuller(filtered_df[column_a])\n\n    # Check if p_value is smaller than 0.05\n    if result[1] < 0.05:\n        return True\n\n    return False", "raw_solution": "import pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\n\ndef task_func(df: pd.DataFrame, column_a: str, column_b: str, column_c: str) -> bool:\n    \"\"\"\n    Determines if a specific subset of data is stationary by filtering rows where column_b bigger than 50 and column_c equal to 900.\n    Data is considered to be stationary if the p_value returned by the Augmented Dickey-Fuller test is smaller than 0.05.\n    If column_a is empty after filtering or if its values are constant, True is returned.\n    \"\"\"\n    # Filter rows where column_b > 50 and column_c == 900\n    filtered_df = df[(df[column_b] > 50) & (df[column_c] == 900)]\n\n    # Check if column_a is empty or if its values are constant\n    if filtered_df[column_a].empty or filtered_df[column_a].nunique() == 1:\n        return True\n\n    # Perform Augmented Dickey-Fuller test on column_a\n    result = adfuller(filtered_df[column_a])\n\n    # Check if p_value is smaller than 0.05\n    if result[1] < 0.05:\n        return True\n\n    return False"}
{"task_id": "BigCodeBench/884", "solution": "import pandas as pd\nfrom scipy.stats import chi2_contingency\ndef task_func(df, columns=['A', 'B', 'C'], larger=50, equal=900):\n    \"\"\"\n    Filters a pandas DataFrame based on the values of specific rows, and performs a chi-square independence test on the first two columns.\n    \"\"\"\n    # Check if the number of specified columns is 3\n    if len(columns) != 3:\n        raise ValueError(\"The number of specified columns must be 3.\")\n\n    # Check if the specified columns are contained in df\n    for col in columns:\n        if col not in df.columns:\n            raise ValueError(f\"The column '{col}' is not contained in df.\")\n\n    # Filter rows based on the criteria\n    filtered_df = df[(df[columns[1]] > larger) & (df[columns[2]] == equal)]\n\n    # Check if there's insufficient data for the test\n    if len(filtered_df) < 30:\n        raise ValueError(\"Insufficient data for the test. At least 30 rows are required.\")\n\n    # Compute a contingency table of the first two columns\n    contingency_table = pd.crosstab(filtered_df[columns[0]], filtered_df[columns[1]])\n\n    # Perform a chi-square independence test\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n\n    return p_value", "raw_solution": "import pandas as pd\nfrom scipy.stats import chi2_contingency\n\ndef task_func(df, columns=['A', 'B', 'C'], larger=50, equal=900):\n    \"\"\"\n    Filters a pandas DataFrame based on the values of specific rows, and performs a chi-square independence test on the first two columns.\n    \"\"\"\n    # Check if the number of specified columns is 3\n    if len(columns) != 3:\n        raise ValueError(\"The number of specified columns must be 3.\")\n\n    # Check if the specified columns are contained in df\n    for col in columns:\n        if col not in df.columns:\n            raise ValueError(f\"The column '{col}' is not contained in df.\")\n\n    # Filter rows based on the criteria\n    filtered_df = df[(df[columns[1]] > larger) & (df[columns[2]] == equal)]\n\n    # Check if there's insufficient data for the test\n    if len(filtered_df) < 30:\n        raise ValueError(\"Insufficient data for the test. At least 30 rows are required.\")\n\n    # Compute a contingency table of the first two columns\n    contingency_table = pd.crosstab(filtered_df[columns[0]], filtered_df[columns[1]])\n\n    # Perform a chi-square independence test\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n\n    return p_value"}
{"task_id": "BigCodeBench/885", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, col_a='A', col_b='B', col_c='C', seed=None):\n    if df.empty or df.isnull().values.any():\n        return None\n    if not pd.api.types.is_numeric_dtype(df[col_a]) or not pd.api.types.is_numeric_dtype(df[col_b]) or not pd.api.types.is_numeric_dtype(df[col_c]):\n        return None\n    if col_a not in df.columns or col_b not in df.columns or col_c not in df.columns:\n        return None\n\n    df = df[df[col_b] > 50]\n    df = df[df[col_c] == 900]\n\n    if df.empty:\n        return None\n\n    X = df[col_a].values.reshape(-1, 1)\n    y = df[col_b].values.reshape(-1, 1)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    return predictions, model", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, col_a='A', col_b='B', col_c='C', seed=None):\n    if df.empty or df.isnull().values.any():\n        return None\n    if not pd.api.types.is_numeric_dtype(df[col_a]) or not pd.api.types.is_numeric_dtype(df[col_b]) or not pd.api.types.is_numeric_dtype(df[col_c]):\n        return None\n    if col_a not in df.columns or col_b not in df.columns or col_c not in df.columns:\n        return None\n\n    df = df[df[col_b] > 50]\n    df = df[df[col_c] == 900]\n\n    if df.empty:\n        return None\n\n    X = df[col_a].values.reshape(-1, 1)\n    y = df[col_b].values.reshape(-1, 1)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    return predictions, model"}
{"task_id": "BigCodeBench/886", "solution": "import pandas as pd\nfrom collections import Counter\ndef task_func(data):\n    \"\"\"\n    Analyzes a dictionary of student data and returns a dataframe sorted by name and age in ascending order,\n    the average score per student as a pandas Series, and the most common age as an integer.\n    \"\"\"\n    # Check if the dictionary has the required keys\n    required_keys = {'Name', 'Age', 'Score'}\n    if not required_keys.issubset(data.keys()):\n        raise ValueError(\"The dictionary does not have the required keys.\")\n\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Sort the DataFrame by 'Name' and 'Age' in ascending order\n    df = df.sort_values(by=['Name', 'Age'])\n\n    # Calculate the average score per student as a Series\n    avg_scores = df.groupby('Name')['Score'].mean()\n\n    # Calculate the most common age\n    most_common_age = Counter(df['Age']).most_common(1)\n    most_common_age = most_common_age[0][0] if most_common_age else None\n\n    return df, avg_scores, most_common_age", "raw_solution": "import pandas as pd\nfrom collections import Counter\n\ndef task_func(data):\n    \"\"\"\n    Analyzes a dictionary of student data and returns a dataframe sorted by name and age in ascending order,\n    the average score per student as a pandas Series, and the most common age as an integer.\n    \"\"\"\n    # Check if the dictionary has the required keys\n    required_keys = {'Name', 'Age', 'Score'}\n    if not required_keys.issubset(data.keys()):\n        raise ValueError(\"The dictionary does not have the required keys.\")\n\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Sort the DataFrame by 'Name' and 'Age' in ascending order\n    df = df.sort_values(by=['Name', 'Age'])\n\n    # Calculate the average score per student as a Series\n    avg_scores = df.groupby('Name')['Score'].mean()\n\n    # Calculate the most common age\n    most_common_age = Counter(df['Age']).most_common(1)\n    most_common_age = most_common_age[0][0] if most_common_age else None\n\n    return df, avg_scores, most_common_age"}
{"task_id": "BigCodeBench/887", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(T1, row_num=50, seed=None):\n    np.random.seed(seed)\n    T1 = list(itertools.chain.from_iterable(T1))\n    T1 = [int(i) for i in T1]\n    df = pd.DataFrame(np.random.randint(0,100,size=(row_num, sum(T1))))\n    df.columns = ['Col_' + str(i+1) for i in range(sum(T1))]\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\ndef task_func(T1, row_num=50, seed=None):\n    np.random.seed(seed)\n    T1 = list(itertools.chain.from_iterable(T1))\n    T1 = [int(i) for i in T1]\n    df = pd.DataFrame(np.random.randint(0,100,size=(row_num, sum(T1))))\n    df.columns = ['Col_' + str(i+1) for i in range(sum(T1))]\n    return df\n\n# Test 1\ndf = task_func(('1', ('1', '3')), row_num=2, seed=32)"}
{"task_id": "BigCodeBench/888", "solution": "import pandas as pd\nimport os\ndef task_func(data_dir: str, csv_files: list) -> pd.DataFrame:\n    \"\"\"\n    Merges / Concatenates multiple CSV files from a specified directory into a single Pandas DataFrame.\n    If an empty list of files is passed, an empty DataFrame is returned.\n    \"\"\"\n    if not csv_files:\n        return pd.DataFrame()\n\n    dfs = []\n    for file in csv_files:\n        file_path = os.path.join(data_dir, file)\n        df = pd.read_csv(file_path)\n        dfs.append(df)\n\n    merged_df = pd.concat(dfs, ignore_index=True)\n    return merged_df", "raw_solution": "import pandas as pd\nimport os\n\ndef task_func(data_dir: str, csv_files: list) -> pd.DataFrame:\n    \"\"\"\n    Merges / Concatenates multiple CSV files from a specified directory into a single Pandas DataFrame.\n    If an empty list of files is passed, an empty DataFrame is returned.\n    \"\"\"\n    if not csv_files:\n        return pd.DataFrame()\n\n    dfs = []\n    for file in csv_files:\n        file_path = os.path.join(data_dir, file)\n        df = pd.read_csv(file_path)\n        dfs.append(df)\n\n    merged_df = pd.concat(dfs, ignore_index=True)\n    return merged_df"}
{"task_id": "BigCodeBench/889", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(data_dir: str, csv_file: str) -> pd.DataFrame:\n    \"\"\"\n    Loads a CSV file into a pandas DataFrame and replaces NaN values in numeric columns with the mean of the corresponding column.\n    If the csv_file does not exist, it raises a FileNotFoundError. If an empty csv is passed, an empty DataFrame is returned.\n    \"\"\"\n    # Check if the file exists\n    file_path = os.path.join(data_dir, csv_file)\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {csv_file} does not exist in the directory {data_dir}\")\n\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n\n    # Replace NaN values in numeric columns with the mean of the corresponding column\n    for col in df.select_dtypes(include=[np.number]):\n        df[col].fillna(df[col].mean(), inplace=True)\n\n    return df", "raw_solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(data_dir: str, csv_file: str) -> pd.DataFrame:\n    \"\"\"\n    Loads a CSV file into a pandas DataFrame and replaces NaN values in numeric columns with the mean of the corresponding column.\n    If the csv_file does not exist, it raises a FileNotFoundError. If an empty csv is passed, an empty DataFrame is returned.\n    \"\"\"\n    # Check if the file exists\n    file_path = os.path.join(data_dir, csv_file)\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {csv_file} does not exist in the directory {data_dir}\")\n\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n\n    # Replace NaN values in numeric columns with the mean of the corresponding column\n    for col in df.select_dtypes(include=[np.number]):\n        df[col].fillna(df[col].mean(), inplace=True)\n\n    return df"}
{"task_id": "BigCodeBench/890", "solution": "import os\nimport random\nimport pandas as pd\ndef task_func(data_dir, csv_files=['file1.csv', 'file2.csv', 'file3.csv'], seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Ensure that all csv_files exist in the data_dir\n    csv_files = [file for file in csv_files if os.path.exists(os.path.join(data_dir, file))]\n\n    if not csv_files:\n        return None, pd.DataFrame()\n\n    # Randomly select a csv_file\n    csv_file = random.choice(csv_files)\n\n    # Read the csv_file into a DataFrame\n    df = pd.read_csv(os.path.join(data_dir, csv_file))\n\n    # If the DataFrame is empty, return an empty DataFrame\n    if df.empty:\n        return csv_file, pd.DataFrame()\n\n    # Randomly select a certain number of records from the DataFrame\n    num_records = random.randint(1, len(df))\n    selected_rows = df.sample(n=num_records)\n\n    return csv_file, selected_rows", "raw_solution": "import os\nimport random\nimport pandas as pd\n\ndef task_func(data_dir, csv_files=['file1.csv', 'file2.csv', 'file3.csv'], seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    # Ensure that all csv_files exist in the data_dir\n    csv_files = [file for file in csv_files if os.path.exists(os.path.join(data_dir, file))]\n\n    if not csv_files:\n        return None, pd.DataFrame()\n\n    # Randomly select a csv_file\n    csv_file = random.choice(csv_files)\n\n    # Read the csv_file into a DataFrame\n    df = pd.read_csv(os.path.join(data_dir, csv_file))\n\n    # If the DataFrame is empty, return an empty DataFrame\n    if df.empty:\n        return csv_file, pd.DataFrame()\n\n    # Randomly select a certain number of records from the DataFrame\n    num_records = random.randint(1, len(df))\n    selected_rows = df.sample(n=num_records)\n\n    return csv_file, selected_rows"}
{"task_id": "BigCodeBench/891", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\ndef task_func(csv_file_path, attribute, test_size=0.2, random_state=42):\n    # Load the data from the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Separate the features and the target\n    X = df.drop(attribute, axis=1)\n    y = df[attribute]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Train the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    predictions = model.predict(X_test)\n\n    return model, predictions", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(csv_file_path, attribute, test_size=0.2, random_state=42):\n    # Load the data from the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Separate the features and the target\n    X = df.drop(attribute, axis=1)\n    y = df[attribute]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Train the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    predictions = model.predict(X_test)\n\n    return model, predictions"}
{"task_id": "BigCodeBench/892", "solution": "from collections import Counter\ndef task_func(strings: list) -> dict:\n    \"\"\"\n    Counts the occurrences of a specific pattern in a list of strings.\n    \"\"\"\n    pattern = \"pattern\"\n    pattern_counts = Counter(s for s in strings if pattern in s)\n    return pattern_counts\nstrings = [\"This is a pattern\", \"This pattern is unique\", \"Pattern pattern pattern\", \"No pattern here\"]", "raw_solution": "from collections import Counter\n\ndef task_func(strings: list) -> dict:\n    \"\"\"\n    Counts the occurrences of a specific pattern in a list of strings.\n    \"\"\"\n    pattern = \"pattern\"\n    pattern_counts = Counter(s for s in strings if pattern in s)\n    return pattern_counts\n\n# Test the function\nstrings = [\"This is a pattern\", \"This pattern is unique\", \"Pattern pattern pattern\", \"No pattern here\"]"}
{"task_id": "BigCodeBench/893", "solution": "import re\nfrom datetime import datetime\ndef task_func(logs: list):\n    error_times = []\n    for log in logs:\n        if 'error' in log:\n            time_str = re.search(r'\\d+:\\d+:\\d+', log).group()\n            time_obj = datetime.strptime(time_str, '%H:%M:%S')\n            error_times.append(time_obj)\n\n    if not error_times:\n        return {'list': [], 'time': 0}\n\n    avg_time = sum(error_times, datetime.min) / len(error_times)\n    avg_time = (avg_time - datetime.min).total_seconds()\n\n    return {'list': [str(time) for time in error_times], 'time': avg_time}\nlogs = [\n    '2022-01-01 12:00:00 [INFO] This is a info message',\n    '2022-01-01 12:00:10 [ERROR] An error occurred',\n    '2022-01-01 12:01:00 [ERROR] Another error occurred',\n    '2022-01-01 12:02:00 [INFO] This is another info message'\n]", "raw_solution": "import re\nfrom datetime import datetime\n\ndef task_func(logs: list):\n    error_times = []\n    for log in logs:\n        if 'error' in log:\n            time_str = re.search(r'\\d+:\\d+:\\d+', log).group()\n            time_obj = datetime.strptime(time_str, '%H:%M:%S')\n            error_times.append(time_obj)\n\n    if not error_times:\n        return {'list': [], 'time': 0}\n\n    avg_time = sum(error_times, datetime.min) / len(error_times)\n    avg_time = (avg_time - datetime.min).total_seconds()\n\n    return {'list': [str(time) for time in error_times], 'time': avg_time}\n\n# Test cases\nlogs = [\n    '2022-01-01 12:00:00 [INFO] This is a info message',\n    '2022-01-01 12:00:10 [ERROR] An error occurred',\n    '2022-01-01 12:01:00 [ERROR] Another error occurred',\n    '2022-01-01 12:02:00 [INFO] This is another info message'\n]"}
{"task_id": "BigCodeBench/894", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nARRAY_SIZE = 10000\ndef task_func():\n    # Create a numeric array of random integers between 1 and 100\n    random_array = np.random.randint(1, 101, ARRAY_SIZE)\n\n    # Calculate the mean and standard deviation\n    mean = np.mean(random_array)\n    std_dev = np.std(random_array)\n\n    # Draw a histogram of the distribution\n    fig, ax = plt.subplots()\n    ax.hist(random_array, bins=100, color='blue', edgecolor='black')\n    ax.set_title('Histogram of Random Integers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    # Plot the mean and standard deviation\n    ax.axvline(mean, color='red', linestyle='--', label='Mean')\n    ax.axvline(mean + std_dev, color='purple', linestyle='--', label='Standard Deviation')\n    ax.axvline(mean - std_dev, color='purple', linestyle='--')\n    ax.legend()\n\n    return (random_array, mean, std_dev, ax)", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_SIZE = 10000\n\ndef task_func():\n    # Create a numeric array of random integers between 1 and 100\n    random_array = np.random.randint(1, 101, ARRAY_SIZE)\n\n    # Calculate the mean and standard deviation\n    mean = np.mean(random_array)\n    std_dev = np.std(random_array)\n\n    # Draw a histogram of the distribution\n    fig, ax = plt.subplots()\n    ax.hist(random_array, bins=100, color='blue', edgecolor='black')\n    ax.set_title('Histogram of Random Integers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    # Plot the mean and standard deviation\n    ax.axvline(mean, color='red', linestyle='--', label='Mean')\n    ax.axvline(mean + std_dev, color='purple', linestyle='--', label='Standard Deviation')\n    ax.axvline(mean - std_dev, color='purple', linestyle='--')\n    ax.legend()\n\n    return (random_array, mean, std_dev, ax)"}
{"task_id": "BigCodeBench/895", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nARRAY_SIZE = 10000\ndef task_func():\n    # Create a numeric array of random integers between 1 and 100\n    random_array = np.random.randint(1, 101, ARRAY_SIZE)\n\n    # Calculate the mean and standard deviation\n    mean = np.mean(random_array)\n    std_dev = np.std(random_array)\n\n    # Draw a histogram of the distribution\n    fig, ax = plt.subplots()\n    ax.hist(random_array, bins=100, color='blue', edgecolor='black')\n    ax.set_title('Histogram of Random Values')\n    ax.set_xlabel('Val')\n    ax.set_ylabel('Freq')\n\n    # Plot the mean and standard deviation\n    ax.axvline(mean, color='red', linestyle='--')\n    ax.axvline(mean + std_dev, color='purple', linestyle='--')\n    ax.axvline(mean - std_dev, color='purple', linestyle='--')\n\n    return (random_array, mean, std_dev, ax)", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_SIZE = 10000\n\ndef task_func():\n    # Create a numeric array of random integers between 1 and 100\n    random_array = np.random.randint(1, 101, ARRAY_SIZE)\n\n    # Calculate the mean and standard deviation\n    mean = np.mean(random_array)\n    std_dev = np.std(random_array)\n\n    # Draw a histogram of the distribution\n    fig, ax = plt.subplots()\n    ax.hist(random_array, bins=100, color='blue', edgecolor='black')\n    ax.set_title('Histogram of Random Values')\n    ax.set_xlabel('Val')\n    ax.set_ylabel('Freq')\n\n    # Plot the mean and standard deviation\n    ax.axvline(mean, color='red', linestyle='--')\n    ax.axvline(mean + std_dev, color='purple', linestyle='--')\n    ax.axvline(mean - std_dev, color='purple', linestyle='--')\n\n    return (random_array, mean, std_dev, ax)\n\n# Test the function\nrandom_array, mean, std_dev, ax = task_func()\nassert isinstance(random_array, np.ndarray)\nassert isinstance(mean, float)\nassert isinstance(std_dev, float)\nassert isinstance(ax, plt.Axes)\nplt.show()"}
{"task_id": "BigCodeBench/896", "solution": "from collections import Counter\nimport random\nimport itertools\ndef task_func(length, count, seed=0):\n    random.seed(seed)\n    letters = ['a', 'b', 'c', 'd', 'e']\n    strings = [''.join(random.choices(letters, k=length)) for _ in range(count)]\n    flattened_string = ''.join(strings)\n    counter = Counter(flattened_string)\n    return counter", "raw_solution": "from collections import Counter\nimport random\nimport itertools\n\ndef task_func(length, count, seed=0):\n    random.seed(seed)\n    letters = ['a', 'b', 'c', 'd', 'e']\n    strings = [''.join(random.choices(letters, k=length)) for _ in range(count)]\n    flattened_string = ''.join(strings)\n    counter = Counter(flattened_string)\n    return counter\n\n# Test the function"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulates a number of dice rolls and calculates the frequency of each result.\n    Returns a tuple containing:\n    - A numpy array with the frequency of each outcome.\n    - A matplotlib Axes object representing the histogram.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n\n    # Calculate frequency of each outcome\n    frequencies = np.bincount(results)\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequencies)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return frequencies, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulates a number of dice rolls and calculates the frequency of each result.\n    Returns a tuple containing:\n    - A numpy array with the frequency of each outcome.\n    - A matplotlib Axes object representing the histogram.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n\n    # Calculate frequency of each outcome\n    frequencies = np.bincount(results)\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequencies)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return frequencies, ax"}
{"task_id": "BigCodeBench/898", "solution": "from collections import Counter\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e']\ndef task_func(count, seed=0):\n    random.seed(seed)\n    pairs = [random.choice(LETTERS) + random.choice(LETTERS) for _ in range(count)]\n    return Counter(pairs)", "raw_solution": "from collections import Counter\nimport random\n\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef task_func(count, seed=0):\n    random.seed(seed)\n    pairs = [random.choice(LETTERS) + random.choice(LETTERS) for _ in range(count)]\n    return Counter(pairs)\n\n# Test the function"}
{"task_id": "BigCodeBench/899", "solution": "import numpy as np\nimport random\ndef task_func(length=10000, seed=0):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n    random.seed(seed)\n    steps = [random.choice([-1, 1]) for _ in range(length)]\n    walk = np.cumsum(steps)\n    return walk", "raw_solution": "import numpy as np\nimport random\n\ndef task_func(length=10000, seed=0):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n    random.seed(seed)\n    steps = [random.choice([-1, 1]) for _ in range(length)]\n    walk = np.cumsum(steps)\n    return walk"}
{"task_id": "BigCodeBench/900", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(d):\n    if not isinstance(d, list):\n        raise ValueError(\"Input is not a list of dictionaries\")\n    if not all(isinstance(i, dict) for i in d):\n        raise ValueError(\"Input is not a list of dictionaries\")\n    if not all(set(i.keys()) == {\"x\", \"y\", \"z\"} for i in d):\n        raise ValueError(\"Not all dictionaries have keys 'x', 'y', and 'z'\")\n    if not all(isinstance(i[k], (int, float)) for i in d for k in i.keys()):\n        raise ValueError(\"Not all values are numeric\")\n\n    df = pd.DataFrame(d)\n    stats = {\n        \"mean\": df.mean().to_dict(),\n        \"sum\": df.sum().to_dict(),\n        \"max\": df.max().to_dict(),\n        \"min\": df.min().to_dict(),\n        \"std\": df.std().to_dict(),\n    }\n\n    return stats", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(d):\n    if not isinstance(d, list):\n        raise ValueError(\"Input is not a list of dictionaries\")\n    if not all(isinstance(i, dict) for i in d):\n        raise ValueError(\"Input is not a list of dictionaries\")\n    if not all(set(i.keys()) == {\"x\", \"y\", \"z\"} for i in d):\n        raise ValueError(\"Not all dictionaries have keys 'x', 'y', and 'z'\")\n    if not all(isinstance(i[k], (int, float)) for i in d for k in i.keys()):\n        raise ValueError(\"Not all values are numeric\")\n\n    df = pd.DataFrame(d)\n    stats = {\n        \"mean\": df.mean().to_dict(),\n        \"sum\": df.sum().to_dict(),\n        \"max\": df.max().to_dict(),\n        \"min\": df.min().to_dict(),\n        \"std\": df.std().to_dict(),\n    }\n\n    return stats"}
{"task_id": "BigCodeBench/901", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(d):\n    if not d:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(d)\n    scaler = MinMaxScaler()\n    df[['x', 'y', 'z']] = scaler.fit_transform(df[['x', 'y', 'z']])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Updated function to handle empty input list\ndef task_func(d):\n    if not d:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(d)\n    scaler = MinMaxScaler()\n    df[['x', 'y', 'z']] = scaler.fit_transform(df[['x', 'y', 'z']])\n\n    return df\n\ndata = [{'x': -1, 'y': 0, 'z': 5}, {'x': 3, 'y': -15, 'z': 0}, {'x': 0, 'y': 1, 'z': -7}]"}
{"task_id": "BigCodeBench/902", "solution": "from collections import Counter\ndef task_func(d):\n    \"\"\"\n    Counts the occurrence of values with the keys \"x,\" \"y\" and \"z\" from a list of dictionaries \"d.\"\n    The function returns a dictionary with keys as 'x', 'y', and 'z' and values as Counter objects.\n    \"\"\"\n    result = {'x': Counter(), 'y': Counter(), 'z': Counter()}\n    for item in d:\n        for key in result.keys():\n            if key in item:\n                result[key].update([item[key]])\n    return result\nd = [{'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}]", "raw_solution": "from collections import Counter\n\ndef task_func(d):\n    \"\"\"\n    Counts the occurrence of values with the keys \"x,\" \"y\" and \"z\" from a list of dictionaries \"d.\"\n    The function returns a dictionary with keys as 'x', 'y', and 'z' and values as Counter objects.\n    \"\"\"\n    result = {'x': Counter(), 'y': Counter(), 'z': Counter()}\n    for item in d:\n        for key in result.keys():\n            if key in item:\n                result[key].update([item[key]])\n    return result\n\n# Test the function\nd = [{'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}, {'x': 'a', 'y': 'b', 'z': 'c'}]"}
{"task_id": "BigCodeBench/903", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(d, target='z'):\n    df = pd.DataFrame(d)\n    X = df[['x', 'y']]\n    y = df[target]\n    model = LinearRegression()\n    model.fit(X, y)\n    return model", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(d, target='z'):\n    df = pd.DataFrame(d)\n    X = df[['x', 'y']]\n    y = df[target]\n    model = LinearRegression()\n    model.fit(X, y)\n    return model"}
{"task_id": "BigCodeBench/904", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(d, keys=['x', 'y', 'z']):\n    \"\"\"\n    Plot values from a list of dictionaries based on specified keys and return the plot as a Matplotlib Axes object.\n    \"\"\"\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(d)\n\n    # Create a new figure and get the Axes object\n    fig, ax = plt.subplots()\n\n    # Plot the values of the specified keys\n    for key in keys:\n        if key in df.columns:\n            ax.plot(df[key])\n\n    return ax\ndata = [{'x': 1, 'y': 2, 'z': 3}, {'x': 4, 'y': 5, 'z': 6}, {'x': 7, 'y': 8, 'z': 9}]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(d, keys=['x', 'y', 'z']):\n    \"\"\"\n    Plot values from a list of dictionaries based on specified keys and return the plot as a Matplotlib Axes object.\n    \"\"\"\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(d)\n\n    # Create a new figure and get the Axes object\n    fig, ax = plt.subplots()\n\n    # Plot the values of the specified keys\n    for key in keys:\n        if key in df.columns:\n            ax.plot(df[key])\n\n    return ax\n\n# Test the function\ndata = [{'x': 1, 'y': 2, 'z': 3}, {'x': 4, 'y': 5, 'z': 6}, {'x': 7, 'y': 8, 'z': 9}]\nax = task_func(data, keys=['x', 'y'])\nassert isinstance(ax, plt.Axes)"}
{"task_id": "BigCodeBench/905", "solution": "import os\nimport glob\nimport csv\ndef task_func(directory_path, file_extension='.csv'):\n    \"\"\"\n    Reads all files with a specified extension in a given directory and returns their data in a dictionary.\n    \"\"\"\n    data = {}\n    for file_path in glob.glob(os.path.join(directory_path, f'*{file_extension}')):\n        with open(file_path, 'r') as file:\n            reader = csv.reader(file)\n            data[os.path.splitext(os.path.basename(file_path))[0]] = list(reader)\n    return data", "raw_solution": "import os\nimport glob\nimport csv\n\ndef task_func(directory_path, file_extension='.csv'):\n    \"\"\"\n    Reads all files with a specified extension in a given directory and returns their data in a dictionary.\n    \"\"\"\n    data = {}\n    for file_path in glob.glob(os.path.join(directory_path, f'*{file_extension}')):\n        with open(file_path, 'r') as file:\n            reader = csv.reader(file)\n            data[os.path.splitext(os.path.basename(file_path))[0]] = list(reader)\n    return data"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from a source directory to a target directory.\n    The function identifies processed files by the '_processed' suffix in the filename.\n    The function returns the path to the created archive.\n    \"\"\"\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(source_dir)\n\n    # Filter the list to only include files with the '_processed' suffix\n    processed_files = [file for file in files if re.search(r'_processed\\.', file)]\n\n    # Archive the processed files\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as zipf:\n        for file in processed_files:\n            file_path = os.path.join(source_dir, file)\n            zipf.write(file_path, arcname=file)\n\n    # Move the archive to the target directory\n    shutil.move(archive_path, target_dir)\n\n    # Return the path to the created archive\n    return os.path.join(target_dir, archive_name)", "raw_solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from a source directory to a target directory.\n    The function identifies processed files by the '_processed' suffix in the filename.\n    The function returns the path to the created archive.\n    \"\"\"\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(source_dir)\n\n    # Filter the list to only include files with the '_processed' suffix\n    processed_files = [file for file in files if re.search(r'_processed\\.', file)]\n\n    # Archive the processed files\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as zipf:\n        for file in processed_files:\n            file_path = os.path.join(source_dir, file)\n            zipf.write(file_path, arcname=file)\n\n    # Move the archive to the target directory\n    shutil.move(archive_path, target_dir)\n\n    # Return the path to the created archive\n    return os.path.join(target_dir, archive_name)"}
{"task_id": "BigCodeBench/907", "solution": "import os\nimport re\ndef task_func(pattern: str, replacement: str, directory: str) -> bool:\n    try:\n        for filename in os.listdir(directory):\n            if re.search(pattern, filename):\n                os.rename(os.path.join(directory, filename), os.path.join(directory, re.sub(pattern, replacement, filename)))\n        return True\n    except Exception:\n        return False", "raw_solution": "import os\nimport re\n\ndef task_func(pattern: str, replacement: str, directory: str) -> bool:\n    try:\n        for filename in os.listdir(directory):\n            if re.search(pattern, filename):\n                os.rename(os.path.join(directory, filename), os.path.join(directory, re.sub(pattern, replacement, filename)))\n        return True\n    except Exception:\n        return False"}
{"task_id": "BigCodeBench/908", "solution": "import os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\ndef task_func(directory: str, pattern: str) -> list:\n    \"\"\"\n    Searches a directory for CSV files matching a given regular expression pattern,\n    reads sales data from these files, and plots the sales data with month on the x-axis\n    and sales on the y-axis.\n    \"\"\"\n    # Initialize an empty list to store the axes objects\n    axes_list = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        if re.match(pattern, filename):\n            # Read the CSV file into a DataFrame\n            df = pd.read_csv(os.path.join(directory, filename))\n\n            # Create a new figure and axis\n            fig, ax = plt.subplots()\n\n            # Plot the sales data\n            ax.plot(df['Month'], df['Sales'])\n\n            # Add the axes object to the list\n            axes_list.append(ax)\n\n    # Return the list of axes objects\n    return axes_list", "raw_solution": "import os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n\ndef task_func(directory: str, pattern: str) -> list:\n    \"\"\"\n    Searches a directory for CSV files matching a given regular expression pattern,\n    reads sales data from these files, and plots the sales data with month on the x-axis\n    and sales on the y-axis.\n    \"\"\"\n    # Initialize an empty list to store the axes objects\n    axes_list = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        if re.match(pattern, filename):\n            # Read the CSV file into a DataFrame\n            df = pd.read_csv(os.path.join(directory, filename))\n\n            # Create a new figure and axis\n            fig, ax = plt.subplots()\n\n            # Plot the sales data\n            ax.plot(df['Month'], df['Sales'])\n\n            # Add the axes object to the list\n            axes_list.append(ax)\n\n    # Return the list of axes objects\n    return axes_list"}
{"task_id": "BigCodeBench/909", "solution": "import pandas as pd\nimport itertools\nfrom random import shuffle\ndef task_func(letters=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'], categories=['Category 1', 'Category 2', 'Category 3']):\n    shuffle(categories)\n    data = list(itertools.product(letters, categories))\n    df = pd.DataFrame(data, columns=['Letter', 'Category'])\n    return df", "raw_solution": "import pandas as pd\nimport itertools\nfrom random import shuffle\n\ndef task_func(letters=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'], categories=['Category 1', 'Category 2', 'Category 3']):\n    shuffle(categories)\n    data = list(itertools.product(letters, categories))\n    df = pd.DataFrame(data, columns=['Letter', 'Category'])\n    return df\n\n# Test the function\ndf = task_func()"}
{"task_id": "BigCodeBench/910", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(letters, repetitions, colors):\n    if len(letters) == 0 or len(colors) == 0:\n        raise ValueError(\"Lists 'letters' and 'colors' cannot be empty\")\n    if len(letters) != len(repetitions) or len(colors) != len(repetitions):\n        raise ValueError(\"Lengths of the input lists do not match\")\n\n    # Flatten the list of letters\n    flat_letters = [letter for letter, rep in zip(letters, repetitions) for _ in range(rep)]\n\n    # Count the frequency of each letter\n    unique_letters, counts = np.unique(flat_letters, return_counts=True)\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(unique_letters, counts, color=colors)\n\n    # Set labels and title\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Letters')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(letters, repetitions, colors):\n    if len(letters) == 0 or len(colors) == 0:\n        raise ValueError(\"Lists 'letters' and 'colors' cannot be empty\")\n    if len(letters) != len(repetitions) or len(colors) != len(repetitions):\n        raise ValueError(\"Lengths of the input lists do not match\")\n\n    # Flatten the list of letters\n    flat_letters = [letter for letter, rep in zip(letters, repetitions) for _ in range(rep)]\n\n    # Count the frequency of each letter\n    unique_letters, counts = np.unique(flat_letters, return_counts=True)\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(unique_letters, counts, color=colors)\n\n    # Set labels and title\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Letters')\n\n    return ax"}
{"task_id": "BigCodeBench/911", "solution": "from functools import reduce\nimport operator\nimport string\ndef task_func(letters):\n    \"\"\"\n    Calculates the product of the corresponding numbers for a list of uppercase letters.\n    For example, given the list [\"A\", \"E\", \"I\"], this function returns 45.\n    \"\"\"\n    # Create a dictionary to map each uppercase letter to its corresponding number\n    letter_to_num = {letter: num for num, letter in enumerate(string.ascii_uppercase, start=1)}\n\n    # Convert each letter to its corresponding number and calculate the product\n    nums = [letter_to_num[letter] for letter in letters]\n    product = reduce(operator.mul, nums)\n\n    return product", "raw_solution": "from functools import reduce\nimport operator\nimport string\n\ndef task_func(letters):\n    \"\"\"\n    Calculates the product of the corresponding numbers for a list of uppercase letters.\n    For example, given the list [\"A\", \"E\", \"I\"], this function returns 45.\n    \"\"\"\n    # Create a dictionary to map each uppercase letter to its corresponding number\n    letter_to_num = {letter: num for num, letter in enumerate(string.ascii_uppercase, start=1)}\n\n    # Convert each letter to its corresponding number and calculate the product\n    nums = [letter_to_num[letter] for letter in letters]\n    product = reduce(operator.mul, nums)\n\n    return product\n\n# Test the function\nassert task_func([\"A\", \"E\", \"I\"]) == 45\nassert task_func([\"A\", \"B\", \"C\"]) == 6\nassert task_func([\"D\", \"E\", \"F\"]) == 15"}
{"task_id": "BigCodeBench/912", "solution": "from collections import Counter\nimport itertools\ndef task_func(letters: list, repetitions: int) -> dict:\n    \"\"\"\n    Counts the frequency of each letter in a list after repeating it a given number of times.\n    \"\"\"\n    # Repeat the list of letters for the given number of times\n    repeated_letters = list(itertools.chain(*[letters for _ in range(repetitions)]))\n\n    # Count the frequency of each letter using Counter\n    frequency_dict = dict(Counter(repeated_letters))\n\n    return frequency_dict", "raw_solution": "from collections import Counter\nimport itertools\n\ndef task_func(letters: list, repetitions: int) -> dict:\n    \"\"\"\n    Counts the frequency of each letter in a list after repeating it a given number of times.\n    \"\"\"\n    # Repeat the list of letters for the given number of times\n    repeated_letters = list(itertools.chain(*[letters for _ in range(repetitions)]))\n\n    # Count the frequency of each letter using Counter\n    frequency_dict = dict(Counter(repeated_letters))\n\n    return frequency_dict"}
{"task_id": "BigCodeBench/913", "solution": "from typing import List, Union\nimport numpy as np\nimport scipy.fft\nfrom collections import Counter\ndef task_func(data: List[Union[int, str]], repetitions: int = 1):\n    if not data or repetitions <= 0:\n        return {'mode': np.array([]), 'count': np.array([])}\n\n    # Repeat the data\n    repeated_data = data * repetitions\n\n    # Calculate the mode(s) and their count(s)\n    counter = Counter(repeated_data)\n    max_count = max(counter.values())\n    modes = [k for k, v in counter.items() if v == max_count]\n    count = np.array([max_count] * len(modes))\n\n    # Calculate the FFT of the data\n    fft_data = scipy.fft.fft(repeated_data)\n\n    return {\n        'mode': np.array(sorted(modes)),\n        'count': count,\n        'fft': fft_data\n    }", "raw_solution": "from typing import List, Union\nimport numpy as np\nimport scipy.fft\nfrom collections import Counter\n\ndef task_func(data: List[Union[int, str]], repetitions: int = 1):\n    if not data or repetitions <= 0:\n        return {'mode': np.array([]), 'count': np.array([])}\n\n    # Repeat the data\n    repeated_data = data * repetitions\n\n    # Calculate the mode(s) and their count(s)\n    counter = Counter(repeated_data)\n    max_count = max(counter.values())\n    modes = [k for k, v in counter.items() if v == max_count]\n    count = np.array([max_count] * len(modes))\n\n    # Calculate the FFT of the data\n    fft_data = scipy.fft.fft(repeated_data)\n\n    return {\n        'mode': np.array(sorted(modes)),\n        'count': count,\n        'fft': fft_data\n    }"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    \"\"\"\n    Predicts the stock closing prices for the next 7 days using simple linear regression and plots the data.\n    \"\"\"\n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Generate future timestamps\n    future_timestamps = pd.date_range(start=df.index[-1], periods=7, freq='D')\n\n    # Reset the index\n    df.reset_index(inplace=True)\n\n    # Create a new DataFrame for the future timestamps\n    future_df = pd.DataFrame(future_timestamps, columns=['Date'])\n\n    # Merge the original DataFrame and the future DataFrame\n    df = pd.concat([df, future_df], ignore_index=True)\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Create a new column 'Time' that represents the time since the first day\n    df['Time'] = (df.index - df.index[0]).days\n\n    # Create a new column 'Time_Squared' that represents the square of the time\n    df['Time_Squared'] = df['Time'] ** 2\n\n    # Initialize the LinearRegression model\n    model = LinearRegression()\n\n    # Fit the model with the 'Time' and 'Time_Squared' columns as features and the 'Close' column as the target\n    model.fit(df[['Time', 'Time_Squared']], df['Close'])\n\n    # Predict the closing prices for the next 7 days\n    predictions = model.predict(df.loc[df.index[-7]:, ['Time', 'Time_Squared']])\n\n    # Plot the actual closing prices and the predicted prices\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df['Close'], label='Actual')\n    plt.plot(df.loc[df.index[-7]:, 'Close'], predictions, label='Predicted')\n    plt.legend()\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Prices Prediction')\n    plt.grid(True)\n\n    return (predictions.tolist(), plt.gca())", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    \"\"\"\n    Predicts the stock closing prices for the next 7 days using simple linear regression and plots the data.\n    \"\"\"\n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Generate future timestamps\n    future_timestamps = pd.date_range(start=df.index[-1], periods=7, freq='D')\n\n    # Reset the index\n    df.reset_index(inplace=True)\n\n    # Create a new DataFrame for the future timestamps\n    future_df = pd.DataFrame(future_timestamps, columns=['Date'])\n\n    # Merge the original DataFrame and the future DataFrame\n    df = pd.concat([df, future_df], ignore_index=True)\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Create a new column 'Time' that represents the time since the first day\n    df['Time'] = (df.index - df.index[0]).days\n\n    # Create a new column 'Time_Squared' that represents the square of the time\n    df['Time_Squared'] = df['Time'] ** 2\n\n    # Initialize the LinearRegression model\n    model = LinearRegression()\n\n    # Fit the model with the 'Time' and 'Time_Squared' columns as features and the 'Close' column as the target\n    model.fit(df[['Time', 'Time_Squared']], df['Close'])\n\n    # Predict the closing prices for the next 7 days\n    predictions = model.predict(df.loc[df.index[-7]:, ['Time', 'Time_Squared']])\n\n    # Plot the actual closing prices and the predicted prices\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df['Close'], label='Actual')\n    plt.plot(df.loc[df.index[-7]:, 'Close'], predictions, label='Predicted')\n    plt.legend()\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Prices Prediction')\n    plt.grid(True)\n\n    return (predictions.tolist(), plt.gca())"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n    df['z_score'] = zscore(df['closing_price'])\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    plot = df['closing_price'].plot(kind='line', title='Outliers in Closing Prices')\n    plot.set_xlabel('Index')\n    plot.set_ylabel('Closing Price')\n    plt.scatter(outliers.index, outliers['closing_price'], color='red')\n    return outliers, plot", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    df['z_score'] = zscore(df['closing_price'])\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    plot = df['closing_price'].plot(kind='line', title='Outliers in Closing Prices')\n    plot.set_xlabel('Index')\n    plot.set_ylabel('Closing Price')\n    plt.scatter(outliers.index, outliers['closing_price'], color='red')\n    return outliers, plot\n\n# Test the function\ndf2 = pd.DataFrame({\n    'closing_price': [10, 20, 30, 40, 50, 100]\n})\noutliers2, plot2 = task_func(df2, z_threshold=1.5)"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualizes the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \"\"\"\n    # Create a new figure\n    fig, axes = plt.subplots(2, figsize=(10, 8))\n\n    # Box plot\n    sns.boxplot(data=df['Close'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n\n    # Histogram\n    sns.histplot(data=df['Close'], ax=axes[1], kde=True)\n    axes[1].set_title('Histogram of Closing Prices')\n\n    plt.tight_layout()\n\n    return axes[0], axes[1]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualizes the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \"\"\"\n    # Create a new figure\n    fig, axes = plt.subplots(2, figsize=(10, 8))\n\n    # Box plot\n    sns.boxplot(data=df['Close'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n\n    # Histogram\n    sns.histplot(data=df['Close'], ax=axes[1], kde=True)\n    axes[1].set_title('Histogram of Closing Prices')\n\n    plt.tight_layout()\n\n    return axes[0], axes[1]"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts the share closing prices for the next 7 days using the ARIMA model and plots the forecast.\n    \"\"\"\n    # Fit the ARIMA model\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit(disp=0)\n\n    # Make forecast\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot forecast\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(df['Date'], df['Close'], label='Historical Prices')\n    ax.plot(pd.date_range(start=df['Date'].iloc[-1], periods=8)[1:], forecast, label='Forecast Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    ax.legend()\n    ax.grid(True)\n\n    return list(forecast), ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts the share closing prices for the next 7 days using the ARIMA model and plots the forecast.\n    \"\"\"\n    # Fit the ARIMA model\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit(disp=0)\n\n    # Make forecast\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot forecast\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(df['Date'], df['Close'], label='Historical Prices')\n    ax.plot(pd.date_range(start=df['Date'].iloc[-1], periods=8)[1:], forecast, label='Forecast Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    ax.legend()\n    ax.grid(True)\n\n    return list(forecast), ax"}
{"task_id": "BigCodeBench/918", "solution": "import pandas as pd\nimport re\ndef task_func(data, mapping):\n    # Create a regular expression pattern from the acronyms in the mapping\n    pattern = re.compile('|'.join(map(re.escape, mapping.keys())), re.IGNORECASE)\n\n    # Replace the acronyms in the DataFrame with their full words\n    data = data.applymap(lambda x: pattern.sub(lambda m: mapping[m.group(0)], str(x)))\n\n    return data\nmapping = {'USA': 'United States of America', 'UK': 'United Kingdom', 'NASA': 'National Aeronautics and Space Administration'}\ndata = pd.DataFrame({'A': ['I visited USA', 'UK is beautiful', 'NASA is doing great'],\n                     'B': ['USA is a beautiful country', 'UK is a big country', 'NASA is the best']})", "raw_solution": "import pandas as pd\nimport re\n\n# Function to replace acronyms in DataFrame\ndef task_func(data, mapping):\n    # Create a regular expression pattern from the acronyms in the mapping\n    pattern = re.compile('|'.join(map(re.escape, mapping.keys())), re.IGNORECASE)\n\n    # Replace the acronyms in the DataFrame with their full words\n    data = data.applymap(lambda x: pattern.sub(lambda m: mapping[m.group(0)], str(x)))\n\n    return data\n\n# Test the function\nmapping = {'USA': 'United States of America', 'UK': 'United Kingdom', 'NASA': 'National Aeronautics and Space Administration'}\ndata = pd.DataFrame({'A': ['I visited USA', 'UK is beautiful', 'NASA is doing great'],\n                     'B': ['USA is a beautiful country', 'UK is a big country', 'NASA is the best']})\n\nresult = task_func(data, mapping)"}
{"task_id": "BigCodeBench/919", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\ndef task_func(data, column):\n    \"\"\"\n    Draws and returns a bar chart that shows the distribution of categories in a specific column of a DataFrame.\n    If some categories are missing in the DataFrame, they will be included in the plot with a count of zero.\n    The x label of the plot is set to 'Category', the y label is set to 'Count', and the title is set to 'Distribution of {column}'.\n    \"\"\"\n    # Ensure the DataFrame is not empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n\n    # Ensure the column exists in the DataFrame\n    if column not in data.columns:\n        raise ValueError(f\"The DataFrame does not contain the column '{column}'.\")\n\n    # Count the occurrences of each category in the column\n    counts = data[column].value_counts()\n\n    # Ensure all categories are included in the plot\n    for category in CATEGORIES:\n        if category not in counts.index:\n            counts[category] = 0\n\n    # Sort the counts by category\n    counts = counts.sort_index()\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(counts.index, counts.values)\n\n    # Set labels and title\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Distribution of {column}')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(data, column):\n    \"\"\"\n    Draws and returns a bar chart that shows the distribution of categories in a specific column of a DataFrame.\n    If some categories are missing in the DataFrame, they will be included in the plot with a count of zero.\n    The x label of the plot is set to 'Category', the y label is set to 'Count', and the title is set to 'Distribution of {column}'.\n    \"\"\"\n    # Ensure the DataFrame is not empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n\n    # Ensure the column exists in the DataFrame\n    if column not in data.columns:\n        raise ValueError(f\"The DataFrame does not contain the column '{column}'.\")\n\n    # Count the occurrences of each category in the column\n    counts = data[column].value_counts()\n\n    # Ensure all categories are included in the plot\n    for category in CATEGORIES:\n        if category not in counts.index:\n            counts[category] = 0\n\n    # Sort the counts by category\n    counts = counts.sort_index()\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(counts.index, counts.values)\n\n    # Set labels and title\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Distribution of {column}')\n\n    return ax"}
{"task_id": "BigCodeBench/920", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Draws and returns a correlation matrix heatmap for a DataFrame containing numerical columns.\n    The title of the heatmap is set to 'Correlation Matrix'.\n    \"\"\"\n    # Calculate the correlation matrix\n    corr = data.corr()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True,\n                xticklabels=corr.columns.values,\n                yticklabels=corr.columns.values)\n    plt.title('Correlation Matrix')\n\n    return plt.gca()", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    Draws and returns a correlation matrix heatmap for a DataFrame containing numerical columns.\n    The title of the heatmap is set to 'Correlation Matrix'.\n    \"\"\"\n    # Calculate the correlation matrix\n    corr = data.corr()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True,\n                xticklabels=corr.columns.values,\n                yticklabels=corr.columns.values)\n    plt.title('Correlation Matrix')\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/921", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data, columns):\n    \"\"\"\n    Normalizes specified columns of a DataFrame using min-max scaling.\n    \"\"\"\n    # Create a MinMaxScaler object\n    scaler = MinMaxScaler()\n\n    # Fit and transform the specified columns\n    data[columns] = scaler.fit_transform(data[columns])\n\n    return data", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data, columns):\n    \"\"\"\n    Normalizes specified columns of a DataFrame using min-max scaling.\n    \"\"\"\n    # Create a MinMaxScaler object\n    scaler = MinMaxScaler()\n\n    # Fit and transform the specified columns\n    data[columns] = scaler.fit_transform(data[columns])\n\n    return data"}
{"task_id": "BigCodeBench/922", "solution": "import pandas as pd\nfrom nltk.corpus import stopwords", "raw_solution": "import pandas as pd\nfrom nltk.corpus import stopwords\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords(data, column):\n    \"\"\"\n    Removes English stopwords from a text column in a DataFrame.\n\n    Parameters:\n    data (pandas.DataFrame): The DataFrame to be processed.\n    column (str): The name of the column containing the text to be processed.\n\n    Returns:\n    pandas.DataFrame: A DataFrame with the stopwords removed from the specified column.\n    \"\"\"\n    # Remove stopwords\n    data[column] = data[column].apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS]))\n\n    return data"}
{"task_id": "BigCodeBench/923", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(person_names, email_domains, num_records=5):\n    if not email_domains:\n        raise ValueError(\"No email domains provided\")\n    if len(person_names) < num_records:\n        raise ValueError(\"Number of names provided is less than the number of records requested\")\n\n    names_emails = []\n    for _ in range(num_records):\n        name = random.choice(person_names)\n        domain = random.choice(email_domains)\n        email = re.sub(r'@', '[at]', f\"{name}@{domain}\")\n        names_emails.append((name, email))\n\n    df = pd.DataFrame(names_emails, columns=['Name', 'Email'])\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport re\n\ndef task_func(person_names, email_domains, num_records=5):\n    if not email_domains:\n        raise ValueError(\"No email domains provided\")\n    if len(person_names) < num_records:\n        raise ValueError(\"Number of names provided is less than the number of records requested\")\n\n    names_emails = []\n    for _ in range(num_records):\n        name = random.choice(person_names)\n        domain = random.choice(email_domains)\n        email = re.sub(r'@', '[at]', f\"{name}@{domain}\")\n        names_emails.append((name, email))\n\n    df = pd.DataFrame(names_emails, columns=['Name', 'Email'])\n    return df"}
{"task_id": "BigCodeBench/924", "solution": "import pandas as pd\nimport os\nimport sys\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Loads a CSV file into a Pandas DataFrame, replaces all occurrences of the string '\\n' with the string '<br>' in the specified column, and returns the cleaned DataFrame.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.exists(file_path):\n        print(f\"File {file_path} does not exist.\")\n        sys.exit(1)\n\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        print(f\"Column {column_name} does not exist in the DataFrame.\")\n        sys.exit(1)\n\n    # Replace all occurrences of '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n\n    return df", "raw_solution": "import pandas as pd\nimport os\nimport sys\n\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Loads a CSV file into a Pandas DataFrame, replaces all occurrences of the string '\\n' with the string '<br>' in the specified column, and returns the cleaned DataFrame.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.exists(file_path):\n        print(f\"File {file_path} does not exist.\")\n        sys.exit(1)\n\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        print(f\"Column {column_name} does not exist in the DataFrame.\")\n        sys.exit(1)\n\n    # Replace all occurrences of '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n\n    return df"}
{"task_id": "BigCodeBench/925", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_size=1000, column_names=['A', ' ','C', 'D', 'E'], seed=0):\n    np.random.seed(seed)\n    data = np.random.randint(1, 101, (data_size, len(column_names)))\n    df = pd.DataFrame(data, columns=column_names)\n    df[df < 10] = -1\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data_size=1000, column_names=['A', ' ','C', 'D', 'E'], seed=0):\n    np.random.seed(seed)\n    data = np.random.randint(1, 101, (data_size, len(column_names)))\n    df = pd.DataFrame(data, columns=column_names)\n    df[df < 10] = -1\n    return df"}
{"task_id": "BigCodeBench/926", "solution": "import pandas as pd\nimport sqlite3\ndef task_func(db_path: str, table_name: str, column_name: str) -> pd.DataFrame:\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_path)\n\n    # Load data from the specified table into a DataFrame\n    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n\n    # Replace all occurrences of '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n\n    # Close the database connection\n    conn.close()\n\n    # Return the modified DataFrame\n    return df", "raw_solution": "import pandas as pd\nimport sqlite3\n\ndef task_func(db_path: str, table_name: str, column_name: str) -> pd.DataFrame:\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_path)\n\n    # Load data from the specified table into a DataFrame\n    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n\n    # Replace all occurrences of '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n\n    # Close the database connection\n    conn.close()\n\n    # Return the modified DataFrame\n    return df"}
{"task_id": "BigCodeBench/927", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    # Load CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n\n    # Replace '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n\n    # Encode the specified column as a categorical variable using LabelEncoder\n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    # Load CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n\n    # Replace '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n\n    # Encode the specified column as a categorical variable using LabelEncoder\n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n\n    return df"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Counts the frequency of all two-letter combinations in a word.\n    \"\"\"\n    # Generate all two-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=2)]\n\n    # Count the frequency of each combination in the word\n    word_counter = Counter(word[i:i+2] for i in range(len(word)-1))\n\n    # Create a dictionary with all combinations and their counts\n    result = {combo: word_counter[combo] for combo in combinations}\n\n    return result", "raw_solution": "from collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Counts the frequency of all two-letter combinations in a word.\n    \"\"\"\n    # Generate all two-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=2)]\n\n    # Count the frequency of each combination in the word\n    word_counter = Counter(word[i:i+2] for i in range(len(word)-1))\n\n    # Create a dictionary with all combinations and their counts\n    result = {combo: word_counter[combo] for combo in combinations}\n\n    return result"}
{"task_id": "BigCodeBench/929", "solution": "import numpy as np\nfrom scipy import stats\ndef task_func(word: str) -> np.ndarray:\n    \"\"\"\n    Calculates the difference between the ASCII values of each pair of adjacent letters in the input word.\n    Also calculates the entropy of the differences.\n    \"\"\"\n    # Calculate the difference between the ASCII values of each pair of adjacent letters\n    diffs = np.array([ord(word[i]) - ord(word[i-1]) for i in range(1, len(word))])\n\n    # Calculate the entropy of the differences\n    entropy = stats.entropy(diffs)\n\n    return diffs, entropy", "raw_solution": "import numpy as np\nfrom scipy import stats\n\ndef task_func(word: str) -> np.ndarray:\n    \"\"\"\n    Calculates the difference between the ASCII values of each pair of adjacent letters in the input word.\n    Also calculates the entropy of the differences.\n    \"\"\"\n    # Calculate the difference between the ASCII values of each pair of adjacent letters\n    diffs = np.array([ord(word[i]) - ord(word[i-1]) for i in range(1, len(word))])\n\n    # Calculate the entropy of the differences\n    entropy = stats.entropy(diffs)\n\n    return diffs, entropy"}
{"task_id": "BigCodeBench/930", "solution": "import random\nimport string\nPOSSIBLE_LETTERS = ['a', 'b', 'c']\ndef task_func(word):\n    if not all(char.isalpha() for char in word):\n        raise ValueError(\"Input contains non-letter characters\")\n\n    if len(word) < 2:\n        return [\"\"] * len(POSSIBLE_LETTERS)\n\n    pairs = []\n    for i in range(len(word) - 1):\n        pair = word[i:i+2]\n        pairs.append(pair)\n\n    return pairs", "raw_solution": "import random\nimport string\n\nPOSSIBLE_LETTERS = ['a', 'b', 'c']\n\ndef task_func(word):\n    if not all(char.isalpha() for char in word):\n        raise ValueError(\"Input contains non-letter characters\")\n\n    if len(word) < 2:\n        return [\"\"] * len(POSSIBLE_LETTERS)\n\n    pairs = []\n    for i in range(len(word) - 1):\n        pair = word[i:i+2]\n        pairs.append(pair)\n\n    return pairs\n\n# Test cases"}
{"task_id": "BigCodeBench/931", "solution": "from collections import defaultdict\nimport re\ndef task_func(word: str) -> dict:\n    # Sanitize the word by removing non-alphabetic characters\n    sanitized_word = re.sub(r'[^a-zA-Z]', '', word)\n\n    # Initialize a defaultdict to store the two-letter combinations and their counts\n    two_letter_combinations = defaultdict(int)\n\n    # Iterate over the sanitized word and count the two-letter combinations\n    for i in range(len(sanitized_word) - 1):\n        two_letter_combinations[sanitized_word[i:i+2]] += 1\n\n    return two_letter_combinations", "raw_solution": "from collections import defaultdict\nimport re\n\ndef task_func(word: str) -> dict:\n    # Sanitize the word by removing non-alphabetic characters\n    sanitized_word = re.sub(r'[^a-zA-Z]', '', word)\n\n    # Initialize a defaultdict to store the two-letter combinations and their counts\n    two_letter_combinations = defaultdict(int)\n\n    # Iterate over the sanitized word and count the two-letter combinations\n    for i in range(len(sanitized_word) - 1):\n        two_letter_combinations[sanitized_word[i:i+2]] += 1\n\n    return two_letter_combinations"}
{"task_id": "BigCodeBench/932", "solution": "from collections import Counter\nimport re\ndef task_func(word: str) -> list:\n    # Clean the word by removing non-alphabetic characters and lowercasing it\n    cleaned_word = re.sub(r'[^a-z]', '', word.lower())\n\n    # If the cleaned word has fewer than 2 letters, return an empty list\n    if len(cleaned_word) < 2:\n        return []\n\n    # Find all two-letter combinations in the cleaned word\n    two_letter_combinations = [cleaned_word[i:i+2] for i in range(len(cleaned_word)-1)]\n\n    # Count the frequency of each two-letter combination\n    counter = Counter(two_letter_combinations)\n\n    # Find the most common two-letter combination and its frequency\n    most_common = counter.most_common(1)[0]\n\n    return [most_common]", "raw_solution": "from collections import Counter\nimport re\n\ndef task_func(word: str) -> list:\n    # Clean the word by removing non-alphabetic characters and lowercasing it\n    cleaned_word = re.sub(r'[^a-z]', '', word.lower())\n\n    # If the cleaned word has fewer than 2 letters, return an empty list\n    if len(cleaned_word) < 2:\n        return []\n\n    # Find all two-letter combinations in the cleaned word\n    two_letter_combinations = [cleaned_word[i:i+2] for i in range(len(cleaned_word)-1)]\n\n    # Count the frequency of each two-letter combination\n    counter = Counter(two_letter_combinations)\n\n    # Find the most common two-letter combination and its frequency\n    most_common = counter.most_common(1)[0]\n\n    return [most_common]"}
{"task_id": "BigCodeBench/933", "solution": "import string\nimport wordninja\ndef task_func(word):\n    alphabet = string.ascii_lowercase\n    word_list = []\n    for char in word:\n        if char.isalpha():\n            position = alphabet.index(char.lower()) + 1\n            word_list.append((char.lower(), position))\n    return word_list", "raw_solution": "import string\nimport wordninja\n\ndef task_func(word):\n    alphabet = string.ascii_lowercase\n    word_list = []\n    for char in word:\n        if char.isalpha():\n            position = alphabet.index(char.lower()) + 1\n            word_list.append((char.lower(), position))\n    return word_list\n\n# Test the function"}
{"task_id": "BigCodeBench/934", "solution": "from collections import Counter\nimport hashlib\ndef task_func(word: str) -> dict:\n    pairs = [word[i:i+2] for i in range(len(word)-1)]\n    count = Counter(pairs)\n    return count", "raw_solution": "from collections import Counter\nimport hashlib\n\ndef task_func(word: str) -> dict:\n    pairs = [word[i:i+2] for i in range(len(word)-1)]\n    count = Counter(pairs)\n    return count\n\ndef test_task_func():\n    assert task_func('hello') == {'he': 1, 'el': 1, 'll': 1, 'lo': 1}\n    assert task_func('mississippi') == {'mi': 1, 'is': 2, 'ss': 2, 'si': 1, 'ip': 1}\n    assert task_func('a') == {}\n    assert task_func('aa') == {'aa': 1}\n    assert task_func('aaa') == {'aa': 2, 'aaa': 1}\n\ntest_task_func()"}
{"task_id": "BigCodeBench/935", "solution": "import pandas as pd\nimport string\ndef task_func(word):\n    if not word.islower():\n        raise ValueError(\"The input word should be in lowercase.\")\n    if not word.isalpha():\n        raise ValueError(\"The input word should not contain non-alphabetic characters.\")\n\n    alphabet = string.ascii_lowercase\n    positions = {letter: index + 1 for index, letter in enumerate(alphabet)}\n\n    data = [(letter, positions[letter]) for letter in word]\n    df = pd.DataFrame(data, columns=['Letter', 'Position'])\n\n    return df", "raw_solution": "import pandas as pd\nimport string\n\ndef task_func(word):\n    if not word.islower():\n        raise ValueError(\"The input word should be in lowercase.\")\n    if not word.isalpha():\n        raise ValueError(\"The input word should not contain non-alphabetic characters.\")\n\n    alphabet = string.ascii_lowercase\n    positions = {letter: index + 1 for index, letter in enumerate(alphabet)}\n\n    data = [(letter, positions[letter]) for letter in word]\n    df = pd.DataFrame(data, columns=['Letter', 'Position'])\n\n    return df"}
{"task_id": "BigCodeBench/936", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport string\nALPHABET = list(string.ascii_lowercase)\ndef task_func(word):\n    \"\"\"\n    Draws a bar chart representing the positions of each letter in the given word within the English alphabet.\n    \"\"\"\n    # Convert the word to lowercase and create a list of positions\n    word = word.lower()\n    positions = [ALPHABET.index(char) + 1 for char in word]\n\n    # Create a numpy array of positions\n    positions_array = np.array(positions)\n\n    # Create a histogram of the positions\n    histogram = np.bincount(positions_array)\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(1, 27), histogram)\n    ax.set_xlabel('Position in alphabet')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of each letter in the word')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport string\n\n# Constants\nALPHABET = list(string.ascii_lowercase)\n\ndef task_func(word):\n    \"\"\"\n    Draws a bar chart representing the positions of each letter in the given word within the English alphabet.\n    \"\"\"\n    # Convert the word to lowercase and create a list of positions\n    word = word.lower()\n    positions = [ALPHABET.index(char) + 1 for char in word]\n\n    # Create a numpy array of positions\n    positions_array = np.array(positions)\n\n    # Create a histogram of the positions\n    histogram = np.bincount(positions_array)\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(1, 27), histogram)\n    ax.set_xlabel('Position in alphabet')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of each letter in the word')\n\n    return ax"}
{"task_id": "BigCodeBench/937", "solution": "import re\nfrom collections import Counter\ndef task_func(input_str):\n    # Remove non-alphanumeric characters and convert to lowercase\n    cleaned_str = re.sub(r'\\W+', '', input_str).lower()\n\n    # Count the frequency of each character\n    freq_dict = Counter(cleaned_str)\n\n    return freq_dict", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(input_str):\n    # Remove non-alphanumeric characters and convert to lowercase\n    cleaned_str = re.sub(r'\\W+', '', input_str).lower()\n\n    # Count the frequency of each character\n    freq_dict = Counter(cleaned_str)\n\n    return freq_dict"}
{"task_id": "BigCodeBench/938", "solution": "import re\nimport pandas as pd\ndef task_func(input_df):\n    \"\"\"\n    Cleans the text in a pandas DataFrame column named 'text' by removing all special characters, punctuation marks, and spaces, then calculates the length of the cleaned text.\n    The function should output with:\n        pandas.DataFrame: A DataFrame with two new columns 'clean_text' and 'text_length', where 'clean_text' is the cleaned text and 'text_length' is its length.\n    \"\"\"\n    # Check if 'text' column exists in the DataFrame\n    if 'text' not in input_df.columns:\n        raise ValueError(\"'text' column not found in the DataFrame\")\n\n    # Remove special characters, punctuation marks, and spaces\n    input_df['clean_text'] = input_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n    input_df['clean_text'] = input_df['clean_text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n\n    # Calculate the length of the cleaned text\n    input_df['text_length'] = input_df['clean_text'].apply(lambda x: len(x))\n\n    return input_df", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(input_df):\n    \"\"\"\n    Cleans the text in a pandas DataFrame column named 'text' by removing all special characters, punctuation marks, and spaces, then calculates the length of the cleaned text.\n    The function should output with:\n        pandas.DataFrame: A DataFrame with two new columns 'clean_text' and 'text_length', where 'clean_text' is the cleaned text and 'text_length' is its length.\n    \"\"\"\n    # Check if 'text' column exists in the DataFrame\n    if 'text' not in input_df.columns:\n        raise ValueError(\"'text' column not found in the DataFrame\")\n\n    # Remove special characters, punctuation marks, and spaces\n    input_df['clean_text'] = input_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n    input_df['clean_text'] = input_df['clean_text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n\n    # Calculate the length of the cleaned text\n    input_df['text_length'] = input_df['clean_text'].apply(lambda x: len(x))\n\n    return input_df"}
{"task_id": "BigCodeBench/939", "solution": "import re\nimport os\nimport glob\ndef task_func(dir_path: str) -> list:\n    \"\"\"\n    Renames all files in the specified directory by removing all special characters,\n    punctuation marks, and spaces, using regular expressions. The function keeps\n    alphanumeric characters and removes the rest.\n    \"\"\"\n    # Get a list of all files in the directory\n    files = glob.glob(os.path.join(dir_path, '*'))\n\n    # Initialize an empty list to store the new names\n    new_names = []\n\n    # Loop through each file\n    for file in files:\n        # Get the base name of the file (without the path)\n        base_name = os.path.basename(file)\n\n        # Remove all special characters, punctuation marks, and spaces\n        new_name = re.sub(r'[^a-zA-Z0-9]', '', base_name)\n\n        # Append the new name to the list\n        new_names.append(new_name)\n\n    return new_names", "raw_solution": "import re\nimport os\nimport glob\n\ndef task_func(dir_path: str) -> list:\n    \"\"\"\n    Renames all files in the specified directory by removing all special characters,\n    punctuation marks, and spaces, using regular expressions. The function keeps\n    alphanumeric characters and removes the rest.\n    \"\"\"\n    # Get a list of all files in the directory\n    files = glob.glob(os.path.join(dir_path, '*'))\n\n    # Initialize an empty list to store the new names\n    new_names = []\n\n    # Loop through each file\n    for file in files:\n        # Get the base name of the file (without the path)\n        base_name = os.path.basename(file)\n\n        # Remove all special characters, punctuation marks, and spaces\n        new_name = re.sub(r'[^a-zA-Z0-9]', '', base_name)\n\n        # Append the new name to the list\n        new_names.append(new_name)\n\n    return new_names"}
{"task_id": "BigCodeBench/940", "solution": "import re\nfrom nltk import word_tokenize\nfrom collections import Counter\ndef task_func(input_str):\n    # Remove all special characters, punctuation marks and spaces\n    cleaned_str = re.sub(r'[^\\w\\s]', '', input_str)\n    cleaned_str = re.sub(r'\\s+', ' ', cleaned_str)\n\n    # Tokenize the string into words\n    words = word_tokenize(cleaned_str)\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    return word_freq", "raw_solution": "import re\nfrom nltk import word_tokenize\nfrom collections import Counter\n\ndef task_func(input_str):\n    # Remove all special characters, punctuation marks and spaces\n    cleaned_str = re.sub(r'[^\\w\\s]', '', input_str)\n    cleaned_str = re.sub(r'\\s+', ' ', cleaned_str)\n\n    # Tokenize the string into words\n    words = word_tokenize(cleaned_str)\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    return word_freq"}
{"task_id": "BigCodeBench/941", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\ndef task_func(start_date: str, periods: int, freq: str, random_seed: int = 0) -> Tuple[pd.DataFrame, plt.Axes]:\n    \"\"\"\n    Generates a sales forecast starting from a given date, for a specified number of periods and frequency.\n    The function outputs a DataFrame with columns ['Date', 'Sales'], where 'Date' is the forecast date and 'Sales' are the forecasted sales.\n    It also returns a matplotlib Axes object for the sales forecast plot.\n    \"\"\"\n    np.random.seed(random_seed)\n    date_rng = pd.date_range(start=start_date, periods=periods, freq=freq)\n    sales_data = pd.DataFrame({\n        'Date': date_rng,\n        'Sales': np.random.randint(100, 500, size=periods)\n    })\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sales_data.plot(x='Date', y='Sales', ax=ax)\n    ax.set_title('Sales Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n\n    return sales_data, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\n\ndef task_func(start_date: str, periods: int, freq: str, random_seed: int = 0) -> Tuple[pd.DataFrame, plt.Axes]:\n    \"\"\"\n    Generates a sales forecast starting from a given date, for a specified number of periods and frequency.\n    The function outputs a DataFrame with columns ['Date', 'Sales'], where 'Date' is the forecast date and 'Sales' are the forecasted sales.\n    It also returns a matplotlib Axes object for the sales forecast plot.\n    \"\"\"\n    np.random.seed(random_seed)\n    date_rng = pd.date_range(start=start_date, periods=periods, freq=freq)\n    sales_data = pd.DataFrame({\n        'Date': date_rng,\n        'Sales': np.random.randint(100, 500, size=periods)\n    })\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sales_data.plot(x='Date', y='Sales', ax=ax)\n    ax.set_title('Sales Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n\n    return sales_data, ax"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create a DataFrame with the date range and sales data\n    df = pd.DataFrame({\n        'Date': dates,\n        'Category': np.random.choice(categories, periods),\n        'Sales': np.random.randint(100, 1000, periods)\n    })\n\n    # Group by 'Category' and 'Date', and sum the 'Sales'\n    df = df.groupby(['Category', 'Date']).sum().reset_index()\n\n    # Plot the 'Sales' over 'Date' for each 'Category'\n    fig, ax = plt.subplots()\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category)\n\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.legend()\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create a DataFrame with the date range and sales data\n    df = pd.DataFrame({\n        'Date': dates,\n        'Category': np.random.choice(categories, periods),\n        'Sales': np.random.randint(100, 1000, periods)\n    })\n\n    # Group by 'Category' and 'Date', and sum the 'Sales'\n    df = df.groupby(['Category', 'Date']).sum().reset_index()\n\n    # Plot the 'Sales' over 'Date' for each 'Category'\n    fig, ax = plt.subplots()\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category)\n\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.legend()\n\n    return df, ax\n\n# Test the function\ndf, ax = task_func()\ndf.head()\nplt.show()"}
{"task_id": "BigCodeBench/943", "solution": "import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(start_date='2016-01-01', periods=24, freq='M', model='additive'):\n    # Generate a sales time-series\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    sales = np.random.randint(100, 500, periods)\n    df = pd.DataFrame({'Date': dates, 'Sales': sales})\n    df.set_index('Date', inplace=True)\n\n    # Decompose the sales time-series\n    decomposition = seasonal_decompose(df['Sales'], model=model)\n\n    # Return the trend, seasonal, and residual components\n    return {'trend': decomposition.trend, 'seasonal': decomposition.seasonal, 'residual': decomposition.resid}", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(start_date='2016-01-01', periods=24, freq='M', model='additive'):\n    # Generate a sales time-series\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    sales = np.random.randint(100, 500, periods)\n    df = pd.DataFrame({'Date': dates, 'Sales': sales})\n    df.set_index('Date', inplace=True)\n\n    # Decompose the sales time-series\n    decomposition = seasonal_decompose(df['Sales'], model=model)\n\n    # Return the trend, seasonal, and residual components\n    return {'trend': decomposition.trend, 'seasonal': decomposition.seasonal, 'residual': decomposition.resid}\n\n# Test the function\nresult = task_func('2020-01-01', 24, 'M', 'multiplicative')\nassert len(result['seasonal']) == 24"}
{"task_id": "BigCodeBench/944", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', seed=0):\n    np.random.seed(seed)\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    prices = np.random.uniform(100, 500, size=periods)\n    df = pd.DataFrame({'Date': dates, 'Price': prices})\n    fig, ax = plt.subplots()\n    df.plot(x='Date', y='Price', ax=ax)\n    ax.set_title('Share Prices Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    return (df, ax)", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', seed=0):\n    np.random.seed(seed)\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    prices = np.random.uniform(100, 500, size=periods)\n    df = pd.DataFrame({'Date': dates, 'Price': prices})\n    fig, ax = plt.subplots()\n    df.plot(x='Date', y='Price', ax=ax)\n    ax.set_title('Share Prices Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    return (df, ax)\n\n# Test the function\ndf, ax = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    if sales_data is None:\n        sales_data = pd.date_range(start=start_date, periods=periods, freq=freq)\n        sales_data = pd.Series(np.random.randint(1, 100, len(sales_data)), index=sales_data)\n    else:\n        sales_data = pd.Series(sales_data, index=pd.date_range(start=start_date, periods=len(sales_data), freq=freq))\n\n    X = np.array(range(len(sales_data))).reshape(-1, 1)\n    Y = sales_data.values.reshape(-1, 1)\n\n    linear_regressor = LinearRegression()\n    linear_regressor.fit(X, Y)\n\n    future_dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    future_X = np.array(range(len(future_dates))).reshape(-1, 1)\n\n    future_sales_forecast = linear_regressor.predict(future_X)\n\n    return future_sales_forecast.flatten()", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    if sales_data is None:\n        sales_data = pd.date_range(start=start_date, periods=periods, freq=freq)\n        sales_data = pd.Series(np.random.randint(1, 100, len(sales_data)), index=sales_data)\n    else:\n        sales_data = pd.Series(sales_data, index=pd.date_range(start=start_date, periods=len(sales_data), freq=freq))\n\n    X = np.array(range(len(sales_data))).reshape(-1, 1)\n    Y = sales_data.values.reshape(-1, 1)\n\n    linear_regressor = LinearRegression()\n    linear_regressor.fit(X, Y)\n\n    future_dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    future_X = np.array(range(len(future_dates))).reshape(-1, 1)\n\n    future_sales_forecast = linear_regressor.predict(future_X)\n\n    return future_sales_forecast.flatten()"}
{"task_id": "BigCodeBench/946", "solution": "import numpy as np\nimport pandas as pd\nimport random\ndef task_func(rows=3, cols=2, min_val=0, max_val=100, seed=0):\n    np.random.seed(seed)\n    matrix = np.random.randint(min_val, max_val, size=(rows, cols))\n    df = pd.DataFrame(matrix)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nimport random\n\ndef task_func(rows=3, cols=2, min_val=0, max_val=100, seed=0):\n    np.random.seed(seed)\n    matrix = np.random.randint(min_val, max_val, size=(rows, cols))\n    df = pd.DataFrame(matrix)\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/947", "solution": "import numpy as np\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(rows=3, columns=2, start_date=datetime(2021, 1, 1), end_date=datetime(2021, 12, 31), seed=0):\n    random.seed(seed)\n    date_range = (end_date - start_date).days + 1\n    dates = [start_date + timedelta(days=i) for i in random.sample(range(date_range), date_range)]\n    matrix = np.array(dates).reshape(rows, columns)\n    return matrix", "raw_solution": "import numpy as np\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(rows=3, columns=2, start_date=datetime(2021, 1, 1), end_date=datetime(2021, 12, 31), seed=0):\n    random.seed(seed)\n    date_range = (end_date - start_date).days + 1\n    dates = [start_date + timedelta(days=i) for i in random.sample(range(date_range), date_range)]\n    matrix = np.array(dates).reshape(rows, columns)\n    return matrix\n\n# Test the function"}
{"task_id": "BigCodeBench/948", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(rows=3, columns=2, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaled_matrix = scaler.fit_transform(matrix)\n    return scaled_matrix", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(rows=3, columns=2, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaled_matrix = scaler.fit_transform(matrix)\n    return scaled_matrix"}
{"task_id": "BigCodeBench/949", "solution": "import numpy as np\nimport pandas as pd", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef generate_random_dataframe(rows, columns, seed=None):\n    \"\"\"\n    Generates a DataFrame with random values within a specified range.\n    The function creates a matrix of given dimensions filled with random values between 0 and 1.\n    The function returns the matrix as a Pandas DataFrame.\n    Users have the option to set a random seed for reproducible results.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    data = np.random.rand(rows, columns)\n    df = pd.DataFrame(data)\n\n    return df"}
{"task_id": "BigCodeBench/950", "solution": "import numpy as np\nfrom scipy.linalg import svd\ndef task_func(rows=3, columns=2, seed=0):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    U, s, Vh = svd(matrix)\n    return U, s, Vh", "raw_solution": "import numpy as np\nfrom scipy.linalg import svd\n\ndef task_func(rows=3, columns=2, seed=0):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    U, s, Vh = svd(matrix)\n    return U, s, Vh"}
{"task_id": "BigCodeBench/951", "solution": "import pandas as pd\nimport numpy as np\nimport random\nfrom random import randint, seed\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Toys & Games']\ndef task_func(mystrings, n_products, seed=0):\n    random.seed(seed)\n    product_names = [name.replace(' ', '_') for name in mystrings]\n    categories = [random.choice(CATEGORIES) for _ in range(n_products)]\n    prices = np.random.normal(50, 10, n_products).round(2)\n\n    df = pd.DataFrame({\n        'Product Name': product_names,\n        'Category': categories,\n        'Price': prices\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport random\nfrom random import randint, seed\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Toys & Games']\n\ndef task_func(mystrings, n_products, seed=0):\n    random.seed(seed)\n    product_names = [name.replace(' ', '_') for name in mystrings]\n    categories = [random.choice(CATEGORIES) for _ in range(n_products)]\n    prices = np.random.normal(50, 10, n_products).round(2)\n\n    df = pd.DataFrame({\n        'Product Name': product_names,\n        'Category': categories,\n        'Price': prices\n    })\n\n    return df"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    today = datetime.now().date()\n\n    assignments = []\n    for _ in range(n_tasks):\n        task = random.choice(task_list)\n        employee = random.choice(employees)\n        task_name = task.replace(\" \", \"_\")\n        assignments.append({\n            \"Task Name\": task_name,\n            \"Assigned To\": employee,\n            \"Due Date\": today,\n        })\n\n    return pd.DataFrame(assignments)", "raw_solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    today = datetime.now().date()\n\n    assignments = []\n    for _ in range(n_tasks):\n        task = random.choice(task_list)\n        employee = random.choice(employees)\n        task_name = task.replace(\" \", \"_\")\n        assignments.append({\n            \"Task Name\": task_name,\n            \"Assigned To\": employee,\n            \"Due Date\": today,\n        })\n\n    return pd.DataFrame(assignments)"}
{"task_id": "BigCodeBench/953", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(mystrings, folder_path, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    if not os.path.exists(folder_path):\n        try:\n            os.makedirs(folder_path)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"The provided directory path {folder_path} does not exist and cannot be created.\")\n\n    file_names = []\n    for mystring in mystrings:\n        data = np.random.rand(10)\n        plt.bar(range(1, 11), data)\n        plt.title(mystring)\n        plt.xlabel('Index')\n        plt.ylabel('Value')\n\n        file_name = f\"{mystring}.png\"\n        file_path = os.path.join(folder_path, file_name)\n        plt.savefig(file_path)\n        plt.clf()\n\n        file_names.append(file_name)\n\n    return file_names", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(mystrings, folder_path, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    if not os.path.exists(folder_path):\n        try:\n            os.makedirs(folder_path)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"The provided directory path {folder_path} does not exist and cannot be created.\")\n\n    file_names = []\n    for mystring in mystrings:\n        data = np.random.rand(10)\n        plt.bar(range(1, 11), data)\n        plt.title(mystring)\n        plt.xlabel('Index')\n        plt.ylabel('Value')\n\n        file_name = f\"{mystring}.png\"\n        file_path = os.path.join(folder_path, file_name)\n        plt.savefig(file_path)\n        plt.clf()\n\n        file_names.append(file_name)\n\n    return file_names"}
{"task_id": "BigCodeBench/954", "solution": "import random\nimport re\ndef task_func(target_words, n_sentences, vocabulary):\n    if n_sentences < 0:\n        raise ValueError(\"n_sentences cannot be negative\")\n    if not vocabulary:\n        raise ValueError(\"Vocabulary cannot be empty\")\n\n    target_words = [word.lower() for word in target_words]\n    sentences = []\n\n    for _ in range(n_sentences):\n        words = random.choices(vocabulary, k=10)\n        sentence = ' '.join(words)\n\n        for word in target_words:\n            sentence = re.sub(r'\\b' + word + r'\\b', word.replace(' ', '_'), sentence)\n\n        sentences.append(sentence.lower())\n\n    return sentences", "raw_solution": "import random\nimport re\n\ndef task_func(target_words, n_sentences, vocabulary):\n    if n_sentences < 0:\n        raise ValueError(\"n_sentences cannot be negative\")\n    if not vocabulary:\n        raise ValueError(\"Vocabulary cannot be empty\")\n\n    target_words = [word.lower() for word in target_words]\n    sentences = []\n\n    for _ in range(n_sentences):\n        words = random.choices(vocabulary, k=10)\n        sentence = ' '.join(words)\n\n        for word in target_words:\n            sentence = re.sub(r'\\b' + word + r'\\b', word.replace(' ', '_'), sentence)\n\n        sentences.append(sentence.lower())\n\n    return sentences"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n\n    # Replace spaces with underscores\n    text = text.lower()\n    text = re.sub(r'\\s', '_', text)\n\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(word_counts.keys(), word_counts.values())\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Each Unique Word')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n\n    # Replace spaces with underscores\n    text = text.lower()\n    text = re.sub(r'\\s', '_', text)\n\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(word_counts.keys(), word_counts.values())\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Each Unique Word')\n\n    return ax"}
{"task_id": "BigCodeBench/956", "solution": "import re\nimport string\nimport random\ndef task_func(text: str, seed=None) -> str:\n    if seed is not None:\n        random.seed(seed)\n\n    # Remove special characters\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n\n    # Normalize whitespace\n    text = re.sub(r'\\s+', '_', text)\n\n    # Randomize casing\n    text = ''.join([char.upper() if random.random() < 0.5 else char.lower() for char in text])\n\n    return text", "raw_solution": "import re\nimport string\nimport random\n\ndef task_func(text: str, seed=None) -> str:\n    if seed is not None:\n        random.seed(seed)\n\n    # Remove special characters\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n\n    # Normalize whitespace\n    text = re.sub(r'\\s+', '_', text)\n\n    # Randomize casing\n    text = ''.join([char.upper() if random.random() < 0.5 else char.lower() for char in text])\n\n    return text"}
{"task_id": "BigCodeBench/957", "solution": "import string\nimport re\ndef task_func(text: str) -> tuple:\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    words = text.split()\n    characters = re.sub(r'\\s', '', text)\n    unique_characters = len(set(characters))\n    return len(words), len(characters), unique_characters\ntext = \"Hello, world! This is a test.\"", "raw_solution": "import string\nimport re\n\ndef task_func(text: str) -> tuple:\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    words = text.split()\n    characters = re.sub(r'\\s', '', text)\n    unique_characters = len(set(characters))\n    return len(words), len(characters), unique_characters\n\n# Test the function\ntext = \"Hello, world! This is a test.\""}
{"task_id": "BigCodeBench/958", "solution": "import random\nimport re\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    words = re.findall(r'\\b\\w+\\b', text)\n    for i in range(len(words)):\n        if len(words[i]) > 3:\n            middle = list(words[i][1:-1])\n            random.shuffle(middle)\n            words[i] = words[i][0] + ''.join(middle) + words[i][-1]\n\n    return ' '.join(words)", "raw_solution": "import random\nimport re\n\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    words = re.findall(r'\\b\\w+\\b', text)\n    for i in range(len(words)):\n        if len(words[i]) > 3:\n            middle = list(words[i][1:-1])\n            random.shuffle(middle)\n            words[i] = words[i][0] + ''.join(middle) + words[i][-1]\n\n    return ' '.join(words)\n\n# Test cases\nassert task_func(\"Hello world\") == \"Hello dlrow\"\nassert task_func(\"Scramble the letters in each word\") == \"Scramble eht srettel ni eahc word\"\nassert task_func(\"Keep the first and last letters of each word unchanged\") == \"Keep eht tirst and last srettel fo eahc word nahciged\"\nassert task_func(\"\") == \"\"\nassert task_func(\"a\") == \"a\"\nassert task_func(\"I\") == \"I\"\nassert task_func(\"I am\") == \"I ma\"\nassert task_func(\"I am a\") == \"I ma a\"\nassert task_func(\"I am a test\") == \"I ma a tset\"\nassert task_func(\"I am a test case\") == \"I ma a tset tac\"\nassert task_func(\"I am a test case\", 1) == \"I ma a tset tac\"\nassert task_func(\"I am a test case\", 2) == \"I ma a tset tac\"\nassert task_func(\"I am a test case\", 3) == \"I ma a tset tac\""}
{"task_id": "BigCodeBench/959", "solution": "import string\nimport random\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    def replace_char(c):\n        if c.isalpha():\n            ascii_lowercase = string.ascii_lowercase\n            ascii_uppercase = string.ascii_uppercase\n            ascii_chars = ascii_lowercase if c.islower() else ascii_uppercase\n            return random.choice(ascii_chars)\n        else:\n            return c\n\n    return ''.join(map(replace_char, text))", "raw_solution": "import string\nimport random\n\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    def replace_char(c):\n        if c.isalpha():\n            ascii_lowercase = string.ascii_lowercase\n            ascii_uppercase = string.ascii_uppercase\n            ascii_chars = ascii_lowercase if c.islower() else ascii_uppercase\n            return random.choice(ascii_chars)\n        else:\n            return c\n\n    return ''.join(map(replace_char, text))\n\n# Testing the function"}
{"task_id": "BigCodeBench/960", "solution": "import string\nimport random\ndef task_func(text, seed=None):\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    password = []\n    for char in text:\n        if char.isalpha():\n            password.append(chr(random.randint(97, 122)))\n        elif char.isdigit():\n            password.append(str(random.randint(0, 9)))\n        elif char.isspace():\n            password.append(str(random.choice([random.randint(0, 9), chr(random.randint(97, 122))])))\n        else:\n            password.append(char)\n\n    return ''.join(password)", "raw_solution": "import string\nimport random\n\ndef task_func(text, seed=None):\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    password = []\n    for char in text:\n        if char.isalpha():\n            password.append(chr(random.randint(97, 122)))\n        elif char.isdigit():\n            password.append(str(random.randint(0, 9)))\n        elif char.isspace():\n            password.append(str(random.choice([random.randint(0, 9), chr(random.randint(97, 122))])))\n        else:\n            password.append(char)\n\n    return ''.join(password)"}
{"task_id": "BigCodeBench/961", "solution": "import os\nimport glob\nfrom collections import Counter\ndef task_func(directory, extensions=[\".txt\", \".docx\", \".xlsx\", \".csv\"], keep_zero=True):\n    \"\"\"\n    Traverses a given directory recursively to count files by specified extensions.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise OSError(f\"The specified directory {directory} does not exist.\")\n\n    counter = Counter()\n\n    for extension in extensions:\n        files = glob.glob(f\"{directory}/**/*{extension}\", recursive=True)\n        counter.update(files)\n\n    if not keep_zero:\n        counter = {k: v for k, v in counter.items() if v > 0}\n\n    return counter", "raw_solution": "import os\nimport glob\nfrom collections import Counter\n\ndef task_func(directory, extensions=[\".txt\", \".docx\", \".xlsx\", \".csv\"], keep_zero=True):\n    \"\"\"\n    Traverses a given directory recursively to count files by specified extensions.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise OSError(f\"The specified directory {directory} does not exist.\")\n\n    counter = Counter()\n\n    for extension in extensions:\n        files = glob.glob(f\"{directory}/**/*{extension}\", recursive=True)\n        counter.update(files)\n\n    if not keep_zero:\n        counter = {k: v for k, v in counter.items() if v > 0}\n\n    return counter"}
{"task_id": "BigCodeBench/962", "solution": "import os\nfrom pathlib import Path\nimport glob\nimport shutil\ndef task_func(source_directory: str, target_directory: str):\n    \"\"\"\n    Moves files with specific extensions from a source directory to a target directory,\n    handling naming conflicts by renaming duplicates.\n    \"\"\"\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n\n    extensions = [\".txt\", \".docx\", \".xlsx\", \".csv\"]\n    files_moved = 0\n\n    for extension in extensions:\n        source_files = glob.glob(f\"{source_directory}/**/*{extension}\", recursive=True)\n\n        for source_file in source_files:\n            source_path = Path(source_file)\n            target_path = Path(target_directory) / source_path.name\n\n            if not target_path.exists():\n                shutil.move(source_file, target_directory)\n                files_moved += 1\n            else:\n                base_name, ext = os.path.splitext(source_path.name)\n                i = 1\n\n                while target_path.exists():\n                    new_name = f\"{base_name}-{i}{ext}\"\n                    target_path = Path(target_directory) / new_name\n                    i += 1\n\n                shutil.move(source_file, target_path)\n                files_moved += 1\n\n    return files_moved", "raw_solution": "import os\nfrom pathlib import Path\nimport glob\nimport shutil\n\ndef task_func(source_directory: str, target_directory: str):\n    \"\"\"\n    Moves files with specific extensions from a source directory to a target directory,\n    handling naming conflicts by renaming duplicates.\n    \"\"\"\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n\n    extensions = [\".txt\", \".docx\", \".xlsx\", \".csv\"]\n    files_moved = 0\n\n    for extension in extensions:\n        source_files = glob.glob(f\"{source_directory}/**/*{extension}\", recursive=True)\n\n        for source_file in source_files:\n            source_path = Path(source_file)\n            target_path = Path(target_directory) / source_path.name\n\n            if not target_path.exists():\n                shutil.move(source_file, target_directory)\n                files_moved += 1\n            else:\n                base_name, ext = os.path.splitext(source_path.name)\n                i = 1\n\n                while target_path.exists():\n                    new_name = f\"{base_name}-{i}{ext}\"\n                    target_path = Path(target_directory) / new_name\n                    i += 1\n\n                shutil.move(source_file, target_path)\n                files_moved += 1\n\n    return files_moved"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory and save it as a zip file saved to a target directory.\n    The valid extensions are: ['.txt', '.docx', '.xlsx', '.csv'].\n    If the source_directory does not exist, it raises an OSError.\n    It returns the full path to the created zip file in the format \"/path/to/target_directory/zip_name.zip\".\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n\n    # Check if target directory exists, if not create it\n    if not os.path.isdir(target_directory):\n        os.makedirs(target_directory)\n\n    # Define valid file extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Get a list of all files with valid extensions in the source directory\n    files_to_zip = [file for file in glob.glob(f\"{source_directory}/*\") if os.path.splitext(file)[1] in valid_extensions]\n\n    # Create the zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files_to_zip:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return zip_path", "raw_solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory and save it as a zip file saved to a target directory.\n    The valid extensions are: ['.txt', '.docx', '.xlsx', '.csv'].\n    If the source_directory does not exist, it raises an OSError.\n    It returns the full path to the created zip file in the format \"/path/to/target_directory/zip_name.zip\".\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n\n    # Check if target directory exists, if not create it\n    if not os.path.isdir(target_directory):\n        os.makedirs(target_directory)\n\n    # Define valid file extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Get a list of all files with valid extensions in the source directory\n    files_to_zip = [file for file in glob.glob(f\"{source_directory}/*\") if os.path.splitext(file)[1] in valid_extensions]\n\n    # Create the zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files_to_zip:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return zip_path"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport openpyxl\nimport csv\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files and saves them in a target directory.\n    Each file's text content is captured and stored in a CSV with a single 'Text' column and no row indices.\n    This function will overwrite existing files in the target directory if they have the same names as the converted files.\n    If the source directory does not exist, this function raises a FileNotFoundError.\n    This function returns the number of files successfully converted to CSV.\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n\n    # Get a list of files in the source directory\n    files = os.listdir(source_directory)\n\n    # Initialize counter for successfully converted files\n    count = 0\n\n    # Loop through each file in the source directory\n    for file in files:\n        # Get the file extension\n        extension = Path(file).suffix\n\n        # Check if the file has a supported extension\n        if extension in ['.txt', '.docx', '.xlsx', '.csv']:\n            # Read the file content\n            if extension == '.txt':\n                with open(os.path.join(source_directory, file), 'r') as f:\n                    content = f.read()\n            elif extension == '.docx':\n                doc = docx.Document(os.path.join(source_directory, file))\n                content = ' '.join([p.text for p in doc.paragraphs])\n            elif extension == '.xlsx':\n                wb = openpyxl.load_workbook(os.path.join(source_directory, file))\n                content = ' '.join([str(cell.value) for row in wb.active.rows for cell in row])\n            else:  # .csv\n                with open(os.path.join(source_directory, file), 'r') as f:\n                    reader = csv.reader(f)\n                    content = ' '.join([', '.join(row) for row in reader])\n\n            # Create a DataFrame with the file content\n            df = pd.DataFrame({'Text': [content]})\n\n            # Save the DataFrame to a CSV file in the target directory\n            df.to_csv(os.path.join(target_directory, f\"{Path(file).stem}.csv\"), index=False)\n\n            # Increment the counter\n            count += 1\n\n    # Return the number of successfully converted files\n    return count", "raw_solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport openpyxl\nimport csv\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files and saves them in a target directory.\n    Each file's text content is captured and stored in a CSV with a single 'Text' column and no row indices.\n    This function will overwrite existing files in the target directory if they have the same names as the converted files.\n    If the source directory does not exist, this function raises a FileNotFoundError.\n    This function returns the number of files successfully converted to CSV.\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n\n    # Get a list of files in the source directory\n    files = os.listdir(source_directory)\n\n    # Initialize counter for successfully converted files\n    count = 0\n\n    # Loop through each file in the source directory\n    for file in files:\n        # Get the file extension\n        extension = Path(file).suffix\n\n        # Check if the file has a supported extension\n        if extension in ['.txt', '.docx', '.xlsx', '.csv']:\n            # Read the file content\n            if extension == '.txt':\n                with open(os.path.join(source_directory, file), 'r') as f:\n                    content = f.read()\n            elif extension == '.docx':\n                doc = docx.Document(os.path.join(source_directory, file))\n                content = ' '.join([p.text for p in doc.paragraphs])\n            elif extension == '.xlsx':\n                wb = openpyxl.load_workbook(os.path.join(source_directory, file))\n                content = ' '.join([str(cell.value) for row in wb.active.rows for cell in row])\n            else:  # .csv\n                with open(os.path.join(source_directory, file), 'r') as f:\n                    reader = csv.reader(f)\n                    content = ' '.join([', '.join(row) for row in reader])\n\n            # Create a DataFrame with the file content\n            df = pd.DataFrame({'Text': [content]})\n\n            # Save the DataFrame to a CSV file in the target directory\n            df.to_csv(os.path.join(target_directory, f\"{Path(file).stem}.csv\"), index=False)\n\n            # Increment the counter\n            count += 1\n\n    # Return the number of successfully converted files\n    return count"}
{"task_id": "BigCodeBench/965", "solution": "import os\nimport re\nimport shutil\ndef task_func(source_directory: str, target_directory: str, pattern: str = r\"\\d{4}\") -> int:\n    if not os.path.exists(source_directory) or not os.path.isdir(source_directory):\n        return 0\n\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    count = 0\n    for filename in os.listdir(source_directory):\n        if re.search(pattern, filename):\n            shutil.move(os.path.join(source_directory, filename), target_directory)\n            count += 1\n\n    return count", "raw_solution": "import os\nimport re\nimport shutil\n\ndef task_func(source_directory: str, target_directory: str, pattern: str = r\"\\d{4}\") -> int:\n    if not os.path.exists(source_directory) or not os.path.isdir(source_directory):\n        return 0\n\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    count = 0\n    for filename in os.listdir(source_directory):\n        if re.search(pattern, filename):\n            shutil.move(os.path.join(source_directory, filename), target_directory)\n            count += 1\n\n    return count"}
{"task_id": "BigCodeBench/966", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if not df.select_dtypes(include=[pd.np.number]).empty:\n        raise ValueError(\"DataFrame contains non-numeric data\")\n\n    cumulative_sum_df = df.cumsum()\n\n    fig, ax = plt.subplots()\n    cumulative_sum_df.plot(kind='bar', ax=ax)\n    ax.set_title('Cumulative Sum per Column')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    plt.legend(loc='upper left')\n\n    return cumulative_sum_df, fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if not df.select_dtypes(include=[pd.np.number]).empty:\n        raise ValueError(\"DataFrame contains non-numeric data\")\n\n    cumulative_sum_df = df.cumsum()\n\n    fig, ax = plt.subplots()\n    cumulative_sum_df.plot(kind='bar', ax=ax)\n    ax.set_title('Cumulative Sum per Column')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    plt.legend(loc='upper left')\n\n    return cumulative_sum_df, fig"}
{"task_id": "BigCodeBench/967", "solution": "import numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    \"\"\"\n    Calculates and plots both a given function and its cumulative integral over a specified range, using a linearly spaced range of x-values.\n    The plot includes a legend and labels for the x and y axes that include the function's name.\n    \"\"\"\n    # Generate x values\n    x = np.linspace(*x_range, num_points)\n\n    # Calculate function values\n    y = func(x)\n\n    # Calculate cumulative integral\n    integral, error = integrate.cumtrapz(y, x, initial=0)\n\n    # Create figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot function and integral\n    ax.plot(x, y, label=f'Function: {func.__name__}')\n    ax.plot(x, integral, label=f'Integral: {func.__name__}')\n\n    # Set labels and legend\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n\n    # Return the Axes object\n    return ax", "raw_solution": "import numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\n\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    \"\"\"\n    Calculates and plots both a given function and its cumulative integral over a specified range, using a linearly spaced range of x-values.\n    The plot includes a legend and labels for the x and y axes that include the function's name.\n    \"\"\"\n    # Generate x values\n    x = np.linspace(*x_range, num_points)\n\n    # Calculate function values\n    y = func(x)\n\n    # Calculate cumulative integral\n    integral, error = integrate.cumtrapz(y, x, initial=0)\n\n    # Create figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot function and integral\n    ax.plot(x, y, label=f'Function: {func.__name__}')\n    ax.plot(x, integral, label=f'Integral: {func.__name__}')\n\n    # Set labels and legend\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/968", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input data should be a pandas DataFrame\")\n    if data.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if not any(data.dtypes == 'number'):\n        raise ValueError(\"No numeric columns are present\")\n\n    # Calculate cumulative sum of each column\n    cumulative_sum = data.select_dtypes(include=['number']).cumsum()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(cumulative_sum, cmap='coolwarm', annot=True, fmt='.0f')\n\n    return heatmap", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input data should be a pandas DataFrame\")\n    if data.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if not any(data.dtypes == 'number'):\n        raise ValueError(\"No numeric columns are present\")\n\n    # Calculate cumulative sum of each column\n    cumulative_sum = data.select_dtypes(include=['number']).cumsum()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(cumulative_sum, cmap='coolwarm', annot=True, fmt='.0f')\n\n    return heatmap"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame cannot be empty\")\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame cannot contain NaN values\")\n    if not df.select_dtypes(include=[np.number]).empty:\n        raise TypeError(\"DataFrame must contain only numeric data types\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_normalized.cumsum()", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame cannot be empty\")\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame cannot contain NaN values\")\n    if not df.select_dtypes(include=[np.number]).empty:\n        raise TypeError(\"DataFrame must contain only numeric data types\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df_normalized.cumsum()"}
{"task_id": "BigCodeBench/970", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data: np.ndarray) -> plt.Axes:\n    if not isinstance(data, np.ndarray):\n        raise TypeError(\"Input data should be a NumPy array.\")\n    if np.issubdtype(data.dtype, np.number):\n        if np.any(np.isnan(data)) or np.any(data < 0):\n            raise ValueError(\"Input data should not contain negative numbers or NaNs.\")\n    else:\n        raise TypeError(\"Input data should contain only numeric values.\")\n\n    sorted_data = np.sort(data)\n    cumulative_prob = np.cumsum(sorted_data) / np.sum(sorted_data)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(len(data)), cumulative_prob, marker='o', linestyle='-')\n    plt.title(\"Cumulative Probability Plot\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Cumulative Probability\")\n    plt.grid(True)\n\n    return plt.gca()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data: np.ndarray) -> plt.Axes:\n    if not isinstance(data, np.ndarray):\n        raise TypeError(\"Input data should be a NumPy array.\")\n    if np.issubdtype(data.dtype, np.number):\n        if np.any(np.isnan(data)) or np.any(data < 0):\n            raise ValueError(\"Input data should not contain negative numbers or NaNs.\")\n    else:\n        raise TypeError(\"Input data should contain only numeric values.\")\n\n    sorted_data = np.sort(data)\n    cumulative_prob = np.cumsum(sorted_data) / np.sum(sorted_data)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(len(data)), cumulative_prob, marker='o', linestyle='-')\n    plt.title(\"Cumulative Probability Plot\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Cumulative Probability\")\n    plt.grid(True)\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n    \"\"\"\n    if not os.path.exists(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n\n    result = []\n    for filename in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, filename)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path)\n            creation_time = os.path.getctime(file_path)\n            modification_time = os.path.getmtime(file_path)\n            creation_time_iso = datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat()\n            modification_time_iso = datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat()\n            result.append((filename, file_size, creation_time_iso, modification_time_iso))\n\n    return result", "raw_solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n    \"\"\"\n    if not os.path.exists(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n\n    result = []\n    for filename in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, filename)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path)\n            creation_time = os.path.getctime(file_path)\n            modification_time = os.path.getmtime(file_path)\n            creation_time_iso = datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat()\n            modification_time_iso = datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat()\n            result.append((filename, file_size, creation_time_iso, modification_time_iso))\n\n    return result"}
{"task_id": "BigCodeBench/972", "solution": "import pathlib\nimport os\ndef task_func(path: str, delimiter: str = os.path.sep) -> list:\n    \"\"\"\n    Validates that a given file path does not contain invalid characters for file paths then splits it into path components using a specified delimiter.\n    \"\"\"\n    # Replace backslashes with forward slashes\n    path = path.replace('\\\\', '/')\n\n    # Define invalid characters\n    invalid_chars = ['<', '>', ':', '\"', '|', '?', '*']\n\n    # Check for invalid characters in the path\n    for char in invalid_chars:\n        if char in path:\n            raise ValueError(f\"Invalid character '{char}' found in the path.\")\n\n    # Split the path into components using the specified delimiter\n    components = path.split(delimiter)\n\n    return components", "raw_solution": "import pathlib\nimport os\n\ndef task_func(path: str, delimiter: str = os.path.sep) -> list:\n    \"\"\"\n    Validates that a given file path does not contain invalid characters for file paths then splits it into path components using a specified delimiter.\n    \"\"\"\n    # Replace backslashes with forward slashes\n    path = path.replace('\\\\', '/')\n\n    # Define invalid characters\n    invalid_chars = ['<', '>', ':', '\"', '|', '?', '*']\n\n    # Check for invalid characters in the path\n    for char in invalid_chars:\n        if char in path:\n            raise ValueError(f\"Invalid character '{char}' found in the path.\")\n\n    # Split the path into components using the specified delimiter\n    components = path.split(delimiter)\n\n    return components"}
{"task_id": "BigCodeBench/973", "solution": "import os\nimport shutil\ndef task_func(path, delimiter=\"/\"):\n    if not isinstance(path, str):\n        raise ValueError(\"'path' must be a string\")\n    if not path:\n        raise ValueError(\"'path' cannot be empty\")\n    if delimiter not in path:\n        raise ValueError(\"'path' must contain the delimiter\")\n    if not os.path.exists(path):\n        raise FileNotFoundError(\"'path' does not exist in the filesystem\")\n\n    components = path.split(delimiter)\n    result = []\n\n    for component in components:\n        if not component:\n            continue\n        usage = shutil.disk_usage(component)\n        result.append((component, {'total': usage.total, 'used': usage.used, 'free': usage.free}))\n\n    return result", "raw_solution": "import os\nimport shutil\n\ndef task_func(path, delimiter=\"/\"):\n    if not isinstance(path, str):\n        raise ValueError(\"'path' must be a string\")\n    if not path:\n        raise ValueError(\"'path' cannot be empty\")\n    if delimiter not in path:\n        raise ValueError(\"'path' must contain the delimiter\")\n    if not os.path.exists(path):\n        raise FileNotFoundError(\"'path' does not exist in the filesystem\")\n\n    components = path.split(delimiter)\n    result = []\n\n    for component in components:\n        if not component:\n            continue\n        usage = shutil.disk_usage(component)\n        result.append((component, {'total': usage.total, 'used': usage.used, 'free': usage.free}))\n\n    return result"}
{"task_id": "BigCodeBench/974", "solution": "import shutil\nimport pathlib\nfrom typing import Tuple, List\ndef task_func(source_path: str, destination_path: str) -> Tuple[str, List[str]]:\n    \"\"\"\n    Lists files in the specified source directory without descending into subdirectories and copies them to a destination directory.\n    Raises ValueError if source_path does not exist or is not a directory.\n    Returns a tuple containing the name of the source directory and a list of filenames (not full paths) that were copied.\n    \"\"\"\n    source_path = pathlib.Path(source_path)\n    if not source_path.exists():\n        raise ValueError(f\"Source path {source_path} does not exist\")\n    if not source_path.is_dir():\n        raise ValueError(f\"Source path {source_path} is not a directory\")\n\n    destination_path = pathlib.Path(destination_path)\n    if not destination_path.exists():\n        destination_path.mkdir(parents=True)\n\n    copied_files = []\n    for file in source_path.iterdir():\n        if file.is_file():\n            shutil.copy2(file, destination_path)\n            copied_files.append(file.name)\n\n    return source_path.name, copied_files", "raw_solution": "import shutil\nimport pathlib\nfrom typing import Tuple, List\n\ndef task_func(source_path: str, destination_path: str) -> Tuple[str, List[str]]:\n    \"\"\"\n    Lists files in the specified source directory without descending into subdirectories and copies them to a destination directory.\n    Raises ValueError if source_path does not exist or is not a directory.\n    Returns a tuple containing the name of the source directory and a list of filenames (not full paths) that were copied.\n    \"\"\"\n    source_path = pathlib.Path(source_path)\n    if not source_path.exists():\n        raise ValueError(f\"Source path {source_path} does not exist\")\n    if not source_path.is_dir():\n        raise ValueError(f\"Source path {source_path} is not a directory\")\n\n    destination_path = pathlib.Path(destination_path)\n    if not destination_path.exists():\n        destination_path.mkdir(parents=True)\n\n    copied_files = []\n    for file in source_path.iterdir():\n        if file.is_file():\n            shutil.copy2(file, destination_path)\n            copied_files.append(file.name)\n\n    return source_path.name, copied_files"}
{"task_id": "BigCodeBench/975", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(rows, columns=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=0) -> pd.DataFrame:\n    np.random.seed(seed)\n    data = np.random.rand(rows, len(columns))\n    df = pd.DataFrame(data, columns=columns)\n    df = df.reindex(np.random.permutation(df.columns), axis=1)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(rows, columns=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=0) -> pd.DataFrame:\n    np.random.seed(seed)\n    data = np.random.rand(rows, len(columns))\n    df = pd.DataFrame(data, columns=columns)\n    df = df.reindex(np.random.permutation(df.columns), axis=1)\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/976", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(records: np.ndarray, random_seed: int = 0) -> pd.DataFrame:\n    \"\"\"\n    Randomly shuffles the given array's features, normalizes its values,\n    and converts to a DataFrame with shuffled feature names.\n    \"\"\"\n    if records.ndim != 2:\n        raise ValueError(\"records must be a 2D array\")\n\n    np.random.seed(random_seed)\n    np.random.shuffle(records)\n\n    scaler = StandardScaler()\n    records = scaler.fit_transform(records)\n\n    feature_names = [f\"f{i+1}\" for i in range(records.shape[1])]\n    np.random.shuffle(feature_names)\n\n    df = pd.DataFrame(records, columns=feature_names)\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(records: np.ndarray, random_seed: int = 0) -> pd.DataFrame:\n    \"\"\"\n    Randomly shuffles the given array's features, normalizes its values,\n    and converts to a DataFrame with shuffled feature names.\n    \"\"\"\n    if records.ndim != 2:\n        raise ValueError(\"records must be a 2D array\")\n\n    np.random.seed(random_seed)\n    np.random.shuffle(records)\n\n    scaler = StandardScaler()\n    records = scaler.fit_transform(records)\n\n    feature_names = [f\"f{i+1}\" for i in range(records.shape[1])]\n    np.random.shuffle(feature_names)\n\n    df = pd.DataFrame(records, columns=feature_names)\n\n    return df"}
{"task_id": "BigCodeBench/977", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(array, features=None, seed=None):\n    \"\"\"\n    Shuffles the columns of a given 2D numpy array and visualizes it as a heatmap.\n    If 'features' is provided, it uses the features list as labels for the heatmap's x-axis.\n    If 'features' is not provided, it defaults to strings of the numerical labels starting from 1 up to the number of columns in the array.\n    If 'array' is empty or not 2-dimensional, it raises a ValueError.\n    \"\"\"\n    if len(array.shape) != 2:\n        raise ValueError(\"'array' must be a 2-dimensional numpy array.\")\n    if array.size == 0:\n        raise ValueError(\"'array' cannot be empty.\")\n    if features is not None and len(features) != array.shape[1]:\n        raise ValueError(\"'features' must match the number of columns in 'array'.\")\n\n    if seed is not None:\n        np.random.seed(seed)\n    shuffled_array = np.random.permutation(array.T).T\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    if features is None:\n        features = [str(i+1) for i in range(array.shape[1])]\n    sns.heatmap(shuffled_array, annot=True, fmt=\".2f\", cmap='viridis', cbar=True,\n                xticklabels=features, yticklabels=range(1, array.shape[0]+1), ax=ax)\n    plt.xlabel('Features')\n    plt.ylabel('Samples')\n    plt.title('Heatmap of Shuffled Columns')\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(array, features=None, seed=None):\n    \"\"\"\n    Shuffles the columns of a given 2D numpy array and visualizes it as a heatmap.\n    If 'features' is provided, it uses the features list as labels for the heatmap's x-axis.\n    If 'features' is not provided, it defaults to strings of the numerical labels starting from 1 up to the number of columns in the array.\n    If 'array' is empty or not 2-dimensional, it raises a ValueError.\n    \"\"\"\n    if len(array.shape) != 2:\n        raise ValueError(\"'array' must be a 2-dimensional numpy array.\")\n    if array.size == 0:\n        raise ValueError(\"'array' cannot be empty.\")\n    if features is not None and len(features) != array.shape[1]:\n        raise ValueError(\"'features' must match the number of columns in 'array'.\")\n\n    if seed is not None:\n        np.random.seed(seed)\n    shuffled_array = np.random.permutation(array.T).T\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    if features is None:\n        features = [str(i+1) for i in range(array.shape[1])]\n    sns.heatmap(shuffled_array, annot=True, fmt=\".2f\", cmap='viridis', cbar=True,\n                xticklabels=features, yticklabels=range(1, array.shape[0]+1), ax=ax)\n    plt.xlabel('Features')\n    plt.ylabel('Samples')\n    plt.title('Heatmap of Shuffled Columns')\n    return ax"}
{"task_id": "BigCodeBench/978", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\ndef task_func(array, seed=None):\n    if array.ndim != 2:\n        raise ValueError(\"Input array must be 2D\")\n    if array.size == 0:\n        return pd.DataFrame()\n\n    np.random.seed(seed)\n    np.random.shuffle(array.T)\n\n    pca = PCA(n_components=2)\n    pca.fit(array)\n\n    df = pd.DataFrame(pca.components_, columns=['PC1', 'PC2'])\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\n\ndef task_func(array, seed=None):\n    if array.ndim != 2:\n        raise ValueError(\"Input array must be 2D\")\n    if array.size == 0:\n        return pd.DataFrame()\n\n    np.random.seed(seed)\n    np.random.shuffle(array.T)\n\n    pca = PCA(n_components=2)\n    pca.fit(array)\n\n    df = pd.DataFrame(pca.components_, columns=['PC1', 'PC2'])\n    return df"}
{"task_id": "BigCodeBench/979", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\ndef task_func(\n    feature_array,\n    target_array,\n    feature_names=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"],\n    target_name=\"target\",\n    seed=None,\n):\n    \"\"\"\n    Shuffles the columns of a given numpy array and trains a Random Forest Classifier on the shuffled data.\n    \"\"\"\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Create a DataFrame from the feature and target arrays\n    df = pd.DataFrame(data=np.column_stack((feature_array, target_array)),\n                      columns=feature_names + [target_name])\n\n    # Shuffle the columns of the DataFrame\n    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n    # Split the DataFrame into features and target\n    X = df[feature_names]\n    y = df[target_name]\n\n    # Train a Random Forest Classifier on the shuffled data\n    clf = RandomForestClassifier(random_state=seed)\n    clf.fit(X, y)\n\n    return clf", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(\n    feature_array,\n    target_array,\n    feature_names=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"],\n    target_name=\"target\",\n    seed=None,\n):\n    \"\"\"\n    Shuffles the columns of a given numpy array and trains a Random Forest Classifier on the shuffled data.\n    \"\"\"\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Create a DataFrame from the feature and target arrays\n    df = pd.DataFrame(data=np.column_stack((feature_array, target_array)),\n                      columns=feature_names + [target_name])\n\n    # Shuffle the columns of the DataFrame\n    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n    # Split the DataFrame into features and target\n    X = df[feature_names]\n    y = df[target_name]\n\n    # Train a Random Forest Classifier on the shuffled data\n    clf = RandomForestClassifier(random_state=seed)\n    clf.fit(X, y)\n\n    return clf"}
{"task_id": "BigCodeBench/980", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if not df.select_dtypes(include=[np.number]).columns.any():\n        raise ValueError(\"No numeric columns are present\")\n\n    # Standardize the numeric columns\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df.select_dtypes(include=[np.number])), columns=df.select_dtypes(include=[np.number]).columns)\n\n    # Create the correlation matrix\n    corr_matrix = df_standardized.corr()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n\n    return df_standardized, plt.gcf()", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if not df.select_dtypes(include=[np.number]).columns.any():\n        raise ValueError(\"No numeric columns are present\")\n\n    # Standardize the numeric columns\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df.select_dtypes(include=[np.number])), columns=df.select_dtypes(include=[np.number]).columns)\n\n    # Create the correlation matrix\n    corr_matrix = df_standardized.corr()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n\n    return df_standardized, plt.gcf()"}
{"task_id": "BigCodeBench/981", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(start_date, end_date, num_series, seed=None):\n    \"\"\"\n    Generates a DataFrame with multiple random integer time series (each ranging from 0 to 100) from a start date to an end date, then returns the generated time series on a line plot.\n    \"\"\"\n    # Validate inputs\n    if start_date > end_date:\n        raise ValueError(\"start_date cannot be later than end_date\")\n    if num_series < 1:\n        raise ValueError(\"num_series must be at least 1\")\n\n    # Set random seed\n    if seed is not None:\n        random.seed(seed)\n\n    # Generate dates\n    dates = pd.date_range(start=start_date, end=end_date)\n\n    # Generate time series\n    data = {f'series_{i+1}': [random.randint(0, 100) for _ in range(len(dates))] for i in range(num_series)}\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=dates)\n\n    # Plot time series\n    plt.figure(figsize=(10, 6))\n    for col in df.columns:\n        plt.plot(df.index, df[col], label=col)\n\n    plt.title(\"Random Time Series\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Value\")\n    plt.legend()\n\n    return df, plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport random\n\ndef task_func(start_date, end_date, num_series, seed=None):\n    \"\"\"\n    Generates a DataFrame with multiple random integer time series (each ranging from 0 to 100) from a start date to an end date, then returns the generated time series on a line plot.\n    \"\"\"\n    # Validate inputs\n    if start_date > end_date:\n        raise ValueError(\"start_date cannot be later than end_date\")\n    if num_series < 1:\n        raise ValueError(\"num_series must be at least 1\")\n\n    # Set random seed\n    if seed is not None:\n        random.seed(seed)\n\n    # Generate dates\n    dates = pd.date_range(start=start_date, end=end_date)\n\n    # Generate time series\n    data = {f'series_{i+1}': [random.randint(0, 100) for _ in range(len(dates))] for i in range(num_series)}\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=dates)\n\n    # Plot time series\n    plt.figure(figsize=(10, 6))\n    for col in df.columns:\n        plt.plot(df.index, df[col], label=col)\n\n    plt.title(\"Random Time Series\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Value\")\n    plt.legend()\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/982", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport pandas as pd\ndef task_func(df, column, bins=30, density=True, alpha=0.6, color=\"g\", seed=None):\n    \"\"\"\n    Plots a histogram for a specified column of a pandas DataFrame and overlays it with a fitted normal distribution curve.\n    \"\"\"\n    # Set the random seed for reproducibility\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Extract the data from the specified column\n    data = df[column].dropna()\n\n    # Create the histogram\n    ax = data.hist(bins=bins, density=density, alpha=alpha, color=color, label='Histogram')\n\n    # Fit a normal distribution to the data\n    mean, std = norm.fit(data)\n\n    # Create the x values for the normal distribution\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n\n    # Create the normal distribution\n    p = norm.pdf(x, mean, std)\n\n    # Overlay the normal distribution on the histogram\n    ax.plot(x, p, 'k', linewidth=2, label='Normal Fit')\n\n    # Add legend\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport pandas as pd\n\ndef task_func(df, column, bins=30, density=True, alpha=0.6, color=\"g\", seed=None):\n    \"\"\"\n    Plots a histogram for a specified column of a pandas DataFrame and overlays it with a fitted normal distribution curve.\n    \"\"\"\n    # Set the random seed for reproducibility\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Extract the data from the specified column\n    data = df[column].dropna()\n\n    # Create the histogram\n    ax = data.hist(bins=bins, density=density, alpha=alpha, color=color, label='Histogram')\n\n    # Fit a normal distribution to the data\n    mean, std = norm.fit(data)\n\n    # Create the x values for the normal distribution\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n\n    # Create the normal distribution\n    p = norm.pdf(x, mean, std)\n\n    # Overlay the normal distribution on the histogram\n    ax.plot(x, p, 'k', linewidth=2, label='Normal Fit')\n\n    # Add legend\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/983", "solution": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    if not np.issubdtype(df.dtypes, np.number).all():\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n\n    covariance_df = df.cov()\n    pair_plot = sns.pairplot(df)\n\n    return covariance_df, pair_plot", "raw_solution": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    if not np.issubdtype(df.dtypes, np.number).all():\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n\n    covariance_df = df.cov()\n    pair_plot = sns.pairplot(df)\n\n    return covariance_df, pair_plot"}
{"task_id": "BigCodeBench/984", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\ndef task_func(df, x_column, y_column):\n    # Create a scatter plot\n    plt.scatter(df[x_column], df[y_column])\n\n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(df[[x_column]], df[y_column])\n\n    # Generate x values for the regression line\n    x_range = np.linspace(df[x_column].min(), df[x_column].max(), 100)\n\n    # Predict y values for the regression line\n    y_range = model.predict(x_range.reshape(-1, 1))\n\n    # Plot the regression line\n    plt.plot(x_range, y_range, color='red')\n\n    return plt.gca()\ndf = pd.DataFrame({\n    'x': np.random.normal(0, 1, 100),\n    'y': np.random.normal(0, 1, 100)\n})", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\ndef task_func(df, x_column, y_column):\n    # Create a scatter plot\n    plt.scatter(df[x_column], df[y_column])\n\n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(df[[x_column]], df[y_column])\n\n    # Generate x values for the regression line\n    x_range = np.linspace(df[x_column].min(), df[x_column].max(), 100)\n\n    # Predict y values for the regression line\n    y_range = model.predict(x_range.reshape(-1, 1))\n\n    # Plot the regression line\n    plt.plot(x_range, y_range, color='red')\n\n    return plt.gca()\n\n# Test the function\ndf = pd.DataFrame({\n    'x': np.random.normal(0, 1, 100),\n    'y': np.random.normal(0, 1, 100)\n})\nax = task_func(df, 'x', 'y')\nassert isinstance(ax, plt.Axes)"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on provided JSON data.\n    \"\"\"\n    # Check if the JSON data is malformed, empty, or contains non-string country names, non-numeric or negative populations\n    try:\n        data = json.loads(json_data)\n        if not isinstance(data, dict):\n            raise ValueError(\"JSON data is malformed\")\n        if not data:\n            raise ValueError(\"JSON data is empty\")\n        for country, population in data.items():\n            if not isinstance(country, str):\n                raise ValueError(\"Non-string country names are not allowed\")\n            if not isinstance(population, (int, float)) or population < 0:\n                raise ValueError(\"Non-numeric or negative populations are not allowed\")\n    except json.JSONDecodeError:\n        raise ValueError(\"JSON data is malformed\")\n\n    # Create a DataFrame from the JSON data\n    df = pd.DataFrame(list(data.items()), columns=[\"Country\", \"Population\"])\n    df[\"Population\"] = df[\"Population\"].apply(lambda x: int(math.floor(x)))  # Round down to the nearest integer\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Write the DataFrame to a CSV file\n    file_path = os.path.join(output_dir, file_name)\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"Cannot write to the specified directory\")\n\n    return file_path, df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on provided JSON data.\n    \"\"\"\n    # Check if the JSON data is malformed, empty, or contains non-string country names, non-numeric or negative populations\n    try:\n        data = json.loads(json_data)\n        if not isinstance(data, dict):\n            raise ValueError(\"JSON data is malformed\")\n        if not data:\n            raise ValueError(\"JSON data is empty\")\n        for country, population in data.items():\n            if not isinstance(country, str):\n                raise ValueError(\"Non-string country names are not allowed\")\n            if not isinstance(population, (int, float)) or population < 0:\n                raise ValueError(\"Non-numeric or negative populations are not allowed\")\n    except json.JSONDecodeError:\n        raise ValueError(\"JSON data is malformed\")\n\n    # Create a DataFrame from the JSON data\n    df = pd.DataFrame(list(data.items()), columns=[\"Country\", \"Population\"])\n    df[\"Population\"] = df[\"Population\"].apply(lambda x: int(math.floor(x)))  # Round down to the nearest integer\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Write the DataFrame to a CSV file\n    file_path = os.path.join(output_dir, file_name)\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"Cannot write to the specified directory\")\n\n    return file_path, df"}
{"task_id": "BigCodeBench/986", "solution": "import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\ndef task_func(json_data: str, key_path: list):\n    \"\"\"\n    Extracts and visualizes numerical data from a JSON structure based on a specified path of keys.\n    Raises KeyError if a specified key is not found.\n    Raises ValueError if no numeric data is found, or the data string is empty or corrupted.\n    Returns a matplotlib figure showing a boxplot of the data values.\n    \"\"\"\n    data = json.loads(json_data)\n    for key in key_path:\n        try:\n            data = data[key]\n        except KeyError:\n            raise KeyError(f\"Key '{key}' not found in JSON data\")\n\n    if not isinstance(data, (list, np.ndarray, pd.Series)):\n        raise ValueError(\"Data is not a list, numpy array, or pandas Series\")\n\n    data = np.array(data)\n    if not np.issubdtype(data.dtype, np.number):\n        raise ValueError(\"Data is not numeric\")\n\n    if data.size == 0:\n        raise ValueError(\"Data is empty\")\n\n    fig, ax = plt.subplots()\n    sns.boxplot(data=data, ax=ax)\n    ax.set_title('Boxplot of Data Values')\n    return fig", "raw_solution": "import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndef task_func(json_data: str, key_path: list):\n    \"\"\"\n    Extracts and visualizes numerical data from a JSON structure based on a specified path of keys.\n    Raises KeyError if a specified key is not found.\n    Raises ValueError if no numeric data is found, or the data string is empty or corrupted.\n    Returns a matplotlib figure showing a boxplot of the data values.\n    \"\"\"\n    data = json.loads(json_data)\n    for key in key_path:\n        try:\n            data = data[key]\n        except KeyError:\n            raise KeyError(f\"Key '{key}' not found in JSON data\")\n\n    if not isinstance(data, (list, np.ndarray, pd.Series)):\n        raise ValueError(\"Data is not a list, numpy array, or pandas Series\")\n\n    data = np.array(data)\n    if not np.issubdtype(data.dtype, np.number):\n        raise ValueError(\"Data is not numeric\")\n\n    if data.size == 0:\n        raise ValueError(\"Data is empty\")\n\n    fig, ax = plt.subplots()\n    sns.boxplot(data=data, ax=ax)\n    ax.set_title('Boxplot of Data Values')\n    return fig"}
{"task_id": "BigCodeBench/987", "solution": "import json\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(json_data: str, data_key: str) -> tuple:\n    \"\"\"\n    Processes a JSON string to extract numerical data, Min-Max normalize them, and generate a line plot.\n    \"\"\"\n    # Load JSON data\n    data = json.loads(json_data)\n\n    # Extract numerical data\n    try:\n        num_data = [float(data[data_key][i]) for i in range(len(data[data_key]))]\n    except KeyError:\n        raise KeyError(f\"Key path '{data_key}' not found in the given data.\")\n\n    # Convert to pandas Series\n    original_data = pd.Series(num_data, name=\"Original Data\")\n\n    # Check if data is empty\n    if original_data.empty:\n        return original_data, None, None\n\n    # Min-Max normalize data\n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(original_data.values.reshape(-1, 1))\n    normalized_data = pd.Series(normalized_data.flatten(), name=\"Normalized Data\")\n\n    # Generate line plot\n    fig, ax = plt.subplots()\n    original_data.plot(ax=ax, label=\"Original Data\")\n    normalized_data.plot(ax=ax, label=\"Normalized Data\")\n    ax.set_title(\"Comparison of Original and Normalized Data\")\n    ax.set_xlabel(\"Index\")\n    ax.set_ylabel(\"Value\")\n    ax.legend()\n\n    return original_data, normalized_data, ax", "raw_solution": "import json\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(json_data: str, data_key: str) -> tuple:\n    \"\"\"\n    Processes a JSON string to extract numerical data, Min-Max normalize them, and generate a line plot.\n    \"\"\"\n    # Load JSON data\n    data = json.loads(json_data)\n\n    # Extract numerical data\n    try:\n        num_data = [float(data[data_key][i]) for i in range(len(data[data_key]))]\n    except KeyError:\n        raise KeyError(f\"Key path '{data_key}' not found in the given data.\")\n\n    # Convert to pandas Series\n    original_data = pd.Series(num_data, name=\"Original Data\")\n\n    # Check if data is empty\n    if original_data.empty:\n        return original_data, None, None\n\n    # Min-Max normalize data\n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(original_data.values.reshape(-1, 1))\n    normalized_data = pd.Series(normalized_data.flatten(), name=\"Normalized Data\")\n\n    # Generate line plot\n    fig, ax = plt.subplots()\n    original_data.plot(ax=ax, label=\"Original Data\")\n    normalized_data.plot(ax=ax, label=\"Normalized Data\")\n    ax.set_title(\"Comparison of Original and Normalized Data\")\n    ax.set_xlabel(\"Index\")\n    ax.set_ylabel(\"Value\")\n    ax.legend()\n\n    return original_data, normalized_data, ax"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n    \"\"\"\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The specified directory {dir_path} does not exist or is not a directory.\")\n\n    result = {}\n\n    for item in os.listdir(dir_path):\n        item_path = os.path.join(dir_path, item)\n\n        if os.path.isfile(item_path):\n            for predicate in predicates:\n                if re.search(predicate, item):\n                    result[item] = True\n                    break\n            else:\n                result[item] = False\n\n        elif os.path.isdir(item_path):\n            for predicate in predicates:\n                if re.search(predicate, item):\n                    result[item] = True\n                    break\n            else:\n                result[item] = False\n\n    return result", "raw_solution": "import os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n    \"\"\"\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The specified directory {dir_path} does not exist or is not a directory.\")\n\n    result = {}\n\n    for item in os.listdir(dir_path):\n        item_path = os.path.join(dir_path, item)\n\n        if os.path.isfile(item_path):\n            for predicate in predicates:\n                if re.search(predicate, item):\n                    result[item] = True\n                    break\n            else:\n                result[item] = False\n\n        elif os.path.isdir(item_path):\n            for predicate in predicates:\n                if re.search(predicate, item):\n                    result[item] = True\n                    break\n            else:\n                result[item] = False\n\n    return result"}
{"task_id": "BigCodeBench/989", "solution": "import random\nimport string\ndef task_func(length: int, predicates: list, seed: int = None):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    valid_predicates = ['lowercase', 'uppercase', 'digits', 'punctuation']\n    for predicate in predicates:\n        if predicate not in valid_predicates:\n            raise KeyError(f\"Predicate '{predicate}' is not recognized\")\n\n    charset = ''\n    for predicate in valid_predicates:\n        if predicate in predicates:\n            if predicate == 'lowercase':\n                charset += string.ascii_lowercase\n            elif predicate == 'uppercase':\n                charset += string.ascii_uppercase\n            elif predicate == 'digits':\n                charset += string.digits\n            elif predicate == 'punctuation':\n                charset += string.punctuation\n\n    random_string = ''.join(random.choices(charset, k=length))\n\n    characteristics = {}\n    if 'lowercase' in predicates:\n        characteristics['lowercase'] = sum(c.islower() for c in random_string)\n    if 'uppercase' in predicates:\n        characteristics['uppercase'] = sum(c.isupper() for c in random_string)\n    if 'digits' in predicates:\n        characteristics['digits'] = sum(c.isdigit() for c in random_string)\n    if 'punctuation' in predicates:\n        characteristics['punctuation'] = sum(c in string.punctuation for c in random_string)\n\n    return (random_string, characteristics)", "raw_solution": "import random\nimport string\n\ndef task_func(length: int, predicates: list, seed: int = None):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    valid_predicates = ['lowercase', 'uppercase', 'digits', 'punctuation']\n    for predicate in predicates:\n        if predicate not in valid_predicates:\n            raise KeyError(f\"Predicate '{predicate}' is not recognized\")\n\n    charset = ''\n    for predicate in valid_predicates:\n        if predicate in predicates:\n            if predicate == 'lowercase':\n                charset += string.ascii_lowercase\n            elif predicate == 'uppercase':\n                charset += string.ascii_uppercase\n            elif predicate == 'digits':\n                charset += string.digits\n            elif predicate == 'punctuation':\n                charset += string.punctuation\n\n    random_string = ''.join(random.choices(charset, k=length))\n\n    characteristics = {}\n    if 'lowercase' in predicates:\n        characteristics['lowercase'] = sum(c.islower() for c in random_string)\n    if 'uppercase' in predicates:\n        characteristics['uppercase'] = sum(c.isupper() for c in random_string)\n    if 'digits' in predicates:\n        characteristics['digits'] = sum(c.isdigit() for c in random_string)\n    if 'punctuation' in predicates:\n        characteristics['punctuation'] = sum(c in string.punctuation for c in random_string)\n\n    return (random_string, characteristics)"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    # Convert hexadecimal to bytes\n    bytes_obj = binascii.unhexlify(hex_string)\n\n    # Convert bytes to UTF-8 string\n    utf8_str = bytes_obj.decode('utf-8')\n\n    # Initialize dictionary to store encodings\n    encodings = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_obj).decode('utf-8'),\n        'utf-8': utf8_str,\n        'utf-16': utf8_str,\n        'utf-32': utf8_str,\n        'ASCII': 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_str),\n        'ROT13': codecs.encode(utf8_str, 'rot_13')\n    }\n\n    # Check if UTF-16, UTF-32, and ASCII encodings are possible\n    try:\n        encodings['utf-16'] = utf8_str.encode('utf-16').hex()\n        encodings['utf-32'] = utf8_str.encode('utf-32').hex()\n        encodings['ASCII'] = utf8_str\n        for char in utf8_str:\n            if ord(char) > 127:\n                encodings['ASCII'] = 'Not representable in ASCII'\n                break\n    except UnicodeEncodeError:\n        pass\n\n    return encodings", "raw_solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    # Convert hexadecimal to bytes\n    bytes_obj = binascii.unhexlify(hex_string)\n\n    # Convert bytes to UTF-8 string\n    utf8_str = bytes_obj.decode('utf-8')\n\n    # Initialize dictionary to store encodings\n    encodings = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_obj).decode('utf-8'),\n        'utf-8': utf8_str,\n        'utf-16': utf8_str,\n        'utf-32': utf8_str,\n        'ASCII': 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_str),\n        'ROT13': codecs.encode(utf8_str, 'rot_13')\n    }\n\n    # Check if UTF-16, UTF-32, and ASCII encodings are possible\n    try:\n        encodings['utf-16'] = utf8_str.encode('utf-16').hex()\n        encodings['utf-32'] = utf8_str.encode('utf-32').hex()\n        encodings['ASCII'] = utf8_str\n        for char in utf8_str:\n            if ord(char) > 127:\n                encodings['ASCII'] = 'Not representable in ASCII'\n                break\n    except UnicodeEncodeError:\n        pass\n\n    return encodings"}
{"task_id": "BigCodeBench/991", "solution": "import binascii\nimport string\nimport random\ndef task_func(length):\n    hex_string = ''.join(random.choices(string.hexdigits, k=length))\n    try:\n        ascii_string = binascii.unhexlify(hex_string).decode('utf-8')\n    except UnicodeDecodeError:\n        ascii_string = 'Decoding failed. The resulting string may contain non-printable characters.'\n    return ascii_string", "raw_solution": "import binascii\nimport string\nimport random\n\ndef task_func(length):\n    hex_string = ''.join(random.choices(string.hexdigits, k=length))\n    try:\n        ascii_string = binascii.unhexlify(hex_string).decode('utf-8')\n    except UnicodeDecodeError:\n        ascii_string = 'Decoding failed. The resulting string may contain non-printable characters.'\n    return ascii_string"}
{"task_id": "BigCodeBench/992", "solution": "import sys\nimport sqlite3\nPATH_TO_APPEND = \"path/to/whatever\"\nDATABASE = \"path/to/database.db\"\ndef task_func(path_to_append=PATH_TO_APPEND, database=DATABASE):\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Connect to the SQLite database\n    conn = sqlite3.connect(database)\n    c = conn.cursor()\n\n    # Create the table if it doesn't exist\n    c.execute('''\n        CREATE TABLE IF NOT EXISTS paths\n        (path TEXT PRIMARY KEY)\n    ''')\n\n    # Insert the path into the database, avoiding duplicates\n    c.execute('''\n        INSERT OR IGNORE INTO paths VALUES (?)\n    ''', (path_to_append,))\n\n    # Commit the changes and close the connection\n    conn.commit()\n    conn.close()\n\n    # Return the path that was appended to sys.path and inserted into the database\n    return path_to_append", "raw_solution": "import sys\nimport sqlite3\n\n# Constants\nPATH_TO_APPEND = \"path/to/whatever\"\nDATABASE = \"path/to/database.db\"\n\ndef task_func(path_to_append=PATH_TO_APPEND, database=DATABASE):\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Connect to the SQLite database\n    conn = sqlite3.connect(database)\n    c = conn.cursor()\n\n    # Create the table if it doesn't exist\n    c.execute('''\n        CREATE TABLE IF NOT EXISTS paths\n        (path TEXT PRIMARY KEY)\n    ''')\n\n    # Insert the path into the database, avoiding duplicates\n    c.execute('''\n        INSERT OR IGNORE INTO paths VALUES (?)\n    ''', (path_to_append,))\n\n    # Commit the changes and close the connection\n    conn.commit()\n    conn.close()\n\n    # Return the path that was appended to sys.path and inserted into the database\n    return path_to_append"}
{"task_id": "BigCodeBench/993", "solution": "import re\nfrom scipy.stats import gaussian_kde\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Calculate the lengths of the words\n    lengths = [len(word) for word in words]\n\n    # Create a subplot\n    fig, ax = plt.subplots()\n\n    # Plot the histogram of word lengths\n    ax.hist(lengths, bins=20, density=True, alpha=0.6, color='g', label='Histogram')\n\n    # Calculate the kernel density estimate (KDE)\n    kde = gaussian_kde(lengths)\n\n    # Determine the x-range for the KDE plot\n    x_range = range(min(lengths), max(lengths))\n\n    # Plot the KDE\n    ax.plot(x_range, kde(x_range), color='r', label='KDE')\n\n    # Set the plot title and labels\n    ax.set_title('Distribution of Word Lengths')\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n\n    # Add a legend\n    ax.legend()\n\n    return ax", "raw_solution": "import re\nfrom scipy.stats import gaussian_kde\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Calculate the lengths of the words\n    lengths = [len(word) for word in words]\n\n    # Create a subplot\n    fig, ax = plt.subplots()\n\n    # Plot the histogram of word lengths\n    ax.hist(lengths, bins=20, density=True, alpha=0.6, color='g', label='Histogram')\n\n    # Calculate the kernel density estimate (KDE)\n    kde = gaussian_kde(lengths)\n\n    # Determine the x-range for the KDE plot\n    x_range = range(min(lengths), max(lengths))\n\n    # Plot the KDE\n    ax.plot(x_range, kde(x_range), color='r', label='KDE')\n\n    # Set the plot title and labels\n    ax.set_title('Distribution of Word Lengths')\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n\n    # Add a legend\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/994", "solution": "import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom requests.exceptions import RequestException\ndef task_func(url: str, csv_file_path: str) -> list:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except RequestException as e:\n        raise RuntimeError(f\"Error fetching URL: {e}\")\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    data = []\n    for container in soup.find_all('div', class_='container'):\n        title = container.find('h1').text if container.find('h1') else 'No Title'\n        date = container.find('span', class_='date').text if container.find('span', class_='date') else 'No Date'\n        author = container.find('span', class_='author').text if container.find('span', class_='author') else 'No Author'\n        data.append((title, date, author))\n\n    df = pd.DataFrame(data, columns=['Title', 'Date', 'Author'])\n    df.to_csv(csv_file_path, index=False)\n\n    return data", "raw_solution": "import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom requests.exceptions import RequestException\n\ndef task_func(url: str, csv_file_path: str) -> list:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except RequestException as e:\n        raise RuntimeError(f\"Error fetching URL: {e}\")\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    data = []\n    for container in soup.find_all('div', class_='container'):\n        title = container.find('h1').text if container.find('h1') else 'No Title'\n        date = container.find('span', class_='date').text if container.find('span', class_='date') else 'No Date'\n        author = container.find('span', class_='author').text if container.find('span', class_='author') else 'No Author'\n        data.append((title, date, author))\n\n    df = pd.DataFrame(data, columns=['Title', 'Date', 'Author'])\n    df.to_csv(csv_file_path, index=False)\n\n    return data"}
{"task_id": "BigCodeBench/995", "solution": "import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(file_path: str, plot_path: str) -> (float, float, str):\n    \"\"\"\n    Processes a CSV file at the given path by reading its contents, cleaning the data,\n    performing statistical analysis, and generating a plot, which is saved to the specified path.\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    # Read CSV file\n    df = pd.read_csv(file_path)\n\n    # Clean data\n    df = df.dropna()\n\n    # Perform statistical analysis\n    mean_value = df.mean().values[0] if df.mean().values[0] == df.mean().values[0] else np.nan\n    median_value = df.median().values[0] if df.median().values[0] == df.median().values[0] else np.nan\n\n    # Generate plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df.values)\n    plt.title(\"Data Visualization\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Value\")\n\n    # Save plot\n    plt.savefig(plot_path)\n\n    return mean_value, median_value, plot_path", "raw_solution": "import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(file_path: str, plot_path: str) -> (float, float, str):\n    \"\"\"\n    Processes a CSV file at the given path by reading its contents, cleaning the data,\n    performing statistical analysis, and generating a plot, which is saved to the specified path.\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    # Read CSV file\n    df = pd.read_csv(file_path)\n\n    # Clean data\n    df = df.dropna()\n\n    # Perform statistical analysis\n    mean_value = df.mean().values[0] if df.mean().values[0] == df.mean().values[0] else np.nan\n    median_value = df.median().values[0] if df.median().values[0] == df.median().values[0] else np.nan\n\n    # Generate plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df.values)\n    plt.title(\"Data Visualization\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Value\")\n\n    # Save plot\n    plt.savefig(plot_path)\n\n    return mean_value, median_value, plot_path"}
{"task_id": "BigCodeBench/996", "solution": "import requests\nimport json\nfrom bs4 import BeautifulSoup\ndef task_func(url: str, file_name: str = \"Output.txt\") -> str:\n    \"\"\"\n    Scrapes the 'title' from a specified web page, saves it in JSON format to a given file, and appends to the file if it exists.\n    If the web page does not have a title, 'None' is saved as the title value in the JSON data.\n    Data is appended to the specified file in JSON format, with each title on a new line.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Parse the HTML content of the page with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find the title tag\n    title = soup.find('title')\n\n    # If a title tag exists, get its text, otherwise set it to 'None'\n    title_text = title.text if title else 'None'\n\n    # Create a dictionary with the title\n    data = {'title': title_text}\n\n    # Convert the dictionary to a JSON string\n    json_data = json.dumps(data)\n\n    # Open the file in append mode and write the JSON data\n    with open(file_name, 'a') as file:\n        file.write(json_data + '\\n')\n\n    # Return the file path\n    return file_name", "raw_solution": "import requests\nimport json\nfrom bs4 import BeautifulSoup\n\ndef task_func(url: str, file_name: str = \"Output.txt\") -> str:\n    \"\"\"\n    Scrapes the 'title' from a specified web page, saves it in JSON format to a given file, and appends to the file if it exists.\n    If the web page does not have a title, 'None' is saved as the title value in the JSON data.\n    Data is appended to the specified file in JSON format, with each title on a new line.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Parse the HTML content of the page with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find the title tag\n    title = soup.find('title')\n\n    # If a title tag exists, get its text, otherwise set it to 'None'\n    title_text = title.text if title else 'None'\n\n    # Create a dictionary with the title\n    data = {'title': title_text}\n\n    # Convert the dictionary to a JSON string\n    json_data = json.dumps(data)\n\n    # Open the file in append mode and write the JSON data\n    with open(file_name, 'a') as file:\n        file.write(json_data + '\\n')\n\n    # Return the file path\n    return file_name"}
{"task_id": "BigCodeBench/997", "solution": "import urllib.request\nimport os\nimport zipfile\nTARGET_DIR = \"downloaded_files\"\nTARGET_ZIP_FILE = \"downloaded_files.zip\"\ndef task_func(url):\n    # Create target directory if it doesn't exist\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n\n    # Download zip file\n    urllib.request.urlretrieve(url, TARGET_ZIP_FILE)\n\n    # Extract zip file\n    with zipfile.ZipFile(TARGET_ZIP_FILE, 'r') as zip_ref:\n        zip_ref.extractall(TARGET_DIR)\n\n    # Delete local zip file\n    os.remove(TARGET_ZIP_FILE)\n\n    return TARGET_DIR", "raw_solution": "import urllib.request\nimport os\nimport zipfile\n\n# Constants\nTARGET_DIR = \"downloaded_files\"\nTARGET_ZIP_FILE = \"downloaded_files.zip\"\n\ndef task_func(url):\n    # Create target directory if it doesn't exist\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n\n    # Download zip file\n    urllib.request.urlretrieve(url, TARGET_ZIP_FILE)\n\n    # Extract zip file\n    with zipfile.ZipFile(TARGET_ZIP_FILE, 'r') as zip_ref:\n        zip_ref.extractall(TARGET_DIR)\n\n    # Delete local zip file\n    os.remove(TARGET_ZIP_FILE)\n\n    return TARGET_DIR"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Failed to download file: {e}\")\n        return False\n\n    # Calculate the MD5 checksum of the downloaded file\n    md5_hash = hashlib.md5()\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(byte_block)\n    calculated_md5_checksum = md5_hash.hexdigest()\n\n    # Check if the calculated checksum matches the expected checksum\n    if calculated_md5_checksum != EXPECTED_MD5_CHECKSUM:\n        print(\"Checksum does not match. Deleting the file.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the contents of the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Failed to extract file: {e}\")\n        return False\n\n    # Remove the downloaded file\n    os.remove(TARGET_TAR_FILE)\n\n    return True", "raw_solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Failed to download file: {e}\")\n        return False\n\n    # Calculate the MD5 checksum of the downloaded file\n    md5_hash = hashlib.md5()\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(byte_block)\n    calculated_md5_checksum = md5_hash.hexdigest()\n\n    # Check if the calculated checksum matches the expected checksum\n    if calculated_md5_checksum != EXPECTED_MD5_CHECKSUM:\n        print(\"Checksum does not match. Deleting the file.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the contents of the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Failed to extract file: {e}\")\n        return False\n\n    # Remove the downloaded file\n    os.remove(TARGET_TAR_FILE)\n\n    return True"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except urllib.error.URLError:\n        print(f\"Failed to download file from {url}\")\n        return\n\n    if not os.path.exists(csv_file_path):\n        print(f\"File {csv_file_path} does not exist\")\n        return\n\n    try:\n        with open(csv_file_path, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            column_values = [row[column_name] for row in csv_reader if column_name in row]\n\n        if not column_values:\n            raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n\n        count_dict = collections.Counter(column_values)\n        os.remove(csv_file_path)\n        return count_dict\n    except csv.Error:\n        print(f\"Failed to read CSV file {csv_file_path}\")\n        os.remove(csv_file_path)\n        return", "raw_solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except urllib.error.URLError:\n        print(f\"Failed to download file from {url}\")\n        return\n\n    if not os.path.exists(csv_file_path):\n        print(f\"File {csv_file_path} does not exist\")\n        return\n\n    try:\n        with open(csv_file_path, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            column_values = [row[column_name] for row in csv_reader if column_name in row]\n\n        if not column_values:\n            raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n\n        count_dict = collections.Counter(column_values)\n        os.remove(csv_file_path)\n        return count_dict\n    except csv.Error:\n        print(f\"Failed to read CSV file {csv_file_path}\")\n        os.remove(csv_file_path)\n        return"}
{"task_id": "BigCodeBench/1000", "solution": "import urllib.request\nimport os\nimport json\nimport pandas as pd\nTARGET_JSON_FILE = \"downloaded_file.json\"\ndef task_func(url):\n    # Download the JSON file\n    urllib.request.urlretrieve(url, TARGET_JSON_FILE)\n\n    # Read the JSON file\n    with open(TARGET_JSON_FILE, 'r') as file:\n        data = json.load(file)\n\n    # Convert the JSON data into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Delete the temporary JSON file\n    os.remove(TARGET_JSON_FILE)\n\n    return df", "raw_solution": "import urllib.request\nimport os\nimport json\nimport pandas as pd\n\n# Constants\nTARGET_JSON_FILE = \"downloaded_file.json\"\n\ndef task_func(url):\n    # Download the JSON file\n    urllib.request.urlretrieve(url, TARGET_JSON_FILE)\n\n    # Read the JSON file\n    with open(TARGET_JSON_FILE, 'r') as file:\n        data = json.load(file)\n\n    # Convert the JSON data into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Delete the temporary JSON file\n    os.remove(TARGET_JSON_FILE)\n\n    return df"}
{"task_id": "BigCodeBench/1001", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(csv_file_path: str):\n    # Read data from CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Normalize 'column1'\n    df['column1'] = (df['column1'] - df['column1'].min()) / (df['column1'].max() - df['column1'].min())\n\n    # Create plot\n    fig, ax = plt.subplots()\n\n    # Plot normalized data\n    ax.hist(df['column1'], bins=30, color='blue')\n\n    # Set plot title\n    ax.set_title('{:^20}: {:^20}'.format('Plot Title', 'Normalized Column 1'))\n\n    # Set x-label\n    ax.set_xlabel('{:^20}: {:^20}'.format('Index', 'Normalized Value'))\n\n    # Set y-label\n    ax.set_ylabel('{:^20}: {:^20}'.format('Frequency', 'Normalized Value'))\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file_path: str):\n    # Read data from CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Normalize 'column1'\n    df['column1'] = (df['column1'] - df['column1'].min()) / (df['column1'].max() - df['column1'].min())\n\n    # Create plot\n    fig, ax = plt.subplots()\n\n    # Plot normalized data\n    ax.hist(df['column1'], bins=30, color='blue')\n\n    # Set plot title\n    ax.set_title('{:^20}: {:^20}'.format('Plot Title', 'Normalized Column 1'))\n\n    # Set x-label\n    ax.set_xlabel('{:^20}: {:^20}'.format('Index', 'Normalized Value'))\n\n    # Set y-label\n    ax.set_ylabel('{:^20}: {:^20}'.format('Frequency', 'Normalized Value'))\n\n    return ax"}
{"task_id": "BigCodeBench/1002", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data, column_name=\"target_column\"):\n    \"\"\"\n    Converts a given JSON data into a Pandas DataFrame and plots a histogram of a specified column.\n    The function handles non-numeric columns by converting them to categorical type and then to numeric codes.\n    It also checks if the specified column exists in the DataFrame.\n    \"\"\"\n    # Convert JSON data to DataFrame\n    df = pd.read_json(data)\n\n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column '{column_name}' does not exist in the DataFrame.\")\n\n    # Convert non-numeric columns to categorical type and then to numeric codes\n    if pd.api.types.is_numeric_dtype(df[column_name]):\n        pass\n    else:\n        df[column_name] = df[column_name].astype('category').cat.codes\n\n    # Plot histogram of the specified column\n    plt.figure(figsize=(10, 6))\n    plt.hist(df[column_name], bins='auto', color='#007BFF', ec='black')\n    plt.title(f'Histogram of {column_name}')\n    plt.xlabel(column_name)\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return df, plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column_name=\"target_column\"):\n    \"\"\"\n    Converts a given JSON data into a Pandas DataFrame and plots a histogram of a specified column.\n    The function handles non-numeric columns by converting them to categorical type and then to numeric codes.\n    It also checks if the specified column exists in the DataFrame.\n    \"\"\"\n    # Convert JSON data to DataFrame\n    df = pd.read_json(data)\n\n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column '{column_name}' does not exist in the DataFrame.\")\n\n    # Convert non-numeric columns to categorical type and then to numeric codes\n    if pd.api.types.is_numeric_dtype(df[column_name]):\n        pass\n    else:\n        df[column_name] = df[column_name].astype('category').cat.codes\n\n    # Plot histogram of the specified column\n    plt.figure(figsize=(10, 6))\n    plt.hist(df[column_name], bins='auto', color='#007BFF', ec='black')\n    plt.title(f'Histogram of {column_name}')\n    plt.xlabel(column_name)\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        response = urllib.request.urlopen(url)\n        xml_content = response.read()\n    except urllib.error.URLError:\n        raise ValueError(\"URL is invalid or XML file cannot be fetched from the URL.\")\n\n    try:\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError:\n        raise ValueError(\"XML file has invalid syntax.\")\n\n    items = root.findall('.//item')\n    if not items:\n        raise ValueError(\"XML structure does not match expected format.\")\n\n    data = []\n    for item in items:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        data.append(item_data)\n\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        response = urllib.request.urlopen(url)\n        xml_content = response.read()\n    except urllib.error.URLError:\n        raise ValueError(\"URL is invalid or XML file cannot be fetched from the URL.\")\n\n    try:\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError:\n        raise ValueError(\"XML file has invalid syntax.\")\n\n    items = root.findall('.//item')\n    if not items:\n        raise ValueError(\"XML structure does not match expected format.\")\n\n    data = []\n    for item in items:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        data.append(item_data)\n\n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Download the file\n    response = urllib.request.urlopen(url)\n    data = response.read()\n    text = data.decode('utf-8')\n\n    # Process the text to count the frequency of each word\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_counts = Counter(words)\n\n    # Plot a bar chart showing the ten most frequently occurring words\n    most_common_words = word_counts.most_common(10)\n    words, counts = zip(*most_common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('Ten Most Common Words')\n\n    return word_counts, ax", "raw_solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Download the file\n    response = urllib.request.urlopen(url)\n    data = response.read()\n    text = data.decode('utf-8')\n\n    # Process the text to count the frequency of each word\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_counts = Counter(words)\n\n    # Plot a bar chart showing the ten most frequently occurring words\n    most_common_words = word_counts.most_common(10)\n    words, counts = zip(*most_common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('Ten Most Common Words')\n\n    return word_counts, ax"}
{"task_id": "BigCodeBench/1005", "solution": "import urllib.request\nimport zipfile\nimport os\nimport urllib.error\ndef task_func(url: str, save_path: str = \"downloaded_file.zip\", extract_path: str = \"extracted_files\") -> str:\n    try:\n        urllib.request.urlretrieve(url, save_path)\n    except urllib.error.URLError as e:\n        return f\"URL Error: {str(e.reason)}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n    try:\n        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_path)\n    except zipfile.BadZipFile as e:\n        os.remove(save_path)\n        return f\"ZIP Error: {str(e)}\"\n    except Exception as e:\n        os.remove(save_path)\n        return f\"Error: {str(e)}\"\n\n    os.remove(save_path)\n    return extract_path", "raw_solution": "import urllib.request\nimport zipfile\nimport os\nimport urllib.error\n\ndef task_func(url: str, save_path: str = \"downloaded_file.zip\", extract_path: str = \"extracted_files\") -> str:\n    try:\n        urllib.request.urlretrieve(url, save_path)\n    except urllib.error.URLError as e:\n        return f\"URL Error: {str(e.reason)}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n    try:\n        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_path)\n    except zipfile.BadZipFile as e:\n        os.remove(save_path)\n        return f\"ZIP Error: {str(e)}\"\n    except Exception as e:\n        os.remove(save_path)\n        return f\"Error: {str(e)}\"\n\n    os.remove(save_path)\n    return extract_path"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n\n        if response.headers.get('content-type') != 'application/zip':\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        file_name = url.split(\"/\")[-1]\n        file_path = os.path.join(download_path, file_name)\n\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n        try:\n            with ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.test_zip()\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        return file_path\n\n    except requests.exceptions.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n\n    except Exception as e:\n        return f\"Error: {str(e)}\"", "raw_solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n\n        if response.headers.get('content-type') != 'application/zip':\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        file_name = url.split(\"/\")[-1]\n        file_path = os.path.join(download_path, file_name)\n\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n        try:\n            with ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.test_zip()\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        return file_path\n\n    except requests.exceptions.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n\n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1007", "solution": "import requests\nimport pandas as pd\nfrom requests.exceptions import RequestException, HTTPError\ndef task_func(url: str) -> pd.DataFrame:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except HTTPError as http_err:\n        raise HTTPError(f'HTTP error occurred: {http_err}')\n    except RequestException as err:\n        raise SystemError(f'Network-related error occurred: {err}')\n\n    try:\n        data = response.json()\n    except ValueError:\n        raise ValueError('Invalid JSON format')\n\n    return pd.DataFrame(data)", "raw_solution": "import requests\nimport pandas as pd\nfrom requests.exceptions import RequestException, HTTPError\n\ndef task_func(url: str) -> pd.DataFrame:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except HTTPError as http_err:\n        raise HTTPError(f'HTTP error occurred: {http_err}')\n    except RequestException as err:\n        raise SystemError(f'Network-related error occurred: {err}')\n\n    try:\n        data = response.json()\n    except ValueError:\n        raise ValueError('Invalid JSON format')\n\n    return pd.DataFrame(data)"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\nfrom requests.exceptions import HTTPError\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except HTTPError as http_err:\n        raise HTTPError(f'HTTP request failed: {http_err}')\n    except Exception as err:\n        raise Exception(f'Other error occurred: {err}')\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if table is None:\n        raise ValueError('Table with the specified ID not found.')\n\n    csv = StringIO()\n    writer = csv.write\n    for row in table.find_all('tr'):\n        cells = row.find_all(['th', 'td'])\n        write_cells = writer(','.join([cell.text.strip() for cell in cells]))\n        write_cells('\\n')\n\n    csv.seek(0)\n    df = pd.read_csv(csv)\n\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\nfrom requests.exceptions import HTTPError\n\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except HTTPError as http_err:\n        raise HTTPError(f'HTTP request failed: {http_err}')\n    except Exception as err:\n        raise Exception(f'Other error occurred: {err}')\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if table is None:\n        raise ValueError('Table with the specified ID not found.')\n\n    csv = StringIO()\n    writer = csv.write\n    for row in table.find_all('tr'):\n        cells = row.find_all(['th', 'td'])\n        write_cells = writer(','.join([cell.text.strip() for cell in cells]))\n        write_cells('\\n')\n\n    csv.seek(0)\n    df = pd.read_csv(csv)\n\n    return df"}
{"task_id": "BigCodeBench/1009", "solution": "import xml.etree.ElementTree as ET\nimport csv\nimport os\ndef task_func(xml_content, output_csv_path):\n    try:\n        root = ET.fromstring(xml_content)\n    except ET.ParseError as e:\n        raise Exception(f\"XML parsing error: {str(e)}\")\n\n    data = []\n    for item in root.iter():\n        data.append({child.tag: child.text for child in item})\n\n    try:\n        with open(output_csv_path, 'w', newline='') as csvfile:\n            fieldnames = data[0].keys()\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n            writer.writeheader()\n            for row in data:\n                writer.writerow(row)\n    except IOError as e:\n        raise Exception(f\"IO error: {str(e)}\")", "raw_solution": "import xml.etree.ElementTree as ET\nimport csv\nimport os\n\ndef task_func(xml_content, output_csv_path):\n    try:\n        root = ET.fromstring(xml_content)\n    except ET.ParseError as e:\n        raise Exception(f\"XML parsing error: {str(e)}\")\n\n    data = []\n    for item in root.iter():\n        data.append({child.tag: child.text for child in item})\n\n    try:\n        with open(output_csv_path, 'w', newline='') as csvfile:\n            fieldnames = data[0].keys()\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n            writer.writeheader()\n            for row in data:\n                writer.writerow(row)\n    except IOError as e:\n        raise Exception(f\"IO error: {str(e)}\")"}
{"task_id": "BigCodeBench/1010", "solution": "import requests\nfrom PIL import Image\nimport io", "raw_solution": "import requests\nfrom PIL import Image\nimport io\n\ndef fetch_image(url):\n    \"\"\"\n    Fetches an image from a given URL and returns it as a PIL Image object.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Invalid URL or timeout: {e}\")\n\n    if response.status_code not in range(200, 300):\n        raise ValueError(f\"Unsuccessful HTTP status code: {response.status_code}\")\n\n    try:\n        image = Image.open(io.BytesIO(response.content))\n    except IOError:\n        raise ValueError(\"Invalid image format\")\n\n    return image"}
{"task_id": "BigCodeBench/1011", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    # Read data from CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Check if the columns exist in the DataFrame\n    if col1_name not in df.columns or col2_name not in df.columns:\n        raise ValueError(f\"One or both columns do not exist in the DataFrame. Available columns: {df.columns}\")\n\n    # Group by col1_name and calculate mean for col2_name\n    grouped_df = df.groupby(col1_name)[col2_name].mean().reset_index()\n\n    # Create a bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(grouped_df[col1_name], grouped_df[col2_name], color='blue')\n\n    # Set title and axis labels\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(col1_name)\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    # Read data from CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Check if the columns exist in the DataFrame\n    if col1_name not in df.columns or col2_name not in df.columns:\n        raise ValueError(f\"One or both columns do not exist in the DataFrame. Available columns: {df.columns}\")\n\n    # Group by col1_name and calculate mean for col2_name\n    grouped_df = df.groupby(col1_name)[col2_name].mean().reset_index()\n\n    # Create a bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(grouped_df[col1_name], grouped_df[col2_name], color='blue')\n\n    # Set title and axis labels\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(col1_name)\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    return ax"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nimport io\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    \"\"\"\n    Downloads and extracts a zip file from a specified URL.\n    \"\"\"\n    try:\n        # Create download directory if it doesn't exist\n        DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()\n\n        # Save the file\n        file_path = DOWNLOAD_DIR / filename\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n        # Extract the zip file\n        ZIP_DIR.mkdir(parents=True, exist_ok=True)\n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # Get a list of filenames in the unzipped directory\n        unzipped_files = [f.name for f in ZIP_DIR.iterdir() if f.is_file()]\n\n        return (\"Success\", unzipped_files)\n\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception\", [])\n    except Exception as e:\n        return (\"Error: File-related exception\", [])", "raw_solution": "import requests\nfrom pathlib import Path\nimport zipfile\nimport io\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    \"\"\"\n    Downloads and extracts a zip file from a specified URL.\n    \"\"\"\n    try:\n        # Create download directory if it doesn't exist\n        DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()\n\n        # Save the file\n        file_path = DOWNLOAD_DIR / filename\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n        # Extract the zip file\n        ZIP_DIR.mkdir(parents=True, exist_ok=True)\n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # Get a list of filenames in the unzipped directory\n        unzipped_files = [f.name for f in ZIP_DIR.iterdir() if f.is_file()]\n\n        return (\"Success\", unzipped_files)\n\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception\", [])\n    except Exception as e:\n        return (\"Error: File-related exception\", [])"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\n    It returns the number of unique absolute links scraped from the webpage.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Parse the content of the response with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all the <a> tags in the soup\n    links = soup.find_all('a')\n\n    # Initialize a set to store the unique absolute links\n    unique_links = set()\n\n    # Iterate over the links\n    for link in links:\n        # Get the href attribute of the link\n        href = link.get('href')\n\n        # If the href attribute exists and is not None, join it with the base URL to get the absolute URL\n        if href and href != '#':\n            absolute_url = urljoin(base_url, href)\n\n            # Add the absolute URL to the set of unique links\n            unique_links.add(absolute_url)\n\n    # Open the CSV file in write mode\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write the unique links to the CSV file\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique links\n    return len(unique_links)", "raw_solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\n    It returns the number of unique absolute links scraped from the webpage.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Parse the content of the response with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all the <a> tags in the soup\n    links = soup.find_all('a')\n\n    # Initialize a set to store the unique absolute links\n    unique_links = set()\n\n    # Iterate over the links\n    for link in links:\n        # Get the href attribute of the link\n        href = link.get('href')\n\n        # If the href attribute exists and is not None, join it with the base URL to get the absolute URL\n        if href and href != '#':\n            absolute_url = urljoin(base_url, href)\n\n            # Add the absolute URL to the set of unique links\n            unique_links.add(absolute_url)\n\n    # Open the CSV file in write mode\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write the unique links to the CSV file\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique links\n    return len(unique_links)"}
{"task_id": "BigCodeBench/1014", "solution": "import requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom requests.exceptions import HTTPError\ndef task_func(api_url):\n    if not isinstance(api_url, str):\n        raise TypeError(\"API URL must be a string\")\n\n    try:\n        response = requests.get(api_url)\n        response.raise_for_status()\n    except HTTPError as http_err:\n        print(f'HTTP error occurred: {http_err}')\n        raise\n    except Exception as err:\n        print(f'Other error occurred: {err}')\n        raise\n\n    data = response.json()\n    df = pd.DataFrame(data)\n\n    if df.empty:\n        print(\"Data is empty, no plot is generated\")\n        return df, None\n\n    ax = df.plot()\n    plt.show()\n\n    return df, ax", "raw_solution": "import requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom requests.exceptions import HTTPError\n\ndef task_func(api_url):\n    if not isinstance(api_url, str):\n        raise TypeError(\"API URL must be a string\")\n\n    try:\n        response = requests.get(api_url)\n        response.raise_for_status()\n    except HTTPError as http_err:\n        print(f'HTTP error occurred: {http_err}')\n        raise\n    except Exception as err:\n        print(f'Other error occurred: {err}')\n        raise\n\n    data = response.json()\n    df = pd.DataFrame(data)\n\n    if df.empty:\n        print(\"Data is empty, no plot is generated\")\n        return df, None\n\n    ax = df.plot()\n    plt.show()\n\n    return df, ax"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\nfrom typing import Union\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> Union[int, None]:\n    try:\n        # Fetch the HTML content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Parse the HTML content\n    try:\n        parsed_content = html.fromstring(response.content)\n        table_content = parsed_content.xpath('//table')\n        if not table_content:\n            return 0\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Convert the HTML table to a pandas DataFrame\n    try:\n        df = pd.read_html(html.tostring(table_content[0]))[0]\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Connect to the SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Replace the table with new data\n    try:\n        df.to_sql(\"my_table\", conn, if_exists='replace', index=False)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Close the connection\n    conn.close()\n\n    # Return the number of rows in the parsed HTML table\n    return df.shape[0]", "raw_solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\nfrom typing import Union\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> Union[int, None]:\n    try:\n        # Fetch the HTML content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Parse the HTML content\n    try:\n        parsed_content = html.fromstring(response.content)\n        table_content = parsed_content.xpath('//table')\n        if not table_content:\n            return 0\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Convert the HTML table to a pandas DataFrame\n    try:\n        df = pd.read_html(html.tostring(table_content[0]))[0]\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Connect to the SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Replace the table with new data\n    try:\n        df.to_sql(\"my_table\", conn, if_exists='replace', index=False)\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Close the connection\n    conn.close()\n\n    # Return the number of rows in the parsed HTML table\n    return df.shape[0]"}
{"task_id": "BigCodeBench/1016", "solution": "import requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\ndef task_func(url: str) -> \"matplotlib.axes._axes.Axes\":\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Error downloading image: {str(e)}\")\n\n    try:\n        image = Image.open(BytesIO(response.content))\n    except IOError as e:\n        raise IOError(f\"Error processing image: {str(e)}\")\n\n    image = image.convert('L')\n    image_array = np.array(image)\n\n    histogram, bins = np.histogram(image_array, bins=256, range=(0, 256))\n\n    plt.figure(figsize=(10, 5))\n    plt.bar(bins[:-1], histogram, width=1)\n    plt.xlabel('Grayscale Value')\n    plt.ylabel('Frequency')\n    plt.title('Grayscale Histogram')\n    plt.xlim([0, 256])\n\n    return plt.gca()", "raw_solution": "import requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\ndef task_func(url: str) -> \"matplotlib.axes._axes.Axes\":\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Error downloading image: {str(e)}\")\n\n    try:\n        image = Image.open(BytesIO(response.content))\n    except IOError as e:\n        raise IOError(f\"Error processing image: {str(e)}\")\n\n    image = image.convert('L')\n    image_array = np.array(image)\n\n    histogram, bins = np.histogram(image_array, bins=256, range=(0, 256))\n\n    plt.figure(figsize=(10, 5))\n    plt.bar(bins[:-1], histogram, width=1)\n    plt.xlabel('Grayscale Value')\n    plt.ylabel('Frequency')\n    plt.title('Grayscale Histogram')\n    plt.xlim([0, 256])\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/1017", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\ndef task_func(csv_file_path, target_column=\"target\", test_size=0.2, n_estimators=100):\n    \"\"\"\n    Processes a CSV file to train a Random Forest classifier and generates a formatted classification report.\n    \"\"\"\n    # Load the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Check if the target_column exists in the DataFrame\n    if target_column not in df.columns:\n        raise ValueError(f\"The specified target_column '{target_column}' is not found in the CSV file.\")\n\n    # Split the DataFrame into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n\n    # Train a Random Forest classifier\n    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n    clf.fit(X_train, y_train)\n\n    # Make predictions on the testing set\n    y_pred = clf.predict(X_test)\n\n    # Generate and return a formatted classification report\n    report = classification_report(y_test, y_pred)\n    return report", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\ndef task_func(csv_file_path, target_column=\"target\", test_size=0.2, n_estimators=100):\n    \"\"\"\n    Processes a CSV file to train a Random Forest classifier and generates a formatted classification report.\n    \"\"\"\n    # Load the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Check if the target_column exists in the DataFrame\n    if target_column not in df.columns:\n        raise ValueError(f\"The specified target_column '{target_column}' is not found in the CSV file.\")\n\n    # Split the DataFrame into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n\n    # Train a Random Forest classifier\n    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n    clf.fit(X_train, y_train)\n\n    # Make predictions on the testing set\n    y_pred = clf.predict(X_test)\n\n    # Generate and return a formatted classification report\n    report = classification_report(y_test, y_pred)\n    return report"}
{"task_id": "BigCodeBench/1018", "solution": "from bs4 import BeautifulSoup\nimport requests\ndef task_func(url=None, from_encoding=\"cp1251\", use_lxml=False):\n    if not url:\n        return None\n\n    try:\n        response = requests.get(url)\n    except requests.exceptions.RequestException:\n        return None\n\n    if response.status_code != 200:\n        return None\n\n    try:\n        decoded_content = response.content.decode(from_encoding)\n    except UnicodeDecodeError:\n        return None\n\n    try:\n        if use_lxml:\n            return BeautifulSoup(decoded_content, 'lxml')\n        else:\n            return BeautifulSoup(decoded_content, 'html.parser')\n    except Exception:\n        return None", "raw_solution": "from bs4 import BeautifulSoup\nimport requests\n\ndef task_func(url=None, from_encoding=\"cp1251\", use_lxml=False):\n    if not url:\n        return None\n\n    try:\n        response = requests.get(url)\n    except requests.exceptions.RequestException:\n        return None\n\n    if response.status_code != 200:\n        return None\n\n    try:\n        decoded_content = response.content.decode(from_encoding)\n    except UnicodeDecodeError:\n        return None\n\n    try:\n        if use_lxml:\n            return BeautifulSoup(decoded_content, 'lxml')\n        else:\n            return BeautifulSoup(decoded_content, 'html.parser')\n    except Exception:\n        return None"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        image = Image.open(filename)\n\n        # Extract text from the image using OCR\n        ocr_text = pytesseract.image_to_string(image, lang='eng')\n\n        # If OCR extraction fails, try to extract text from the image comment\n        if not ocr_text:\n            ocr_text = image.comment\n\n        # Convert the text encoding\n        ocr_text = ocr_text.encode(from_encoding).decode(to_encoding)\n\n        return ocr_text\n\n    except (ValueError, LookupError) as e:\n        raise ValueError(f\"Error during conversion: {e}\")\n\n    except Exception as e:\n        return \"\"", "raw_solution": "from PIL import Image\nimport codecs\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        image = Image.open(filename)\n\n        # Extract text from the image using OCR\n        ocr_text = pytesseract.image_to_string(image, lang='eng')\n\n        # If OCR extraction fails, try to extract text from the image comment\n        if not ocr_text:\n            ocr_text = image.comment\n\n        # Convert the text encoding\n        ocr_text = ocr_text.encode(from_encoding).decode(to_encoding)\n\n        return ocr_text\n\n    except (ValueError, LookupError) as e:\n        raise ValueError(f\"Error during conversion: {e}\")\n\n    except Exception as e:\n        return \"\""}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        content = response.content\n\n        if not content:\n            return {}\n\n        if not from_encoding:\n            from_encoding = chardet.detect(content)['encoding']\n\n        if not from_encoding:\n            raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        decoded_content = content.decode(from_encoding)\n        reencoded_content = decoded_content.encode(to_encoding)\n        json_data = json.loads(reencoded_content)\n\n        return json_data\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: {e}\")\n        return {}", "raw_solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        content = response.content\n\n        if not content:\n            return {}\n\n        if not from_encoding:\n            from_encoding = chardet.detect(content)['encoding']\n\n        if not from_encoding:\n            raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        decoded_content = content.decode(from_encoding)\n        reencoded_content = decoded_content.encode(to_encoding)\n        json_data = json.loads(reencoded_content)\n\n        return json_data\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: {e}\")\n        return {}"}
{"task_id": "BigCodeBench/1021", "solution": "import hashlib\nimport binascii\ndef task_func(input_string, verify_hash=None):\n    if not isinstance(input_string, str):\n        raise TypeError(\"input_string must be a string\")\n    if verify_hash is not None and not isinstance(verify_hash, str):\n        raise TypeError(\"verify_hash must be a string or None\")\n\n    hash_object = hashlib.sha256(input_string.encode())\n    hex_dig = hash_object.hexdigest()\n\n    if verify_hash is not None:\n        return hex_dig, hex_dig == verify_hash\n    else:\n        return hex_dig, None", "raw_solution": "import hashlib\nimport binascii\n\ndef task_func(input_string, verify_hash=None):\n    if not isinstance(input_string, str):\n        raise TypeError(\"input_string must be a string\")\n    if verify_hash is not None and not isinstance(verify_hash, str):\n        raise TypeError(\"verify_hash must be a string or None\")\n\n    hash_object = hashlib.sha256(input_string.encode())\n    hex_dig = hash_object.hexdigest()\n\n    if verify_hash is not None:\n        return hex_dig, hex_dig == verify_hash\n    else:\n        return hex_dig, None"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads a CSV file and processes its date-related data.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    # Read the CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the CSV file.\")\n\n    # Convert date values to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n\n    # Filter rows based on the current date\n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n\n    # Sort the resulting data\n    df = df.sort_values(by=column_name)\n\n    return df", "raw_solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads a CSV file and processes its date-related data.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    # Read the CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the CSV file.\")\n\n    # Convert date values to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n\n    # Filter rows based on the current date\n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n\n    # Sort the resulting data\n    df = df.sort_values(by=column_name)\n\n    return df"}
{"task_id": "BigCodeBench/1023", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(dataframe):\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input should be a pandas DataFrame\")\n    if dataframe.empty:\n        raise ValueError(\"Input DataFrame is empty\")\n    if dataframe.shape[1] < 2:\n        raise ValueError(\"DataFrame should have at least two columns\")\n    if not all(dataframe.dtypes == 'float64'):\n        raise TypeError(\"All columns in the DataFrame should be numeric\")\n\n    corr_matrix = dataframe.corr().abs()\n    unstacked_corr_matrix = corr_matrix.unstack()\n    sorted_pairs = unstacked_corr_matrix.sort_values(ascending=False, kind=\"quicksort\")\n    highest_corr_pair = sorted_pairs.index[1]\n\n    x_col, y_col = highest_corr_pair\n\n    fig, ax = plt.subplots()\n    ax.scatter(dataframe[x_col], dataframe[y_col])\n    ax.set_xlabel(x_col)\n    ax.set_ylabel(y_col)\n    ax.set_title(f\"Scatter plot of {x_col} and {y_col} with highest correlation\")\n\n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(dataframe):\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input should be a pandas DataFrame\")\n    if dataframe.empty:\n        raise ValueError(\"Input DataFrame is empty\")\n    if dataframe.shape[1] < 2:\n        raise ValueError(\"DataFrame should have at least two columns\")\n    if not all(dataframe.dtypes == 'float64'):\n        raise TypeError(\"All columns in the DataFrame should be numeric\")\n\n    corr_matrix = dataframe.corr().abs()\n    unstacked_corr_matrix = corr_matrix.unstack()\n    sorted_pairs = unstacked_corr_matrix.sort_values(ascending=False, kind=\"quicksort\")\n    highest_corr_pair = sorted_pairs.index[1]\n\n    x_col, y_col = highest_corr_pair\n\n    fig, ax = plt.subplots()\n    ax.scatter(dataframe[x_col], dataframe[y_col])\n    ax.set_xlabel(x_col)\n    ax.set_ylabel(y_col)\n    ax.set_title(f\"Scatter plot of {x_col} and {y_col} with highest correlation\")\n\n    return ax"}
{"task_id": "BigCodeBench/1024", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nPLOT_TITLE = \"Value Distribution\"\ndef task_func(data_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(data_dict.items()), columns=['Key', 'Value'])\n    df = df.dropna()  # Remove None values\n\n    # Check if the DataFrame is empty or all values are the same\n    if df.empty or df['Value'].nunique() == 1:\n        return df, None\n\n    # Calculate the number of bins dynamically\n    num_bins = max(2, min(11, int(len(df) / 2)))\n\n    # Generate the histogram\n    plot = sns.histplot(df['Value'], bins=num_bins, kde=False)\n    plot.set_title(PLOT_TITLE)\n\n    return df, plot", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nPLOT_TITLE = \"Value Distribution\"\n\ndef task_func(data_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(data_dict.items()), columns=['Key', 'Value'])\n    df = df.dropna()  # Remove None values\n\n    # Check if the DataFrame is empty or all values are the same\n    if df.empty or df['Value'].nunique() == 1:\n        return df, None\n\n    # Calculate the number of bins dynamically\n    num_bins = max(2, min(11, int(len(df) / 2)))\n\n    # Generate the histogram\n    plot = sns.histplot(df['Value'], bins=num_bins, kde=False)\n    plot.set_title(PLOT_TITLE)\n\n    return df, plot"}
{"task_id": "BigCodeBench/1025", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nPLOT_TITLE = \"Scaled Values\"\ndef task_func(data_dict):\n    \"\"\"\n    Scales the values in a given dictionary using MinMaxScaler and plots the scaled data.\n    \"\"\"\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(list(data_dict.items()), columns=['Key', 'Value'])\n\n    # Scale the 'Value' column using MinMaxScaler\n    scaler = MinMaxScaler()\n    df['Value'] = scaler.fit_transform(df['Value'].values.reshape(-1, 1))\n\n    # Plot the scaled data\n    fig, ax = plt.subplots()\n    ax.plot(df['Key'], df['Value'], marker='o')\n    ax.set_title(PLOT_TITLE)\n    ax.set_xlabel('Key')\n    ax.set_ylabel('Scaled Value')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nPLOT_TITLE = \"Scaled Values\"\n\ndef task_func(data_dict):\n    \"\"\"\n    Scales the values in a given dictionary using MinMaxScaler and plots the scaled data.\n    \"\"\"\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(list(data_dict.items()), columns=['Key', 'Value'])\n\n    # Scale the 'Value' column using MinMaxScaler\n    scaler = MinMaxScaler()\n    df['Value'] = scaler.fit_transform(df['Value'].values.reshape(-1, 1))\n\n    # Plot the scaled data\n    fig, ax = plt.subplots()\n    ax.plot(df['Key'], df['Value'], marker='o')\n    ax.set_title(PLOT_TITLE)\n    ax.set_xlabel('Key')\n    ax.set_ylabel('Scaled Value')\n\n    return df, ax"}
{"task_id": "BigCodeBench/1026", "solution": "import numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\ndef task_func(group1, group2, alpha=0.05, min_samples=2, var_threshold=1e-8):\n    \"\"\"\n    Performs a two-sample t-test on numerical data from two groups to determine if there is a significant difference in their means.\n    \"\"\"\n    # Remove NaN values\n    group1 = group1[~np.isnan(group1)]\n    group2 = group2[~np.isnan(group2)]\n\n    # Check for sufficient non-NaN data points and adequate variance in each group\n    if len(group1) < min_samples or len(group2) < min_samples:\n        raise ValueError(\"Each group must have at least two non-NaN values.\")\n    if np.var(group1) < var_threshold or np.var(group2) < var_threshold:\n        raise ValueError(\"The variance in each group must be above a threshold.\")\n\n    # Perform t-test\n    t_stat, p_val = ttest_ind(group1, group2, nan_policy='omit')\n\n    # Generate descriptive statistics\n    group1_stats = {'mean': np.mean(group1), 'std': np.std(group1)}\n    group2_stats = {'mean': np.mean(group2), 'std': np.std(group2)}\n\n    # Create boxplot and histogram\n    fig, (ax_boxplot, ax_histogram) = plt.subplots(2, gridspec_kw={\"height_ratios\": (.15, .85)})\n    ax_boxplot.boxplot([group1, group2], labels=['Group 1', 'Group 2'])\n    ax_boxplot.set_title('Boxplot of Group 1 and Group 2')\n    ax_histogram.hist(group1, alpha=0.5, label='Group 1')\n    ax_histogram.hist(group2, alpha=0.5, label='Group 2')\n    ax_histogram.set_title('Histogram of Group 1 and Group 2')\n    ax_histogram.legend()\n\n    # Return results\n    return {\n        'significant': p_val < alpha,\n        'group1_stats': group1_stats,\n        'group2_stats': group2_stats,\n        'ax_boxplot': ax_boxplot,\n        'ax_histogram': ax_histogram,\n    }", "raw_solution": "import numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\n\ndef task_func(group1, group2, alpha=0.05, min_samples=2, var_threshold=1e-8):\n    \"\"\"\n    Performs a two-sample t-test on numerical data from two groups to determine if there is a significant difference in their means.\n    \"\"\"\n    # Remove NaN values\n    group1 = group1[~np.isnan(group1)]\n    group2 = group2[~np.isnan(group2)]\n\n    # Check for sufficient non-NaN data points and adequate variance in each group\n    if len(group1) < min_samples or len(group2) < min_samples:\n        raise ValueError(\"Each group must have at least two non-NaN values.\")\n    if np.var(group1) < var_threshold or np.var(group2) < var_threshold:\n        raise ValueError(\"The variance in each group must be above a threshold.\")\n\n    # Perform t-test\n    t_stat, p_val = ttest_ind(group1, group2, nan_policy='omit')\n\n    # Generate descriptive statistics\n    group1_stats = {'mean': np.mean(group1), 'std': np.std(group1)}\n    group2_stats = {'mean': np.mean(group2), 'std': np.std(group2)}\n\n    # Create boxplot and histogram\n    fig, (ax_boxplot, ax_histogram) = plt.subplots(2, gridspec_kw={\"height_ratios\": (.15, .85)})\n    ax_boxplot.boxplot([group1, group2], labels=['Group 1', 'Group 2'])\n    ax_boxplot.set_title('Boxplot of Group 1 and Group 2')\n    ax_histogram.hist(group1, alpha=0.5, label='Group 1')\n    ax_histogram.hist(group2, alpha=0.5, label='Group 2')\n    ax_histogram.set_title('Histogram of Group 1 and Group 2')\n    ax_histogram.legend()\n\n    # Return results\n    return {\n        'significant': p_val < alpha,\n        'group1_stats': group1_stats,\n        'group2_stats': group2_stats,\n        'ax_boxplot': ax_boxplot,\n        'ax_histogram': ax_histogram,\n    }"}
{"task_id": "BigCodeBench/1027", "solution": "import binascii\nimport urllib.parse\ndef task_func(url):\n    query_string = urllib.parse.urlparse(url).query\n    query_dict = urllib.parse.parse_qs(query_string)\n    if 'q' in query_dict:\n        hex_string = query_dict['q'][0]\n        try:\n            decoded_string = binascii.unhexlify(hex_string).decode('utf-8')\n            return decoded_string\n        except (binascii.Error, UnicodeDecodeError):\n            return None\n    return None", "raw_solution": "import binascii\nimport urllib.parse\n\ndef task_func(url):\n    query_string = urllib.parse.urlparse(url).query\n    query_dict = urllib.parse.parse_qs(query_string)\n    if 'q' in query_dict:\n        hex_string = query_dict['q'][0]\n        try:\n            decoded_string = binascii.unhexlify(hex_string).decode('utf-8')\n            return decoded_string\n        except (binascii.Error, UnicodeDecodeError):\n            return None\n    return None"}
{"task_id": "BigCodeBench/1028", "solution": "import subprocess\nimport time\nimport json\nimport platform\nimport psutil\nLOGFILE_PATH = \"logfile.log\"\ndef task_func(interval, duration):\n    if interval <= 0 or duration <= 0:\n        raise ValueError(\"Interval and duration must be greater than 0\")\n\n    start_time = time.time()\n    cpu_percentages = []\n\n    while time.time() - start_time <= duration:\n        cpu_percent = psutil.cpu_percent(interval=interval)\n        timestamp = time.time()\n        cpu_percentages.append({\"timestamp\": timestamp, \"cpu_percent\": cpu_percent})\n\n    try:\n        with open(LOGFILE_PATH, \"w\") as logfile:\n            json.dump(cpu_percentages, logfile)\n    except IOError:\n        return None\n\n    return LOGFILE_PATH", "raw_solution": "import subprocess\nimport time\nimport json\nimport platform\nimport psutil\n\nLOGFILE_PATH = \"logfile.log\"\n\ndef task_func(interval, duration):\n    if interval <= 0 or duration <= 0:\n        raise ValueError(\"Interval and duration must be greater than 0\")\n\n    start_time = time.time()\n    cpu_percentages = []\n\n    while time.time() - start_time <= duration:\n        cpu_percent = psutil.cpu_percent(interval=interval)\n        timestamp = time.time()\n        cpu_percentages.append({\"timestamp\": timestamp, \"cpu_percent\": cpu_percent})\n\n    try:\n        with open(LOGFILE_PATH, \"w\") as logfile:\n            json.dump(cpu_percentages, logfile)\n    except IOError:\n        return None\n\n    return LOGFILE_PATH"}
{"task_id": "BigCodeBench/1029", "solution": "import pandas as pd\nimport numpy as np\nimport string\ndef task_func(rows=100, columns=3):\n    # Generate column names\n    column_names = [string.ascii_lowercase[i % 26] for i in range(columns)]\n\n    # Generate random alphabets\n    data = np.random.choice(list(string.ascii_lowercase), size=(rows, columns))\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=column_names)\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport string\n\ndef task_func(rows=100, columns=3):\n    # Generate column names\n    column_names = [string.ascii_lowercase[i % 26] for i in range(columns)]\n\n    # Generate random alphabets\n    data = np.random.choice(list(string.ascii_lowercase), size=(rows, columns))\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=column_names)\n\n    return df"}
{"task_id": "BigCodeBench/1030", "solution": "import itertools\nimport string\nimport pandas as pd", "raw_solution": "import itertools\nimport string\nimport pandas as pd\n\ndef generate_combinations():\n    \"\"\"\n    Generates all possible combinations (with replacement) of three letters from the alphabet and saves them in a pandas DataFrame.\n    \"\"\"\n    # Generate all possible combinations of three letters from the alphabet\n    combinations = list(itertools.product(string.ascii_lowercase, repeat=3))\n\n    # Convert the list of tuples to a list of strings\n    combinations = [''.join(comb) for comb in combinations]\n\n    # Create a pandas DataFrame from the list of combinations\n    df = pd.DataFrame(combinations, columns=['Combinations'])\n\n    return df"}
{"task_id": "BigCodeBench/1031", "solution": "import random\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(n_rows=1000):\n    if n_rows <= 0:\n        raise ValueError(\"n_rows should be greater than 0\")\n\n    # Generate random 3-letter strings\n    strings = [''.join(random.choices(string.ascii_lowercase, k=3)) for _ in range(n_rows)]\n\n    # Count the frequency of each string\n    freq = pd.Series(strings).value_counts()\n\n    # Get the top 30 most common strings\n    top_30 = freq.nlargest(30)\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    top_30.plot(kind='bar', ax=ax)\n    ax.set_title('Frequency of the top 30 unique random 3-letter strings')\n    ax.set_xlabel('String')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import random\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(n_rows=1000):\n    if n_rows <= 0:\n        raise ValueError(\"n_rows should be greater than 0\")\n\n    # Generate random 3-letter strings\n    strings = [''.join(random.choices(string.ascii_lowercase, k=3)) for _ in range(n_rows)]\n\n    # Count the frequency of each string\n    freq = pd.Series(strings).value_counts()\n\n    # Get the top 30 most common strings\n    top_30 = freq.nlargest(30)\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    top_30.plot(kind='bar', ax=ax)\n    ax.set_title('Frequency of the top 30 unique random 3-letter strings')\n    ax.set_xlabel('String')\n    ax.set_ylabel('Frequency')\n\n    return ax"}
{"task_id": "BigCodeBench/1032", "solution": "import matplotlib.pyplot as plt\nimport random\nimport string\nimport pandas as pd\nimport seaborn as sns\nLETTERS = list(string.ascii_lowercase)\ndef task_func(rows=1000, string_length=3):\n    if rows == 0:\n        print(\"No data to generate heatmap.\")\n        return None\n\n    # Generate random strings\n    random_strings = [''.join(random.choices(LETTERS, k=string_length)) for _ in range(rows)]\n\n    # Create DataFrame\n    df = pd.DataFrame(random_strings, columns=['String'])\n\n    # Convert strings to one-hot encoded format\n    df = pd.get_dummies(df['String'])\n\n    # Calculate frequency of each letter\n    letter_frequencies = df.sum().sort_values(ascending=False)\n\n    # Create correlation matrix\n    correlation_matrix = df.corr()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.title('Heatmap of Correlation Matrix')\n    plt.show()\n\n    return letter_frequencies", "raw_solution": "import matplotlib.pyplot as plt\nimport random\nimport string\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\n\ndef task_func(rows=1000, string_length=3):\n    if rows == 0:\n        print(\"No data to generate heatmap.\")\n        return None\n\n    # Generate random strings\n    random_strings = [''.join(random.choices(LETTERS, k=string_length)) for _ in range(rows)]\n\n    # Create DataFrame\n    df = pd.DataFrame(random_strings, columns=['String'])\n\n    # Convert strings to one-hot encoded format\n    df = pd.get_dummies(df['String'])\n\n    # Calculate frequency of each letter\n    letter_frequencies = df.sum().sort_values(ascending=False)\n\n    # Create correlation matrix\n    correlation_matrix = df.corr()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.title('Heatmap of Correlation Matrix')\n    plt.show()\n\n    return letter_frequencies"}
{"task_id": "BigCodeBench/1033", "solution": "import itertools\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func():\n    # Generate all possible 3-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=3)]\n\n    # Create a DataFrame from these combinations\n    df = pd.DataFrame(combinations, columns=['Combinations'])\n\n    # Count the frequency of the first letters in these combinations\n    first_letter_counts = df['Combinations'].str[0].value_counts()\n\n    # Plot a histogram of the frequency of the first letters\n    fig, ax = plt.subplots()\n    first_letter_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('First Letter')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of First Letters in 3-Letter Combinations')\n\n    return (combinations, df, ax)", "raw_solution": "import itertools\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    # Generate all possible 3-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=3)]\n\n    # Create a DataFrame from these combinations\n    df = pd.DataFrame(combinations, columns=['Combinations'])\n\n    # Count the frequency of the first letters in these combinations\n    first_letter_counts = df['Combinations'].str[0].value_counts()\n\n    # Plot a histogram of the frequency of the first letters\n    fig, ax = plt.subplots()\n    first_letter_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('First Letter')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of First Letters in 3-Letter Combinations')\n\n    return (combinations, df, ax)"}
{"task_id": "BigCodeBench/1034", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance\nCATEGORIES = [\"Electronics\", \"Clothing\", \"Home Decor\", \"Automotive\", \"Books\"]\ndef task_func(s1, s2):\n    # Ensure the inputs are pandas Series\n    if not isinstance(s1, pd.Series) or not isinstance(s2, pd.Series):\n        raise ValueError(\"Both inputs must be pandas Series\")\n\n    # Ensure the series have the same index\n    if not s1.index.equals(s2.index):\n        raise ValueError(\"Both series must have the same index\")\n\n    # Ensure the series have the same categories\n    if not set(s1.index).issubset(CATEGORIES) or not set(s2.index).issubset(CATEGORIES):\n        raise ValueError(\"Both series must have the same categories\")\n\n    # Filter the series to only include categories with sales exceeding the threshold\n    s1_filtered = s1[s1 > 200]\n    s2_filtered = s2[s2 > 200]\n\n    # If no categories meet the threshold, return None for the plot and 0.0 for the distance\n    if s1_filtered.empty or s2_filtered.empty:\n        return None, 0.0\n\n    # Compute the Euclidean distance between the two series\n    euclidean_distance = distance.euclidean(s1_filtered, s2_filtered)\n\n    # Generate a bar plot for the filtered series\n    fig, ax = plt.subplots()\n    ax.bar(s1_filtered.index, s1_filtered.values, label='Store 1')\n    ax.bar(s2_filtered.index, s2_filtered.values, label='Store 2')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Sales')\n    ax.legend()\n    plt.show()\n\n    return ax, euclidean_distance", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance\n\nCATEGORIES = [\"Electronics\", \"Clothing\", \"Home Decor\", \"Automotive\", \"Books\"]\n\ndef task_func(s1, s2):\n    # Ensure the inputs are pandas Series\n    if not isinstance(s1, pd.Series) or not isinstance(s2, pd.Series):\n        raise ValueError(\"Both inputs must be pandas Series\")\n\n    # Ensure the series have the same index\n    if not s1.index.equals(s2.index):\n        raise ValueError(\"Both series must have the same index\")\n\n    # Ensure the series have the same categories\n    if not set(s1.index).issubset(CATEGORIES) or not set(s2.index).issubset(CATEGORIES):\n        raise ValueError(\"Both series must have the same categories\")\n\n    # Filter the series to only include categories with sales exceeding the threshold\n    s1_filtered = s1[s1 > 200]\n    s2_filtered = s2[s2 > 200]\n\n    # If no categories meet the threshold, return None for the plot and 0.0 for the distance\n    if s1_filtered.empty or s2_filtered.empty:\n        return None, 0.0\n\n    # Compute the Euclidean distance between the two series\n    euclidean_distance = distance.euclidean(s1_filtered, s2_filtered)\n\n    # Generate a bar plot for the filtered series\n    fig, ax = plt.subplots()\n    ax.bar(s1_filtered.index, s1_filtered.values, label='Store 1')\n    ax.bar(s2_filtered.index, s2_filtered.values, label='Store 2')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Sales')\n    ax.legend()\n    plt.show()\n\n    return ax, euclidean_distance"}
{"task_id": "BigCodeBench/1035", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(feature: pd.Series, target: pd.Series) -> (np.ndarray, plt.Axes):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(feature.values.reshape(-1, 1), target, test_size=0.2, random_state=42)\n\n    # Train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the target for the test set\n    y_pred = model.predict(X_test)\n\n    # Create the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Create the confusion matrix plot\n    fig, ax = plt.subplots(figsize=(6, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', cbar=False, square=True, xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'], ax=ax)\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('Actual')\n    ax.set_title('Confusion Matrix')\n\n    return cm, ax", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(feature: pd.Series, target: pd.Series) -> (np.ndarray, plt.Axes):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(feature.values.reshape(-1, 1), target, test_size=0.2, random_state=42)\n\n    # Train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the target for the test set\n    y_pred = model.predict(X_test)\n\n    # Create the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Create the confusion matrix plot\n    fig, ax = plt.subplots(figsize=(6, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', cbar=False, square=True, xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'], ax=ax)\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('Actual')\n    ax.set_title('Confusion Matrix')\n\n    return cm, ax"}
{"task_id": "BigCodeBench/1036", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(s1, s2):\n    \"\"\"\n    Visualizes two Series using a swarm plot with a highlight on their intersecting data points.\n    \"\"\"\n    # Create a DataFrame from the two Series\n    df = pd.DataFrame({'s1': s1, 's2': s2})\n\n    # Create a swarm plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.swarmplot(x='s1', y='s2', data=df)\n\n    # Highlight the intersecting data points\n    intersecting_data = df[df['s1'] == df['s2']]\n    for i, row in intersecting_data.iterrows():\n        ax.plot(row['s1'], row['s2'], 'or')\n\n    # Count the number of unique intersecting data points\n    intersection_count = len(intersecting_data['s1'].unique())\n\n    return ax, intersection_count", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(s1, s2):\n    \"\"\"\n    Visualizes two Series using a swarm plot with a highlight on their intersecting data points.\n    \"\"\"\n    # Create a DataFrame from the two Series\n    df = pd.DataFrame({'s1': s1, 's2': s2})\n\n    # Create a swarm plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.swarmplot(x='s1', y='s2', data=df)\n\n    # Highlight the intersecting data points\n    intersecting_data = df[df['s1'] == df['s2']]\n    for i, row in intersecting_data.iterrows():\n        ax.plot(row['s1'], row['s2'], 'or')\n\n    # Count the number of unique intersecting data points\n    intersection_count = len(intersecting_data['s1'].unique())\n\n    return ax, intersection_count"}
{"task_id": "BigCodeBench/1037", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(s1, s2, n_clusters=3):\n    if not isinstance(s1, pd.Series) or not isinstance(s2, pd.Series):\n        raise ValueError(\"s1 and s2 must be pandas Series\")\n    if len(s1) != len(s2):\n        raise ValueError(\"s1 and s2 must have the same length\")\n\n    data = pd.concat([s1, s2], axis=1)\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n\n    plt.figure(figsize=(8, 6))\n    plt.scatter(data[0], data[1], c=labels, cmap='viridis')\n    plt.title(\"K-Means Clustering\")\n    plt.xlabel(s1.name)\n    plt.ylabel(s2.name)\n    plt.legend(title='Clusters')\n\n    return labels, plt.gca()", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(s1, s2, n_clusters=3):\n    if not isinstance(s1, pd.Series) or not isinstance(s2, pd.Series):\n        raise ValueError(\"s1 and s2 must be pandas Series\")\n    if len(s1) != len(s2):\n        raise ValueError(\"s1 and s2 must have the same length\")\n\n    data = pd.concat([s1, s2], axis=1)\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n\n    plt.figure(figsize=(8, 6))\n    plt.scatter(data[0], data[1], c=labels, cmap='viridis')\n    plt.title(\"K-Means Clustering\")\n    plt.xlabel(s1.name)\n    plt.ylabel(s2.name)\n    plt.legend(title='Clusters')\n\n    return labels, plt.gca()"}
{"task_id": "BigCodeBench/1038", "solution": "from datetime import datetime\nimport json\ndef task_func(client_socket):\n    \"\"\"\n    Responds to a client's request by sending a JSON-formatted message containing the current server time and a greeting.\n    \"\"\"\n    # Get the current server time\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Create a JSON message\n    message = {\n        \"time\": current_time,\n        \"greeting\": \"Hello, client!\"\n    }\n\n    # Convert the message to JSON format\n    json_message = json.dumps(message)\n\n    # Send the JSON message to the client\n    client_socket.send(json_message.encode())\n\n    # Close the client socket\n    client_socket.close()\n\n    # Return None\n    return None", "raw_solution": "from datetime import datetime\nimport json\n\nSERVER_ADDRESS = \"localhost\"\nBUFFER_SIZE = 1024\n\ndef task_func(client_socket):\n    \"\"\"\n    Responds to a client's request by sending a JSON-formatted message containing the current server time and a greeting.\n    \"\"\"\n    # Get the current server time\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Create a JSON message\n    message = {\n        \"time\": current_time,\n        \"greeting\": \"Hello, client!\"\n    }\n\n    # Convert the message to JSON format\n    json_message = json.dumps(message)\n\n    # Send the JSON message to the client\n    client_socket.send(json_message.encode())\n\n    # Close the client socket\n    client_socket.close()\n\n    # Return None\n    return None"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n\n    secure_socket = context.wrap_socket(client_socket, server_side=True)\n\n    data = secure_socket.recv(buffer_size).decode()\n\n    if os.path.isfile(data):\n        with open(data, 'rb') as file:\n            file_data = file.read()\n            file_hash = hashlib.sha256(file_data).hexdigest()\n            return file_hash\n    else:\n        return 'File not found'\n\n    secure_socket.close()", "raw_solution": "import ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n\n    secure_socket = context.wrap_socket(client_socket, server_side=True)\n\n    data = secure_socket.recv(buffer_size).decode()\n\n    if os.path.isfile(data):\n        with open(data, 'rb') as file:\n            file_data = file.read()\n            file_hash = hashlib.sha256(file_data).hexdigest()\n            return file_hash\n    else:\n        return 'File not found'\n\n    secure_socket.close()"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # List for sockets and their corresponding buffers\n    sockets_list = [server_socket]\n    clients_list = {}\n    start_time = datetime.now()\n\n    while (datetime.now() - start_time).seconds < run_duration:\n        read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list)\n\n        # Handle received data\n        for notified_socket in read_sockets:\n            if notified_socket == server_socket:\n                client_socket, client_address = server_socket.accept()\n                sockets_list.append(client_socket)\n                clients_list[client_socket] = queue.Queue()\n            else:\n                data = notified_socket.recv(buffer_size)\n                if data:\n                    data = data.decode(\"utf-8\")\n                    data = f\"{data} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n                    notified_socket.send(data.encode(\"utf-8\"))\n                else:\n                    sockets_list.remove(notified_socket)\n                    del clients_list[notified_socket]\n\n        # Handle exceptions\n        for notified_socket in exception_sockets:\n            sockets_list.remove(notified_socket)\n            del clients_list[notified_socket]\n\n    server_socket.close()\n    return f\"Echo server has run for {run_duration} seconds.\"", "raw_solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # List for sockets and their corresponding buffers\n    sockets_list = [server_socket]\n    clients_list = {}\n    start_time = datetime.now()\n\n    while (datetime.now() - start_time).seconds < run_duration:\n        read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list)\n\n        # Handle received data\n        for notified_socket in read_sockets:\n            if notified_socket == server_socket:\n                client_socket, client_address = server_socket.accept()\n                sockets_list.append(client_socket)\n                clients_list[client_socket] = queue.Queue()\n            else:\n                data = notified_socket.recv(buffer_size)\n                if data:\n                    data = data.decode(\"utf-8\")\n                    data = f\"{data} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n                    notified_socket.send(data.encode(\"utf-8\"))\n                else:\n                    sockets_list.remove(notified_socket)\n                    del clients_list[notified_socket]\n\n        # Handle exceptions\n        for notified_socket in exception_sockets:\n            sockets_list.remove(notified_socket)\n            del clients_list[notified_socket]\n\n    server_socket.close()\n    return f\"Echo server has run for {run_duration} seconds.\"\n"}
{"task_id": "BigCodeBench/1041", "solution": "import re\nimport os\ndef task_func(request):\n    if request.startswith('GET /'):\n        filename = request.split(' ')[1]\n        if os.path.exists(filename):\n            with open(filename, 'r') as file:\n                content = file.read()\n            response = f'HTTP/1.1 200 OK\\r\\nContent-Length: {len(content)}\\r\\n\\r\\n{content}'\n        else:\n            response = 'HTTP/1.1 404 Not Found\\r\\nContent-Length: 0\\r\\n\\r\\n'\n    else:\n        response = 'HTTP/1.1 400 Bad Request\\r\\nContent-Length: 0\\r\\n\\r\\n'\n    return response", "raw_solution": "import re\nimport os\n\ndef task_func(request):\n    if request.startswith('GET /'):\n        filename = request.split(' ')[1]\n        if os.path.exists(filename):\n            with open(filename, 'r') as file:\n                content = file.read()\n            response = f'HTTP/1.1 200 OK\\r\\nContent-Length: {len(content)}\\r\\n\\r\\n{content}'\n        else:\n            response = 'HTTP/1.1 404 Not Found\\r\\nContent-Length: 0\\r\\n\\r\\n'\n    else:\n        response = 'HTTP/1.1 400 Bad Request\\r\\nContent-Length: 0\\r\\n\\r\\n'\n    return response"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass", "raw_solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef send_email(sender, recipient, password, message):\n    \"\"\"\n    Sends an email from a client socket.\n    \"\"\"\n    msg = EmailMessage()\n    msg['Subject'] = 'Message from Client'\n    msg['From'] = sender\n    msg['To'] = recipient\n    msg.set_content(message)\n\n    server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n    server.starttls()\n    server.login(sender, password)\n    server.send_message(msg)\n    server.quit()\n\n# Test the function\nsender = input(\"Enter your email: \")\nrecipient = input(\"Enter recipient's email: \")\npassword = getpass.getpass(\"Enter your email password: \")\nmessage = \"Hello, this is a test message from a client socket.\"\nsend_email(sender, recipient, password, message)"}
{"task_id": "BigCodeBench/1043", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCATEGORIES = [\"A\", \"B\", \"C\", \"D\", \"E\"]\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"The data list is empty.\")\n\n    # Create a DataFrame from the data_list\n    df = pd.DataFrame(data_list, columns=[\"Category\"])\n\n    # Count the occurrences of each category\n    category_counts = df[\"Category\"].value_counts()\n\n    # Combine the predefined categories and the categories in the data_list\n    all_categories = CATEGORIES + list(category_counts.index)\n\n    # Reindex the category_counts to include all categories\n    category_counts = category_counts.reindex(all_categories, fill_value=0)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(all_categories, category_counts.values, width=0.8, align=\"center\")\n\n    # Check if the distribution of predefined categories is uniform\n    if not category_counts.std() == 0:\n        print(\"The distribution of predefined categories is not uniform.\")\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"The data list is empty.\")\n\n    # Create a DataFrame from the data_list\n    df = pd.DataFrame(data_list, columns=[\"Category\"])\n\n    # Count the occurrences of each category\n    category_counts = df[\"Category\"].value_counts()\n\n    # Combine the predefined categories and the categories in the data_list\n    all_categories = CATEGORIES + list(category_counts.index)\n\n    # Reindex the category_counts to include all categories\n    category_counts = category_counts.reindex(all_categories, fill_value=0)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(all_categories, category_counts.values, width=0.8, align=\"center\")\n\n    # Check if the distribution of predefined categories is uniform\n    if not category_counts.std() == 0:\n        print(\"The distribution of predefined categories is not uniform.\")\n\n    return ax"}
{"task_id": "BigCodeBench/1044", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nROOMS = [\"Room1\", \"Room2\", \"Room3\", \"Room4\", \"Room5\"]\ndef task_func(date_str, booking_data):\n    # Validate date_str\n    try:\n        date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    except ValueError:\n        raise ValueError(\"Invalid date format. Please use 'yyyy-mm-dd' format.\")\n\n    # Validate date_str is not a past date\n    if date < datetime.now():\n        raise ValueError(\"Date cannot be in the past.\")\n\n    # Generate status report\n    status_report = pd.DataFrame(columns=[\"Room\", \"Status\"])\n    for room in ROOMS:\n        status = \"Booked\" if room in booking_data.keys() and booking_data[room] == date_str else \"Available\"\n        status_report = status_report.append({\"Room\": room, \"Status\": status}, ignore_index=True)\n\n    # Plot booking status\n    plt.figure(figsize=(10, 5))\n    plt.bar(status_report[\"Room\"], status_report[\"Status\"] == \"Booked\", color='b')\n    plt.xlabel('Room')\n    plt.ylabel('Booking Status')\n    plt.title(f'Booking Status for {date_str}')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n\n    return status_report, plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Constants\nROOMS = [\"Room1\", \"Room2\", \"Room3\", \"Room4\", \"Room5\"]\n\ndef task_func(date_str, booking_data):\n    # Validate date_str\n    try:\n        date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    except ValueError:\n        raise ValueError(\"Invalid date format. Please use 'yyyy-mm-dd' format.\")\n\n    # Validate date_str is not a past date\n    if date < datetime.now():\n        raise ValueError(\"Date cannot be in the past.\")\n\n    # Generate status report\n    status_report = pd.DataFrame(columns=[\"Room\", \"Status\"])\n    for room in ROOMS:\n        status = \"Booked\" if room in booking_data.keys() and booking_data[room] == date_str else \"Available\"\n        status_report = status_report.append({\"Room\": room, \"Status\": status}, ignore_index=True)\n\n    # Plot booking status\n    plt.figure(figsize=(10, 5))\n    plt.bar(status_report[\"Room\"], status_report[\"Status\"] == \"Booked\", color='b')\n    plt.xlabel('Room')\n    plt.ylabel('Booking Status')\n    plt.title(f'Booking Status for {date_str}')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n\n    return status_report, plt.gca()"}
{"task_id": "BigCodeBench/1045", "solution": "from datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\nLEAP_SECONDS = np.array(\n    [\n        1972,\n        1973,\n        1974,\n        1975,\n        1976,\n        1977,\n        1978,\n        1979,\n        1980,\n        1981,\n        1982,\n        1983,\n        1985,\n        1988,\n        1990,\n        1993,\n        1994,\n        1997,\n        1999,\n        2006,\n        2009,\n        2012,\n        2015,\n        2016,\n        2020,\n    ]\n)\ndef task_func(date_str):\n    given_date = parse(date_str)\n    current_date = datetime.now()\n    total_seconds = (current_date - given_date).total_seconds()\n    leap_seconds = sum(LEAP_SECONDS[LEAP_SECONDS <= current_date.year])\n    return int(total_seconds + leap_seconds)", "raw_solution": "from datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\n\nLEAP_SECONDS = np.array(\n    [\n        1972,\n        1973,\n        1974,\n        1975,\n        1976,\n        1977,\n        1978,\n        1979,\n        1980,\n        1981,\n        1982,\n        1983,\n        1985,\n        1988,\n        1990,\n        1993,\n        1994,\n        1997,\n        1999,\n        2006,\n        2009,\n        2012,\n        2015,\n        2016,\n        2020,\n    ]\n)\n\ndef task_func(date_str):\n    given_date = parse(date_str)\n    current_date = datetime.now()\n    total_seconds = (current_date - given_date).total_seconds()\n    leap_seconds = sum(LEAP_SECONDS[LEAP_SECONDS <= current_date.year])\n    return int(total_seconds + leap_seconds)"}
{"task_id": "BigCodeBench/1046", "solution": "from datetime import datetime, timedelta\nimport pandas as pd", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\n\n# Constants\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\n\ndef generate_dates(date_str):\n    \"\"\"\n    Generates a pandas DataFrame containing a series of dates for a predefined list of employees.\n    The function takes a string date_str in the format 'YYYY-MM-DD' and outputs a pandas DataFrame\n    with 'Employee' and 'Date' columns, listing the next 10 days for each employee.\n    \"\"\"\n    # Convert string to datetime object\n    date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n\n    # Generate a list of dates for the next 10 days\n    dates = [(date_obj + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(10)]\n\n    # Create a DataFrame from the list of dates\n    df = pd.DataFrame(list(product(EMPLOYEES, dates)), columns=['Employee', 'Date'])\n\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/1047", "solution": "from datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(date_str):\n    \"\"\"\n    Generates a list of random integers, where the count of integers equals the day of the month in the provided date,\n    then generates a line plot of these integers and returns the Axes object of the plot.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n\n    # Generate a list of random integers\n    random_list = [random.randint(0, 100) for _ in range(date.day)]\n\n    # Generate a line plot of the random integers\n    ax = plt.plot(random_list)\n\n    return ax", "raw_solution": "from datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(date_str):\n    \"\"\"\n    Generates a list of random integers, where the count of integers equals the day of the month in the provided date,\n    then generates a line plot of these integers and returns the Axes object of the plot.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n\n    # Generate a list of random integers\n    random_list = [random.randint(0, 100) for _ in range(date.day)]\n\n    # Generate a line plot of the random integers\n    ax = plt.plot(random_list)\n\n    return ax"}
{"task_id": "BigCodeBench/1048", "solution": "from datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(date_str):\n    \"\"\"\n    Plots a sine wave whose frequency is determined by the day of the month from the given date.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n\n    # Extract the day of the month\n    day_of_month = date.day\n\n    # Generate x values for the sine wave\n    x = np.linspace(0, 2 * np.pi, 1000)\n\n    # Calculate the frequency based on the day of the month\n    frequency = day_of_month / 30  # Assuming a month has 30 days\n\n    # Generate y values for the sine wave\n    y = np.sin(frequency * x)\n\n    # Create a new figure and plot the sine wave\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n\n    # Set the title and labels\n    ax.set_title(f'Sine Wave with Frequency Determined by Day of Month ({date_str})')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    return ax", "raw_solution": "from datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(date_str):\n    \"\"\"\n    Plots a sine wave whose frequency is determined by the day of the month from the given date.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n\n    # Extract the day of the month\n    day_of_month = date.day\n\n    # Generate x values for the sine wave\n    x = np.linspace(0, 2 * np.pi, 1000)\n\n    # Calculate the frequency based on the day of the month\n    frequency = day_of_month / 30  # Assuming a month has 30 days\n\n    # Generate y values for the sine wave\n    y = np.sin(frequency * x)\n\n    # Create a new figure and plot the sine wave\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n\n    # Set the title and labels\n    ax.set_title(f'Sine Wave with Frequency Determined by Day of Month ({date_str})')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    return ax"}
{"task_id": "BigCodeBench/1049", "solution": "import re\nimport pandas as pd\ndef task_func(input_string: str) -> pd.DataFrame:\n    lines = input_string.split('\\n')\n    lines = [line.strip() for line in lines if line.strip()]\n    lines = [re.sub('\\t', ' ', line) for line in lines]\n    return pd.DataFrame(lines, columns=['Text'])", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(input_string: str) -> pd.DataFrame:\n    lines = input_string.split('\\n')\n    lines = [line.strip() for line in lines if line.strip()]\n    lines = [re.sub('\\t', ' ', line) for line in lines]\n    return pd.DataFrame(lines, columns=['Text'])"}
{"task_id": "BigCodeBench/1050", "solution": "import os\nimport hashlib\nDIRECTORY = \"./hashed_files\"\ndef task_func(input_string):\n    # Create directory if it doesn't exist\n    if not os.path.exists(DIRECTORY):\n        os.makedirs(DIRECTORY)\n\n    # Split the input string into lines\n    lines = input_string.split('\\n')\n\n    # List to store the file paths\n    file_paths = []\n\n    # Iterate over the lines\n    for line in lines:\n        # Ignore empty lines\n        if line.strip():\n            # Hash the line\n            hashed_line = hashlib.sha256(line.encode()).hexdigest()\n\n            # Get the first 10 characters of the hash\n            file_name = hashed_line[:10]\n\n            # Create the file path\n            file_path = os.path.join(DIRECTORY, file_name + '.txt')\n\n            # Write the hashed line to the file\n            with open(file_path, 'w') as file:\n                file.write(hashed_line)\n\n            # Add the file path to the list\n            file_paths.append(file_path)\n\n    # Return the list of file paths\n    return file_paths", "raw_solution": "import os\nimport hashlib\n\n# Constants\nDIRECTORY = \"./hashed_files\"\n\ndef task_func(input_string):\n    # Create directory if it doesn't exist\n    if not os.path.exists(DIRECTORY):\n        os.makedirs(DIRECTORY)\n\n    # Split the input string into lines\n    lines = input_string.split('\\n')\n\n    # List to store the file paths\n    file_paths = []\n\n    # Iterate over the lines\n    for line in lines:\n        # Ignore empty lines\n        if line.strip():\n            # Hash the line\n            hashed_line = hashlib.sha256(line.encode()).hexdigest()\n\n            # Get the first 10 characters of the hash\n            file_name = hashed_line[:10]\n\n            # Create the file path\n            file_path = os.path.join(DIRECTORY, file_name + '.txt')\n\n            # Write the hashed line to the file\n            with open(file_path, 'w') as file:\n                file.write(hashed_line)\n\n            # Add the file path to the list\n            file_paths.append(file_path)\n\n    # Return the list of file paths\n    return file_paths"}
{"task_id": "BigCodeBench/1051", "solution": "import collections\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    if not data_dict:\n        return None, \"The distribution is uniform.\"\n\n    counts = list(data_dict.values())\n    avg_count = np.mean(counts)\n\n    is_uniform = all(abs(count - avg_count) <= 1e-5 for count in counts)\n\n    fig, ax = plt.subplots()\n    ax.hist(data_dict.keys(), bins=min(10, len(set(counts))), weights=counts)\n    ax.set_xticks(data_dict.keys())\n    ax.set_xticklabels(data_dict.keys())\n    plt.close(fig)\n\n    return ax, \"The distribution is uniform.\" if is_uniform else \"The distribution is not uniform.\"", "raw_solution": "import collections\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict):\n    if not data_dict:\n        return None, \"The distribution is uniform.\"\n\n    counts = list(data_dict.values())\n    avg_count = np.mean(counts)\n\n    is_uniform = all(abs(count - avg_count) <= 1e-5 for count in counts)\n\n    fig, ax = plt.subplots()\n    ax.hist(data_dict.keys(), bins=min(10, len(set(counts))), weights=counts)\n    ax.set_xticks(data_dict.keys())\n    ax.set_xticklabels(data_dict.keys())\n    plt.close(fig)\n\n    return ax, \"The distribution is uniform.\" if is_uniform else \"The distribution is not uniform.\""}
{"task_id": "BigCodeBench/1052", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    # Load the CSV file\n    try:\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        print(f\"Error loading file: {e}\")\n        return None\n\n    # Check if the dataframe is empty\n    if df.empty:\n        print(\"The input file is empty or contains only stop words.\")\n        return None\n\n    # Perform text vectorization\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    X = vectorizer.fit_transform(df['text'])\n\n    # Get the most common words\n    word_counts = X.toarray().sum(axis=0)\n    word_counts_df = pd.DataFrame(word_counts, index=vectorizer.get_feature_names_out(), columns=['count'])\n    most_common_words = word_counts_df.nlargest(10, 'count')\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    most_common_words.plot(kind='bar', ax=ax)\n    ax.set_title('Most Common Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n\n    # Save or display the plot\n    if save_path:\n        plt.savefig(save_path)\n        return None\n    else:\n        return ax", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    # Load the CSV file\n    try:\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        print(f\"Error loading file: {e}\")\n        return None\n\n    # Check if the dataframe is empty\n    if df.empty:\n        print(\"The input file is empty or contains only stop words.\")\n        return None\n\n    # Perform text vectorization\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    X = vectorizer.fit_transform(df['text'])\n\n    # Get the most common words\n    word_counts = X.toarray().sum(axis=0)\n    word_counts_df = pd.DataFrame(word_counts, index=vectorizer.get_feature_names_out(), columns=['count'])\n    most_common_words = word_counts_df.nlargest(10, 'count')\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    most_common_words.plot(kind='bar', ax=ax)\n    ax.set_title('Most Common Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n\n    # Save or display the plot\n    if save_path:\n        plt.savefig(save_path)\n        return None\n    else:\n        return ax"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File {file_path} not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    # If the CSV file does not have a header, use the first column as the text data\n    if 'Text' in df.columns:\n        text_data = df['Text']\n    else:\n        text_data = df[df.columns[0]]\n\n    # Split text into words\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    word_counts = vectorizer.fit_transform(text_data)\n\n    # Create a histogram of the frequency of the top ten most common words\n    word_freq = pd.Series(word_counts.toarray().sum(axis=0)).sort_values(ascending=False)\n    top_10_words = word_freq[:10]\n\n    # Plot the histogram\n    ax = top_10_words.plot(kind='bar')\n    ax.set_title('Top 10 Most Common Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n\n    # Save the plot to a file if a save_path is provided\n    if save_path:\n        plt.savefig(save_path)\n        return None\n\n    return ax", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File {file_path} not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    # If the CSV file does not have a header, use the first column as the text data\n    if 'Text' in df.columns:\n        text_data = df['Text']\n    else:\n        text_data = df[df.columns[0]]\n\n    # Split text into words\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    word_counts = vectorizer.fit_transform(text_data)\n\n    # Create a histogram of the frequency of the top ten most common words\n    word_freq = pd.Series(word_counts.toarray().sum(axis=0)).sort_values(ascending=False)\n    top_10_words = word_freq[:10]\n\n    # Plot the histogram\n    ax = top_10_words.plot(kind='bar')\n    ax.set_title('Top 10 Most Common Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n\n    # Save the plot to a file if a save_path is provided\n    if save_path:\n        plt.savefig(save_path)\n        return None\n\n    return ax"}
{"task_id": "BigCodeBench/1054", "solution": "import csv\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    try:\n        with open(file_path, 'r') as file:\n            reader = csv.reader(file)\n            population = [float(row[0]) for row in reader if row]\n    except (FileNotFoundError, IOError) as e:\n        return f\"Error: {str(e)}\"\n\n    sample = np.random.choice(population, size=30, replace=False)\n    sample_mean = np.mean(sample)\n    sample_stddev = np.std(sample, ddof=1)\n\n    fig, ax = plt.subplots()\n    ax.hist(sample, bins='auto', alpha=0.7, rwidth=0.85, color='#003f5c', density=True)\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, sample_mean, sample_stddev)\n    ax.plot(x, p, 'k', linewidth=2)\n    title = f\"Histogram of Sample Mean: {sample_mean:.2f}, Sample Std Dev: {sample_stddev:.2f}\"\n    ax.set_title(title)\n\n    return (sample_mean, sample_stddev, ax)", "raw_solution": "import csv\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    try:\n        with open(file_path, 'r') as file:\n            reader = csv.reader(file)\n            population = [float(row[0]) for row in reader if row]\n    except (FileNotFoundError, IOError) as e:\n        return f\"Error: {str(e)}\"\n\n    sample = np.random.choice(population, size=30, replace=False)\n    sample_mean = np.mean(sample)\n    sample_stddev = np.std(sample, ddof=1)\n\n    fig, ax = plt.subplots()\n    ax.hist(sample, bins='auto', alpha=0.7, rwidth=0.85, color='#003f5c', density=True)\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, sample_mean, sample_stddev)\n    ax.plot(x, p, 'k', linewidth=2)\n    title = f\"Histogram of Sample Mean: {sample_mean:.2f}, Sample Std Dev: {sample_stddev:.2f}\"\n    ax.set_title(title)\n\n    return (sample_mean, sample_stddev, ax)"}
{"task_id": "BigCodeBench/1055", "solution": "import pandas as pd\nimport itertools\nimport random\ndef task_func(colors, states):\n    # Generate all possible combinations of colors and states\n    combinations = list(itertools.product(colors, states))\n\n    # Shuffle the combinations\n    random.shuffle(combinations)\n\n    # Determine the number of columns\n    num_columns = min(len(colors), len(states))\n\n    # Split the combinations into chunks of equal size\n    chunks = [combinations[i:i + num_columns] for i in range(0, len(combinations), num_columns)]\n\n    # Create a DataFrame from the chunks\n    df = pd.DataFrame(chunks).T\n\n    # Format the DataFrame\n    df.columns = [f\"Color:State {i+1}\" for i in range(num_columns)]\n\n    return df", "raw_solution": "import pandas as pd\nimport itertools\nimport random\n\ndef task_func(colors, states):\n    # Generate all possible combinations of colors and states\n    combinations = list(itertools.product(colors, states))\n\n    # Shuffle the combinations\n    random.shuffle(combinations)\n\n    # Determine the number of columns\n    num_columns = min(len(colors), len(states))\n\n    # Split the combinations into chunks of equal size\n    chunks = [combinations[i:i + num_columns] for i in range(0, len(combinations), num_columns)]\n\n    # Create a DataFrame from the chunks\n    df = pd.DataFrame(chunks).T\n\n    # Format the DataFrame\n    df.columns = [f\"Color:State {i+1}\" for i in range(num_columns)]\n\n    return df"}
{"task_id": "BigCodeBench/1056", "solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\nNUMBERS = list(range(1, 27))\ndef task_func(n_pairs=26):\n    if n_pairs < 1 or n_pairs > 26:\n        raise ValueError(\"'n_pairs' must be within the range of 1 to 26, inclusive.\")\n\n    # Generate letter-number pairs\n    pairs = [f\"{LETTERS[i]}:{NUMBERS[i]}\" for i in range(n_pairs)]\n    random.shuffle(pairs)\n\n    # Generate random counts\n    counts = [random.randint(1, 9) for _ in range(n_pairs)]\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(pairs, counts)\n\n    # Set chart title and labels\n    ax.set_title(\"Random Letter:Number Pairs Chart\")\n    ax.set_xlabel(\"Letter:Number Pairs\")\n    ax.set_ylabel(\"Counts\")\n\n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=90)\n\n    return ax", "raw_solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\nNUMBERS = list(range(1, 27))\n\ndef task_func(n_pairs=26):\n    if n_pairs < 1 or n_pairs > 26:\n        raise ValueError(\"'n_pairs' must be within the range of 1 to 26, inclusive.\")\n\n    # Generate letter-number pairs\n    pairs = [f\"{LETTERS[i]}:{NUMBERS[i]}\" for i in range(n_pairs)]\n    random.shuffle(pairs)\n\n    # Generate random counts\n    counts = [random.randint(1, 9) for _ in range(n_pairs)]\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(pairs, counts)\n\n    # Set chart title and labels\n    ax.set_title(\"Random Letter:Number Pairs Chart\")\n    ax.set_xlabel(\"Letter:Number Pairs\")\n    ax.set_ylabel(\"Counts\")\n\n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=90)\n\n    return ax"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    if animals is None:\n        animals = ['Dog', 'Cat', 'Bird']\n    if foods is None:\n        foods = ['Bread', 'Milk', 'Fish']\n\n    if not animals or not foods:\n        return pd.DataFrame()\n\n    combinations = list(itertools.product(animals, foods))\n    np.random.shuffle(combinations)\n\n    df = pd.DataFrame(combinations, columns=foods)\n    df.index = animals\n\n    return df.applymap(lambda x: f\"{x[0]}:{x[1]}\")", "raw_solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    if animals is None:\n        animals = ['Dog', 'Cat', 'Bird']\n    if foods is None:\n        foods = ['Bread', 'Milk', 'Fish']\n\n    if not animals or not foods:\n        return pd.DataFrame()\n\n    combinations = list(itertools.product(animals, foods))\n    np.random.shuffle(combinations)\n\n    df = pd.DataFrame(combinations, columns=foods)\n    df.index = animals\n\n    return df.applymap(lambda x: f\"{x[0]}:{x[1]}\")"}
{"task_id": "BigCodeBench/1058", "solution": "import itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nSHAPES = [\n    \"Circle\",\n    \"Square\",\n    \"Triangle\",\n    \"Rectangle\",\n    \"Pentagon\",\n    \"Hexagon\",\n    \"Heptagon\",\n    \"Octagon\",\n    \"Nonagon\",\n    \"Decagon\",\n]\nCOLORS = [\n    \"Red\",\n    \"Blue\",\n    \"Green\",\n    \"Yellow\",\n    \"Black\",\n    \"White\",\n    \"Purple\",\n    \"Orange\",\n    \"Pink\",\n    \"Brown\",\n]\ndef task_func(num_pairs=10):\n    # Generate shape-color pairs\n    shape_color_pairs = list(itertools.product(SHAPES, COLORS))[:num_pairs]\n\n    # Create a DataFrame from the shape-color pairs\n    df = pd.DataFrame(shape_color_pairs, columns=[\"Shape\", \"Color\"])\n\n    # Generate the countplot\n    plt.figure(figsize=(10, 6))\n    ax = sns.countplot(x=\"Shape\", hue=\"Color\", data=df)\n    ax.set_title(\"Shape-Color Countplot\")\n\n    return ax", "raw_solution": "import itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nSHAPES = [\n    \"Circle\",\n    \"Square\",\n    \"Triangle\",\n    \"Rectangle\",\n    \"Pentagon\",\n    \"Hexagon\",\n    \"Heptagon\",\n    \"Octagon\",\n    \"Nonagon\",\n    \"Decagon\",\n]\nCOLORS = [\n    \"Red\",\n    \"Blue\",\n    \"Green\",\n    \"Yellow\",\n    \"Black\",\n    \"White\",\n    \"Purple\",\n    \"Orange\",\n    \"Pink\",\n    \"Brown\",\n]\n\ndef task_func(num_pairs=10):\n    # Generate shape-color pairs\n    shape_color_pairs = list(itertools.product(SHAPES, COLORS))[:num_pairs]\n\n    # Create a DataFrame from the shape-color pairs\n    df = pd.DataFrame(shape_color_pairs, columns=[\"Shape\", \"Color\"])\n\n    # Generate the countplot\n    plt.figure(figsize=(10, 6))\n    ax = sns.countplot(x=\"Shape\", hue=\"Color\", data=df)\n    ax.set_title(\"Shape-Color Countplot\")\n\n    return ax"}
{"task_id": "BigCodeBench/1059", "solution": "import numpy as np\nimport random\nimport itertools\nimport pandas as pd\nPLANETS = [\n    \"Mercury\",\n    \"Venus\",\n    \"Earth\",\n    \"Mars\",\n    \"Jupiter\",\n    \"Saturn\",\n    \"Uranus\",\n    \"Neptune\",\n]\nELEMENTS = [\n    \"Hydrogen\",\n    \"Helium\",\n    \"Oxygen\",\n    \"Carbon\",\n    \"Nitrogen\",\n    \"Magnesium\",\n    \"Silicon\",\n    \"Iron\",\n    \"Nickel\",\n]\ndef task_func():\n    # Generate all possible planet-element pairs\n    pairs = list(itertools.product(PLANETS, ELEMENTS))\n\n    # Shuffle the pairs\n    random.shuffle(pairs)\n\n    # Create a DataFrame from the pairs\n    df = pd.DataFrame(pairs, columns=['Planet', 'Element'])\n\n    # Format the pairs as 'Planet:Element'\n    df['Planet:Element'] = df['Planet'] + ': ' + df['Element']\n\n    # Drop the 'Planet' and 'Element' columns\n    df = df.drop(columns=['Planet', 'Element'])\n\n    return df", "raw_solution": "import numpy as np\nimport random\nimport itertools\nimport pandas as pd\n\n# Constants\nPLANETS = [\n    \"Mercury\",\n    \"Venus\",\n    \"Earth\",\n    \"Mars\",\n    \"Jupiter\",\n    \"Saturn\",\n    \"Uranus\",\n    \"Neptune\",\n]\nELEMENTS = [\n    \"Hydrogen\",\n    \"Helium\",\n    \"Oxygen\",\n    \"Carbon\",\n    \"Nitrogen\",\n    \"Magnesium\",\n    \"Silicon\",\n    \"Iron\",\n    \"Nickel\",\n]\n\ndef task_func():\n    # Generate all possible planet-element pairs\n    pairs = list(itertools.product(PLANETS, ELEMENTS))\n\n    # Shuffle the pairs\n    random.shuffle(pairs)\n\n    # Create a DataFrame from the pairs\n    df = pd.DataFrame(pairs, columns=['Planet', 'Element'])\n\n    # Format the pairs as 'Planet:Element'\n    df['Planet:Element'] = df['Planet'] + ': ' + df['Element']\n\n    # Drop the 'Planet' and 'Element' columns\n    df = df.drop(columns=['Planet', 'Element'])\n\n    return df"}
{"task_id": "BigCodeBench/1060", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame, column_name: str) -> (str, plt.Axes):\n    if df.empty or column_name not in df.columns or df[column_name].isnull().all():\n        plt.figure()\n        plt.title(f\"Distribution of values in {column_name} (No Data)\")\n        plt.xlabel(\"Values\")\n        plt.ylabel(\"Frequency\")\n        return \"The DataFrame is empty or the specified column has no data.\", plt.gca()\n\n    unique_values = df[column_name].nunique()\n    value_counts = df[column_name].value_counts()\n    is_uniform = unique_values == len(value_counts)\n\n    plt.figure()\n    plt.hist(value_counts, bins=unique_values, edgecolor='black', alpha=0.7)\n    plt.xlabel(\"Values\")\n    plt.ylabel(\"Frequency\")\n    plt.title(f\"Distribution of values in {column_name}\")\n\n    return (\"The distribution of values is uniform.\" if is_uniform else \"The distribution of values is not uniform.\"), plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame, column_name: str) -> (str, plt.Axes):\n    if df.empty or column_name not in df.columns or df[column_name].isnull().all():\n        plt.figure()\n        plt.title(f\"Distribution of values in {column_name} (No Data)\")\n        plt.xlabel(\"Values\")\n        plt.ylabel(\"Frequency\")\n        return \"The DataFrame is empty or the specified column has no data.\", plt.gca()\n\n    unique_values = df[column_name].nunique()\n    value_counts = df[column_name].value_counts()\n    is_uniform = unique_values == len(value_counts)\n\n    plt.figure()\n    plt.hist(value_counts, bins=unique_values, edgecolor='black', alpha=0.7)\n    plt.xlabel(\"Values\")\n    plt.ylabel(\"Frequency\")\n    plt.title(f\"Distribution of values in {column_name}\")\n\n    return (\"The distribution of values is uniform.\" if is_uniform else \"The distribution of values is not uniform.\"), plt.gca()"}
{"task_id": "BigCodeBench/1061", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(arr: np.ndarray) -> (plt.Axes, np.ndarray):\n    # Calculate the sum of elements in each row\n    row_sums = np.sum(arr, axis=1)\n\n    # Normalize the row sums\n    mean = np.mean(row_sums)\n    std_dev = np.std(row_sums)\n    normalized_data = (row_sums - mean) / std_dev if std_dev != 0 else np.zeros_like(row_sums)\n\n    # Plot the histogram of the normalized data\n    fig, ax = plt.subplots()\n    ax.hist(normalized_data, bins=30, density=True, alpha=0.6, color='g')\n    ax.set_title(\"Histogram of Normalized Data with Standard Normal PDF\")\n\n    # Plot the PDF of a standard normal distribution\n    x = np.linspace(-4, 4, 1000)\n    ax.plot(x, norm.pdf(x), color='r', linewidth=2)\n\n    return ax, normalized_data", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(arr: np.ndarray) -> (plt.Axes, np.ndarray):\n    # Calculate the sum of elements in each row\n    row_sums = np.sum(arr, axis=1)\n\n    # Normalize the row sums\n    mean = np.mean(row_sums)\n    std_dev = np.std(row_sums)\n    normalized_data = (row_sums - mean) / std_dev if std_dev != 0 else np.zeros_like(row_sums)\n\n    # Plot the histogram of the normalized data\n    fig, ax = plt.subplots()\n    ax.hist(normalized_data, bins=30, density=True, alpha=0.6, color='g')\n    ax.set_title(\"Histogram of Normalized Data with Standard Normal PDF\")\n\n    # Plot the PDF of a standard normal distribution\n    x = np.linspace(-4, 4, 1000)\n    ax.plot(x, norm.pdf(x), color='r', linewidth=2)\n\n    return ax, normalized_data"}
{"task_id": "BigCodeBench/1062", "solution": "import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndef task_func(arr):\n    if arr.size == 0:\n        fig, ax = plt.subplots()\n        ax.set_title('Time Series of Row Sums')\n        return ax\n\n    dates = pd.date_range(start='1/1/2020', periods=arr.shape[0])\n    sums = np.sum(arr, axis=1)\n    df = pd.DataFrame({'Date': dates, 'Sum': sums})\n    df.set_index('Date', inplace=True)\n\n    fig, ax = plt.subplots()\n    df['Sum'].plot(ax=ax)\n    ax.set_title('Time Series of Row Sums')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sum')\n\n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef task_func(arr):\n    if arr.size == 0:\n        fig, ax = plt.subplots()\n        ax.set_title('Time Series of Row Sums')\n        return ax\n\n    dates = pd.date_range(start='1/1/2020', periods=arr.shape[0])\n    sums = np.sum(arr, axis=1)\n    df = pd.DataFrame({'Date': dates, 'Sum': sums})\n    df.set_index('Date', inplace=True)\n\n    fig, ax = plt.subplots()\n    df['Sum'].plot(ax=ax)\n    ax.set_title('Time Series of Row Sums')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sum')\n\n    return ax"}
{"task_id": "BigCodeBench/1063", "solution": "from matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\ndef task_func(arr):\n    # Perform PCA on the sum of rows of the array\n    pca = PCA(n_components=1)\n    pca.fit(arr)\n\n    # Create a plot of the explained variance ratio\n    fig, ax = plt.subplots()\n    ax.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n    ax.set_title(\"Explained Variance Ratio of Principal Components\")\n    ax.set_xlabel(\"Principal Component\")\n    ax.set_ylabel(\"Explained Variance Ratio\")\n\n    return ax", "raw_solution": "from matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\ndef task_func(arr):\n    # Perform PCA on the sum of rows of the array\n    pca = PCA(n_components=1)\n    pca.fit(arr)\n\n    # Create a plot of the explained variance ratio\n    fig, ax = plt.subplots()\n    ax.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n    ax.set_title(\"Explained Variance Ratio of Principal Components\")\n    ax.set_xlabel(\"Principal Component\")\n    ax.set_ylabel(\"Explained Variance Ratio\")\n\n    return ax"}
{"task_id": "BigCodeBench/1064", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(arr):\n    \"\"\"\n    Plots a heatmap of a given 2D numerical array and prints the sum of each row.\n    The heatmap's color range is set based on the minimum and maximum values in the array.\n    \"\"\"\n    # Calculate the sum of each row\n    row_sums = np.sum(arr, axis=1)\n    print(f\"Row sums: {row_sums}\")\n\n    # Create a heatmap\n    ax = sns.heatmap(arr, cmap='viridis', vmin=np.min(arr), vmax=np.max(arr))\n\n    return ax\narr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(arr):\n    \"\"\"\n    Plots a heatmap of a given 2D numerical array and prints the sum of each row.\n    The heatmap's color range is set based on the minimum and maximum values in the array.\n    \"\"\"\n    # Calculate the sum of each row\n    row_sums = np.sum(arr, axis=1)\n    print(f\"Row sums: {row_sums}\")\n\n    # Create a heatmap\n    ax = sns.heatmap(arr, cmap='viridis', vmin=np.min(arr), vmax=np.max(arr))\n\n    return ax\n\n# Test the function\narr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nax = task_func(arr)\nplt.show()"}
{"task_id": "BigCodeBench/1065", "solution": "from scipy import fftpack\nfrom matplotlib import pyplot as plt\nimport numpy as np\ndef task_func(arr):\n    \"\"\"\n    Performs a Fast Fourier Transform (FFT) on the sum of each row in a 2D array and plots the absolute values of the FFT coefficients.\n    \"\"\"\n    # Calculate the sum of each row\n    row_sums = np.sum(arr, axis=1)\n\n    # Perform FFT on the row sums\n    fft_coefficients = fftpack.fft(row_sums)\n\n    # Calculate the absolute values of the FFT coefficients\n    abs_fft_coefficients = np.abs(fft_coefficients)\n\n    # Plot the absolute values of the FFT coefficients\n    plt.plot(abs_fft_coefficients)\n    plt.title('Absolute Values of FFT Coefficients')\n    plt.xlabel('Frequency')\n    plt.ylabel('Magnitude')\n\n    return plt.gca()", "raw_solution": "from scipy import fftpack\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef task_func(arr):\n    \"\"\"\n    Performs a Fast Fourier Transform (FFT) on the sum of each row in a 2D array and plots the absolute values of the FFT coefficients.\n    \"\"\"\n    # Calculate the sum of each row\n    row_sums = np.sum(arr, axis=1)\n\n    # Perform FFT on the row sums\n    fft_coefficients = fftpack.fft(row_sums)\n\n    # Calculate the absolute values of the FFT coefficients\n    abs_fft_coefficients = np.abs(fft_coefficients)\n\n    # Plot the absolute values of the FFT coefficients\n    plt.plot(abs_fft_coefficients)\n    plt.title('Absolute Values of FFT Coefficients')\n    plt.xlabel('Frequency')\n    plt.ylabel('Magnitude')\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/1066", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nNUM_SAMPLES = 100\nNUM_OUTLIERS = 5\ndef task_func(num_samples=NUM_SAMPLES, num_outliers=NUM_OUTLIERS):\n    # Generate normally distributed data\n    normal_data = np.random.normal(loc=0, scale=1, size=num_samples)\n\n    # Generate artificially introduced outliers\n    outlier_data = np.random.uniform(low=-3, high=3, size=num_outliers)\n\n    # Combine the data\n    data = np.concatenate([normal_data, outlier_data])\n\n    # Detect outliers using the IQR method\n    Q1 = np.percentile(normal_data, 25)\n    Q3 = np.percentile(normal_data, 75)\n    IQR = Q3 - Q1\n    outliers_detected = data[(data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))]\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=20, color='blue', alpha=0.6, rwidth=0.85)\n    ax.set_xlabel('Data')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Combined Data')\n\n    return data, outliers_detected, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 100\nNUM_OUTLIERS = 5\n\ndef task_func(num_samples=NUM_SAMPLES, num_outliers=NUM_OUTLIERS):\n    # Generate normally distributed data\n    normal_data = np.random.normal(loc=0, scale=1, size=num_samples)\n\n    # Generate artificially introduced outliers\n    outlier_data = np.random.uniform(low=-3, high=3, size=num_outliers)\n\n    # Combine the data\n    data = np.concatenate([normal_data, outlier_data])\n\n    # Detect outliers using the IQR method\n    Q1 = np.percentile(normal_data, 25)\n    Q3 = np.percentile(normal_data, 75)\n    IQR = Q3 - Q1\n    outliers_detected = data[(data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))]\n\n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=20, color='blue', alpha=0.6, rwidth=0.85)\n    ax.set_xlabel('Data')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Combined Data')\n\n    return data, outliers_detected, ax"}
{"task_id": "BigCodeBench/1067", "solution": "import requests\nimport logging\nfrom requests.exceptions import HTTPError, RequestException\ndef task_func(repo_url: str) -> dict:\n    \"\"\"\n    Fetches and returns information about a GitHub repository using its API URL.\n    \"\"\"\n    try:\n        response = requests.get(repo_url)\n        # If the response was successful, no Exception will be raised\n        response.raise_for_status()\n    except HTTPError as http_err:\n        logging.error(f'HTTP error occurred: {http_err}')\n        raise\n    except RequestException as req_err:\n        logging.error(f'Request error occurred: {req_err}')\n        raise\n    else:\n        repo_info = response.json()\n        open_issues_count = repo_info.get('open_issues_count', 0)\n        if open_issues_count > 100:\n            logging.warning(f'Warning: The repository has more than 100 open issues.')\n        return repo_info", "raw_solution": "import requests\nimport logging\nfrom requests.exceptions import HTTPError, RequestException\n\ndef task_func(repo_url: str) -> dict:\n    \"\"\"\n    Fetches and returns information about a GitHub repository using its API URL.\n    \"\"\"\n    try:\n        response = requests.get(repo_url)\n        # If the response was successful, no Exception will be raised\n        response.raise_for_status()\n    except HTTPError as http_err:\n        logging.error(f'HTTP error occurred: {http_err}')\n        raise\n    except RequestException as req_err:\n        logging.error(f'Request error occurred: {req_err}')\n        raise\n    else:\n        repo_info = response.json()\n        open_issues_count = repo_info.get('open_issues_count', 0)\n        if open_issues_count > 100:\n            logging.warning(f'Warning: The repository has more than 100 open issues.')\n        return repo_info"}
{"task_id": "BigCodeBench/1068", "solution": "import warnings\nimport sqlite3\nimport pandas as pd\ndef task_func(db_path, query, warn_large_dataset=True):\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(db_path)\n    except Exception as e:\n        raise Exception(f\"Error fetching data from the database: {str(e)}\")\n\n    try:\n        # Execute the SQL query\n        df = pd.read_sql_query(query, conn)\n    except Exception as e:\n        raise Exception(f\"Error fetching data from the database: {str(e)}\")\n\n    # Check if the data contains more than 10000 rows\n    if warn_large_dataset and len(df) > 10000:\n        warnings.warn(\"The data contains more than 10000 rows.\")\n\n    return df", "raw_solution": "import warnings\nimport sqlite3\nimport pandas as pd\n\ndef task_func(db_path, query, warn_large_dataset=True):\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(db_path)\n    except Exception as e:\n        raise Exception(f\"Error fetching data from the database: {str(e)}\")\n\n    try:\n        # Execute the SQL query\n        df = pd.read_sql_query(query, conn)\n    except Exception as e:\n        raise Exception(f\"Error fetching data from the database: {str(e)}\")\n\n    # Check if the data contains more than 10000 rows\n    if warn_large_dataset and len(df) > 10000:\n        warnings.warn(\"The data contains more than 10000 rows.\")\n\n    return df"}
{"task_id": "BigCodeBench/1069", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Initialize an empty list to store the Axes objects\n    axes_list = []\n\n    # Iterate over each column in the DataFrame\n    for col in df.columns:\n        # Generate a histogram for the column\n        ax = df[col].hist(bins=10)\n\n        # Add the Axes object to the list\n        axes_list.append(ax)\n\n        # Check if the distribution is uniform\n        if not df[col].nunique() == df[col].size:\n            print(f\"The distribution of '{col}' is not uniform.\")\n\n    # Return the list of Axes objects\n    return axes_list", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict):\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Initialize an empty list to store the Axes objects\n    axes_list = []\n\n    # Iterate over each column in the DataFrame\n    for col in df.columns:\n        # Generate a histogram for the column\n        ax = df[col].hist(bins=10)\n\n        # Add the Axes object to the list\n        axes_list.append(ax)\n\n        # Check if the distribution is uniform\n        if not df[col].nunique() == df[col].size:\n            print(f\"The distribution of '{col}' is not uniform.\")\n\n    # Return the list of Axes objects\n    return axes_list"}
{"task_id": "BigCodeBench/1070", "solution": "import pandas as pd\nfrom random import shuffle\nPOSSIBLE_VALUES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]\ndef task_func(list_of_lists):\n    # Shuffle the possible values\n    shuffle(POSSIBLE_VALUES)\n\n    # Create a list to store the DataFrames\n    df_list = []\n\n    # For each sublist in list_of_lists\n    for sublist in list_of_lists:\n        # Create a DataFrame with columns named as per the elements of the sublist\n        df = pd.DataFrame(columns=sublist)\n\n        # Fill each column with the shuffled values\n        for column in sublist:\n            df[column] = POSSIBLE_VALUES\n\n        # Append the DataFrame to the list\n        df_list.append(df)\n\n    # Return the list of DataFrames\n    return df_list", "raw_solution": "import pandas as pd\nfrom random import shuffle\n\n# Constants\nPOSSIBLE_VALUES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]\n\ndef task_func(list_of_lists):\n    # Shuffle the possible values\n    shuffle(POSSIBLE_VALUES)\n\n    # Create a list to store the DataFrames\n    df_list = []\n\n    # For each sublist in list_of_lists\n    for sublist in list_of_lists:\n        # Create a DataFrame with columns named as per the elements of the sublist\n        df = pd.DataFrame(columns=sublist)\n\n        # Fill each column with the shuffled values\n        for column in sublist:\n            df[column] = POSSIBLE_VALUES\n\n        # Append the DataFrame to the list\n        df_list.append(df)\n\n    # Return the list of DataFrames\n    return df_list"}
{"task_id": "BigCodeBench/1071", "solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\nimport numpy as np\nfrom random import shuffle\nCOLORS = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\ndef task_func(list_of_lists):\n    \"\"\"\n    Plots a series of lines for each list in `list_of_lists`. Each line is plotted with shuffled y-values and sequential x-values starting from 1.\n    The function shuffles the y-values of each inner list before plotting. Each line is plotted with a different color from a predetermined set of colors.\n    The function cycles through these colors for each inner list. If an inner list is empty, it will be skipped and no line will be plotted for it.\n    The colors are reused cyclically if there are more inner lists than colors available. The shuffling of y-values is random and different each time the function is called,\n    unless a random seed is set externally. The function uses a default set of colors defined in the COLORS constant.\n    The function returns a tuple containing the figure and axes objects of the plotted graph.\n    \"\"\"\n    # Shuffle the y-values of each inner list\n    for i in range(len(list_of_lists)):\n        shuffle(list_of_lists[i])\n\n    # Create a cycle iterator for the colors\n    color_cycle = cycle(COLORS)\n\n    # Create the figure and axes\n    fig, ax = plt.subplots()\n\n    # Plot each list as a line with a different color\n    for i, y_values in enumerate(list_of_lists):\n        if y_values:  # Skip empty lists\n            color = next(color_cycle)\n            x_values = np.arange(1, len(y_values) + 1)\n            ax.plot(x_values, y_values, color=color, label=f\"Line {i+1}\")\n\n    # Add a legend\n    ax.legend()\n\n    return fig, ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\nimport numpy as np\nfrom random import shuffle\n\nCOLORS = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\n\ndef task_func(list_of_lists):\n    \"\"\"\n    Plots a series of lines for each list in `list_of_lists`. Each line is plotted with shuffled y-values and sequential x-values starting from 1.\n    The function shuffles the y-values of each inner list before plotting. Each line is plotted with a different color from a predetermined set of colors.\n    The function cycles through these colors for each inner list. If an inner list is empty, it will be skipped and no line will be plotted for it.\n    The colors are reused cyclically if there are more inner lists than colors available. The shuffling of y-values is random and different each time the function is called,\n    unless a random seed is set externally. The function uses a default set of colors defined in the COLORS constant.\n    The function returns a tuple containing the figure and axes objects of the plotted graph.\n    \"\"\"\n    # Shuffle the y-values of each inner list\n    for i in range(len(list_of_lists)):\n        shuffle(list_of_lists[i])\n\n    # Create a cycle iterator for the colors\n    color_cycle = cycle(COLORS)\n\n    # Create the figure and axes\n    fig, ax = plt.subplots()\n\n    # Plot each list as a line with a different color\n    for i, y_values in enumerate(list_of_lists):\n        if y_values:  # Skip empty lists\n            color = next(color_cycle)\n            x_values = np.arange(1, len(y_values) + 1)\n            ax.plot(x_values, y_values, color=color, label=f\"Line {i+1}\")\n\n    # Add a legend\n    ax.legend()\n\n    return fig, ax"}
{"task_id": "BigCodeBench/1072", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(list_of_lists):\n    np.random.seed(0)\n    series_list = []\n    for sublist in list_of_lists:\n        n = len(sublist)\n        values = np.arange(1, n + 1)\n        np.random.shuffle(values)\n        series = pd.Series(values, index=sublist)\n        series_list.append(series)\n    return series_list\nlist_of_lists = [['a', 'b', 'c'], ['x', 'y', 'z'], ['1', '2', '3']]", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(list_of_lists):\n    np.random.seed(0)\n    series_list = []\n    for sublist in list_of_lists:\n        n = len(sublist)\n        values = np.arange(1, n + 1)\n        np.random.shuffle(values)\n        series = pd.Series(values, index=sublist)\n        series_list.append(series)\n    return series_list\n\n# Test the function\nlist_of_lists = [['a', 'b', 'c'], ['x', 'y', 'z'], ['1', '2', '3']]\nseries_list = task_func(list_of_lists)\nfor series in series_list:\n    print(series)"}
{"task_id": "BigCodeBench/1073", "solution": "import time\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\ndef task_func(time_strings, time_format=\"%d/%m/%Y %H:%M:%S.%f\"):\n    seconds_list = []\n    for time_string in time_strings:\n        try:\n            parsed_time = datetime.strptime(time_string, time_format)\n            seconds_list.append(parsed_time.second)\n        except ValueError:\n            print(f\"Error: {time_string} cannot be parsed according to {time_format}\")\n            return None\n\n    fig, ax = plt.subplots()\n    ax.hist(seconds_list, bins=60, edgecolor='black', color='skyblue')\n    ax.set_xlabel('Seconds')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Seconds Component')\n    return ax", "raw_solution": "import time\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\ndef task_func(time_strings, time_format=\"%d/%m/%Y %H:%M:%S.%f\"):\n    seconds_list = []\n    for time_string in time_strings:\n        try:\n            parsed_time = datetime.strptime(time_string, time_format)\n            seconds_list.append(parsed_time.second)\n        except ValueError:\n            print(f\"Error: {time_string} cannot be parsed according to {time_format}\")\n            return None\n\n    fig, ax = plt.subplots()\n    ax.hist(seconds_list, bins=60, edgecolor='black', color='skyblue')\n    ax.set_xlabel('Seconds')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Seconds Component')\n    return ax"}
{"task_id": "BigCodeBench/1074", "solution": "from datetime import datetime\nfrom dateutil.parser import parse\nfrom pytz import timezone\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_string, from_tz, to_tz):\n    # Parse the time string into a datetime object\n    dt = parse(time_string)\n\n    # Convert the datetime object to the source timezone\n    dt = dt.replace(tzinfo=timezone(from_tz))\n\n    # Convert the datetime object to the target timezone\n    dt = dt.astimezone(timezone(to_tz))\n\n    # Format the datetime object into a string\n    result = dt.strftime(TIME_FORMAT)\n\n    return result", "raw_solution": "from datetime import datetime\nfrom dateutil.parser import parse\nfrom pytz import timezone\n\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\n\ndef task_func(time_string, from_tz, to_tz):\n    # Parse the time string into a datetime object\n    dt = parse(time_string)\n\n    # Convert the datetime object to the source timezone\n    dt = dt.replace(tzinfo=timezone(from_tz))\n\n    # Convert the datetime object to the target timezone\n    dt = dt.astimezone(timezone(to_tz))\n\n    # Format the datetime object into a string\n    result = dt.strftime(TIME_FORMAT)\n\n    return result"}
{"task_id": "BigCodeBench/1075", "solution": "import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_strings):\n    # Convert the time strings to datetime objects\n    time_objs = [datetime.datetime.strptime(time, TIME_FORMAT) for time in time_strings]\n\n    # Calculate the differences in seconds\n    diffs = [(time_objs[i+1] - time_objs[i]).total_seconds() for i in range(len(time_objs)-1)]\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(len(diffs)), diffs)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Time Difference (s)')\n    ax.set_title('Time Difference Between Consecutive Datetime Strings')\n\n    return ax", "raw_solution": "import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\n\ndef task_func(time_strings):\n    # Convert the time strings to datetime objects\n    time_objs = [datetime.datetime.strptime(time, TIME_FORMAT) for time in time_strings]\n\n    # Calculate the differences in seconds\n    diffs = [(time_objs[i+1] - time_objs[i]).total_seconds() for i in range(len(time_objs)-1)]\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(len(diffs)), diffs)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Time Difference (s)')\n    ax.set_title('Time Difference Between Consecutive Datetime Strings')\n\n    return ax"}
{"task_id": "BigCodeBench/1076", "solution": "from datetime import datetime\nimport pandas as pd\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_strings, target_tz):\n    \"\"\"\n    Converts a list of time strings from UTC to a specified timezone and returns a DataFrame.\n    \"\"\"\n    # Convert time strings to datetime objects\n    utc_times = [datetime.strptime(time, TIME_FORMAT) for time in time_strings]\n\n    # Convert UTC times to target timezone\n    converted_times = [time.astimezone(ZoneInfo(target_tz)) for time in utc_times]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'Original Time': utc_times,\n        'Converted Time': converted_times\n    })\n\n    return df", "raw_solution": "from datetime import datetime\nimport pandas as pd\n# For Python versions lower than 3.9, use 'pytz' instead of 'zoneinfo'\ntry:\n    from zoneinfo import ZoneInfo\nexcept ImportError:\n    from pytz import timezone as ZoneInfo\n\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\n\ndef task_func(time_strings, target_tz):\n    \"\"\"\n    Converts a list of time strings from UTC to a specified timezone and returns a DataFrame.\n    \"\"\"\n    # Convert time strings to datetime objects\n    utc_times = [datetime.strptime(time, TIME_FORMAT) for time in time_strings]\n\n    # Convert UTC times to target timezone\n    converted_times = [time.astimezone(ZoneInfo(target_tz)) for time in utc_times]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'Original Time': utc_times,\n        'Converted Time': converted_times\n    })\n\n    return df"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert time strings to datetime objects and convert to specified timezone\n    time_objs = [pytz.timezone(timezone).localize(datetime.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")) for time_string in time_strings]\n\n    # Calculate time differences in seconds\n    time_diffs = [(time_objs[i+1] - time_objs[i]).total_seconds() for i in range(len(time_objs)-1)]\n\n    # Return the average time difference\n    return float(np.mean(time_diffs))", "raw_solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert time strings to datetime objects and convert to specified timezone\n    time_objs = [pytz.timezone(timezone).localize(datetime.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")) for time_string in time_strings]\n\n    # Calculate time differences in seconds\n    time_diffs = [(time_objs[i+1] - time_objs[i]).total_seconds() for i in range(len(time_objs)-1)]\n\n    # Return the average time difference\n    return float(np.mean(time_diffs))"}
{"task_id": "BigCodeBench/1078", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(arr):\n    unique, counts = np.unique(arr, return_counts=True)\n    uniform_distribution = np.all(counts == counts[0])\n\n    fig, ax = plt.subplots()\n    ax.hist(arr, bins=np.arange(len(unique) + 1) - 0.5, edgecolor='black')\n    ax.set_xticks(unique)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Array Values')\n\n    return uniform_distribution, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(arr):\n    unique, counts = np.unique(arr, return_counts=True)\n    uniform_distribution = np.all(counts == counts[0])\n\n    fig, ax = plt.subplots()\n    ax.hist(arr, bins=np.arange(len(unique) + 1) - 0.5, edgecolor='black')\n    ax.set_xticks(unique)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Array Values')\n\n    return uniform_distribution, ax"}
{"task_id": "BigCodeBench/1079", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert string prices to float\n    data['Price'] = data['Price_String'].apply(lambda x: float(x.replace(',', '')))\n\n    # Calculate statistical measures\n    stats = {\n        'mean': data['Price'].mean(),\n        'median': data['Price'].median(),\n        'std_dev': data['Price'].std()\n    }\n\n    # Generate histogram\n    fig, ax = plt.subplots()\n    ax.hist(data['Price'], bins='auto', color='blue', alpha=0.7, rwidth=0.85)\n    ax.set_title('Histogram of Product Prices')\n    ax.set_xlabel('Price')\n    ax.set_ylabel('Frequency')\n\n    return stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert string prices to float\n    data['Price'] = data['Price_String'].apply(lambda x: float(x.replace(',', '')))\n\n    # Calculate statistical measures\n    stats = {\n        'mean': data['Price'].mean(),\n        'median': data['Price'].median(),\n        'std_dev': data['Price'].std()\n    }\n\n    # Generate histogram\n    fig, ax = plt.subplots()\n    ax.hist(data['Price'], bins='auto', color='blue', alpha=0.7, rwidth=0.85)\n    ax.set_title('Histogram of Product Prices')\n    ax.set_xlabel('Price')\n    ax.set_ylabel('Frequency')\n\n    return stats, ax"}
{"task_id": "BigCodeBench/1080", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nDATA = {\n    \"Area_String\": [\"1,000\", \"2,000\", \"3,000\", \"4,000\", \"5,000\"],\n    \"Price\": [100, 200, 300, 400, 500],\n}\ndef task_func(area_string, data=DATA):\n    # Convert the area_string to integer\n    data['Area'] = [int(area.replace(',', '')) for area in data['Area_String']]\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    # Reshape the data for sklearn\n    X = df['Area'].values.reshape(-1, 1)\n    y = df['Price'].values.reshape(-1, 1)\n\n    # Create and fit the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Predict the price for the given area\n    area_value = int(area_string.replace(',', ''))\n    prediction = model.predict([[area_value]])\n\n    return float(prediction[0])", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nDATA = {\n    \"Area_String\": [\"1,000\", \"2,000\", \"3,000\", \"4,000\", \"5,000\"],\n    \"Price\": [100, 200, 300, 400, 500],\n}\n\ndef task_func(area_string, data=DATA):\n    # Convert the area_string to integer\n    data['Area'] = [int(area.replace(',', '')) for area in data['Area_String']]\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    # Reshape the data for sklearn\n    X = df['Area'].values.reshape(-1, 1)\n    y = df['Price'].values.reshape(-1, 1)\n\n    # Create and fit the model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Predict the price for the given area\n    area_value = int(area_string.replace(',', ''))\n    prediction = model.predict([[area_value]])\n\n    return float(prediction[0])"}
{"task_id": "BigCodeBench/1081", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data=None):\n    if data is None:\n        data = {\n            'Weight_String': ['70', '65', '75', '80', '75'],\n            'Height': [170, 165, 175, 180, 175]\n        }\n\n    weight_string = data.get('Weight_String')\n    height = data.get('Height')\n\n    if not all(isinstance(x, str) for x in weight_string):\n        raise ValueError(\"All weight values must be strings\")\n\n    try:\n        weight = [float(x) for x in weight_string]\n    except ValueError:\n        raise ValueError(\"All weight values must be convertible to float\")\n\n    df = pd.DataFrame({\n        'Weight': weight,\n        'Height': height\n    })\n\n    sns.set(style=\"whitegrid\")\n    ax = sns.scatterplot(x=\"Weight\", y=\"Height\", data=df)\n    ax.set_title(\"Weight vs Height\")\n\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data=None):\n    if data is None:\n        data = {\n            'Weight_String': ['70', '65', '75', '80', '75'],\n            'Height': [170, 165, 175, 180, 175]\n        }\n\n    weight_string = data.get('Weight_String')\n    height = data.get('Height')\n\n    if not all(isinstance(x, str) for x in weight_string):\n        raise ValueError(\"All weight values must be strings\")\n\n    try:\n        weight = [float(x) for x in weight_string]\n    except ValueError:\n        raise ValueError(\"All weight values must be convertible to float\")\n\n    df = pd.DataFrame({\n        'Weight': weight,\n        'Height': height\n    })\n\n    sns.set(style=\"whitegrid\")\n    ax = sns.scatterplot(x=\"Weight\", y=\"Height\", data=df)\n    ax.set_title(\"Weight vs Height\")\n\n    return ax"}
{"task_id": "BigCodeBench/1082", "solution": "import pandas as pd\nfrom scipy.stats import pearsonr\ndef task_func(data):\n    if len(data) < 2:\n        return float('NaN')\n\n    # Convert scores from string format to floats\n    data['score'] = data['score'].astype(float)\n\n    # Encode categorical grades into numerical values based on their rank order\n    grade_ranks = data['grade'].rank(method='min')\n    data['grade'] = grade_ranks\n\n    # Compute the Pearson correlation coefficient between the numerical scores and the encoded grades\n    correlation, _ = pearsonr(data['score'], data['grade'])\n\n    return correlation", "raw_solution": "import pandas as pd\nfrom scipy.stats import pearsonr\n\ndef task_func(data):\n    if len(data) < 2:\n        return float('NaN')\n\n    # Convert scores from string format to floats\n    data['score'] = data['score'].astype(float)\n\n    # Encode categorical grades into numerical values based on their rank order\n    grade_ranks = data['grade'].rank(method='min')\n    data['grade'] = grade_ranks\n\n    # Compute the Pearson correlation coefficient between the numerical scores and the encoded grades\n    correlation, _ = pearsonr(data['score'], data['grade'])\n\n    return correlation"}
{"task_id": "BigCodeBench/1083", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # 1. Input Validation\n    if not all(key in data for key in ('Salary_String', 'Experience')):\n        raise ValueError(\"Input data dictionary must contain 'Salary_String' and 'Experience' keys\")\n\n    # 2. DataFrame Conversion\n    df = pd.DataFrame(data)\n\n    # 3. Empty Data Handling\n    if df.empty:\n        return plt.gca()\n\n    # 4. Salary Conversion\n    try:\n        df['Salary'] = df['Salary_String'].apply(lambda x: float(x.replace(',', '')))\n    except ValueError as e:\n        raise ValueError(\"Failed to convert Salary_String to float: {}\".format(str(e)))\n\n    # 5. Salary Normalization\n    scaler = MinMaxScaler()\n    df[['Salary']] = scaler.fit_transform(df[['Salary']])\n\n    # 6. Data Plotting\n    ax = df.plot(kind='scatter', x='Experience', y='Salary', figsize=(8, 6))\n    ax.set_xlabel('Experience')\n    ax.set_ylabel('Normalized Salary')\n    ax.set_title('Normalized Salary against Experience')\n\n    return ax", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # 1. Input Validation\n    if not all(key in data for key in ('Salary_String', 'Experience')):\n        raise ValueError(\"Input data dictionary must contain 'Salary_String' and 'Experience' keys\")\n\n    # 2. DataFrame Conversion\n    df = pd.DataFrame(data)\n\n    # 3. Empty Data Handling\n    if df.empty:\n        return plt.gca()\n\n    # 4. Salary Conversion\n    try:\n        df['Salary'] = df['Salary_String'].apply(lambda x: float(x.replace(',', '')))\n    except ValueError as e:\n        raise ValueError(\"Failed to convert Salary_String to float: {}\".format(str(e)))\n\n    # 5. Salary Normalization\n    scaler = MinMaxScaler()\n    df[['Salary']] = scaler.fit_transform(df[['Salary']])\n\n    # 6. Data Plotting\n    ax = df.plot(kind='scatter', x='Experience', y='Salary', figsize=(8, 6))\n    ax.set_xlabel('Experience')\n    ax.set_ylabel('Normalized Salary')\n    ax.set_title('Normalized Salary against Experience')\n\n    return ax"}
{"task_id": "BigCodeBench/1084", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom typing import Tuple, List\ndef task_func(data_file_path: str) -> Tuple[pd.Series, pd.Series, List[plt.Axes], pd.DataFrame]:\n    \"\"\"\n    Analyzes numerical data from a CSV file.\n    \"\"\"\n    # Read the CSV file\n    df = pd.read_csv(data_file_path)\n\n    # Convert string representations of numbers with commas into floating point numbers\n    df = df.apply(pd.to_numeric, errors='ignore')\n\n    # Calculate the mean and standard deviation for each numerical column\n    means = df.mean()\n    std_devs = df.std()\n\n    # Generate a histogram plot for each numerical column\n    axes = []\n    for col in df.select_dtypes(include=[pd.np.number]).columns:\n        ax = df[col].hist(bins=10, figsize=(10, 5))\n        axes.append(ax)\n\n    # Perform an ANOVA test to check the statistical significance of differences between means of numerical columns (if applicable)\n    anova_results = pd.DataFrame()\n    if len(df.select_dtypes(include=[pd.np.number]).columns) > 1:\n        for col1 in df.select_dgroups_dtypes(include=[pd.np.number]).columns:\n            for col2 in df.select_dtypes(include=[pd.np.number]).columns:\n                if col1 != col2:\n                    f_val, p_val = stats.f_oneway(df[col1], df[col2])\n                    anova_results = anova_results.append(pd.Series({\"F-value\": f_val, \"P-value\": p_val}, name=f\"{col1} vs {col2}\"))\n\n    return means, std_devs, axes, anova_results", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom typing import Tuple, List\n\ndef task_func(data_file_path: str) -> Tuple[pd.Series, pd.Series, List[plt.Axes], pd.DataFrame]:\n    \"\"\"\n    Analyzes numerical data from a CSV file.\n    \"\"\"\n    # Read the CSV file\n    df = pd.read_csv(data_file_path)\n\n    # Convert string representations of numbers with commas into floating point numbers\n    df = df.apply(pd.to_numeric, errors='ignore')\n\n    # Calculate the mean and standard deviation for each numerical column\n    means = df.mean()\n    std_devs = df.std()\n\n    # Generate a histogram plot for each numerical column\n    axes = []\n    for col in df.select_dtypes(include=[pd.np.number]).columns:\n        ax = df[col].hist(bins=10, figsize=(10, 5))\n        axes.append(ax)\n\n    # Perform an ANOVA test to check the statistical significance of differences between means of numerical columns (if applicable)\n    anova_results = pd.DataFrame()\n    if len(df.select_dtypes(include=[pd.np.number]).columns) > 1:\n        for col1 in df.select_dgroups_dtypes(include=[pd.np.number]).columns:\n            for col2 in df.select_dtypes(include=[pd.np.number]).columns:\n                if col1 != col2:\n                    f_val, p_val = stats.f_oneway(df[col1], df[col2])\n                    anova_results = anova_results.append(pd.Series({\"F-value\": f_val, \"P-value\": p_val}, name=f\"{col1} vs {col2}\"))\n\n    return means, std_devs, axes, anova_results"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Split into words\n    words = text.split()\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Get the 10 most common words\n    most_common_words = word_counts.most_common(10)\n\n    # Create a bar chart\n    words, counts = zip(*most_common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n\n    return most_common_words, ax", "raw_solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Split into words\n    words = text.split()\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Get the 10 most common words\n    most_common_words = word_counts.most_common(10)\n\n    # Create a bar chart\n    words, counts = zip(*most_common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n\n    return most_common_words, ax"}
{"task_id": "BigCodeBench/1086", "solution": "import string\nimport random\nimport pandas as pd\nimport numpy as np\nNUM_SAMPLES = 1000\ndef task_func():\n    # Generate random strings\n    random_strings = [''.join(random.choices(string.ascii_letters, k=10)) for _ in range(NUM_SAMPLES)]\n\n    # Generate random floats\n    random_floats = [round(random.uniform(0, 10000), 2) for _ in range(NUM_SAMPLES)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'String Field': random_strings,\n        'Float Field': random_floats\n    })\n\n    # Format 'Float Field' as string with comma as thousands separator\n    df['Float Field'] = df['Float Field'].apply(lambda x: \"{:,.2f}\".format(x))\n\n    return df", "raw_solution": "import string\nimport random\nimport pandas as pd\nimport numpy as np\n\n# Constants\nNUM_SAMPLES = 1000  # Number of samples\n\ndef task_func():\n    # Generate random strings\n    random_strings = [''.join(random.choices(string.ascii_letters, k=10)) for _ in range(NUM_SAMPLES)]\n\n    # Generate random floats\n    random_floats = [round(random.uniform(0, 10000), 2) for _ in range(NUM_SAMPLES)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'String Field': random_strings,\n        'Float Field': random_floats\n    })\n\n    # Format 'Float Field' as string with comma as thousands separator\n    df['Float Field'] = df['Float Field'].apply(lambda x: \"{:,.2f}\".format(x))\n\n    return df"}
{"task_id": "BigCodeBench/1087", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport os\ndef task_func(mean=123456.908, std_dev=1.2, save_plots=False):\n    # Generate a random sample from a normal distribution\n    sample = np.random.normal(mean, std_dev, 1000)\n\n    # Calculate skewness and kurtosis\n    skewness = stats.skew(sample)\n    kurtosis = stats.kurtosis(sample)\n\n    # Create a histogram and a QQ plot\n    if save_plots:\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.hist(sample, bins=30, edgecolor='black')\n        plt.title('Histogram')\n        plt.xlabel('Value')\n        plt.ylabel('Frequency')\n\n        plt.subplot(1, 2, 2)\n        stats.probplot(sample, plot=plt)\n        plt.title('QQ Plot')\n        plt.tight_layout()\n\n        # Save the plots\n        plot_paths = []\n        for i in range(1, 3):\n            plot_path = f'plot_{i}.png'\n            plt.savefig(plot_path)\n            plot_paths.append(plot_path)\n\n        return skewness, kurtosis, plot_paths\n    else:\n        return skewness, kurtosis, []", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport os\n\ndef task_func(mean=123456.908, std_dev=1.2, save_plots=False):\n    # Generate a random sample from a normal distribution\n    sample = np.random.normal(mean, std_dev, 1000)\n\n    # Calculate skewness and kurtosis\n    skewness = stats.skew(sample)\n    kurtosis = stats.kurtosis(sample)\n\n    # Create a histogram and a QQ plot\n    if save_plots:\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.hist(sample, bins=30, edgecolor='black')\n        plt.title('Histogram')\n        plt.xlabel('Value')\n        plt.ylabel('Frequency')\n\n        plt.subplot(1, 2, 2)\n        stats.probplot(sample, plot=plt)\n        plt.title('QQ Plot')\n        plt.tight_layout()\n\n        # Save the plots\n        plot_paths = []\n        for i in range(1, 3):\n            plot_path = f'plot_{i}.png'\n            plt.savefig(plot_path)\n            plot_paths.append(plot_path)\n\n        return skewness, kurtosis, plot_paths\n    else:\n        return skewness, kurtosis, []"}
{"task_id": "BigCodeBench/1088", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data=None):\n    if data is None:\n        return None\n\n    # Convert to DataFrame\n    df = pd.DataFrame(data)\n\n    # Replace values less than 0.5 with zeros\n    df[df < 0.5] = 0\n\n    # Standardize the data\n    scaler = StandardScaler()\n    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data=None):\n    if data is None:\n        return None\n\n    # Convert to DataFrame\n    df = pd.DataFrame(data)\n\n    # Replace values less than 0.5 with zeros\n    df[df < 0.5] = 0\n\n    # Standardize the data\n    scaler = StandardScaler()\n    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df"}
{"task_id": "BigCodeBench/1089", "solution": "from collections import Counter\ndef task_func(list_of_tuples):\n    sum_of_values = 0\n    categories = []\n    for value, category in list_of_tuples:\n        sum_of_values += value\n        categories.append(category)\n    count_of_categories = Counter(categories)\n    return (sum_of_values, dict(count_of_categories))\nlist_of_tuples = [(1, 'cat'), (2, 'dog'), (3, 'cat'), (4, 'bird'), (2, 'cat'), (3, 'dog')]", "raw_solution": "from collections import Counter\n\ndef task_func(list_of_tuples):\n    sum_of_values = 0\n    categories = []\n    for value, category in list_of_tuples:\n        sum_of_values += value\n        categories.append(category)\n    count_of_categories = Counter(categories)\n    return (sum_of_values, dict(count_of_categories))\n\n# Test the function\nlist_of_tuples = [(1, 'cat'), (2, 'dog'), (3, 'cat'), (4, 'bird'), (2, 'cat'), (3, 'dog')]"}
{"task_id": "BigCodeBench/1090", "solution": "import ast\nimport json\nfrom collections import Counter\ndef task_func(file_pointer):\n    \"\"\"\n    Reads from a given file pointer to a JSON file, evaluates strings that represent dictionaries to actual dictionaries,\n    and counts the frequency of each key across all dictionary entries in the JSON data.\n    \"\"\"\n    # Read the JSON data from the file\n    with open(file_pointer, 'r') as file:\n        data = json.load(file)\n\n    # Initialize a Counter object\n    key_counter = Counter()\n\n    # Iterate over each entry in the JSON data\n    for entry in data:\n        # If the entry is a string, evaluate it as a dictionary\n        if isinstance(entry, str):\n            entry = ast.literal_eval(entry)\n\n        # If the entry is a dictionary, update the Counter object with its keys\n        if isinstance(entry, dict):\n            key_counter.update(entry.keys())\n\n    return key_counter", "raw_solution": "import ast\nimport json\nfrom collections import Counter\n\ndef task_func(file_pointer):\n    \"\"\"\n    Reads from a given file pointer to a JSON file, evaluates strings that represent dictionaries to actual dictionaries,\n    and counts the frequency of each key across all dictionary entries in the JSON data.\n    \"\"\"\n    # Read the JSON data from the file\n    with open(file_pointer, 'r') as file:\n        data = json.load(file)\n\n    # Initialize a Counter object\n    key_counter = Counter()\n\n    # Iterate over each entry in the JSON data\n    for entry in data:\n        # If the entry is a string, evaluate it as a dictionary\n        if isinstance(entry, str):\n            entry = ast.literal_eval(entry)\n\n        # If the entry is a dictionary, update the Counter object with its keys\n        if isinstance(entry, dict):\n            key_counter.update(entry.keys())\n\n    return key_counter"}
{"task_id": "BigCodeBench/1091", "solution": "import ast\nimport os\nimport glob\nDIRECTORY = 'data'\ndef task_func(directory):\n    \"\"\"\n    Converts all Unicode string representations of dictionaries in all text files in the specified directory to Python dictionaries.\n    \"\"\"\n    # Get all text files in the directory\n    files = glob.glob(os.path.join(directory, '*.txt'))\n\n    dict_list = []\n\n    for file in files:\n        with open(file, 'r', encoding='utf-8') as f:\n            content = f.read()\n\n            # Try to convert the Unicode string representation to a Python dictionary\n            try:\n                dict_obj = ast.literal_eval(content)\n\n                # Check if the object is a dictionary\n                if not isinstance(dict_obj, dict):\n                    raise ValueError(f\"Invalid dictionary representation in file: {file}\")\n\n                dict_list.append(dict_obj)\n            except (SyntaxError, ValueError) as e:\n                raise ValueError(f\"Invalid dictionary representation in file: {file}\") from e\n\n    return dict_list", "raw_solution": "import ast\nimport os\nimport glob\n\n# Constants\nDIRECTORY = 'data'\n\ndef task_func(directory):\n    \"\"\"\n    Converts all Unicode string representations of dictionaries in all text files in the specified directory to Python dictionaries.\n    \"\"\"\n    # Get all text files in the directory\n    files = glob.glob(os.path.join(directory, '*.txt'))\n\n    dict_list = []\n\n    for file in files:\n        with open(file, 'r', encoding='utf-8') as f:\n            content = f.read()\n\n            # Try to convert the Unicode string representation to a Python dictionary\n            try:\n                dict_obj = ast.literal_eval(content)\n\n                # Check if the object is a dictionary\n                if not isinstance(dict_obj, dict):\n                    raise ValueError(f\"Invalid dictionary representation in file: {file}\")\n\n                dict_list.append(dict_obj)\n            except (SyntaxError, ValueError) as e:\n                raise ValueError(f\"Invalid dictionary representation in file: {file}\") from e\n\n    return dict_list\n\n# Test the function\ndict_list = task_func(DIRECTORY)\nfor i, dict_obj in enumerate(dict_list, 1):\n    print(f\"Dictionary {i}: {dict_obj}\")"}
{"task_id": "BigCodeBench/1092", "solution": "import ast\nimport requests\nfrom bs4 import BeautifulSoup\ndef task_func(url):\n    try:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, 'html.parser')\n        scripts = soup.find_all('script')\n        result = []\n\n        for script in scripts:\n            script_text = script.string\n            if script_text is not None:\n                try:\n                    # Try to parse the script text as a Python dictionary\n                    dictionary = ast.literal_eval(script_text)\n                    if isinstance(dictionary, dict):\n                        result.append(dictionary)\n                except (SyntaxError, ValueError):\n                    pass\n\n        return result\n    except requests.exceptions.RequestException:\n        return []", "raw_solution": "import ast\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef task_func(url):\n    try:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, 'html.parser')\n        scripts = soup.find_all('script')\n        result = []\n\n        for script in scripts:\n            script_text = script.string\n            if script_text is not None:\n                try:\n                    # Try to parse the script text as a Python dictionary\n                    dictionary = ast.literal_eval(script_text)\n                    if isinstance(dictionary, dict):\n                        result.append(dictionary)\n                except (SyntaxError, ValueError):\n                    pass\n\n        return result\n    except requests.exceptions.RequestException:\n        return []"}
{"task_id": "BigCodeBench/1093", "solution": "import re\nimport ast\ndef task_func(text_file: str) -> list:\n    with open(text_file, 'r') as file:\n        data = file.read()\n\n    dict_strs = re.findall(r'\\{.*?\\}', data)\n    dicts = [ast.literal_eval(d) for d in dict_strs]\n\n    return dicts", "raw_solution": "import re\nimport ast\n\ndef task_func(text_file: str) -> list:\n    with open(text_file, 'r') as file:\n        data = file.read()\n\n    dict_strs = re.findall(r'\\{.*?\\}', data)\n    dicts = [ast.literal_eval(d) for d in dict_strs]\n\n    return dicts"}
{"task_id": "BigCodeBench/1094", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\nimport re\ndef task_func(text):\n    # Tokenize the text using regex\n    tokenizer = RegexpTokenizer(r'\\$\\w+')\n    tokens = tokenizer.tokenize(text)\n\n    # Filter out words solely consisting of \"$\" symbols without any following alphanumeric characters\n    tokens = [token[1:] for token in tokens if re.match(r'\\$\\w+', token)]\n\n    # Count the frequency of each word\n    counter = Counter(tokens)\n\n    # Return the five most frequent words along with their counts, ordered by most to least common\n    return counter.most_common(5)", "raw_solution": "from nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\nimport re\n\ndef task_func(text):\n    # Tokenize the text using regex\n    tokenizer = RegexpTokenizer(r'\\$\\w+')\n    tokens = tokenizer.tokenize(text)\n\n    # Filter out words solely consisting of \"$\" symbols without any following alphanumeric characters\n    tokens = [token[1:] for token in tokens if re.match(r'\\$\\w+', token)]\n\n    # Count the frequency of each word\n    counter = Counter(tokens)\n\n    # Return the five most frequent words along with their counts, ordered by most to least common\n    return counter.most_common(5)"}
{"task_id": "BigCodeBench/1095", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport os\ndef task_func(text, output_filename):\n    \"\"\"\n    Extracts words from the input text that begin with the '$' character and saves them to a specified file, excluding any words that are solely composed of punctuation characters.\n    \"\"\"\n    # Tokenize the text into words\n    tokenizer = RegexpTokenizer(r'\\w+')\n    words = tokenizer.tokenize(text)\n\n    # Filter out words that start with '$' and are not solely composed of punctuation characters\n    special_words = [word for word in words if word.startswith('$') and not all(char in punctuation for char in word)]\n\n    # Save the special words to the output file\n    with open(output_filename, 'w') as file:\n        for word in special_words:\n            file.write(word + '\\n')\n\n    # Return the absolute path to the output file\n    return os.path.abspath(output_filename)", "raw_solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport os\n\ndef task_func(text, output_filename):\n    \"\"\"\n    Extracts words from the input text that begin with the '$' character and saves them to a specified file, excluding any words that are solely composed of punctuation characters.\n    \"\"\"\n    # Tokenize the text into words\n    tokenizer = RegexpTokenizer(r'\\w+')\n    words = tokenizer.tokenize(text)\n\n    # Filter out words that start with '$' and are not solely composed of punctuation characters\n    special_words = [word for word in words if word.startswith('$') and not all(char in punctuation for char in word)]\n\n    # Save the special words to the output file\n    with open(output_filename, 'w') as file:\n        for word in special_words:\n            file.write(word + '\\n')\n\n    # Return the absolute path to the output file\n    return os.path.abspath(output_filename)"}
{"task_id": "BigCodeBench/1096", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport csv\nimport os\nPUNCTUATION = set(punctuation)\ndef task_func(text, filename):\n    \"\"\"\n    Saves all words in a text beginning with the \"$\" character in a CSV file,\n    excluding any words that are solely composed of punctuation characters.\n    \"\"\"\n    tokenizer = RegexpTokenizer(r'\\b\\$\\w+\\b')\n    words = tokenizer.tokenize(text)\n    words = [word for word in words if not set(word).issubset(PUNCTUATION)]\n\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Word\"])\n        writer.writerows([[word] for word in words])\n\n    return os.path.abspath(filename)", "raw_solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport csv\nimport os\n\nPUNCTUATION = set(punctuation)\n\ndef task_func(text, filename):\n    \"\"\"\n    Saves all words in a text beginning with the \"$\" character in a CSV file,\n    excluding any words that are solely composed of punctuation characters.\n    \"\"\"\n    tokenizer = RegexpTokenizer(r'\\b\\$\\w+\\b')\n    words = tokenizer.tokenize(text)\n    words = [word for word in words if not set(word).issubset(PUNCTUATION)]\n\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Word\"])\n        writer.writerows([[word] for word in words])\n\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/1097", "solution": "import re\nfrom string import punctuation\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\ndef task_func(text):\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Remove punctuation\n    text = \"\".join([char for char in text if char not in punctuation])\n\n    # Remove stopwords\n    words = text.split()\n    text = \" \".join([word for word in words if word not in PREDEFINED_STOPWORDS])\n\n    return text", "raw_solution": "import re\nfrom string import punctuation\n\n# Predefined list of common stopwords\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\n\ndef task_func(text):\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Remove punctuation\n    text = \"\".join([char for char in text if char not in punctuation])\n\n    # Remove stopwords\n    words = text.split()\n    text = \" \".join([word for word in words if word not in PREDEFINED_STOPWORDS])\n\n    return text"}
{"task_id": "BigCodeBench/1098", "solution": "import re\nfrom collections import Counter\ndef task_func(text, top_n):\n    # Remove URLs from the text\n    url_pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = url_pattern.sub('', text)\n\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Return the N most common words\n    return word_counts.most_common(top_n)", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(text, top_n):\n    # Remove URLs from the text\n    url_pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = url_pattern.sub('', text)\n\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Return the N most common words\n    return word_counts.most_common(top_n)"}
{"task_id": "BigCodeBench/1099", "solution": "import re\nfrom collections import Counter\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\ndef task_func(text):\n    # Remove URLs from the text\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = url_pattern.sub('', text)\n\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_count = Counter(words)\n\n    # Filter out stopwords and sort the result\n    result = sorted((word, count) for word, count in word_count.items() if word.lower() not in PREDEFINED_STOPWORDS)\n\n    return result", "raw_solution": "import re\nfrom collections import Counter\n\n# Predefined list of common stopwords\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\n\ndef task_func(text):\n    # Remove URLs from the text\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = url_pattern.sub('', text)\n\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_count = Counter(words)\n\n    # Filter out stopwords and sort the result\n    result = sorted((word, count) for word, count in word_count.items() if word.lower() not in PREDEFINED_STOPWORDS)\n\n    return result"}
{"task_id": "BigCodeBench/1100", "solution": "import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef task_func(texts):\n    # Remove URLs from the texts\n    texts = [re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) for text in texts]\n\n    # Initialize TfidfVectorizer\n    vectorizer = TfidfVectorizer(use_idf=True)\n\n    # Fit and transform the texts\n    tfidf_matrix = vectorizer.fit_transform(texts)\n\n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Convert the tf-idf matrix to a dense matrix and round the scores to 8 decimal places\n    tfidf_scores = [tuple(round(score, 8) for score in doc_tfidf.toarray()[0]) for doc_tfidf in tfidf_matrix]\n\n    return tfidf_scores, list(feature_names)", "raw_solution": "import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef task_func(texts):\n    # Remove URLs from the texts\n    texts = [re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) for text in texts]\n\n    # Initialize TfidfVectorizer\n    vectorizer = TfidfVectorizer(use_idf=True)\n\n    # Fit and transform the texts\n    tfidf_matrix = vectorizer.fit_transform(texts)\n\n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Convert the tf-idf matrix to a dense matrix and round the scores to 8 decimal places\n    tfidf_scores = [tuple(round(score, 8) for score in doc_tfidf.toarray()[0]) for doc_tfidf in tfidf_matrix]\n\n    return tfidf_scores, list(feature_names)"}
{"task_id": "BigCodeBench/1101", "solution": "import subprocess\nimport os\nimport glob\nimport time\ndef task_func(test_dir):\n    \"\"\"\n    Runs all Python scripts in a specific directory and returns their execution times.\n    \"\"\"\n    # Get all Python files in the directory\n    python_files = glob.glob(os.path.join(test_dir, '*.py'))\n\n    # Dictionary to store script names and their execution times\n    execution_times = {}\n\n    # Run each Python script and measure its execution time\n    for file in python_files:\n        start_time = time.time()\n        subprocess.run(['python', file])\n        end_time = time.time()\n        execution_times[file] = end_time - start_time\n\n    return execution_times", "raw_solution": "import subprocess\nimport os\nimport glob\nimport time\n\ndef task_func(test_dir):\n    \"\"\"\n    Runs all Python scripts in a specific directory and returns their execution times.\n    \"\"\"\n    # Get all Python files in the directory\n    python_files = glob.glob(os.path.join(test_dir, '*.py'))\n\n    # Dictionary to store script names and their execution times\n    execution_times = {}\n\n    # Run each Python script and measure its execution time\n    for file in python_files:\n        start_time = time.time()\n        subprocess.run(['python', file])\n        end_time = time.time()\n        execution_times[file] = end_time - start_time\n\n    return execution_times"}
{"task_id": "BigCodeBench/1102", "solution": "import subprocess\nimport shlex\nfrom datetime import datetime\ndef task_func(script_path: str) -> dict:\n    start_time = datetime.now()\n    with subprocess.Popen(shlex.split(f\"Rscript {script_path}\"), stdout=subprocess.PIPE, stderr=subprocess.PIPE) as process:\n        stdout, stderr = process.communicate()\n    end_time = datetime.now()\n    return {\n        \"start_time\": start_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n        \"end_time\": end_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n        \"stdout\": stdout.decode(),\n        \"stderr\": stderr.decode(),\n    }", "raw_solution": "import subprocess\nimport shlex\nfrom datetime import datetime\n\ndef task_func(script_path: str) -> dict:\n    start_time = datetime.now()\n    with subprocess.Popen(shlex.split(f\"Rscript {script_path}\"), stdout=subprocess.PIPE, stderr=subprocess.PIPE) as process:\n        stdout, stderr = process.communicate()\n    end_time = datetime.now()\n    return {\n        \"start_time\": start_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n        \"end_time\": end_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n        \"stdout\": stdout.decode(),\n        \"stderr\": stderr.decode(),\n    }"}
{"task_id": "BigCodeBench/1103", "solution": "import subprocess\nimport shutil\nimport os\ndef task_func(script_path: str, temp_dir: str) -> str:\n    \"\"\"\n    Executes a given Python script in a temporary directory.\n    :param script_path: The path to the script to be executed.\n    :param temp_dir: The path to the temporary directory.\n    :return: A string indicating the success or failure of the script execution.\n    \"\"\"\n    # Create a temporary directory\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n\n    # Copy the script to the temporary directory\n    shutil.copy(script_path, temp_dir)\n\n    # Change the working directory to the temporary directory\n    os.chdir(temp_dir)\n\n    # Execute the script\n    try:\n        subprocess.check_output(['python', os.path.basename(script_path)], stderr=subprocess.STDOUT)\n        return \"Script executed successfully!\"\n    except subprocess.CalledProcessError:\n        return \"Script execution failed!\"", "raw_solution": "import subprocess\nimport shutil\nimport os\n\ndef task_func(script_path: str, temp_dir: str) -> str:\n    \"\"\"\n    Executes a given Python script in a temporary directory.\n    :param script_path: The path to the script to be executed.\n    :param temp_dir: The path to the temporary directory.\n    :return: A string indicating the success or failure of the script execution.\n    \"\"\"\n    # Create a temporary directory\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n\n    # Copy the script to the temporary directory\n    shutil.copy(script_path, temp_dir)\n\n    # Change the working directory to the temporary directory\n    os.chdir(temp_dir)\n\n    # Execute the script\n    try:\n        subprocess.check_output(['python', os.path.basename(script_path)], stderr=subprocess.STDOUT)\n        return \"Script executed successfully!\"\n    except subprocess.CalledProcessError:\n        return \"Script execution failed!\""}
{"task_id": "BigCodeBench/1104", "solution": "import subprocess\nimport os\nimport threading\ndef task_func(script_path: str, timeout: int = 60) -> str:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"File {script_path} not found\")\n\n    def target():\n        subprocess.run([\"python\", script_path])\n\n    thread = threading.Thread(target=target)\n    thread.start()\n    thread.join(timeout)\n\n    if thread.is_alive():\n        thread.terminate()\n        return \"Terminating process due to timeout.\"\n    else:\n        return \"Script executed successfully.\"", "raw_solution": "import subprocess\nimport os\nimport threading\n\ndef task_func(script_path: str, timeout: int = 60) -> str:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"File {script_path} not found\")\n\n    def target():\n        subprocess.run([\"python\", script_path])\n\n    thread = threading.Thread(target=target)\n    thread.start()\n    thread.join(timeout)\n\n    if thread.is_alive():\n        thread.terminate()\n        return \"Terminating process due to timeout.\"\n    else:\n        return \"Script executed successfully.\""}
{"task_id": "BigCodeBench/1105", "solution": "import subprocess\nimport os\nimport time\nimport glob\ndef task_func(r_script_path: str, output_path: str, duration: int) -> (bool, str):\n    \"\"\"\n    This function executes an R script and verifies if the output file is generated within a given duration.\n    \"\"\"\n    start_time = time.time()\n    subprocess.run([\"Rscript\", r_script_path])\n    while time.time() - start_time <= duration:\n        if glob.glob(output_path):\n            return True, 'File generated successfully within the specified duration.'\n        time.sleep(1)\n    return False, 'File not generated within the specified duration.'\nr_script_path = \"/path/to/your/r_script.R\"\noutput_path = \"/path/to/your/output_file.txt\"\nduration = 10", "raw_solution": "import subprocess\nimport os\nimport time\nimport glob\n\ndef task_func(r_script_path: str, output_path: str, duration: int) -> (bool, str):\n    \"\"\"\n    This function executes an R script and verifies if the output file is generated within a given duration.\n    \"\"\"\n    start_time = time.time()\n    subprocess.run([\"Rscript\", r_script_path])\n    while time.time() - start_time <= duration:\n        if glob.glob(output_path):\n            return True, 'File generated successfully within the specified duration.'\n        time.sleep(1)\n    return False, 'File not generated within the specified duration.'\n\n# Test the function\nr_script_path = \"/path/to/your/r_script.R\"\noutput_path = \"/path/to/your/output_file.txt\"\nduration = 10  # in seconds\n\nresult = task_func(r_script_path, output_path, duration)"}
{"task_id": "BigCodeBench/1106", "solution": "from datetime import datetime\nimport os\nfrom pathlib import Path\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(file_path):\n    \"\"\"\n    Determines the creation time of a file and converts it to a formatted string.\n    :param file_path: str: The path to the file.\n    :return: str: The creation time of the file in the format '%Y-%m-%d %H:%M:%S'.\n    \"\"\"\n    # Get the creation time of the file\n    creation_time = os.path.getctime(file_path)\n\n    # Convert the creation time to a datetime object\n    creation_time_dt = datetime.fromtimestamp(creation_time)\n\n    # Format the datetime object to the desired format\n    formatted_time = creation_time_dt.strftime(DATE_FORMAT)\n\n    return formatted_time", "raw_solution": "from datetime import datetime\nimport os\nfrom pathlib import Path\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(file_path):\n    \"\"\"\n    Determines the creation time of a file and converts it to a formatted string.\n    :param file_path: str: The path to the file.\n    :return: str: The creation time of the file in the format '%Y-%m-%d %H:%M:%S'.\n    \"\"\"\n    # Get the creation time of the file\n    creation_time = os.path.getctime(file_path)\n\n    # Convert the creation time to a datetime object\n    creation_time_dt = datetime.fromtimestamp(creation_time)\n\n    # Format the datetime object to the desired format\n    formatted_time = creation_time_dt.strftime(DATE_FORMAT)\n\n    return formatted_time"}
{"task_id": "BigCodeBench/1107", "solution": "from datetime import datetime\nimport pytz\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(unix_timestamp, target_timezone):\n    \"\"\"\n    Converts a Unix timestamp to a formatted date and time string in a specified timezone.\n    :param unix_timestamp: The Unix timestamp to convert.\n    :param target_timezone: The target timezone to convert to.\n    :return: A string representing the date and time in the target timezone, formatted as '%Y-%m-%d %H:%M:%S'.\n    \"\"\"\n    # Convert Unix timestamp to datetime object\n    dt = datetime.utcfromtimestamp(unix_timestamp)\n\n    # Convert datetime object to target timezone\n    target_tz = pytz.timezone(target_timezone)\n    dt = dt.replace(tzinfo=pytz.utc).astimezone(target_tz)\n\n    # Format datetime object as string\n    return dt.strftime(DATE_FORMAT)", "raw_solution": "from datetime import datetime\nimport pytz\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(unix_timestamp, target_timezone):\n    \"\"\"\n    Converts a Unix timestamp to a formatted date and time string in a specified timezone.\n    :param unix_timestamp: The Unix timestamp to convert.\n    :param target_timezone: The target timezone to convert to.\n    :return: A string representing the date and time in the target timezone, formatted as '%Y-%m-%d %H:%M:%S'.\n    \"\"\"\n    # Convert Unix timestamp to datetime object\n    dt = datetime.utcfromtimestamp(unix_timestamp)\n\n    # Convert datetime object to target timezone\n    target_tz = pytz.timezone(target_timezone)\n    dt = dt.replace(tzinfo=pytz.utc).astimezone(target_tz)\n\n    # Format datetime object as string\n    return dt.strftime(DATE_FORMAT)\n\n# Test the function"}
{"task_id": "BigCodeBench/1108", "solution": "from collections import Counter\nimport re\ndef task_func(result):\n    url_values = [item['url'] for item in result if 'url' in item]\n    most_common_values = Counter(url_values).most_common(1)\n    return {'most_common_url': most_common_values[0][0], 'count': most_common_values[0][1]}", "raw_solution": "from collections import Counter\nimport re\n\ndef task_func(result):\n    url_values = [item['url'] for item in result if 'url' in item]\n    most_common_values = Counter(url_values).most_common(1)\n    return {'most_common_url': most_common_values[0][0], 'count': most_common_values[0][1]}"}
{"task_id": "BigCodeBench/1109", "solution": "import os\nfrom nltk import word_tokenize\ndef task_func(file_path='File.txt'):\n    if not os.path.exists(file_path):\n        return \"File not found.\"\n\n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n\n    tokens = []\n    for line in lines:\n        tokens.extend(word_tokenize(line))\n\n    return tokens", "raw_solution": "import os\nfrom nltk import word_tokenize\n\ndef task_func(file_path='File.txt'):\n    if not os.path.exists(file_path):\n        return \"File not found.\"\n\n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n\n    tokens = []\n    for line in lines:\n        tokens.extend(word_tokenize(line))\n\n    return tokens"}
{"task_id": "BigCodeBench/1110", "solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\ndef task_func(word_dict):\n    # Combine all words into a single string\n    all_words = ''.join(word_dict.keys())\n\n    # Count the frequency of each letter\n    letter_counts = Counter(all_words)\n\n    return dict(letter_counts)\nword_dict = {'hello': 'abc', 'world': 'def'}", "raw_solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\n\ndef task_func(word_dict):\n    # Combine all words into a single string\n    all_words = ''.join(word_dict.keys())\n\n    # Count the frequency of each letter\n    letter_counts = Counter(all_words)\n\n    return dict(letter_counts)\n\n# Test the function\nword_dict = {'hello': 'abc', 'world': 'def'}"}
{"task_id": "BigCodeBench/1111", "solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\nANIMAL = ['cat', 'camel', 'cow', 'dog', 'elephant', 'fox', 'giraffe', 'hippo', 'iguana', 'jaguar']\ndef task_func(animal_dict):\n    # Remove key in the dictionary if it is not an animal\n    animal_dict = {key: value for key, value in animal_dict.items() if key in ANIMAL}\n\n    # Flatten the values in the dictionary and count the frequency of each letter\n    letter_counts = Counter(itertools.chain.from_iterable(animal_dict.values()))\n\n    # Sort the dictionary by frequency in descending order and return\n    return dict(sorted(letter_counts.items(), key=itemgetter(1), reverse=True))\nanimal_dict = {'cat': 'meow', 'camel': 'hum', 'cow': 'moo', 'dog': 'bark', 'elephant': 'trumpet', 'fox': 'what', 'giraffe': 'hum', 'hippo': 'grunt', 'iguana': 'squawk', 'jaguar': 'roar', 'lion': 'roar'}", "raw_solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\n\n#CONSTANT\nANIMAL = ['cat', 'camel', 'cow', 'dog', 'elephant', 'fox', 'giraffe', 'hippo', 'iguana', 'jaguar']\n\ndef task_func(animal_dict):\n    # Remove key in the dictionary if it is not an animal\n    animal_dict = {key: value for key, value in animal_dict.items() if key in ANIMAL}\n\n    # Flatten the values in the dictionary and count the frequency of each letter\n    letter_counts = Counter(itertools.chain.from_iterable(animal_dict.values()))\n\n    # Sort the dictionary by frequency in descending order and return\n    return dict(sorted(letter_counts.items(), key=itemgetter(1), reverse=True))\n\n# Test the function\nanimal_dict = {'cat': 'meow', 'camel': 'hum', 'cow': 'moo', 'dog': 'bark', 'elephant': 'trumpet', 'fox': 'what', 'giraffe': 'hum', 'hippo': 'grunt', 'iguana': 'squawk', 'jaguar': 'roar', 'lion': 'roar'}"}
{"task_id": "BigCodeBench/1112", "solution": "import csv\nimport random\nfrom datetime import datetime, timedelta", "raw_solution": "import csv\nimport random\nfrom datetime import datetime, timedelta\n\n# Constants\nDATA = ['Temperature', 'Humidity', 'Pressure']\nRANGE = {\n    'Temperature': (-50, 50),\n    'Humidity': (0, 100),\n    'Pressure': (980, 1040)\n}\n\ndef generate_weather_data(file_name=\"data.csv\"):\n    \"\"\"\n    Generates a CSV file with weather data for each hour of the current day.\n    The row names for the csv are 'Temperature', 'Humidity', and 'Pressure'.\n    The function returns the path to the created file.\n    \"\"\"\n    # Generate weather data for each hour of the current day\n    weather_data = [\n        {\n            'Time': (datetime.now() + timedelta(hours=i)).strftime('%Y-%m-%d %H:00:00'),\n            'Temperature': random.randint(*RANGE['Temperature']),\n            'Humidity': random.randint(*RANGE['Humidity']),\n            'Pressure': random.randint(*RANGE['Pressure'])\n        }\n        for i in range(24)\n    ]\n\n    # Write the weather data to a CSV file\n    with open(file_name, 'w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=DATA)\n        writer.writeheader()\n        writer.writerows(weather_data)\n\n    return file_name"}
{"task_id": "BigCodeBench/1113", "solution": "import csv\nimport collections\ndef task_func(csv_file, emp_prefix='EMP$$'):\n    \"\"\"\n    Count the number of records for each employee in a CSV file.\n    The function returns a dictionary with the count of records for each employee.\n    \"\"\"\n    # Initialize a dictionary to store the counts\n    counts = collections.defaultdict(int)\n\n    # Open the CSV file\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file)\n\n        # Iterate over the rows in the CSV file\n        for row in reader:\n            # Check if the row contains the employee prefix\n            for cell in row:\n                if cell.startswith(emp_prefix):\n                    # Increment the count for the employee\n                    counts[cell] += 1\n\n    return counts", "raw_solution": "import csv\nimport collections\n\ndef task_func(csv_file, emp_prefix='EMP$$'):\n    \"\"\"\n    Count the number of records for each employee in a CSV file.\n    The function returns a dictionary with the count of records for each employee.\n    \"\"\"\n    # Initialize a dictionary to store the counts\n    counts = collections.defaultdict(int)\n\n    # Open the CSV file\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file)\n\n        # Iterate over the rows in the CSV file\n        for row in reader:\n            # Check if the row contains the employee prefix\n            for cell in row:\n                if cell.startswith(emp_prefix):\n                    # Increment the count for the employee\n                    counts[cell] += 1\n\n    return counts"}
{"task_id": "BigCodeBench/1114", "solution": "from collections import defaultdict\nfrom random import randint\ndef task_func(dict1):\n    \"\"\"\n    This function takes a dictionary of employee data and returns a new dictionary with department codes starting with 'EMP$$' as keys and lists of employee salaries as values.\n    \"\"\"\n    result = defaultdict(list)\n    for key, value in dict1.items():\n        if key.startswith('EMP$$'):\n            for emp in value:\n                result[key].append(emp['salary'])\n    return result", "raw_solution": "from collections import defaultdict\nfrom random import randint\n\ndef task_func(dict1):\n    \"\"\"\n    This function takes a dictionary of employee data and returns a new dictionary with department codes starting with 'EMP$$' as keys and lists of employee salaries as values.\n    \"\"\"\n    result = defaultdict(list)\n    for key, value in dict1.items():\n        if key.startswith('EMP$$'):\n            for emp in value:\n                result[key].append(emp['salary'])\n    return result\n\n# Test the function\nemployee_data = {\n    'EMP$$1': [{'name': 'John', 'salary': 5000}, {'name': 'Jane', 'salary': 6000}],\n    'EMP$$2': [{'name': 'Mike', 'salary': 7000}, {'name': 'Sara', 'salary': 8000}],\n    'HR1': [{'name': 'Tom', 'salary': 5500}, {'name': 'Alice', 'salary': 6500}],\n    'EMP$$3': [{'name': 'Bob', 'salary': 7500}, {'name': 'Emily', 'salary': 8500}],\n}\n"}
{"task_id": "BigCodeBench/1115", "solution": "import random\nfrom string import ascii_uppercase\ndef task_func(dict1):\n    def generate_id(department_code):\n        random_string = ''.join(random.choice(ascii_uppercase) for _ in range(5))\n        return department_code + random_string\n\n    unique_ids = []\n    for department_code in dict1:\n        for _ in range(dict1[department_code]):\n            unique_ids.append(generate_id(department_code))\n\n    return unique_ids", "raw_solution": "import random\nfrom string import ascii_uppercase\n\ndef task_func(dict1):\n    def generate_id(department_code):\n        random_string = ''.join(random.choice(ascii_uppercase) for _ in range(5))\n        return department_code + random_string\n\n    unique_ids = []\n    for department_code in dict1:\n        for _ in range(dict1[department_code]):\n            unique_ids.append(generate_id(department_code))\n\n    return unique_ids\n\n# Test the function"}
{"task_id": "BigCodeBench/1116", "solution": "import random\nimport statistics\nfrom collections import Counter\nfrom typing import Dict, List, Tuple\ndef task_func(dict1: Dict[str, List[int]]) -> Tuple[float, float, List[int]]:\n    \"\"\"\n    Calculates the mean, median, and mode(s) of the ages of employees in the department \"EMP$$\".\n    \"\"\"\n    # Extract ages of employees in \"EMP$$\"\n    ages = dict1[\"EMP$$\"]\n\n    # Calculate mean\n    mean_age = statistics.mean(ages)\n\n    # Calculate median\n    median_age = statistics.median(ages)\n\n    # Calculate mode(s)\n    age_counts = Counter(ages)\n    max_count = max(age_counts.values())\n    modes = [age for age, count in age_counts.items() if count == max_count]\n\n    return (mean_age, median_age, modes)", "raw_solution": "import random\nimport statistics\nfrom collections import Counter\nfrom typing import Dict, List, Tuple\n\n# Constants\nAGE_RANGE = (22, 60)\n\ndef task_func(dict1: Dict[str, List[int]]) -> Tuple[float, float, List[int]]:\n    \"\"\"\n    Calculates the mean, median, and mode(s) of the ages of employees in the department \"EMP$$\".\n    \"\"\"\n    # Extract ages of employees in \"EMP$$\"\n    ages = dict1[\"EMP$$\"]\n\n    # Calculate mean\n    mean_age = statistics.mean(ages)\n\n    # Calculate median\n    median_age = statistics.median(ages)\n\n    # Calculate mode(s)\n    age_counts = Counter(ages)\n    max_count = max(age_counts.values())\n    modes = [age for age, count in age_counts.items() if count == max_count]\n\n    return (mean_age, median_age, modes)\n\n# Test the function\nrandom.seed(0)  # For reproducibility\nemployee_ages = {\n    \"EMP$$\": [random.randint(*AGE_RANGE) for _ in range(100)]\n}"}
{"task_id": "BigCodeBench/1117", "solution": "import collections\nimport random\nimport json\nPREFICES = ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']\nLEVELS = ['Junior', 'Mid', 'Senior']\ndef task_func(department_data):\n    \"\"\"\n    Generates a JSON object from employee data based on given department codes and their employee counts.\n    \"\"\"\n    # Create a dictionary to store the department data\n    department_dict = collections.defaultdict(list)\n\n    # Generate employee data for each department\n    for department in PREFICES:\n        for _ in range(department_data[department]):\n            department_dict[department].append(random.choice(LEVELS))\n\n    # Convert the dictionary to a JSON object\n    json_object = json.dumps(department_dict, indent=4)\n\n    return json_object", "raw_solution": "import collections\nimport random\nimport json\n\n# Constants\nPREFICES = ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']\nLEVELS = ['Junior', 'Mid', 'Senior']\n\ndef task_func(department_data):\n    \"\"\"\n    Generates a JSON object from employee data based on given department codes and their employee counts.\n    \"\"\"\n    # Create a dictionary to store the department data\n    department_dict = collections.defaultdict(list)\n\n    # Generate employee data for each department\n    for department in PREFICES:\n        for _ in range(department_data[department]):\n            department_dict[department].append(random.choice(LEVELS))\n\n    # Convert the dictionary to a JSON object\n    json_object = json.dumps(department_dict, indent=4)\n\n    return json_object\n\n# Test the function"}
{"task_id": "BigCodeBench/1118", "solution": "import json\nimport csv\nimport requests\nfrom io import StringIO\nCSV_URL = 'https://example.com/data.csv'\nJSON_FILE = 'data.json'\ndef task_func(csv_url=CSV_URL, json_file_path=JSON_FILE):\n    \"\"\"\n    Downloads a CSV file from a specified URL, converts it to JSON format, and saves it to a specified file path.\n    \"\"\"\n    # Download CSV file\n    response = requests.get(csv_url)\n    response.raise_for_status()\n\n    # Convert CSV to JSON\n    csv_data = response.text\n    dict_data = csv.DictReader(StringIO(csv_data))\n    json_data = json.dumps(list(dict_data))\n\n    # Save JSON to file\n    with open(json_file_path, 'w') as json_file:\n        json_file.write(json_data)\n\n    return json_file_path", "raw_solution": "import json\nimport csv\nimport requests\nfrom io import StringIO\n\n# Constants\nCSV_URL = 'https://example.com/data.csv'\nJSON_FILE = 'data.json'\n\ndef task_func(csv_url=CSV_URL, json_file_path=JSON_FILE):\n    \"\"\"\n    Downloads a CSV file from a specified URL, converts it to JSON format, and saves it to a specified file path.\n    \"\"\"\n    # Download CSV file\n    response = requests.get(csv_url)\n    response.raise_for_status()\n\n    # Convert CSV to JSON\n    csv_data = response.text\n    dict_data = csv.DictReader(StringIO(csv_data))\n    json_data = json.dumps(list(dict_data))\n\n    # Save JSON to file\n    with open(json_file_path, 'w') as json_file:\n        json_file.write(json_data)\n\n    return json_file_path"}
{"task_id": "BigCodeBench/1119", "solution": "import codecs\nimport random\nimport string\nimport hashlib\ndef task_func(password_length=10, salt=\"salty\"):\n    # Generate a random password\n    password = ''.join(random.choice(string.ascii_letters + string.digits + string.punctuation) for _ in range(password_length))\n\n    # Mix the password with the salt\n    salted_password = password + salt\n\n    # Hash the salted password using SHA256\n    hashed_password = hashlib.sha256(salted_password.encode()).hexdigest()\n\n    return hashed_password", "raw_solution": "import codecs\nimport random\nimport string\nimport hashlib\n\ndef task_func(password_length=10, salt=\"salty\"):\n    # Generate a random password\n    password = ''.join(random.choice(string.ascii_letters + string.digits + string.punctuation) for _ in range(password_length))\n\n    # Mix the password with the salt\n    salted_password = password + salt\n\n    # Hash the salted password using SHA256\n    hashed_password = hashlib.sha256(salted_password.encode()).hexdigest()\n\n    return hashed_password"}
{"task_id": "BigCodeBench/1120", "solution": "import re\nimport urllib.parse\nimport requests\nimport json\ndef task_func(myString, API_KEY):\n    # Extract URLs from the string\n    urls = re.findall('(?:(?:https?|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', myString)\n\n    # Extract domain from URLs\n    domains = [urllib.parse.urlparse(url).netloc for url in urls]\n\n    # Initialize an empty dictionary to store the geolocation data\n    geolocation_data = {}\n\n    # Iterate over the domains\n    for domain in domains:\n        # Construct the API request URL\n        url = f\"https://ipapi.co/{domain}/json/\"\n\n        # Make the API request\n        response = requests.get(url)\n\n        # Check if the request was successful\n        if response.status_code == 200:\n            # Parse the response JSON\n            data = response.json()\n\n            # Add the data to the dictionary\n            geolocation_data[domain] = data\n        else:\n            # If the request failed, add None to the dictionary\n            geolocation_data[domain] = None\n\n    return geolocation_data", "raw_solution": "import re\nimport urllib.parse\nimport requests\nimport json\n\ndef task_func(myString, API_KEY):\n    # Extract URLs from the string\n    urls = re.findall('(?:(?:https?|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', myString)\n\n    # Extract domain from URLs\n    domains = [urllib.parse.urlparse(url).netloc for url in urls]\n\n    # Initialize an empty dictionary to store the geolocation data\n    geolocation_data = {}\n\n    # Iterate over the domains\n    for domain in domains:\n        # Construct the API request URL\n        url = f\"https://ipapi.co/{domain}/json/\"\n\n        # Make the API request\n        response = requests.get(url)\n\n        # Check if the request was successful\n        if response.status_code == 200:\n            # Parse the response JSON\n            data = response.json()\n\n            # Add the data to the dictionary\n            geolocation_data[domain] = data\n        else:\n            # If the request failed, add None to the dictionary\n            geolocation_data[domain] = None\n\n    return geolocation_data"}
{"task_id": "BigCodeBench/1121", "solution": "import re\nimport urllib.parse\nimport requests\nimport json\ndef task_func(myString, API_KEY):\n    # Extract URLs from the string\n    urls = re.findall('(?:(?:https?|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', myString)\n\n    # Extract domain from URLs\n    domains = [urllib.parse.urlparse(url).netloc for url in urls]\n\n    # Initialize an empty dictionary to store the geolocation data\n    geolocation_data = {}\n\n    # Iterate over the domains\n    for domain in domains:\n        # Construct the API request URL\n        url = f\"https://ipapi.co/{domain}/json/\"\n\n        # Make the API request\n        response = requests.get(url)\n\n        # Check if the request was successful\n        if response.status_code == 200:\n            # Parse the response JSON\n            data = response.json()\n\n            # Add the data to the dictionary\n            geolocation_data[domain] = data\n        else:\n            # If the request failed, add None to the dictionary\n            geolocation_data[domain] = None\n\n    return geolocation_data", "raw_solution": "import re\nimport urllib.parse\nimport requests\nimport json\n\ndef task_func(myString, API_KEY):\n    # Extract URLs from the string\n    urls = re.findall('(?:(?:https?|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', myString)\n\n    # Extract domain from URLs\n    domains = [urllib.parse.urlparse(url).netloc for url in urls]\n\n    # Initialize an empty dictionary to store the geolocation data\n    geolocation_data = {}\n\n    # Iterate over the domains\n    for domain in domains:\n        # Construct the API request URL\n        url = f\"https://ipapi.co/{domain}/json/\"\n\n        # Make the API request\n        response = requests.get(url)\n\n        # Check if the request was successful\n        if response.status_code == 200:\n            # Parse the response JSON\n            data = response.json()\n\n            # Add the data to the dictionary\n            geolocation_data[domain] = data\n        else:\n            # If the request failed, add None to the dictionary\n            geolocation_data[domain] = None\n\n    return geolocation_data"}
{"task_id": "BigCodeBench/1122", "solution": "import re\nimport socket\nimport urllib.parse\ndef task_func(myString):\n    # Extract URLs from the string\n    urls = re.findall('(?:(?:https?|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', myString)\n\n    # Initialize an empty dictionary to store the domain and IP address pairs\n    domain_ip_dict = {}\n\n    for url in urls:\n        # Extract the domain from the URL\n        domain = urllib.parse.urlparse(url).netloc\n\n        try:\n            # Retrieve the IP address of the domain\n            ip = socket.gethostbyname(domain)\n        except socket.gaierror:\n            # If the domain cannot be resolved, set the IP address to None\n            ip = None\n\n        # Add the domain and IP address pair to the dictionary\n        domain_ip_dict[domain] = ip\n\n    return domain_ip_dict", "raw_solution": "import re\nimport socket\nimport urllib.parse\n\ndef task_func(myString):\n    # Extract URLs from the string\n    urls = re.findall('(?:(?:https?|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', myString)\n\n    # Initialize an empty dictionary to store the domain and IP address pairs\n    domain_ip_dict = {}\n\n    for url in urls:\n        # Extract the domain from the URL\n        domain = urllib.parse.urlparse(url).netloc\n\n        try:\n            # Retrieve the IP address of the domain\n            ip = socket.gethostbyname(domain)\n        except socket.gaierror:\n            # If the domain cannot be resolved, set the IP address to None\n            ip = None\n\n        # Add the domain and IP address pair to the dictionary\n        domain_ip_dict[domain] = ip\n\n    return domain_ip_dict"}
{"task_id": "BigCodeBench/1123", "solution": "import re\nimport urllib.parse\nimport ssl\nimport socket\ndef task_func(myString):\n    # Extract all URLs from the string\n    urls = re.findall('https://[^\\s]*', myString)\n\n    # Dictionary to store domain and SSL certificate expiry date\n    ssl_dict = {}\n\n    for url in urls:\n        try:\n            # Parse the URL to get the domain\n            domain = urllib.parse.urlparse(url).netloc\n\n            # Get the SSL certificate\n            context = ssl.create_default_context()\n            with socket.create_connection((domain, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=domain) as ssock:\n                    print(ssock.getpeercert())\n                    # Get the certificate expiry date\n                    cert_expiry_date = ssock.getpeercert()['notAfter']\n\n                    # Add the domain and expiry date to the dictionary\n                    ssl_dict[domain] = cert_expiry_date\n        except:\n            # Ignore any URLs that cannot be processed\n            continue\n\n    return ssl_dict", "raw_solution": "import re\nimport urllib.parse\nimport ssl\nimport socket\n\ndef task_func(myString):\n    # Extract all URLs from the string\n    urls = re.findall('https://[^\\s]*', myString)\n\n    # Dictionary to store domain and SSL certificate expiry date\n    ssl_dict = {}\n\n    for url in urls:\n        try:\n            # Parse the URL to get the domain\n            domain = urllib.parse.urlparse(url).netloc\n\n            # Get the SSL certificate\n            context = ssl.create_default_context()\n            with socket.create_connection((domain, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=domain) as ssock:\n                    print(ssock.getpeercert())\n                    # Get the certificate expiry date\n                    cert_expiry_date = ssock.getpeercert()['notAfter']\n\n                    # Add the domain and expiry date to the dictionary\n                    ssl_dict[domain] = cert_expiry_date\n        except:\n            # Ignore any URLs that cannot be processed\n            continue\n\n    return ssl_dict"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Extract URL from the string\n    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    if not url:\n        return \"No valid URL found in the provided string.\"\n    url = url[0]\n\n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n    except requests.exceptions.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # Parse the content and extract the title\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.find('title')\n    if not title:\n        return \"No title tag found in the webpage.\"\n    return title.text", "raw_solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Extract URL from the string\n    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    if not url:\n        return \"No valid URL found in the provided string.\"\n    url = url[0]\n\n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n    except requests.exceptions.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # Parse the content and extract the title\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.find('title')\n    if not title:\n        return \"No title tag found in the webpage.\"\n    return title.text"}
{"task_id": "BigCodeBench/1125", "solution": "import re\nimport json\nimport requests\ndef task_func(myString, token):\n    # Extract URL from string\n    url = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    if url:\n        url = url[0]\n    else:\n        return \"No URL found in the string\"\n\n    # Prepare headers\n    headers = {\n        'Authorization': 'Bearer ' + token,\n        'Content-Type': 'application/json'\n    }\n\n    # Prepare payload\n    payload = {\n        'url': url\n    }\n\n    # Send POST request\n    response = requests.post('https://api.example.com/url', headers=headers, data=json.dumps(payload))\n\n    # Return response\n    return response.json()", "raw_solution": "import re\nimport json\nimport requests\n\ndef task_func(myString, token):\n    # Extract URL from string\n    url = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    if url:\n        url = url[0]\n    else:\n        return \"No URL found in the string\"\n\n    # Prepare headers\n    headers = {\n        'Authorization': 'Bearer ' + token,\n        'Content-Type': 'application/json'\n    }\n\n    # Prepare payload\n    payload = {\n        'url': url\n    }\n\n    # Send POST request\n    response = requests.post('https://api.example.com/url', headers=headers, data=json.dumps(payload))\n\n    # Return response\n    return response.json()"}
{"task_id": "BigCodeBench/1126", "solution": "import re\nimport hashlib\ndef task_func(input_str):\n    # Remove all special characters, punctuation marks, and spaces from the input string\n    cleaned_str = re.sub(r'[^A-Za-z0-9]', '', input_str)\n\n    # Hash the cleaned string with SHA256\n    hashed_str = hashlib.sha256(cleaned_str.encode()).hexdigest()\n\n    return hashed_str", "raw_solution": "import re\nimport hashlib\n\ndef task_func(input_str):\n    # Remove all special characters, punctuation marks, and spaces from the input string\n    cleaned_str = re.sub(r'[^A-Za-z0-9]', '', input_str)\n\n    # Hash the cleaned string with SHA256\n    hashed_str = hashlib.sha256(cleaned_str.encode()).hexdigest()\n\n    return hashed_str"}
{"task_id": "BigCodeBench/1127", "solution": "import os\nimport re\nimport hashlib\ndef task_func(path, delimiter):\n    \"\"\"\n    Splits a file path by a specified delimiter, retaining the delimiter in the output, and computes the hash of each component if it is a file.\n    \"\"\"\n    components = re.split(delimiter, path)\n    result = []\n\n    for component in components:\n        if os.path.isfile(component):\n            with open(component, 'rb') as file:\n                file_hash = hashlib.sha256(file.read()).hexdigest()\n            result.append((component, file_hash))\n        else:\n            result.append((component, None))\n\n    return result", "raw_solution": "import os\nimport re\nimport hashlib\n\ndef task_func(path, delimiter):\n    \"\"\"\n    Splits a file path by a specified delimiter, retaining the delimiter in the output, and computes the hash of each component if it is a file.\n    \"\"\"\n    components = re.split(delimiter, path)\n    result = []\n\n    for component in components:\n        if os.path.isfile(component):\n            with open(component, 'rb') as file:\n                file_hash = hashlib.sha256(file.read()).hexdigest()\n            result.append((component, file_hash))\n        else:\n            result.append((component, None))\n\n    return result"}
{"task_id": "BigCodeBench/1128", "solution": "import json\nimport os\nimport hashlib\nimport base64\nimport time\ndef task_func(file_path, unknown_key):\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    value_to_hash = data['A'][unknown_key]['maindata'][0]['Info']\n\n    hash_object = hashlib.sha256(value_to_hash.encode())\n    hashed_value = base64.b64encode(hash_object.digest()).decode()\n\n    timestamp = str(int(time.time()))\n    new_file_path = os.path.join(os.getcwd(), f'hashed_value_{timestamp}.txt')\n\n    with open(new_file_path, 'w') as new_file:\n        new_file.write(hashed_value)\n\n    return new_file_path", "raw_solution": "import json\nimport os\nimport hashlib\nimport base64\nimport time\n\ndef task_func(file_path, unknown_key):\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    value_to_hash = data['A'][unknown_key]['maindata'][0]['Info']\n\n    hash_object = hashlib.sha256(value_to_hash.encode())\n    hashed_value = base64.b64encode(hash_object.digest()).decode()\n\n    timestamp = str(int(time.time()))\n    new_file_path = os.path.join(os.getcwd(), f'hashed_value_{timestamp}.txt')\n\n    with open(new_file_path, 'w') as new_file:\n        new_file.write(hashed_value)\n\n    return new_file_path"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with a specified key, downloads the file from the URL,\n    and saves it with a timestamped filename. The filename format is '{unknown_key}_{timestamp}.txt',\n    where 'timestamp' is formatted as '%Y%m%d%H%M%S%f' to include the date and time down to microseconds.\n    The file is saved in the specified directory or in the current working directory by default.\n    \"\"\"\n    data = json.loads(json_data)\n    url = data[unknown_key]\n\n    response = requests.get(url)\n    content = response.content\n\n    if save_dir is None:\n        save_dir = os.getcwd()\n\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    filepath = os.path.join(save_dir, filename)\n\n    with open(filepath, 'wb') as f:\n        f.write(content)\n\n    return os.path.abspath(filepath)", "raw_solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with a specified key, downloads the file from the URL,\n    and saves it with a timestamped filename. The filename format is '{unknown_key}_{timestamp}.txt',\n    where 'timestamp' is formatted as '%Y%m%d%H%M%S%f' to include the date and time down to microseconds.\n    The file is saved in the specified directory or in the current working directory by default.\n    \"\"\"\n    data = json.loads(json_data)\n    url = data[unknown_key]\n\n    response = requests.get(url)\n    content = response.content\n\n    if save_dir is None:\n        save_dir = os.getcwd()\n\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    filepath = os.path.join(save_dir, filename)\n\n    with open(filepath, 'wb') as f:\n        f.write(content)\n\n    return os.path.abspath(filepath)"}
{"task_id": "BigCodeBench/1130", "solution": "import os\nimport hashlib\nimport json\nfrom pathlib import Path\ndef task_func(directory: str) -> str:\n    \"\"\"\n    Creates SHA256 hashes for all files in the specified directory, including files in subdirectories,\n    and saves these hashes in a JSON file named 'hashes.json' in the given directory.\n    \"\"\"\n    hashes = {}\n\n    for foldername, subfolders, filenames in os.walk(directory):\n        for filename in filenames:\n            file_path = os.path.join(foldername, filename)\n            with open(file_path, 'rb') as f:\n                file_hash = hashlib.sha256()\n                while chunk := f.read(8192):\n                    file_hash.update(chunk)\n            hashes[file_path] = file_hash.hexdigest()\n\n    json_file_path = os.path.join(directory, 'hashes.json')\n    with open(json_file_path, 'w') as f:\n        json.dump(hashes, f)\n\n    return json_file_path", "raw_solution": "import os\nimport hashlib\nimport json\nfrom pathlib import Path\n\ndef task_func(directory: str) -> str:\n    \"\"\"\n    Creates SHA256 hashes for all files in the specified directory, including files in subdirectories,\n    and saves these hashes in a JSON file named 'hashes.json' in the given directory.\n    \"\"\"\n    hashes = {}\n\n    for foldername, subfolders, filenames in os.walk(directory):\n        for filename in filenames:\n            file_path = os.path.join(foldername, filename)\n            with open(file_path, 'rb') as f:\n                file_hash = hashlib.sha256()\n                while chunk := f.read(8192):\n                    file_hash.update(chunk)\n            hashes[file_path] = file_hash.hexdigest()\n\n    json_file_path = os.path.join(directory, 'hashes.json')\n    with open(json_file_path, 'w') as f:\n        json.dump(hashes, f)\n\n    return json_file_path"}
{"task_id": "BigCodeBench/1131", "solution": "import hashlib\nimport binascii\nimport sqlite3\ndef task_func(salt, cursor):\n    if not isinstance(salt, str):\n        raise TypeError(\"Salt must be a string\")\n\n    # Prepare the salt\n    salt = salt.encode('utf-8')\n\n    # Get the list of users\n    cursor.execute(\"SELECT username, password FROM users\")\n    users = cursor.fetchall()\n\n    # Update the passwords\n    count = 0\n    for user in users:\n        username, password = user\n        hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, 100000)\n        hashed_password = binascii.hexlify(hashed_password).decode('utf-8')\n\n        cursor.execute(\"UPDATE users SET password = ? WHERE username = ?\", (hashed_password, username))\n        count += 1\n\n    return count", "raw_solution": "import hashlib\nimport binascii\nimport sqlite3\n\ndef task_func(salt, cursor):\n    if not isinstance(salt, str):\n        raise TypeError(\"Salt must be a string\")\n\n    # Prepare the salt\n    salt = salt.encode('utf-8')\n\n    # Get the list of users\n    cursor.execute(\"SELECT username, password FROM users\")\n    users = cursor.fetchall()\n\n    # Update the passwords\n    count = 0\n    for user in users:\n        username, password = user\n        hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, 100000)\n        hashed_password = binascii.hexlify(hashed_password).decode('utf-8')\n\n        cursor.execute(\"UPDATE users SET password = ? WHERE username = ?\", (hashed_password, username))\n        count += 1\n\n    return count"}
{"task_id": "BigCodeBench/1132", "solution": "import os\nimport hashlib\nimport base64\ndef task_func(password, PREFIX=\"ME\", SALT_LENGTH=16):\n    if SALT_LENGTH < 0:\n        raise ValueError(\"SALT_LENGTH cannot be negative\")\n\n    # Generate a random salt\n    salt = os.urandom(SALT_LENGTH)\n\n    # Combine the password, prefix, and salt\n    combined = (password + PREFIX).encode() + salt\n\n    # Hash the combined string using SHA256\n    hashed = hashlib.sha256(combined)\n\n    # Encode the hashed result in base64\n    encoded = base64.b64encode(hashed.digest())\n\n    return encoded.decode()", "raw_solution": "import os\nimport hashlib\nimport base64\n\ndef task_func(password, PREFIX=\"ME\", SALT_LENGTH=16):\n    if SALT_LENGTH < 0:\n        raise ValueError(\"SALT_LENGTH cannot be negative\")\n\n    # Generate a random salt\n    salt = os.urandom(SALT_LENGTH)\n\n    # Combine the password, prefix, and salt\n    combined = (password + PREFIX).encode() + salt\n\n    # Hash the combined string using SHA256\n    hashed = hashlib.sha256(combined)\n\n    # Encode the hashed result in base64\n    encoded = base64.b64encode(hashed.digest())\n\n    return encoded.decode()"}
{"task_id": "BigCodeBench/1133", "solution": "import json\nimport requests\ndef task_func(API_URL, endpoint, PREFIX):\n    try:\n        response = requests.get(API_URL + endpoint)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error fetching data from API: {e}\")\n\n    data = response.json()\n\n    filename = f\"{PREFIX}_{endpoint}.json\"\n    try:\n        with open(filename, 'w') as file:\n            json.dump(data, file)\n    except IOError as e:\n        raise RuntimeError(f\"Error writing to file: {e}\")\n\n    return filename", "raw_solution": "import json\nimport requests\n\ndef task_func(API_URL, endpoint, PREFIX):\n    try:\n        response = requests.get(API_URL + endpoint)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error fetching data from API: {e}\")\n\n    data = response.json()\n\n    filename = f\"{PREFIX}_{endpoint}.json\"\n    try:\n        with open(filename, 'w') as file:\n            json.dump(data, file)\n    except IOError as e:\n        raise RuntimeError(f\"Error writing to file: {e}\")\n\n    return filename"}
{"task_id": "BigCodeBench/1134", "solution": "import os\nimport glob\nimport hashlib\ndef task_func(source_dir, target_dir, prefix='#Hash: '):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory {source_dir} does not exist.\")\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    file_paths = glob.glob(os.path.join(source_dir, '*'))\n\n    new_file_paths = []\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            content = file.read()\n            md5_hash = hashlib.md5(content.encode()).hexdigest()\n            new_content = prefix + md5_hash + '\\n' + content\n\n            new_file_path = os.path.join(target_dir, os.path.basename(file_path))\n            with open(new_file_path, 'w') as new_file:\n                new_file.write(new_content)\n\n            new_file_paths.append(new_file_path)\n\n    return new_file_paths", "raw_solution": "import os\nimport glob\nimport hashlib\n\ndef task_func(source_dir, target_dir, prefix='#Hash: '):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory {source_dir} does not exist.\")\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    file_paths = glob.glob(os.path.join(source_dir, '*'))\n\n    new_file_paths = []\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            content = file.read()\n            md5_hash = hashlib.md5(content.encode()).hexdigest()\n            new_content = prefix + md5_hash + '\\n' + content\n\n            new_file_path = os.path.join(target_dir, os.path.basename(file_path))\n            with open(new_file_path, 'w') as new_file:\n                new_file.write(new_content)\n\n            new_file_paths.append(new_file_path)\n\n    return new_file_paths"}
{"task_id": "BigCodeBench/1135", "solution": "import collections\nimport json\nimport requests\ndef task_func(user, API_URL = 'https://api.github.com/users/'):\n    \"\"\"\n    Retrieves the names of the repositories of a specified GitHub user, sorted in ascending order by their creation date.\n    \"\"\"\n    response = requests.get(API_URL + user + '/repos')\n    data = json.loads(response.text)\n\n    # Extract the names and creation dates of the repositories\n    repos = [(repo['name'], repo['created_at']) for repo in data]\n\n    # Sort the repositories by their creation dates\n    repos.sort(key=lambda x: x[1])\n\n    # Extract the names of the repositories\n    repo_names = [repo[0] for repo in repos]\n\n    return repo_names", "raw_solution": "import collections\nimport json\nimport requests\n\ndef task_func(user, API_URL = 'https://api.github.com/users/'):\n    \"\"\"\n    Retrieves the names of the repositories of a specified GitHub user, sorted in ascending order by their creation date.\n    \"\"\"\n    response = requests.get(API_URL + user + '/repos')\n    data = json.loads(response.text)\n\n    # Extract the names and creation dates of the repositories\n    repos = [(repo['name'], repo['created_at']) for repo in data]\n\n    # Sort the repositories by their creation dates\n    repos.sort(key=lambda x: x[1])\n\n    # Extract the names of the repositories\n    repo_names = [repo[0] for repo in repos]\n\n    return repo_names"}
{"task_id": "BigCodeBench/1136", "solution": "import bs4\nimport requests\nimport re\nimport csv\ndef task_func(url=\"http://example.com\", csv_path=\"emails.csv\", \n          regex=r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\", \n          headers={'User-Agent': 'Mozilla/5.0'}):\n    \"\"\"\n    Scrapes a web page to extract all email addresses using a specified regular expression pattern and writes them to a CSV file.\n    The csv file is always created eventhough no email is found in the url. The header of the csv should be \"Emails\".\n    The function returns the path to the CSV file where the extracted email addresses have been saved.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n\n    # Parse the HTML content of the page with BeautifulSoup\n    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n\n    # Find all text in the page that matches the regular expression for email addresses\n    emails = re.findall(regex, soup.text)\n\n    # Write the emails to a CSV file\n    with open(csv_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Emails\"])\n        for email in emails:\n            writer.writerow([email])\n\n    return csv_path", "raw_solution": "import bs4\nimport requests\nimport re\nimport csv\n\ndef task_func(url=\"http://example.com\", csv_path=\"emails.csv\", \n          regex=r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\", \n          headers={'User-Agent': 'Mozilla/5.0'}):\n    \"\"\"\n    Scrapes a web page to extract all email addresses using a specified regular expression pattern and writes them to a CSV file.\n    The csv file is always created eventhough no email is found in the url. The header of the csv should be \"Emails\".\n    The function returns the path to the CSV file where the extracted email addresses have been saved.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n\n    # Parse the HTML content of the page with BeautifulSoup\n    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n\n    # Find all text in the page that matches the regular expression for email addresses\n    emails = re.findall(regex, soup.text)\n\n    # Write the emails to a CSV file\n    with open(csv_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Emails\"])\n        for email in emails:\n            writer.writerow([email])\n\n    return csv_path"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n    \"\"\"\n    if \"http\" in url:\n        response = requests.get(url)\n        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    else:\n        with open(url, 'r') as file:\n            soup = bs4.BeautifulSoup(file.read(), 'html.parser')\n\n    phone_numbers = re.findall(r'\\(?\\b[2-9][0-9]{2}\\)?[-. ]?[2-9][0-9]{2}[-. ]?[0-9]{4}\\b', soup.text)\n\n    with open(output_path, 'w') as file:\n        json.dump(phone_numbers, file)\n\n    return phone_numbers", "raw_solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n    \"\"\"\n    if \"http\" in url:\n        response = requests.get(url)\n        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    else:\n        with open(url, 'r') as file:\n            soup = bs4.BeautifulSoup(file.read(), 'html.parser')\n\n    phone_numbers = re.findall(r'\\(?\\b[2-9][0-9]{2}\\)?[-. ]?[2-9][0-9]{2}[-. ]?[0-9]{4}\\b', soup.text)\n\n    with open(output_path, 'w') as file:\n        json.dump(phone_numbers, file)\n\n    return phone_numbers"}
{"task_id": "BigCodeBench/1138", "solution": "import numpy as np\nimport itertools\ndef task_func(matrix):\n    # Flatten the 2D array and sort it\n    flattened_sorted_array = np.sort(matrix.flatten())\n\n    # Generate all unique combinations of two elements\n    combinations = list(itertools.combinations(flattened_sorted_array, 2))\n\n    return flattened_sorted_array, combinations", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(matrix):\n    # Flatten the 2D array and sort it\n    flattened_sorted_array = np.sort(matrix.flatten())\n\n    # Generate all unique combinations of two elements\n    combinations = list(itertools.combinations(flattened_sorted_array, 2))\n\n    return flattened_sorted_array, combinations"}
{"task_id": "BigCodeBench/1139", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\ndef task_func(data):\n    # Split the data into train and test sets\n    X = data['Hours'].values.reshape(-1, 1)\n    y = data['Scores'].values.reshape(-1, 1)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Train the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate and return the mean squared error\n    mse = mean_squared_error(y_test, y_pred)\n    return mse", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\ndef task_func(data):\n    # Split the data into train and test sets\n    X = data['Hours'].values.reshape(-1, 1)\n    y = data['Scores'].values.reshape(-1, 1)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Train the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate and return the mean squared error\n    mse = mean_squared_error(y_test, y_pred)\n    return mse"}
