{"task_id": "BigCodeBench/0", "solution": "import itertools\nfrom random import shuffle\ndef task_func(numbers=list(range(1, 11))):\n    # Generate all permutations of the list\n    perms = list(itertools.permutations(numbers))\n\n    # Initialize sum of absolute differences\n    sum_diffs = 0\n\n    # For each permutation, calculate the sum of absolute differences\n    for perm in perms:\n        shuffle(perm)  # Shuffle the permutation\n        sum_diffs += sum(abs(perm[i] - perm[i+1]) for i in range(len(perm)-1))\n\n    # Calculate the average\n    avg_diffs = sum_diffs / len(perms)\n\n    return avg_diffs", "raw_solution": "import itertools\nfrom random import shuffle\n\ndef task_func(numbers=list(range(1, 11))):\n    # Generate all permutations of the list\n    perms = list(itertools.permutations(numbers))\n\n    # Initialize sum of absolute differences\n    sum_diffs = 0\n\n    # For each permutation, calculate the sum of absolute differences\n    for perm in perms:\n        shuffle(perm)  # Shuffle the permutation\n        sum_diffs += sum(abs(perm[i] - perm[i+1]) for i in range(len(perm)-1))\n\n    # Calculate the average\n    avg_diffs = sum_diffs / len(perms)\n\n    return avg_diffs\n\n# Test the function"}
{"task_id": "BigCodeBench/1", "solution": "import collections\nimport random\nimport string\ndef task_func(length=100):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n    random_string = ''.join(random.choices(string.ascii_letters, k=length))\n    count_dict = collections.Counter(random_string)\n    return count_dict", "raw_solution": "import collections\nimport random\nimport string\n\ndef task_func(length=100):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n    random_string = ''.join(random.choices(string.ascii_letters, k=length))\n    count_dict = collections.Counter(random_string)\n    return count_dict"}
{"task_id": "BigCodeBench/2", "solution": "import random\nimport statistics\ndef task_func(LETTERS):\n    # Create a dictionary with random letters as keys and random lists of integers as values\n    random_dict = {chr(65 + i): [random.randint(1, 100) for _ in range(10)] for i in range(LETTERS)}\n\n    # Calculate the mean of the values in the dictionary and sort it in descending order\n    sorted_dict = {k: v for k, v in sorted(random_dict.items(), key=lambda item: statistics.mean(item[1]), reverse=True)}\n\n    return sorted_dict", "raw_solution": "import random\nimport statistics\n\ndef task_func(LETTERS):\n    # Create a dictionary with random letters as keys and random lists of integers as values\n    random_dict = {chr(65 + i): [random.randint(1, 100) for _ in range(10)] for i in range(LETTERS)}\n\n    # Calculate the mean of the values in the dictionary and sort it in descending order\n    sorted_dict = {k: v for k, v in sorted(random_dict.items(), key=lambda item: statistics.mean(item[1]), reverse=True)}\n\n    return sorted_dict\n\n# Test the function"}
{"task_id": "BigCodeBench/3", "solution": "import random\nimport numpy as np\ndef task_func(LETTERS):\n    random.seed(0)\n    result = {}\n    for letter in LETTERS:\n        random_list = [random.randint(0, 100) for _ in range(random.randint(1, 10))]\n        result[letter] = np.mean(random_list)\n    return result\nLETTERS = ['A', 'B', 'C', 'D', 'E']", "raw_solution": "import random\nimport numpy as np\n\ndef task_func(LETTERS):\n    random.seed(0)\n    result = {}\n    for letter in LETTERS:\n        random_list = [random.randint(0, 100) for _ in range(random.randint(1, 10))]\n        result[letter] = np.mean(random_list)\n    return result\n\n# Test the function\nLETTERS = ['A', 'B', 'C', 'D', 'E']"}
{"task_id": "BigCodeBench/4", "solution": "from collections import Counter\nimport itertools\ndef task_func(d):\n    \"\"\"\n    Count the occurrence of each integer in the values of the input dictionary, where each value is a list of integers, and return a dictionary with these counts.\n    \"\"\"\n    # Flatten the list of values from the dictionary and count the occurrences of each integer\n    counts = Counter(itertools.chain.from_iterable(d.values()))\n    return counts\nd = {\n    \"a\": [1, 2, 2, 3],\n    \"b\": [2, 3, 4],\n    \"c\": [1, 1, 1, 2, 2, 3, 3, 3]\n}", "raw_solution": "from collections import Counter\nimport itertools\n\ndef task_func(d):\n    \"\"\"\n    Count the occurrence of each integer in the values of the input dictionary, where each value is a list of integers, and return a dictionary with these counts.\n    \"\"\"\n    # Flatten the list of values from the dictionary and count the occurrences of each integer\n    counts = Counter(itertools.chain.from_iterable(d.values()))\n    return counts\n\n# Test the function\nd = {\n    \"a\": [1, 2, 2, 3],\n    \"b\": [2, 3, 4],\n    \"c\": [1, 1, 1, 2, 2, 3, 3, 3]\n}"}
{"task_id": "BigCodeBench/5", "solution": "import random\nimport math\nfrom typing import List, Dict\ndef task_func(LETTERS: List[str] = [chr(i) for i in range(97, 123)]) -> Dict[str, float]:\n    \"\"\"\n    Creates a dictionary where keys are letters from a predefined list LETTERS and values are lists of random integers.\n    Then, calculates the population standard deviation for each list of integers and returns a dictionary of these values.\n    The random integers for each key are generated within the range 0 to 100, and each list contains between 1 to 10 integers.\n    \"\"\"\n    def calculate_std_dev(numbers: List[int]) -> float:\n        \"\"\"\n        Helper function to calculate the population standard deviation of a list of numbers.\n        \"\"\"\n        mean = sum(numbers) / len(numbers)\n        variance = sum((x - mean) ** 2 for x in numbers) / len(numbers)\n        return math.sqrt(variance)\n\n    random.seed(0)  # for reproducibility\n    result = {}\n    for letter in LETTERS:\n        num_integers = random.randint(1, 10)\n        random_integers = [random.randint(0, 100) for _ in range(num_integers)]\n        result[letter] = calculate_std_dev(random_integers)\n    return result", "raw_solution": "import random\nimport math\nfrom typing import List, Dict\n\ndef task_func(LETTERS: List[str] = [chr(i) for i in range(97, 123)]) -> Dict[str, float]:\n    \"\"\"\n    Creates a dictionary where keys are letters from a predefined list LETTERS and values are lists of random integers.\n    Then, calculates the population standard deviation for each list of integers and returns a dictionary of these values.\n    The random integers for each key are generated within the range 0 to 100, and each list contains between 1 to 10 integers.\n    \"\"\"\n    def calculate_std_dev(numbers: List[int]) -> float:\n        \"\"\"\n        Helper function to calculate the population standard deviation of a list of numbers.\n        \"\"\"\n        mean = sum(numbers) / len(numbers)\n        variance = sum((x - mean) ** 2 for x in numbers) / len(numbers)\n        return math.sqrt(variance)\n\n    random.seed(0)  # for reproducibility\n    result = {}\n    for letter in LETTERS:\n        num_integers = random.randint(1, 10)\n        random_integers = [random.randint(0, 100) for _ in range(num_integers)]\n        result[letter] = calculate_std_dev(random_integers)\n    return result\n\n# Test the function"}
{"task_id": "BigCodeBench/6", "solution": "import os\nimport re\nfrom typing import Optional\ndef task_func(pattern: str, log_dir: str = '/var/log/') -> Optional[str]:\n    \"\"\"\n    Finds the latest log file in a specified directory that matches a given regex pattern.\n    \"\"\"\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # Initialize variables to store the latest file and its modification time\n    latest_file = None\n    latest_mtime = 0\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(log_dir):\n        # Check if the file matches the regex pattern\n        if regex.match(filename):\n            # Get the file's modification time\n            mtime = os.path.getmtime(os.path.join(log_dir, filename))\n\n            # If the file's modification time is newer than the current latest file, update the latest file and its modification time\n            if mtime > latest_mtime:\n                latest_file = os.path.join(log_dir, filename)\n                latest_mtime = mtime\n\n    # Return the path to the latest file or None if no files match the pattern\n    return latest_file", "raw_solution": "import os\nimport re\nfrom typing import Optional\n\ndef task_func(pattern: str, log_dir: str = '/var/log/') -> Optional[str]:\n    \"\"\"\n    Finds the latest log file in a specified directory that matches a given regex pattern.\n    \"\"\"\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # Initialize variables to store the latest file and its modification time\n    latest_file = None\n    latest_mtime = 0\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(log_dir):\n        # Check if the file matches the regex pattern\n        if regex.match(filename):\n            # Get the file's modification time\n            mtime = os.path.getmtime(os.path.join(log_dir, filename))\n\n            # If the file's modification time is newer than the current latest file, update the latest file and its modification time\n            if mtime > latest_mtime:\n                latest_file = os.path.join(log_dir, filename)\n                latest_mtime = mtime\n\n    # Return the path to the latest file or None if no files match the pattern\n    return latest_file"}
{"task_id": "BigCodeBench/7", "solution": "import csv\nfrom collections import defaultdict\ndef task_func(csv_file_path):\n    \"\"\"\n    Finds the best-selling product from a given CSV file with sales data.\n    \"\"\"\n    # Initialize a dictionary to store product names and their total sales\n    product_sales = defaultdict(int)\n\n    # Open the CSV file and read it line by line\n    with open(csv_file_path, 'r') as csv_file:\n        csv_reader = csv.DictReader(csv_file)\n        for row in csv_reader:\n            # Add the quantity sold for the current product to the total sales\n            product_sales[row['product']] += int(row['quantity'])\n\n    # Find the product with the highest total sales\n    best_selling_product = max(product_sales.items(), key=lambda x: x[1])[0]\n\n    return best_selling_product", "raw_solution": "import csv\nfrom collections import defaultdict\n\ndef task_func(csv_file_path):\n    \"\"\"\n    Finds the best-selling product from a given CSV file with sales data.\n    \"\"\"\n    # Initialize a dictionary to store product names and their total sales\n    product_sales = defaultdict(int)\n\n    # Open the CSV file and read it line by line\n    with open(csv_file_path, 'r') as csv_file:\n        csv_reader = csv.DictReader(csv_file)\n        for row in csv_reader:\n            # Add the quantity sold for the current product to the total sales\n            product_sales[row['product']] += int(row['quantity'])\n\n    # Find the product with the highest total sales\n    best_selling_product = max(product_sales.items(), key=lambda x: x[1])[0]\n\n    return best_selling_product"}
{"task_id": "BigCodeBench/8", "solution": "from collections import Counter\nimport itertools\nfrom random import randint\ndef task_func(T1, RANGE=100):\n    # Convert elements in 'T1' to integers\n    T1 = list(map(int, T1))\n\n    # Create a list of random integers\n    random_list = [randint(0, RANGE) for _ in itertools.repeat(None, sum(T1))]\n\n    # Count the occurrences of each number in the generated list\n    counter = Counter(random_list)\n\n    return counter", "raw_solution": "from collections import Counter\nimport itertools\nfrom random import randint\n\ndef task_func(T1, RANGE=100):\n    # Convert elements in 'T1' to integers\n    T1 = list(map(int, T1))\n\n    # Create a list of random integers\n    random_list = [randint(0, RANGE) for _ in itertools.repeat(None, sum(T1))]\n\n    # Count the occurrences of each number in the generated list\n    counter = Counter(random_list)\n\n    return counter"}
{"task_id": "BigCodeBench/9", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(list_of_pairs):\n    # Create a DataFrame from the list of pairs\n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n\n    # Create a bar plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='Category', y='Value', data=df)\n    ax.set_title('Category vs Value')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(list_of_pairs):\n    # Create a DataFrame from the list of pairs\n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n\n    # Create a bar plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='Category', y='Value', data=df)\n    ax.set_title('Category vs Value')\n\n    return df, ax"}
{"task_id": "BigCodeBench/10", "solution": "import numpy as np\nimport itertools\nimport random\nimport statistics\nfrom collections import Counter\ndef task_func(T1, RANGE=100):\n    \"\"\"\n    Converts elements in 'T1' to integers, creates a list of random integers,\n    and calculates the mean, median, and mode of the list.\n    \"\"\"\n    if not T1:\n        raise statistics.StatisticsError(\"T1 is empty\")\n\n    # Convert elements in 'T1' to integers\n    T1 = [int(i) for i in T1]\n\n    # Create a list of random integers\n    random_list = [random.randint(0, RANGE) for _ in itertools.repeat(None, sum(T1))]\n\n    # Calculate mean\n    mean_val = np.mean(random_list)\n\n    # Calculate median\n    median_val = np.median(random_list)\n\n    # Calculate mode\n    counter = Counter(random_list)\n    max_count = max(counter.values())\n    mode_val = [k for k, v in counter.items() if v == max_count]\n    mode_val = min(mode_val)  # in case of multiple modes, return the smallest one\n\n    return (mean_val, median_val, mode_val)", "raw_solution": "import numpy as np\nimport itertools\nimport random\nimport statistics\nfrom collections import Counter\n\ndef task_func(T1, RANGE=100):\n    \"\"\"\n    Converts elements in 'T1' to integers, creates a list of random integers,\n    and calculates the mean, median, and mode of the list.\n    \"\"\"\n    if not T1:\n        raise statistics.StatisticsError(\"T1 is empty\")\n\n    # Convert elements in 'T1' to integers\n    T1 = [int(i) for i in T1]\n\n    # Create a list of random integers\n    random_list = [random.randint(0, RANGE) for _ in itertools.repeat(None, sum(T1))]\n\n    # Calculate mean\n    mean_val = np.mean(random_list)\n\n    # Calculate median\n    median_val = np.median(random_list)\n\n    # Calculate mode\n    counter = Counter(random_list)\n    max_count = max(counter.values())\n    mode_val = [k for k, v in counter.items() if v == max_count]\n    mode_val = min(mode_val)  # in case of multiple modes, return the smallest one\n\n    return (mean_val, median_val, mode_val)"}
{"task_id": "BigCodeBench/11", "solution": "import numpy as np\nimport itertools\nimport random\ndef task_func(T1, max_value=100):\n    # Convert elements in T1 to integers\n    T1 = [int(i) for i in itertools.chain.from_iterable(T1)]\n\n    # Create a list of random integers with size equal to the sum of T1\n    T2 = [random.randint(0, max_value) for _ in range(sum(T1))]\n\n    # Calculate 25th, 50th, and 75th percentiles\n    p25 = np.percentile(T2, 25)\n    p50 = np.percentile(T2, 50)\n    p75 = np.percentile(T2, 75)\n\n    return (p25, p50, p75)\nT1 = (('1', '2', '3'), ('4', '5', '6'), ('7', '8', '9'))", "raw_solution": "import numpy as np\nimport itertools\nimport random\n\ndef task_func(T1, max_value=100):\n    # Convert elements in T1 to integers\n    T1 = [int(i) for i in itertools.chain.from_iterable(T1)]\n\n    # Create a list of random integers with size equal to the sum of T1\n    T2 = [random.randint(0, max_value) for _ in range(sum(T1))]\n\n    # Calculate 25th, 50th, and 75th percentiles\n    p25 = np.percentile(T2, 25)\n    p50 = np.percentile(T2, 50)\n    p75 = np.percentile(T2, 75)\n\n    return (p25, p50, p75)\n\n# Test the function\nT1 = (('1', '2', '3'), ('4', '5', '6'), ('7', '8', '9'))"}
{"task_id": "BigCodeBench/12", "solution": "import subprocess\nimport os\nimport json\nfrom datetime import datetime\ndef task_func(script_name='backup.sh', log_file='/home/user/backup_log.json'):\n    \"\"\"\n    Runs the provided backup shell script and logs the start time, end time, and exit status in a specified JSON log file.\n    \"\"\"\n    if not os.path.exists(script_name):\n        raise FileNotFoundError(f\"The script file {script_name} does not exist.\")\n\n    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    try:\n        output = subprocess.run([script_name], capture_output=True, text=True, check=True)\n        exit_status = 0\n    except subprocess.CalledProcessError as e:\n        output = e.output\n        exit_status = e.returncode\n        raise RuntimeError(f\"Error executing the script: {output}\")\n    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    log_data = {\n        'start_time': start_time,\n        'end_time': end_time,\n        'exit_status': exit_status,\n        'output': output\n    }\n\n    with open(log_file, 'w') as file:\n        json.dump(log_data, file)\n\n    return log_data", "raw_solution": "import subprocess\nimport os\nimport json\nfrom datetime import datetime\n\ndef task_func(script_name='backup.sh', log_file='/home/user/backup_log.json'):\n    \"\"\"\n    Runs the provided backup shell script and logs the start time, end time, and exit status in a specified JSON log file.\n    \"\"\"\n    if not os.path.exists(script_name):\n        raise FileNotFoundError(f\"The script file {script_name} does not exist.\")\n\n    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    try:\n        output = subprocess.run([script_name], capture_output=True, text=True, check=True)\n        exit_status = 0\n    except subprocess.CalledProcessError as e:\n        output = e.output\n        exit_status = e.returncode\n        raise RuntimeError(f\"Error executing the script: {output}\")\n    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n    log_data = {\n        'start_time': start_time,\n        'end_time': end_time,\n        'exit_status': exit_status,\n        'output': output\n    }\n\n    with open(log_file, 'w') as file:\n        json.dump(log_data, file)\n\n    return log_data"}
{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\nfrom typing import List\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test') -> List[str]:\n    \"\"\"\n    Downloads all files from a specific directory on an FTP server using wget.\n    \"\"\"\n    # List to store filenames\n    filenames = []\n\n    try:\n        # Connect to FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        print(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n        return filenames\n\n    try:\n        # Login to FTP server\n        ftp.login(ftp_user, ftp_password)\n    except Exception as e:\n        print(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n        return filenames\n\n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        print(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        return filenames\n\n    # Get a list of files in the directory\n    try:\n        ftp.retrlines('LIST', filenames.append)\n    except Exception as e:\n        print(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        return filenames\n\n    # Download each file using wget\n    for filename in filenames:\n        try:\n            subprocess.run(['wget', f'ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}'])\n        except Exception as e:\n            print(f\"Failed to download file {filename} from server {ftp_server}: {str(e)}\")\n\n    # Close the FTP connection\n    ftp.quit()\n\n    return filenames", "raw_solution": "import subprocess\nimport ftplib\nimport os\nfrom typing import List\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test') -> List[str]:\n    \"\"\"\n    Downloads all files from a specific directory on an FTP server using wget.\n    \"\"\"\n    # List to store filenames\n    filenames = []\n\n    try:\n        # Connect to FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        print(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n        return filenames\n\n    try:\n        # Login to FTP server\n        ftp.login(ftp_user, ftp_password)\n    except Exception as e:\n        print(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n        return filenames\n\n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        print(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        return filenames\n\n    # Get a list of files in the directory\n    try:\n        ftp.retrlines('LIST', filenames.append)\n    except Exception as e:\n        print(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        return filenames\n\n    # Download each file using wget\n    for filename in filenames:\n        try:\n            subprocess.run(['wget', f'ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}'])\n        except Exception as e:\n            print(f\"Failed to download file {filename} from server {ftp_server}: {str(e)}\")\n\n    # Close the FTP connection\n    ftp.quit()\n\n    return filenames"}
{"task_id": "BigCodeBench/14", "solution": "import configparser\nimport os\nimport shutil\nimport zipfile\ndef task_func(config_file_path, archive_dir='/home/user/archive'):\n    \"\"\"\n    Archives a specified project directory into a ZIP file based on the configuration specified in a config file.\n    \"\"\"\n    # Check if the config file exists\n    if not os.path.isfile(config_file_path):\n        raise FileNotFoundError(f\"The config file {config_file_path} does not exist.\")\n\n    # Parse the config file\n    config = configparser.ConfigParser()\n    config.read(config_file_path)\n\n    # Check if the project directory exists\n    project_dir = config['Project']['directory']\n    if not os.path.isdir(project_dir):\n        raise FileNotFoundError(f\"The project directory {project_dir} does not exist.\")\n\n    # Create the archive directory if it doesn't exist\n    if not os.path.isdir(archive_dir):\n        os.makedirs(archive_dir)\n\n    # Create the ZIP archive\n    zip_file_path = os.path.join(archive_dir, os.path.basename(project_dir) + '.zip')\n    try:\n        with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for root, dirs, files in os.walk(project_dir):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    zipf.write(file_path, arcname=file)\n    except Exception as e:\n        raise Exception(f\"Failed to create the ZIP archive: {e}\")\n\n    return True", "raw_solution": "import configparser\nimport os\nimport shutil\nimport zipfile\n\ndef task_func(config_file_path, archive_dir='/home/user/archive'):\n    \"\"\"\n    Archives a specified project directory into a ZIP file based on the configuration specified in a config file.\n    \"\"\"\n    # Check if the config file exists\n    if not os.path.isfile(config_file_path):\n        raise FileNotFoundError(f\"The config file {config_file_path} does not exist.\")\n\n    # Parse the config file\n    config = configparser.ConfigParser()\n    config.read(config_file_path)\n\n    # Check if the project directory exists\n    project_dir = config['Project']['directory']\n    if not os.path.isdir(project_dir):\n        raise FileNotFoundError(f\"The project directory {project_dir} does not exist.\")\n\n    # Create the archive directory if it doesn't exist\n    if not os.path.isdir(archive_dir):\n        os.makedirs(archive_dir)\n\n    # Create the ZIP archive\n    zip_file_path = os.path.join(archive_dir, os.path.basename(project_dir) + '.zip')\n    try:\n        with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for root, dirs, files in os.walk(project_dir):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    zipf.write(file_path, arcname=file)\n    except Exception as e:\n        raise Exception(f\"Failed to create the ZIP archive: {e}\")\n\n    return True"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    Executes a list of shell commands read from a CSV file and saves the outputs in separate files.\n    Each command's output is written to a unique file in the specified output directory.\n    If a command fails, the error message along with the exit code is appended to the respective output file.\n    \"\"\"\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_file_paths = []\n\n    with open(commands_file_path, 'r') as commands_file:\n        reader = csv.reader(commands_file)\n        for i, row in enumerate(reader):\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{i}_output.txt')\n            output_file_paths.append(output_file_path)\n\n            try:\n                result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n            except subprocess.CalledProcessError as e:\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(f\"Command '{command}' failed with exit code {e.returncode}.\\n\")\n                    output_file.write(f\"Error message: {str(e)}\")\n\n    return output_file_paths", "raw_solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    Executes a list of shell commands read from a CSV file and saves the outputs in separate files.\n    Each command's output is written to a unique file in the specified output directory.\n    If a command fails, the error message along with the exit code is appended to the respective output file.\n    \"\"\"\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_file_paths = []\n\n    with open(commands_file_path, 'r') as commands_file:\n        reader = csv.reader(commands_file)\n        for i, row in enumerate(reader):\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{i}_output.txt')\n            output_file_paths.append(output_file_path)\n\n            try:\n                result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n            except subprocess.CalledProcessError as e:\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(f\"Command '{command}' failed with exit code {e.returncode}.\\n\")\n                    output_file.write(f\"Error message: {str(e)}\")\n\n    return output_file_paths"}
{"task_id": "BigCodeBench/16", "solution": "import os\nimport glob\nimport subprocess\ndef task_func(directory, backup_dir='/path/to/backup'):\n    \"\"\"\n    Backup all '.log' files in a specified directory to a tar.gz file and delete the original files after backup.\n    The backup file is named 'logs_backup.tar.gz' and placed in the specified backup directory.\n    The function raises FileNotFoundError if the specified directory does not exist.\n    The function returns a message 'No logs found to backup' if no '.log' files are found in the directory.\n    \"\"\"\n    # Check if the directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    # Get a list of all '.log' files in the directory\n    log_files = glob.glob(os.path.join(directory, '*.log'))\n\n    # If no '.log' files are found, return a message\n    if not log_files:\n        return 'No logs found to backup.'\n\n    # Create a backup file path\n    backup_file = os.path.join(backup_dir, 'logs_backup.tar.gz')\n\n    # Create a tar.gz file of the '.log' files in the directory\n    subprocess.run(['tar', '-czvf', backup_file] + log_files, check=True)\n\n    # Delete the original '.log' files\n    for log_file in log_files:\n        os.remove(log_file)\n\n    # Return the path to the backup file\n    return backup_file", "raw_solution": "import os\nimport glob\nimport subprocess\n\ndef task_func(directory, backup_dir='/path/to/backup'):\n    \"\"\"\n    Backup all '.log' files in a specified directory to a tar.gz file and delete the original files after backup.\n    The backup file is named 'logs_backup.tar.gz' and placed in the specified backup directory.\n    The function raises FileNotFoundError if the specified directory does not exist.\n    The function returns a message 'No logs found to backup' if no '.log' files are found in the directory.\n    \"\"\"\n    # Check if the directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    # Get a list of all '.log' files in the directory\n    log_files = glob.glob(os.path.join(directory, '*.log'))\n\n    # If no '.log' files are found, return a message\n    if not log_files:\n        return 'No logs found to backup.'\n\n    # Create a backup file path\n    backup_file = os.path.join(backup_dir, 'logs_backup.tar.gz')\n\n    # Create a tar.gz file of the '.log' files in the directory\n    subprocess.run(['tar', '-czvf', backup_file] + log_files, check=True)\n\n    # Delete the original '.log' files\n    for log_file in log_files:\n        os.remove(log_file)\n\n    # Return the path to the backup file\n    return backup_file"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process is running, so we need to terminate it\n            proc.terminate()\n            time.sleep(1)  # Wait for the process to terminate\n            return f\"Process found. Restarting {process_name}.\"\n\n    # Process is not running, so we need to start it\n    subprocess.Popen(process_name, shell=True)\n    time.sleep(1)  # Wait for the process to start\n    return f\"Process not found. Starting {process_name}.\"", "raw_solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process is running, so we need to terminate it\n            proc.terminate()\n            time.sleep(1)  # Wait for the process to terminate\n            return f\"Process found. Restarting {process_name}.\"\n\n    # Process is not running, so we need to start it\n    subprocess.Popen(process_name, shell=True)\n    time.sleep(1)  # Wait for the process to start\n    return f\"Process not found. Starting {process_name}.\"\n\n# Test the function"}
{"task_id": "BigCodeBench/18", "solution": "import subprocess\nimport csv\nimport glob\nimport random\nimport os\ndef task_func(file):\n    \"\"\"\n    Divides a CSV file into several smaller files and shuffles the lines in each file.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(file):\n        return []\n\n    # Check if file is a CSV file\n    if not file.endswith('.csv'):\n        return []\n\n    # Split the file into smaller files\n    try:\n        subprocess.run(['split', '-l', '1000', file, 'split_'], check=True)\n    except subprocess.CalledProcessError:\n        return []\n\n    # Get the list of split files\n    split_files = glob.glob('split_*')\n\n    # Shuffle the lines in each file\n    for split_file in split_files:\n        with open(split_file, 'r') as f:\n            lines = f.readlines()\n            random.shuffle(lines)\n        with open(split_file, 'w') as f:\n            f.writelines(lines)\n\n    return split_files", "raw_solution": "import subprocess\nimport csv\nimport glob\nimport random\nimport os\n\ndef task_func(file):\n    \"\"\"\n    Divides a CSV file into several smaller files and shuffles the lines in each file.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(file):\n        return []\n\n    # Check if file is a CSV file\n    if not file.endswith('.csv'):\n        return []\n\n    # Split the file into smaller files\n    try:\n        subprocess.run(['split', '-l', '1000', file, 'split_'], check=True)\n    except subprocess.CalledProcessError:\n        return []\n\n    # Get the list of split files\n    split_files = glob.glob('split_*')\n\n    # Shuffle the lines in each file\n    for split_file in split_files:\n        with open(split_file, 'r') as f:\n            lines = f.readlines()\n            random.shuffle(lines)\n        with open(split_file, 'w') as f:\n            f.writelines(lines)\n\n    return split_files"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    \"\"\"\n    Zips all files (not including subdirectories) located in the specified directory and returns the path to the created zip file.\n    If the specified directory does not exist, it raises a FileNotFoundError.\n    If the directory does not contain any files, it returns None.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    files = glob.glob(os.path.join(directory, '*'))\n    files = [file for file in files if os.path.isfile(file)]\n\n    if not files:\n        return None\n\n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return zip_path", "raw_solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    \"\"\"\n    Zips all files (not including subdirectories) located in the specified directory and returns the path to the created zip file.\n    If the specified directory does not exist, it raises a FileNotFoundError.\n    If the directory does not contain any files, it returns None.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    files = glob.glob(os.path.join(directory, '*'))\n    files = [file for file in files if os.path.isfile(file)]\n\n    if not files:\n        return None\n\n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, arcname=os.path.basename(file))\n\n    return zip_path"}
{"task_id": "BigCodeBench/20", "solution": "import ast\nimport pandas as pd\nimport seaborn as sns\ndef task_func(csv_file):\n    # Read CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert string representations of dictionaries to Python dictionaries\n    df['dict_column'] = df['dict_column'].apply(lambda x: ast.literal_eval(x))\n\n    # Visualize data with Seaborn's pairplot\n    ax = sns.pairplot(df)\n\n    return df, ax", "raw_solution": "import ast\nimport pandas as pd\nimport seaborn as sns\n\ndef task_func(csv_file):\n    # Read CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert string representations of dictionaries to Python dictionaries\n    df['dict_column'] = df['dict_column'].apply(lambda x: ast.literal_eval(x))\n\n    # Visualize data with Seaborn's pairplot\n    ax = sns.pairplot(df)\n\n    return df, ax"}
{"task_id": "BigCodeBench/21", "solution": "import psutil\nimport platform\ndef task_func():\n    \"\"\"\n    Obtains system details, including operating system, architecture, and memory usage.\n    Returns a dictionary with the details.\n    \"\"\"\n    # Get the OS name\n    os_name = platform.system()\n\n    # Get the architecture\n    architecture = platform.architecture()[0]\n\n    # Get the memory usage\n    memory_info = psutil.virtual_memory()\n    total_memory = memory_info.total / (1024.0 ** 3)  # Convert bytes to GB\n    used_memory = memory_info.used / (1024.0 ** 3)  # Convert bytes to GB\n    memory_usage = (used_memory / total_memory) * 100  # Calculate percentage\n\n    # Create a dictionary with the details\n    details = {\n        'OS': os_name,\n        'Architecture': architecture,\n        'Memory Usage': f'{memory_usage:.2f}%'  # Format the usage as a percentage with 2 decimal places\n    }\n\n    return details", "raw_solution": "import psutil\nimport platform\n\ndef task_func():\n    \"\"\"\n    Obtains system details, including operating system, architecture, and memory usage.\n    Returns a dictionary with the details.\n    \"\"\"\n    # Get the OS name\n    os_name = platform.system()\n\n    # Get the architecture\n    architecture = platform.architecture()[0]\n\n    # Get the memory usage\n    memory_info = psutil.virtual_memory()\n    total_memory = memory_info.total / (1024.0 ** 3)  # Convert bytes to GB\n    used_memory = memory_info.used / (1024.0 ** 3)  # Convert bytes to GB\n    memory_usage = (used_memory / total_memory) * 100  # Calculate percentage\n\n    # Create a dictionary with the details\n    details = {\n        'OS': os_name,\n        'Architecture': architecture,\n        'Memory Usage': f'{memory_usage:.2f}%'  # Format the usage as a percentage with 2 decimal places\n    }\n\n    return details"}
{"task_id": "BigCodeBench/22", "solution": "import collections\nfrom itertools import zip_longest\nfrom random import choices\ndef task_func(l1, l2, K=10):\n    # Combine two lists by alternating their elements\n    combined = [item for sublist in zip_longest(l1, l2) for item in sublist if item is not None]\n    \n    # Create a random sample of size K from the combined list\n    sample = choices(combined, k=K)\n    \n    # Calculate the frequency of each element in the sample\n    frequency = collections.Counter(sample)\n    \n    return frequency", "raw_solution": "import collections\nfrom itertools import zip_longest\nfrom random import choices\n\ndef task_func(l1, l2, K=10):\n    # Combine two lists by alternating their elements\n    combined = [item for sublist in zip_longest(l1, l2) for item in sublist if item is not None]\n    \n    # Create a random sample of size K from the combined list\n    sample = choices(combined, k=K)\n    \n    # Calculate the frequency of each element in the sample\n    frequency = collections.Counter(sample)\n    \n    return frequency"}
{"task_id": "BigCodeBench/23", "solution": "import numpy as np\nfrom itertools import zip_longest\ndef task_func(l1, l2, THRESHOLD=0.5):\n    # Filter out non-numeric elements\n    l1 = [i for i in l1 if isinstance(i, (int, float))]\n    l2 = [i for i in l2 if isinstance(i, (int, float))]\n\n    # Alternates elements from two lists\n    combined = [i for pair in zip_longest(l1, l2) for i in pair if i is not None]\n\n    # Calculate absolute difference from threshold\n    diffs = [abs(i - THRESHOLD) for i in combined]\n\n    # Return element closest to threshold\n    return combined[np.argmin(diffs)]\nl1 = [0.1, 0.2, 0.3, 0.4, 0.5]\nl2 = [0.6, 0.7, 0.8, 0.9, 1.0]", "raw_solution": "import numpy as np\nfrom itertools import zip_longest\n\ndef task_func(l1, l2, THRESHOLD=0.5):\n    # Filter out non-numeric elements\n    l1 = [i for i in l1 if isinstance(i, (int, float))]\n    l2 = [i for i in l2 if isinstance(i, (int, float))]\n\n    # Alternates elements from two lists\n    combined = [i for pair in zip_longest(l1, l2) for i in pair if i is not None]\n\n    # Calculate absolute difference from threshold\n    diffs = [abs(i - THRESHOLD) for i in combined]\n\n    # Return element closest to threshold\n    return combined[np.argmin(diffs)]\n\n# Test the function\nl1 = [0.1, 0.2, 0.3, 0.4, 0.5]\nl2 = [0.6, 0.7, 0.8, 0.9, 1.0]"}
{"task_id": "BigCodeBench/24", "solution": "import base64\nimport hashlib\nimport os\nfrom typing import Tuple\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes", "raw_solution": "import base64\nimport hashlib\nimport os\nfrom typing import Tuple\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives"}
{"task_id": "BigCodeBench/25", "solution": "import base64\nimport json\nimport zlib\ndef task_func(data_dict):\n    \"\"\"\n    Serializes a dictionary to a JSON string, compresses it using zlib, and then encodes the compressed data with base64.\n    \"\"\"\n    # Convert the dictionary to a JSON string\n    json_str = json.dumps(data_dict)\n\n    # Compress the JSON string using zlib\n    compressed_data = zlib.compress(json_str.encode('utf-8'))\n\n    # Encode the compressed data with base64\n    base64_encoded_data = base64.b64encode(compressed_data)\n\n    # Convert the base64-encoded data to a string and return it\n    return base64_encoded_data.decode('utf-8')\ndata_dict = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}", "raw_solution": "import base64\nimport json\nimport zlib\n\ndef task_func(data_dict):\n    \"\"\"\n    Serializes a dictionary to a JSON string, compresses it using zlib, and then encodes the compressed data with base64.\n    \"\"\"\n    # Convert the dictionary to a JSON string\n    json_str = json.dumps(data_dict)\n\n    # Compress the JSON string using zlib\n    compressed_data = zlib.compress(json_str.encode('utf-8'))\n\n    # Encode the compressed data with base64\n    base64_encoded_data = base64.b64encode(compressed_data)\n\n    # Convert the base64-encoded data to a string and return it\n    return base64_encoded_data.decode('utf-8')\n\n# Test the function\ndata_dict = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\nresult = task_func(data_dict)"}
{"task_id": "BigCodeBench/26", "solution": "import base64\nfrom cryptography.fernet import Fernet\ndef task_func(message, encryption_key):\n    \"\"\"\n    Encrypts a message with a symmetric encryption key using Fernet encryption, and then encode the encrypted message using base64.\n    \"\"\"\n    # Create a Fernet object with the encryption key\n    cipher_suite = Fernet(encryption_key)\n\n    # Encrypt the message\n    encrypted_message = cipher_suite.encrypt(message.encode())\n\n    # Encode the encrypted message using base64\n    base64_encoded_message = base64.b64encode(encrypted_message).decode()\n\n    return base64_encoded_message", "raw_solution": "import base64\nfrom cryptography.fernet import Fernet\n\ndef task_func(message, encryption_key):\n    \"\"\"\n    Encrypts a message with a symmetric encryption key using Fernet encryption, and then encode the encrypted message using base64.\n    \"\"\"\n    # Create a Fernet object with the encryption key\n    cipher_suite = Fernet(encryption_key)\n\n    # Encrypt the message\n    encrypted_message = cipher_suite.encrypt(message.encode())\n\n    # Encode the encrypted message using base64\n    base64_encoded_message = base64.b64encode(encrypted_message).decode()\n\n    return base64_encoded_message"}
{"task_id": "BigCodeBench/27", "solution": "import json\nimport base64\nfrom datetime import datetime\ndef task_func(data: dict, DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\") -> str:\n    \"\"\"\n    Takes a Python dictionary, adds a current timestamp to it, serializes the modified dictionary to a JSON-formatted string, and then encodes this string using base64 encoding with ASCII character encoding.\n    \"\"\"\n    # Add current timestamp to the dictionary\n    data['timestamp'] = datetime.now().strftime(DATE_FORMAT)\n\n    # Convert dictionary to JSON-formatted string\n    json_str = json.dumps(data)\n\n    # Encode the JSON-formatted string using base64 and ASCII character encoding\n    base64_encoded_str = base64.b64encode(json_str.encode('ascii'))\n\n    return base64_encoded_str.decode('ascii')\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}", "raw_solution": "import json\nimport base64\nfrom datetime import datetime\n\ndef task_func(data: dict, DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\") -> str:\n    \"\"\"\n    Takes a Python dictionary, adds a current timestamp to it, serializes the modified dictionary to a JSON-formatted string, and then encodes this string using base64 encoding with ASCII character encoding.\n    \"\"\"\n    # Add current timestamp to the dictionary\n    data['timestamp'] = datetime.now().strftime(DATE_FORMAT)\n\n    # Convert dictionary to JSON-formatted string\n    json_str = json.dumps(data)\n\n    # Encode the JSON-formatted string using base64 and ASCII character encoding\n    base64_encoded_str = base64.b64encode(json_str.encode('ascii'))\n\n    return base64_encoded_str.decode('ascii')\n\n# Test the function\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}"}
{"task_id": "BigCodeBench/28", "solution": "import requests\nimport json\nimport base64\ndef task_func(data, url=\"http://your-api-url.com\"):\n    \"\"\"\n    This function takes a Python dictionary and a URL as input.\n    It converts the dictionary into a JSON-formatted string, encodes this string in base64 format,\n    and sends it as a 'payload' in a POST request to the provided URL.\n    It returns the response object received from the API endpoint after the POST request.\n    \"\"\"\n    # Convert the dictionary to a JSON-formatted string\n    json_str = json.dumps(data)\n\n    # Encode the JSON string in base64 format\n    encoded_str = base64.b64encode(json_str.encode('utf-8')).decode('utf-8')\n\n    # Create the payload\n    payload = {'payload': encoded_str}\n\n    # Send a POST request to the API endpoint\n    response = requests.post(url, data=payload)\n\n    return response", "raw_solution": "import requests\nimport json\nimport base64\n\ndef task_func(data, url=\"http://your-api-url.com\"):\n    \"\"\"\n    This function takes a Python dictionary and a URL as input.\n    It converts the dictionary into a JSON-formatted string, encodes this string in base64 format,\n    and sends it as a 'payload' in a POST request to the provided URL.\n    It returns the response object received from the API endpoint after the POST request.\n    \"\"\"\n    # Convert the dictionary to a JSON-formatted string\n    json_str = json.dumps(data)\n\n    # Encode the JSON string in base64 format\n    encoded_str = base64.b64encode(json_str.encode('utf-8')).decode('utf-8')\n\n    # Create the payload\n    payload = {'payload': encoded_str}\n\n    # Send a POST request to the API endpoint\n    response = requests.post(url, data=payload)\n\n    return response"}
{"task_id": "BigCodeBench/29", "solution": "from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport base64\ndef task_func(data):\n    \"\"\"\n    Standardize a numeric array using sklearn's StandardScaler and encode the standardized data in base64 format as an ASCII string.\n    \"\"\"\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Fit and transform the data\n    standardized_data = scaler.fit_transform(data)\n\n    # Convert the standardized data to a numpy array\n    standardized_data = np.array(standardized_data)\n\n    # Convert the numpy array to a string\n    standardized_data_str = standardized_data.tostring()\n\n    # Encode the string in base64\n    base64_encoded_data = base64.b64encode(standardized_data_str)\n\n    # Convert the base64-encoded data to an ASCII string\n    ascii_encoded_data = base64_encoded_data.decode('ascii')\n\n    return ascii_encoded_data", "raw_solution": "from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport base64\n\ndef task_func(data):\n    \"\"\"\n    Standardize a numeric array using sklearn's StandardScaler and encode the standardized data in base64 format as an ASCII string.\n    \"\"\"\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Fit and transform the data\n    standardized_data = scaler.fit_transform(data)\n\n    # Convert the standardized data to a numpy array\n    standardized_data = np.array(standardized_data)\n\n    # Convert the numpy array to a string\n    standardized_data_str = standardized_data.tostring()\n\n    # Encode the string in base64\n    base64_encoded_data = base64.b64encode(standardized_data_str)\n\n    # Convert the base64-encoded data to an ASCII string\n    ascii_encoded_data = base64_encoded_data.decode('ascii')\n\n    return ascii_encoded_data"}
{"task_id": "BigCodeBench/30", "solution": "import json\nimport os\nimport re\ndef task_func(\n    file_path,\n    attribute,\n    INPUT_JSON={\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},  \n            \"age\": {\"type\": \"integer\"},   \n            \"email\": {\"type\": \"string\"}  \n        },\n        \"required\": [\"name\", \"age\", \"email\"]\n    },\n    EMAIL_REGEX=r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"):\n\n    # Check if file exists\n    if not os.path.isfile(file_path):\n        raise ValueError(f\"File {file_path} does not exist.\")\n\n    # Load JSON file\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    # Validate structure and contents of JSON file\n    if not isinstance(data, dict):\n        raise ValueError(\"JSON data is not an object.\")\n\n    # Check required fields\n    for field in INPUT_JSON[\"required\"]:\n        if field not in data:\n            raise ValueError(f\"Required field {field} is missing.\")\n\n    # Check types\n    for key, value in INPUT_JSON[\"properties\"].items():\n        if key in data:\n            if not isinstance(data[key], value[\"type\"]):\n                raise ValueError(f\"Field {key} is not of type {value['type']}.\")\n\n    # Check email format\n    if \"email\" in data:\n        if not re.match(EMAIL_REGEX, data[\"email\"]):\n            raise ValueError(\"Invalid email format.\")\n\n    # Return the value of the specified attribute\n    if attribute in data:\n        return data[attribute]\n    else:\n        raise ValueError(f\"Attribute {attribute} not found in JSON data.\")", "raw_solution": "import json\nimport os\nimport re\n\ndef task_func(\n    file_path,\n    attribute,\n    INPUT_JSON={\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},  \n            \"age\": {\"type\": \"integer\"},   \n            \"email\": {\"type\": \"string\"}  \n        },\n        \"required\": [\"name\", \"age\", \"email\"]\n    },\n    EMAIL_REGEX=r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\"):\n\n    # Check if file exists\n    if not os.path.isfile(file_path):\n        raise ValueError(f\"File {file_path} does not exist.\")\n\n    # Load JSON file\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    # Validate structure and contents of JSON file\n    if not isinstance(data, dict):\n        raise ValueError(\"JSON data is not an object.\")\n\n    # Check required fields\n    for field in INPUT_JSON[\"required\"]:\n        if field not in data:\n            raise ValueError(f\"Required field {field} is missing.\")\n\n    # Check types\n    for key, value in INPUT_JSON[\"properties\"].items():\n        if key in data:\n            if not isinstance(data[key], value[\"type\"]):\n                raise ValueError(f\"Field {key} is not of type {value['type']}.\")\n\n    # Check email format\n    if \"email\" in data:\n        if not re.match(EMAIL_REGEX, data[\"email\"]):\n            raise ValueError(\"Invalid email format.\")\n\n    # Return the value of the specified attribute\n    if attribute in data:\n        return data[attribute]\n    else:\n        raise ValueError(f\"Attribute {attribute} not found in JSON data.\")"}
{"task_id": "BigCodeBench/31", "solution": "import nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom typing import Optional\ndef task_func(text: str) -> Optional[plt.Axes]:\n    \"\"\"\n    Draws a bar chart of the frequency of words in a text beginning with the \"$\" character.\n    Words that start with the '$' character but consist only of punctuation (e.g., '$!$' and '$.$') are not included in the frequency count.\n    If there is no word respecting the above conditions, the plot is None.\n    \"\"\"\n    # Tokenize the text\n    words = nltk.word_tokenize(text)\n\n    # Filter out words that start with '$' and are not punctuation\n    dollar_words = [word[1:] for word in words if word.startswith('$') and word[1:]]\n\n    # Count the frequency of each word\n    word_freq = Counter(dollar_words)\n\n    # If there are no words, return None\n    if not word_freq:\n        return None\n\n    # Create a bar plot\n    ax = sns.barplot(x=list(word_freq.keys()), y=list(word_freq.values()))\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of words beginning with \"$\"')\n\n    return ax", "raw_solution": "import nltk\nfrom string import punctuation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom typing import Optional\n\n# Constants\nPUNCTUATION = set(punctuation)\n\ndef task_func(text: str) -> Optional[plt.Axes]:\n    \"\"\"\n    Draws a bar chart of the frequency of words in a text beginning with the \"$\" character.\n    Words that start with the '$' character but consist only of punctuation (e.g., '$!$' and '$.$') are not included in the frequency count.\n    If there is no word respecting the above conditions, the plot is None.\n    \"\"\"\n    # Tokenize the text\n    words = nltk.word_tokenize(text)\n\n    # Filter out words that start with '$' and are not punctuation\n    dollar_words = [word[1:] for word in words if word.startswith('$') and word[1:]]\n\n    # Count the frequency of each word\n    word_freq = Counter(dollar_words)\n\n    # If there are no words, return None\n    if not word_freq:\n        return None\n\n    # Create a bar plot\n    ax = sns.barplot(x=list(word_freq.keys()), y=list(word_freq.values()))\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of words beginning with \"$\"')\n\n    return ax"}
{"task_id": "BigCodeBench/32", "solution": "import requests\nfrom bs4 import BeautifulSoup\ndef task_func(url, tag):\n    \"\"\"\n    Scrapes a web page for the first occurrence of a specified HTML tag and returns its text content.\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    first_tag = soup.find(tag)\n    if first_tag is not None:\n        return first_tag.text\n    else:\n        return None\nurl = \"https://www.example.com\"\ntag = \"h1\"", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\n\ndef task_func(url, tag):\n    \"\"\"\n    Scrapes a web page for the first occurrence of a specified HTML tag and returns its text content.\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    first_tag = soup.find(tag)\n    if first_tag is not None:\n        return first_tag.text\n    else:\n        return None\n\n# Test the function\nurl = \"https://www.example.com\"\ntag = \"h1\""}
{"task_id": "BigCodeBench/33", "solution": "import numpy as np\nfrom functools import reduce\nfrom operator import mul\ndef task_func(list_of_pairs):\n    \"\"\"\n    Calculates the product of the second values in each tuple in a list of tuples and returns the product as a single-element numeric array.\n    \"\"\"\n    # Extract the second elements of each tuple\n    second_elements = [pair[1] for pair in list_of_pairs]\n\n    # Calculate the product of the second elements\n    product = reduce(mul, second_elements)\n\n    # Return the product as a numpy array\n    return np.array([product])\nlist_of_pairs = [(1, 2), (3, 4), (5, 6)]", "raw_solution": "import numpy as np\nfrom functools import reduce\nfrom operator import mul\n\ndef task_func(list_of_pairs):\n    \"\"\"\n    Calculates the product of the second values in each tuple in a list of tuples and returns the product as a single-element numeric array.\n    \"\"\"\n    # Extract the second elements of each tuple\n    second_elements = [pair[1] for pair in list_of_pairs]\n\n    # Calculate the product of the second elements\n    product = reduce(mul, second_elements)\n\n    # Return the product as a numpy array\n    return np.array([product])\n\n# Test the function\nlist_of_pairs = [(1, 2), (3, 4), (5, 6)]"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    \"\"\"\n    Generates a word cloud from the given text after removing URLs.\n    \"\"\"\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Split the text into words\n    words = text.split()\n\n    # If there are no words available, raise an exception\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n\n    # Join the words back into a string\n    text = ' '.join(words)\n\n    # Generate a word cloud\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = None, \n                min_font_size = 10).generate(text)\n\n    # Plot the word cloud\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 0)\n\n    plt.show()\n\n    return wordcloud", "raw_solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    \"\"\"\n    Generates a word cloud from the given text after removing URLs.\n    \"\"\"\n    # Remove URLs from the text\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Split the text into words\n    words = text.split()\n\n    # If there are no words available, raise an exception\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n\n    # Join the words back into a string\n    text = ' '.join(words)\n\n    # Generate a word cloud\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = None, \n                min_font_size = 10).generate(text)\n\n    # Plot the word cloud\n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 0)\n\n    plt.show()\n\n    return wordcloud"}
{"task_id": "BigCodeBench/35", "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_values=[1, 3, 4]):\n    \"\"\"\n    Replaces all elements in DataFrame columns that do not exist in the target_values array with zeros,\n    and then outputs the distribution of each column after replacing.\n    \"\"\"\n    # Replace non-target values with zeros\n    df = df.apply(lambda x: x.apply(lambda y: y if y in target_values else 0))\n\n    # Plot distribution of each column\n    for col in df.columns:\n        plt.figure(figsize=(10, 6))\n        sns.histplot(df[col], kde=True, color='blue')\n        plt.title(f'Distribution of {col}')\n        plt.xlabel(col)\n        plt.ylabel('Frequency')\n        plt.show()", "raw_solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, target_values=[1, 3, 4]):\n    \"\"\"\n    Replaces all elements in DataFrame columns that do not exist in the target_values array with zeros,\n    and then outputs the distribution of each column after replacing.\n    \"\"\"\n    # Replace non-target values with zeros\n    df = df.apply(lambda x: x.apply(lambda y: y if y in target_values else 0))\n\n    # Plot distribution of each column\n    for col in df.columns:\n        plt.figure(figsize=(10, 6))\n        sns.histplot(df[col], kde=True, color='blue')\n        plt.title(f'Distribution of {col}')\n        plt.xlabel(col)\n        plt.ylabel('Frequency')\n        plt.show()"}
{"task_id": "BigCodeBench/36", "solution": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nTARGET_VALUES = np.array([1, 3, 4])\ndef task_func(df):\n    # Replace all elements in DataFrame columns that do not exist in the TARGET_VALUES array with zeros\n    df = df.replace(set(df.values.flatten()) - set(TARGET_VALUES), 0)\n\n    # Perform a Box-Cox transformation on each column (if data is not constant, add 1 to account for zeros)\n    for col in df.columns:\n        df[col] = stats.boxcox(df[col] + 1)[0]\n\n    # Display the resulting KDE plots\n    fig, ax = plt.subplots(figsize=(10, 10))\n    df.plot(kind='kde', ax=ax)\n    plt.show()\n\n    return df, fig", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nTARGET_VALUES = np.array([1, 3, 4])\n\ndef task_func(df):\n    # Replace all elements in DataFrame columns that do not exist in the TARGET_VALUES array with zeros\n    df = df.replace(set(df.values.flatten()) - set(TARGET_VALUES), 0)\n\n    # Perform a Box-Cox transformation on each column (if data is not constant, add 1 to account for zeros)\n    for col in df.columns:\n        df[col] = stats.boxcox(df[col] + 1)[0]\n\n    # Display the resulting KDE plots\n    fig, ax = plt.subplots(figsize=(10, 10))\n    df.plot(kind='kde', ax=ax)\n    plt.show()\n\n    return df, fig"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_column):\n    \"\"\"\n    Trains a random forest classifier on the input dataframe and plots the feature importance.\n    \"\"\"\n    # Split the dataframe into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train the random forest classifier\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X, y)\n\n    # Get the feature importances\n    importances = clf.feature_importances_\n    feature_names = X.columns\n\n    # Create a dataframe for the feature importances\n    df_importances = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances\n    })\n\n    # Sort the dataframe by importance\n    df_importances = df_importances.sort_values('Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Feature', data=df_importances)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.tight_layout()\n\n    return clf, plt.gca()", "raw_solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, target_column):\n    \"\"\"\n    Trains a random forest classifier on the input dataframe and plots the feature importance.\n    \"\"\"\n    # Split the dataframe into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train the random forest classifier\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X, y)\n\n    # Get the feature importances\n    importances = clf.feature_importances_\n    feature_names = X.columns\n\n    # Create a dataframe for the feature importances\n    df_importances = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances\n    })\n\n    # Sort the dataframe by importance\n    df_importances = df_importances.sort_values('Importance', ascending=False)\n\n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Feature', data=df_importances)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.tight_layout()\n\n    return clf, plt.gca()"}
{"task_id": "BigCodeBench/38", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nFEATURE_NAMES = [\"Feature 1\", \"Feature 2\", \"Feature 3\", \"Feature 4\", \"Feature 5\"]\ndef task_func(data_matrix):\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n\n    # Calculate the mean of each row\n    means = standardized_data.mean(axis=1)\n\n    # Create a DataFrame\n    df = pd.DataFrame(standardized_data, columns=FEATURE_NAMES)\n    df['Mean'] = means\n\n    # Plot the distribution of the means\n    plt.figure(figsize=(10, 6))\n    plt.hist(means, bins=30, color='blue', edgecolor='black')\n    plt.title('Distribution of Means')\n    plt.xlabel('Mean')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return df, plt.gca()", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURE_NAMES = [\"Feature 1\", \"Feature 2\", \"Feature 3\", \"Feature 4\", \"Feature 5\"]\n\ndef task_func(data_matrix):\n    # Standardize the data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n\n    # Calculate the mean of each row\n    means = standardized_data.mean(axis=1)\n\n    # Create a DataFrame\n    df = pd.DataFrame(standardized_data, columns=FEATURE_NAMES)\n    df['Mean'] = means\n\n    # Plot the distribution of the means\n    plt.figure(figsize=(10, 6))\n    plt.hist(means, bins=30, color='blue', edgecolor='black')\n    plt.title('Distribution of Means')\n    plt.xlabel('Mean')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/39", "solution": "import numpy as np\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\nALPHA = 0.05\ndef task_func(data_matrix):\n    # Calculate the mean value of each row\n    row_means = np.mean(data_matrix, axis=1)\n\n    # Calculate the population mean\n    pop_mean = np.mean(data_matrix)\n\n    # Run a t-test from a sample against the population value\n    _, p_values = ttest_1samp(data_matrix, pop_mean)\n\n    # Record the mean values that differ significantly\n    significant_indices = [i for i, p in enumerate(p_values) if p < ALPHA]\n\n    # Create a lineplot with the mean of rows in red\n    plt.plot(row_means, color='red', label='Means')\n\n    # Create a line plot with the significant_indices on the x-axis and the corresponding means on the y-axis\n    plt.plot([i for i in range(len(row_means)) if i in significant_indices], \n             [row_means[i] for i in significant_indices], \n             color='blue', \n             label='Significant Means')\n\n    # Create an horizontal line which represent the mean computed on the whole 2D matrix\n    plt.axhline(pop_mean, color='green', label='Population Mean')\n\n    plt.legend()\n    plt.show()\n\n    return significant_indices, plt.gca()", "raw_solution": "import numpy as np\nfrom scipy.stats import ttest_1samp\nimport matplotlib.pyplot as plt\n\n# Constants\nALPHA = 0.05\n\ndef task_func(data_matrix):\n    # Calculate the mean value of each row\n    row_means = np.mean(data_matrix, axis=1)\n\n    # Calculate the population mean\n    pop_mean = np.mean(data_matrix)\n\n    # Run a t-test from a sample against the population value\n    _, p_values = ttest_1samp(data_matrix, pop_mean)\n\n    # Record the mean values that differ significantly\n    significant_indices = [i for i, p in enumerate(p_values) if p < ALPHA]\n\n    # Create a lineplot with the mean of rows in red\n    plt.plot(row_means, color='red', label='Means')\n\n    # Create a line plot with the significant_indices on the x-axis and the corresponding means on the y-axis\n    plt.plot([i for i in range(len(row_means)) if i in significant_indices], \n             [row_means[i] for i in significant_indices], \n             color='blue', \n             label='Significant Means')\n\n    # Create an horizontal line which represent the mean computed on the whole 2D matrix\n    plt.axhline(pop_mean, color='green', label='Population Mean')\n\n    plt.legend()\n    plt.show()\n\n    return significant_indices, plt.gca()"}
{"task_id": "BigCodeBench/40", "solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(data_matrix):\n    # Calculate the Z-values of a 2D data matrix\n    z_scores = pd.DataFrame(zscore(data_matrix, axis=1), columns=data_matrix.columns)\n    \n    # Calculate the mean value of each row\n    z_scores['Mean'] = z_scores.mean(axis=1)\n    \n    # Visualize the correlation matrix of the Z-values with a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(z_scores.corr(), annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix of Z-Scores')\n    plt.show()\n    \n    return z_scores, plt.gca()", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n\ndef task_func(data_matrix):\n    # Calculate the Z-values of a 2D data matrix\n    z_scores = pd.DataFrame(zscore(data_matrix, axis=1), columns=data_matrix.columns)\n    \n    # Calculate the mean value of each row\n    z_scores['Mean'] = z_scores.mean(axis=1)\n    \n    # Visualize the correlation matrix of the Z-values with a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(z_scores.corr(), annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix of Z-Scores')\n    plt.show()\n    \n    return z_scores, plt.gca()"}
{"task_id": "BigCodeBench/41", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\ndef task_func(data_matrix):\n    \"\"\"\n    Calculates the skewness of each row in a 2D data matrix and plots the distribution.\n    \"\"\"\n    # Convert the data matrix to a pandas DataFrame\n    df = pd.DataFrame(data_matrix)\n\n    # Calculate the skewness of each row and store it in a new column\n    df['Skewness'] = df.apply(lambda row: skew(row), axis=1)\n\n    # Plot the distribution of the skewness\n    fig, ax = plt.subplots()\n    df['Skewness'].plot(kind='hist', ax=ax)\n    ax.set_xlabel('Skewness')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Skewness')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\n\ndef task_func(data_matrix):\n    \"\"\"\n    Calculates the skewness of each row in a 2D data matrix and plots the distribution.\n    \"\"\"\n    # Convert the data matrix to a pandas DataFrame\n    df = pd.DataFrame(data_matrix)\n\n    # Calculate the skewness of each row and store it in a new column\n    df['Skewness'] = df.apply(lambda row: skew(row), axis=1)\n\n    # Plot the distribution of the skewness\n    fig, ax = plt.subplots()\n    df['Skewness'].plot(kind='hist', ax=ax)\n    ax.set_xlabel('Skewness')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Skewness')\n\n    return df, ax"}
{"task_id": "BigCodeBench/42", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\ndef task_func(data_matrix, n_components=2):\n    pca = PCA(n_components=n_components)\n    pca_result = pca.fit_transform(data_matrix)\n    df = pd.DataFrame(pca_result, columns=[f'Component {i+1}' for i in range(n_components)])\n    df['Mean'] = df.mean(axis=1)\n    explained_variance_ratio = pca.explained_variance_ratio_\n    cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n\n    fig, ax = plt.subplots()\n    x = range(1, n_components + 1)\n    ax.plot(x, cumulative_explained_variance_ratio)\n    ax.set_xlabel('Number of Components')\n    ax.set_ylabel('Cumulative Explained Variance')\n    plt.grid(True)\n\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\ndef task_func(data_matrix, n_components=2):\n    pca = PCA(n_components=n_components)\n    pca_result = pca.fit_transform(data_matrix)\n    df = pd.DataFrame(pca_result, columns=[f'Component {i+1}' for i in range(n_components)])\n    df['Mean'] = df.mean(axis=1)\n    explained_variance_ratio = pca.explained_variance_ratio_\n    cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n\n    fig, ax = plt.subplots()\n    x = range(1, n_components + 1)\n    ax.plot(x, cumulative_explained_variance_ratio)\n    ax.set_xlabel('Number of Components')\n    ax.set_ylabel('Cumulative Explained Variance')\n    plt.grid(True)\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/43", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\ndef task_func(df):\n    # Describe the dataframe\n    df_desc = df.describe(include=[np.number]).transpose()\n\n    # Replace NaN values with the average of the column\n    df.fillna(df.mean(), inplace=True)\n\n    # Initialize list to store the Axes objects\n    axes = []\n\n    # Loop through each numeric column and create a distribution plot\n    for col in df.select_dtypes(include=[np.number]).columns:\n        ax = sns.distplot(df[col], bins=10, kde=False)\n        ax.set_title(f'Distribution of {col}')\n        axes.append(ax)\n\n    # Return the DataFrame and list of Axes\n    return df_desc, axes", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(df):\n    # Describe the dataframe\n    df_desc = df.describe(include=[np.number]).transpose()\n\n    # Replace NaN values with the average of the column\n    df.fillna(df.mean(), inplace=True)\n\n    # Initialize list to store the Axes objects\n    axes = []\n\n    # Loop through each numeric column and create a distribution plot\n    for col in df.select_dtypes(include=[np.number]).columns:\n        ax = sns.distplot(df[col], bins=10, kde=False)\n        ax.set_title(f'Distribution of {col}')\n        axes.append(ax)\n\n    # Return the DataFrame and list of Axes\n    return df_desc, axes"}
{"task_id": "BigCodeBench/44", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df):\n    \"\"\"\n    Normalize numeric columns in a DataFrame and draw a box plot for each column.\n    Missing values are replaced by column's average.\n    \"\"\"\n    # Normalize numeric columns\n    scaler = MinMaxScaler()\n    df[df.columns] = scaler.fit_transform(df[df.columns])\n\n    # Replace missing values with column's average\n    df.fillna(df.mean(), inplace=True)\n\n    # Create a figure and a set of subplots\n    fig, axes = plt.subplots(nrows=df.shape[1], ncols=1, figsize=(10, 10))\n\n    # Draw a box plot for each column\n    for i, column in enumerate(df.columns):\n        axes[i].boxplot(df[column])\n        axes[i].set_title(column)\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return df, axes", "raw_solution": "from sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    \"\"\"\n    Normalize numeric columns in a DataFrame and draw a box plot for each column.\n    Missing values are replaced by column's average.\n    \"\"\"\n    # Normalize numeric columns\n    scaler = MinMaxScaler()\n    df[df.columns] = scaler.fit_transform(df[df.columns])\n\n    # Replace missing values with column's average\n    df.fillna(df.mean(), inplace=True)\n\n    # Create a figure and a set of subplots\n    fig, axes = plt.subplots(nrows=df.shape[1], ncols=1, figsize=(10, 10))\n\n    # Draw a box plot for each column\n    for i, column in enumerate(df.columns):\n        axes[i].boxplot(df[column])\n        axes[i].set_title(column)\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return df, axes"}
{"task_id": "BigCodeBench/45", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[pd.DataFrame, plt.Axes]:\n    \"\"\"\n    Perform PCA on a DataFrame (excluding non-numeric columns) and draw a scatter plot of the first two main components.\n    The principal columns should be named 'Component 1' and 'Component 2'. Missing values are replaced by column's average.\n    \"\"\"\n    # Replace missing values with column's average\n    df.fillna(df.mean(), inplace=True)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df)\n\n    # Create a DataFrame for the principal components\n    principal_df = pd.DataFrame(data=principal_components, columns=['Component 1', 'Component 2'])\n\n    # Create a scatter plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.scatterplot(x='Component 1', y='Component 2', data=principal_df, ax=ax)\n    ax.set_xlabel('Component 1')\n    ax.set_ylabel('Component 2')\n\n    return principal_df, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[pd.DataFrame, plt.Axes]:\n    \"\"\"\n    Perform PCA on a DataFrame (excluding non-numeric columns) and draw a scatter plot of the first two main components.\n    The principal columns should be named 'Component 1' and 'Component 2'. Missing values are replaced by column's average.\n    \"\"\"\n    # Replace missing values with column's average\n    df.fillna(df.mean(), inplace=True)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df)\n\n    # Create a DataFrame for the principal components\n    principal_df = pd.DataFrame(data=principal_components, columns=['Component 1', 'Component 2'])\n\n    # Create a scatter plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sns.scatterplot(x='Component 1', y='Component 2', data=principal_df, ax=ax)\n    ax.set_xlabel('Component 1')\n    ax.set_ylabel('Component 2')\n\n    return principal_df, ax"}
{"task_id": "BigCodeBench/46", "solution": "from scipy.stats import zscore\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df):\n    \"\"\"\n    Calculate Z-scores for numeric columns in a DataFrame and draw a histogram for each column.\n    Missing values are replaced by the column's average.\n    The histograms are plotted with 10 bins.\n    \"\"\"\n    # Calculate z-scores\n    df_z_scores = df.apply(zscore, nan_policy='omit')\n\n    # Replace missing values with the column's average\n    df_filled = df.fillna(df.mean())\n\n    # Create a list to store the histograms\n    histograms = []\n\n    # Plot histograms for each numeric column\n    for col in df_filled.select_dtypes(include=[np.number]):\n        fig, ax = plt.subplots()\n        ax.hist(df_filled[col], bins=10)\n        ax.set_title(f'Histogram of {col}')\n        ax.set_xlabel(col)\n        ax.set_ylabel('Frequency')\n        histograms.append(ax)\n\n    return df_z_scores, histograms", "raw_solution": "from scipy.stats import zscore\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    \"\"\"\n    Calculate Z-scores for numeric columns in a DataFrame and draw a histogram for each column.\n    Missing values are replaced by the column's average.\n    The histograms are plotted with 10 bins.\n    \"\"\"\n    # Calculate z-scores\n    df_z_scores = df.apply(zscore, nan_policy='omit')\n\n    # Replace missing values with the column's average\n    df_filled = df.fillna(df.mean())\n\n    # Create a list to store the histograms\n    histograms = []\n\n    # Plot histograms for each numeric column\n    for col in df_filled.select_dtypes(include=[np.number]):\n        fig, ax = plt.subplots()\n        ax.hist(df_filled[col], bins=10)\n        ax.set_title(f'Histogram of {col}')\n        ax.set_xlabel(col)\n        ax.set_ylabel('Frequency')\n        histograms.append(ax)\n\n    return df_z_scores, histograms"}
{"task_id": "BigCodeBench/47", "solution": "from sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df):\n    \"\"\"\n    Standardize numeric columns in a DataFrame and return the heatmap of the correlation matrix.\n    Missing values are replaced by the column's average.\n    \"\"\"\n    # Fill missing values with the column's average\n    df.fillna(df.mean(), inplace=True)\n\n    # Standardize numeric columns\n    scaler = StandardScaler()\n    df[df.select_dtypes(include=[np.number]).columns] = scaler.fit_transform(df[df.select_dtypes(include=[np.number]).columns])\n\n    # Compute the correlation matrix\n    corr = df.corr()\n\n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix')\n    plt.show()\n\n    return df, plt.gca()", "raw_solution": "from sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    \"\"\"\n    Standardize numeric columns in a DataFrame and return the heatmap of the correlation matrix.\n    Missing values are replaced by the column's average.\n    \"\"\"\n    # Fill missing values with the column's average\n    df.fillna(df.mean(), inplace=True)\n\n    # Standardize numeric columns\n    scaler = StandardScaler()\n    df[df.select_dtypes(include=[np.number]).columns] = scaler.fit_transform(df[df.select_dtypes(include=[np.number]).columns])\n\n    # Compute the correlation matrix\n    corr = df.corr()\n\n    # Plot the heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix')\n    plt.show()\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/48", "solution": "import time\nfrom datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\ndef task_func(n, output_path=None):\n    \"\"\"\n    Generate n random Unix timestamps and convert them to strings formatted as UTC DATE_FORMAT.\n    Plot a histogram of the distribution of the generated timestamps.\n    If an output path is provided, save the histogram to the specified path.\n    Otherwise, display the plot.\n    \"\"\"\n    timestamps = [random.randint(0, int(time.time())) for _ in range(n)]\n    formatted_timestamps = [datetime.utcfromtimestamp(ts).strftime(DATE_FORMAT) for ts in timestamps]\n\n    plt.hist([datetime.strptime(ts, DATE_FORMAT) for ts in formatted_timestamps], bins=10)\n    plt.xlabel('Date')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Unix Timestamps')\n\n    if output_path is None:\n        plt.show()\n    else:\n        plt.savefig(output_path)\n\n    return formatted_timestamps", "raw_solution": "import time\nfrom datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef task_func(n, output_path=None):\n    \"\"\"\n    Generate n random Unix timestamps and convert them to strings formatted as UTC DATE_FORMAT.\n    Plot a histogram of the distribution of the generated timestamps.\n    If an output path is provided, save the histogram to the specified path.\n    Otherwise, display the plot.\n    \"\"\"\n    timestamps = [random.randint(0, int(time.time())) for _ in range(n)]\n    formatted_timestamps = [datetime.utcfromtimestamp(ts).strftime(DATE_FORMAT) for ts in timestamps]\n\n    plt.hist([datetime.strptime(ts, DATE_FORMAT) for ts in formatted_timestamps], bins=10)\n    plt.xlabel('Date')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Unix Timestamps')\n\n    if output_path is None:\n        plt.show()\n    else:\n        plt.savefig(output_path)\n\n    return formatted_timestamps"}
{"task_id": "BigCodeBench/49", "solution": "from datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\ndef task_func(timestamps):\n    \"\"\"\n    Converts a list of Unix timestamps to date objects, creates a Pandas DataFrame, and draws a histogram.\n    \"\"\"\n    if not timestamps:\n        raise ValueError(\"Input list of timestamps is empty.\")\n\n    # Convert Unix timestamps to datetime objects\n    datetimes = [datetime.fromtimestamp(ts) for ts in timestamps]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Timestamp': timestamps,\n        'Datetime': datetimes\n    })\n\n    # Convert 'Datetime' column to string format\n    df['Datetime'] = df['Datetime'].dt.strftime(DATE_FORMAT)\n\n    # Draw a histogram\n    ax = df['Datetime'].hist(bins=10)\n\n    return df, ax", "raw_solution": "from datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef task_func(timestamps):\n    \"\"\"\n    Converts a list of Unix timestamps to date objects, creates a Pandas DataFrame, and draws a histogram.\n    \"\"\"\n    if not timestamps:\n        raise ValueError(\"Input list of timestamps is empty.\")\n\n    # Convert Unix timestamps to datetime objects\n    datetimes = [datetime.fromtimestamp(ts) for ts in timestamps]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Timestamp': timestamps,\n        'Datetime': datetimes\n    })\n\n    # Convert 'Datetime' column to string format\n    df['Datetime'] = df['Datetime'].dt.strftime(DATE_FORMAT)\n\n    # Draw a histogram\n    ax = df['Datetime'].hist(bins=10)\n\n    return df, ax"}
{"task_id": "BigCodeBench/50", "solution": "from datetime import datetime\nimport pandas as pd\nimport pytz\nimport matplotlib.pyplot as plt\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\nTIMEZONES = [\n    \"America/New_York\",\n    \"Europe/London\",\n    \"Asia/Shanghai\",\n    \"Asia/Tokyo\",\n    \"Australia/Sydney\",\n]\ndef task_func(timestamp):\n    \"\"\"\n    Converts a Unix timestamp to date objects in different time zones,\n    creates a Pandas DataFrame, and draws a bar chart.\n    \"\"\"\n    # Create a list to store the datetime objects for each timezone\n    datetime_list = []\n\n    # Convert the Unix timestamp to datetime objects in different timezones\n    for tz in TIMEZONES:\n        dt = datetime.fromtimestamp(timestamp)\n        dt = dt.astimezone(pytz.timezone(tz))\n        datetime_list.append(dt.strftime(DATE_FORMAT))\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(datetime_list, columns=['Datetime'])\n    df.index = TIMEZONES\n    df.rename_axis('Timezone', axis='index', inplace=True)\n\n    # Draw a bar chart\n    ax = df.plot(kind='bar', legend=False)\n    ax.set_xlabel('Timezone')\n    ax.set_ylabel('Datetime')\n    ax.set_title('Datetime = f(Timezone)')\n\n    return df, ax", "raw_solution": "from datetime import datetime\nimport pandas as pd\nimport pytz\nimport matplotlib.pyplot as plt\n\n# Constants\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\nTIMEZONES = [\n    \"America/New_York\",\n    \"Europe/London\",\n    \"Asia/Shanghai\",\n    \"Asia/Tokyo\",\n    \"Australia/Sydney\",\n]\n\ndef task_func(timestamp):\n    \"\"\"\n    Converts a Unix timestamp to date objects in different time zones,\n    creates a Pandas DataFrame, and draws a bar chart.\n    \"\"\"\n    # Create a list to store the datetime objects for each timezone\n    datetime_list = []\n\n    # Convert the Unix timestamp to datetime objects in different timezones\n    for tz in TIMEZONES:\n        dt = datetime.fromtimestamp(timestamp)\n        dt = dt.astimezone(pytz.timezone(tz))\n        datetime_list.append(dt.strftime(DATE_FORMAT))\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(datetime_list, columns=['Datetime'])\n    df.index = TIMEZONES\n    df.rename_axis('Timezone', axis='index', inplace=True)\n\n    # Draw a bar chart\n    ax = df.plot(kind='bar', legend=False)\n    ax.set_xlabel('Timezone')\n    ax.set_ylabel('Datetime')\n    ax.set_title('Datetime = f(Timezone)')\n\n    return df, ax"}
{"task_id": "BigCodeBench/51", "solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df, age: int, height: int):\n    # Filter the dataframe based on the specified 'Age' and 'Height' conditions\n    df_filtered = df[(df['Age'] >= age) & (df['Height'] >= height)]\n\n    # If the filtered dataframe has less than 3 columns, add a column 'Cluster' with 0 for each row\n    if len(df_filtered.columns) < 3:\n        df_filtered['Cluster'] = 0\n    else:\n        # Otherwise, do a KMeans clustering (by Age and Height) with 3 clusters and add a column 'Cluster' to the dataframe\n        kmeans = KMeans(n_clusters=3)\n        df_filtered['Cluster'] = kmeans.fit_predict(df_filtered[['Age', 'Height']])\n\n        # Plot a scatter plot of the 'Age' and 'Height' and colored by the cluster indices\n        plt.scatter(df_filtered['Age'], df_filtered['Height'], c=df_filtered['Cluster'])\n        plt.xlabel('Age')\n        plt.ylabel('Height')\n        plt.title('KMeans Clustering based on Age and Height')\n\n    return df_filtered, plt.gca() if 'Cluster' in df_filtered.columns else None", "raw_solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df, age: int, height: int):\n    # Filter the dataframe based on the specified 'Age' and 'Height' conditions\n    df_filtered = df[(df['Age'] >= age) & (df['Height'] >= height)]\n\n    # If the filtered dataframe has less than 3 columns, add a column 'Cluster' with 0 for each row\n    if len(df_filtered.columns) < 3:\n        df_filtered['Cluster'] = 0\n    else:\n        # Otherwise, do a KMeans clustering (by Age and Height) with 3 clusters and add a column 'Cluster' to the dataframe\n        kmeans = KMeans(n_clusters=3)\n        df_filtered['Cluster'] = kmeans.fit_predict(df_filtered[['Age', 'Height']])\n\n        # Plot a scatter plot of the 'Age' and 'Height' and colored by the cluster indices\n        plt.scatter(df_filtered['Age'], df_filtered['Height'], c=df_filtered['Cluster'])\n        plt.xlabel('Age')\n        plt.ylabel('Height')\n        plt.title('KMeans Clustering based on Age and Height')\n\n    return df_filtered, plt.gca() if 'Cluster' in df_filtered.columns else None"}
{"task_id": "BigCodeBench/52", "solution": "import pandas as pd\nfrom collections import Counter\nSTOPWORDS = [\"a\", \"an\", \"the\", \"in\", \"is\", \"are\"]\ndef task_func(text):\n    \"\"\"\n    Count the frequency of each word in a text after removing specific stopwords.\n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    text = ''.join(c for c in text if c.isalnum() or c.isspace()).lower()\n\n    # Split the text into words\n    words = text.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Convert the Counter to a pandas Series\n    series = pd.Series(word_counts)\n\n    return series\ntext = \"The sun sets in the east. The sun rises in the west.\"", "raw_solution": "import pandas as pd\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = [\"a\", \"an\", \"the\", \"in\", \"is\", \"are\"]\n\ndef task_func(text):\n    \"\"\"\n    Count the frequency of each word in a text after removing specific stopwords.\n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    text = ''.join(c for c in text if c.isalnum() or c.isspace()).lower()\n\n    # Split the text into words\n    words = text.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Convert the Counter to a pandas Series\n    series = pd.Series(word_counts)\n\n    return series\n\n# Test the function\ntext = \"The sun sets in the east. The sun rises in the west.\""}
{"task_id": "BigCodeBench/53", "solution": "import pandas as pd\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMN_NAMES = [\"Name\", \"Email\", \"Age\", \"Country\"]\ndef task_func(text):\n    \"\"\"\n    Extracts data from the given text using a regular expression pattern and creates a pandas DataFrame.\n    Plots the age distribution using seaborn.\n    \"\"\"\n    # Extract data using regular expression\n    data = re.findall(r\"Name: (.*?), Email: (.*?), Age: (.*?), Country: (.*?)($|\\n)\", text)\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Convert Age to integer\n    df['Age'] = df['Age'].astype(int)\n\n    # Plot age distribution\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df, x='Age', bins=30, kde=False)\n    plt.title('Age Distribution')\n    plt.show()\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nCOLUMN_NAMES = [\"Name\", \"Email\", \"Age\", \"Country\"]\n\ndef task_func(text):\n    \"\"\"\n    Extracts data from the given text using a regular expression pattern and creates a pandas DataFrame.\n    Plots the age distribution using seaborn.\n    \"\"\"\n    # Extract data using regular expression\n    data = re.findall(r\"Name: (.*?), Email: (.*?), Age: (.*?), Country: (.*?)($|\\n)\", text)\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Convert Age to integer\n    df['Age'] = df['Age'].astype(int)\n\n    # Plot age distribution\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df, x='Age', bins=30, kde=False)\n    plt.title('Age Distribution')\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/54", "solution": "import pandas as pd\nimport regex as re\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef task_func(text):\n    # Split the text into sentences\n    sentences = [sentence.strip() for sentence in re.split(r'[.!?]', text) if sentence.strip()]\n\n    # Create a CountVectorizer object\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    dtm = vectorizer.fit_transform(sentences)\n\n    # Create a DataFrame from the document-term matrix\n    df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df", "raw_solution": "import pandas as pd\nimport regex as re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef task_func(text):\n    # Split the text into sentences\n    sentences = [sentence.strip() for sentence in re.split(r'[.!?]', text) if sentence.strip()]\n\n    # Create a CountVectorizer object\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    dtm = vectorizer.fit_transform(sentences)\n\n    # Create a DataFrame from the document-term matrix\n    df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df"}
{"task_id": "BigCodeBench/55", "solution": "import re\nimport pandas as pd\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(text):\n    sentences = re.split('(?<=[.?!]) +', text)\n    sentence_word_counts = {}\n    for i, sentence in enumerate(sentences, 1):\n        words = re.findall(r'\\b\\w+\\b', sentence)\n        words = [word for word in words if word not in STOPWORDS]\n        if words:\n            sentence_word_counts[f\"Sentence {i}\"] = len(words)\n    return pd.Series(sentence_word_counts)\ntext = \"This is the first sentence. This is the second sentence. This sentence is ignored.\"", "raw_solution": "import re\nimport pandas as pd\nfrom nltk.corpus import stopwords\n\nSTOPWORDS = set(stopwords.words('english'))\nSTOPWORDS.update([\"Those\", \"are\", \"the\", \"words\", \"to\", \"ignore\"])\n\ndef task_func(text):\n    sentences = re.split('(?<=[.?!]) +', text)\n    sentence_word_counts = {}\n    for i, sentence in enumerate(sentences, 1):\n        words = re.findall(r'\\b\\w+\\b', sentence)\n        words = [word for word in words if word not in STOPWORDS]\n        if words:\n            sentence_word_counts[f\"Sentence {i}\"] = len(words)\n    return pd.Series(sentence_word_counts)\n\n# Test the function\ntext = \"This is the first sentence. This is the second sentence. This sentence is ignored.\""}
{"task_id": "BigCodeBench/56", "solution": "import pandas as pd\nimport re\ndef task_func(text):\n    \"\"\"\n    Extracts data from a text and creates a Pandas DataFrame.\n    The text contains several lines, each formatted as 'Score: 85, Category: Math'.\n    The scores are converted to integers.\n    \"\"\"\n    # Define the pattern for the data extraction\n    pattern = r'Score: (\\d+), Category: (\\w+)'\n\n    # Find all matches in the text\n    matches = re.findall(pattern, text)\n\n    # Create a DataFrame from the matches\n    df = pd.DataFrame(matches, columns=['Score', 'Category'])\n\n    # Convert the 'Score' column to integers\n    df['Score'] = df['Score'].astype(int)\n\n    return df", "raw_solution": "import pandas as pd\nimport re\n\ndef task_func(text):\n    \"\"\"\n    Extracts data from a text and creates a Pandas DataFrame.\n    The text contains several lines, each formatted as 'Score: 85, Category: Math'.\n    The scores are converted to integers.\n    \"\"\"\n    # Define the pattern for the data extraction\n    pattern = r'Score: (\\d+), Category: (\\w+)'\n\n    # Find all matches in the text\n    matches = re.findall(pattern, text)\n\n    # Create a DataFrame from the matches\n    df = pd.DataFrame(matches, columns=['Score', 'Category'])\n\n    # Convert the 'Score' column to integers\n    df['Score'] = df['Score'].astype(int)\n\n    return df"}
{"task_id": "BigCodeBench/57", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(csv_file_path: str, title: str):\n    # Read the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Calculate the correlation matrix\n    corr = df.corr().round(2)\n\n    # Create a mask for the upper triangle of the correlation matrix\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n    plt.title(title)\n\n    return corr, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(csv_file_path: str, title: str):\n    # Read the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Calculate the correlation matrix\n    corr = df.corr().round(2)\n\n    # Create a mask for the upper triangle of the correlation matrix\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n    plt.title(title)\n\n    return corr, ax"}
{"task_id": "BigCodeBench/58", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mu, sigma, num_samples):\n    \"\"\"\n    Generates a normal distribution with given mean (mu) and standard deviation (sigma),\n    and plots a histogram of randomly generated samples from this distribution.\n    \"\"\"\n    # Generate a normal distribution\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Normal Distribution')\n\n    # Generate samples from the distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Plot the histogram\n    plt.hist(samples, bins=30, density=True, alpha=0.5, label='Histogram')\n\n    # Set plot title and labels\n    plt.title('Normal Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.legend()\n\n    # Save the figure\n    fig = plt.gcf()\n\n    return fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(mu, sigma, num_samples):\n    \"\"\"\n    Generates a normal distribution with given mean (mu) and standard deviation (sigma),\n    and plots a histogram of randomly generated samples from this distribution.\n    \"\"\"\n    # Generate a normal distribution\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Normal Distribution')\n\n    # Generate samples from the distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Plot the histogram\n    plt.hist(samples, bins=30, density=True, alpha=0.5, label='Histogram')\n\n    # Set plot title and labels\n    plt.title('Normal Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.legend()\n\n    # Save the figure\n    fig = plt.gcf()\n\n    return fig"}
{"task_id": "BigCodeBench/59", "solution": "import wikipedia\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(page_title):\n    try:\n        # Get the page content\n        page_content = wikipedia.page(page_title).content\n\n        # Create a word cloud\n        wordcloud = WordCloud().generate(page_content)\n\n        # Display the word cloud\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis('off')\n        plt.show()\n\n        return plt.gca()\n    except wikipedia.exceptions.PageError:\n        # If there is no wikipedia page with the title given as input\n        return None", "raw_solution": "import wikipedia\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(page_title):\n    try:\n        # Get the page content\n        page_content = wikipedia.page(page_title).content\n\n        # Create a word cloud\n        wordcloud = WordCloud().generate(page_content)\n\n        # Display the word cloud\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis('off')\n        plt.show()\n\n        return plt.gca()\n    except wikipedia.exceptions.PageError:\n        # If there is no wikipedia page with the title given as input\n        return None"}
{"task_id": "BigCodeBench/60", "solution": "import json\nimport pandas as pd\ndef task_func(result, csv_file_path=\"test.csv\", json_file_path=\"test.json\"):\n    \"\"\"\n    Saves the list of dictionaries provided in the 'result' parameter to a CSV file (without index) and a JSON file.\n    \"\"\"\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(result)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file_path, index=False)\n\n    # Save the DataFrame to a JSON file\n    with open(json_file_path, 'w') as f:\n        json.dump(result, f)\n\n    return None", "raw_solution": "import json\nimport pandas as pd\n\ndef task_func(result, csv_file_path=\"test.csv\", json_file_path=\"test.json\"):\n    \"\"\"\n    Saves the list of dictionaries provided in the 'result' parameter to a CSV file (without index) and a JSON file.\n    \"\"\"\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(result)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file_path, index=False)\n\n    # Save the DataFrame to a JSON file\n    with open(json_file_path, 'w') as f:\n        json.dump(result, f)\n\n    return None"}
{"task_id": "BigCodeBench/61", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nPLOT_TITLE = 'Square root plot'\nX_LABEL = 'x'\nY_LABEL = 'sqrt(x)'\nTIME_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(input_list, plot_title=PLOT_TITLE, x_label=X_LABEL, y_label=Y_LABEL):\n    \"\"\"\n    Plots the square root function for values associated with the key 'from_user' from the input list of dictionaries.\n    Annotates the graph with the current date and time.\n    \"\"\"\n    # Extract 'from_user' values from the input list of dictionaries\n    x_values = [d['from_user'] for d in input_list]\n\n    # Calculate square root values\n    y_values = np.sqrt(x_values)\n\n    # Create the plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(x_values, y_values, label='sqrt(x)')\n    plt.title(plot_title)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n\n    # Annotate the graph with the current date and time\n    current_time = datetime.now().strftime(TIME_FORMAT)\n    plt.annotate(f'Current time: {current_time}', xy=(1, 1), xycoords='axes fraction',\n                 xytext=(5, -5), textcoords='offset points', ha='right', va='top')\n\n    plt.legend()\n    plt.grid(True)\n\n    return y_values, plt.gca()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Constants\nPLOT_TITLE = 'Square root plot'\nX_LABEL = 'x'\nY_LABEL = 'sqrt(x)'\nTIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(input_list, plot_title=PLOT_TITLE, x_label=X_LABEL, y_label=Y_LABEL):\n    \"\"\"\n    Plots the square root function for values associated with the key 'from_user' from the input list of dictionaries.\n    Annotates the graph with the current date and time.\n    \"\"\"\n    # Extract 'from_user' values from the input list of dictionaries\n    x_values = [d['from_user'] for d in input_list]\n\n    # Calculate square root values\n    y_values = np.sqrt(x_values)\n\n    # Create the plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(x_values, y_values, label='sqrt(x)')\n    plt.title(plot_title)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n\n    # Annotate the graph with the current date and time\n    current_time = datetime.now().strftime(TIME_FORMAT)\n    plt.annotate(f'Current time: {current_time}', xy=(1, 1), xycoords='axes fraction',\n                 xytext=(5, -5), textcoords='offset points', ha='right', va='top')\n\n    plt.legend()\n    plt.grid(True)\n\n    return y_values, plt.gca()"}
{"task_id": "BigCodeBench/62", "solution": "import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(result, colors=['b', 'g', 'r', 'c', 'm', 'y', 'k']):\n    \"\"\"\n    Draws a histogram of the \"from_user\" values in the provided result. The color of the histogram bars is selected at random from the provided colors list.\n    \"\"\"\n    # Extract the \"from_user\" values\n    from_user_values = [row['from_user'] for row in result]\n\n    # Select a random color for the histogram bars\n    color = random.choice(colors)\n\n    # Create the histogram\n    plt.hist(from_user_values, color=color)\n    plt.xlabel('from_user')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of from_user values')\n    plt.show()\n\n    return None", "raw_solution": "import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(result, colors=['b', 'g', 'r', 'c', 'm', 'y', 'k']):\n    \"\"\"\n    Draws a histogram of the \"from_user\" values in the provided result. The color of the histogram bars is selected at random from the provided colors list.\n    \"\"\"\n    # Extract the \"from_user\" values\n    from_user_values = [row['from_user'] for row in result]\n\n    # Select a random color for the histogram bars\n    color = random.choice(colors)\n\n    # Create the histogram\n    plt.hist(from_user_values, color=color)\n    plt.xlabel('from_user')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of from_user values')\n    plt.show()\n\n    return None"}
{"task_id": "BigCodeBench/63", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(car_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(car_dict.items()), columns=['Car', 'Color'])\n\n    # Count the occurrence of each color\n    color_counts = df['Color'].value_counts()\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    color_counts.plot(kind='bar', ax=ax)\n    ax.set_title('Distribution of Vehicle Colors')\n    ax.set_xlabel('Color')\n    ax.set_ylabel('Count')\n\n    return df, ax.figure\ncar_dict = {'Tesla Model S': 'Red', 'Ford Mustang': 'Blue', 'Chevrolet Camaro': 'Red', 'Dodge Charger': 'Black', 'BMW M3': 'Blue'}", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(car_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(list(car_dict.items()), columns=['Car', 'Color'])\n\n    # Count the occurrence of each color\n    color_counts = df['Color'].value_counts()\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    color_counts.plot(kind='bar', ax=ax)\n    ax.set_title('Distribution of Vehicle Colors')\n    ax.set_xlabel('Color')\n    ax.set_ylabel('Count')\n\n    return df, ax.figure\n\n# Test the function\ncar_dict = {'Tesla Model S': 'Red', 'Ford Mustang': 'Blue', 'Chevrolet Camaro': 'Red', 'Dodge Charger': 'Black', 'BMW M3': 'Blue'}\ndf, fig = task_func(car_dict)"}
{"task_id": "BigCodeBench/64", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    \"\"\"\n    Given a list of elements, each element is a list with the same length as COLUMNS,\n    representing one row of a DataFrame df.\n    Visualize the distribution of different values in a column \"col3\" of a pandas DataFrame df,\n    grouped by \"col1\" and \"col2,\" using a heatmap.\n    \"\"\"\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Group by 'col1' and 'col2' and calculate the mean of 'col3'\n    grouped_df = df.groupby(['col1', 'col2']).mean().reset_index()\n\n    # Pivot the DataFrame to get the values for the heatmap\n    pivot_df = grouped_df.pivot('col1', 'col2', 'col3')\n\n    # Create the heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(pivot_df, cmap='viridis', annot=True, fmt=\".2f\")\n    plt.title('Heatmap of col3 grouped by col1 and col2')\n    plt.xlabel('col2')\n    plt.ylabel('col1')\n\n    return df, plt.gca()", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    \"\"\"\n    Given a list of elements, each element is a list with the same length as COLUMNS,\n    representing one row of a DataFrame df.\n    Visualize the distribution of different values in a column \"col3\" of a pandas DataFrame df,\n    grouped by \"col1\" and \"col2,\" using a heatmap.\n    \"\"\"\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Group by 'col1' and 'col2' and calculate the mean of 'col3'\n    grouped_df = df.groupby(['col1', 'col2']).mean().reset_index()\n\n    # Pivot the DataFrame to get the values for the heatmap\n    pivot_df = grouped_df.pivot('col1', 'col2', 'col3')\n\n    # Create the heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(pivot_df, cmap='viridis', annot=True, fmt=\".2f\")\n    plt.title('Heatmap of col3 grouped by col1 and col2')\n    plt.xlabel('col2')\n    plt.ylabel('col1')\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/65", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    df = pd.DataFrame(data, columns=COLUMNS)\n    df.set_index(COLUMNS[:-1], inplace=True)\n    df = df.groupby(level=list(range(len(COLUMNS)-1)))[COLUMNS[-1]].unique()\n\n    fig, ax = plt.subplots()\n    df.plot(kind='line', ax=ax)\n    ax.set_xlabel('-'.join(COLUMNS[:-1]))\n    ax.set_ylabel(COLUMNS[-1])\n    plt.tight_layout()\n\n    return df, ax\ndata = [\n    ['a', 'b', 1],\n    ['a', 'b', 2],\n    ['a', 'c', 3],\n    ['b', 'c', 4],\n    ['b', 'c', 5],\n]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    df = pd.DataFrame(data, columns=COLUMNS)\n    df.set_index(COLUMNS[:-1], inplace=True)\n    df = df.groupby(level=list(range(len(COLUMNS)-1)))[COLUMNS[-1]].unique()\n\n    fig, ax = plt.subplots()\n    df.plot(kind='line', ax=ax)\n    ax.set_xlabel('-'.join(COLUMNS[:-1]))\n    ax.set_ylabel(COLUMNS[-1])\n    plt.tight_layout()\n\n    return df, ax\n\n# Test the function\ndata = [\n    ['a', 'b', 1],\n    ['a', 'b', 2],\n    ['a', 'c', 3],\n    ['b', 'c', 4],\n    ['b', 'c', 5],\n]\ndf, ax = task_func(data)\nplt.show()"}
{"task_id": "BigCodeBench/66", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMNS = ['col1', 'col2', 'col3']\ndef task_func(data):\n    # Create a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Group by 'col1' and 'col2' and create a new dataframe\n    grouped_df = df.groupby(['col1', 'col2']).mean().reset_index()\n\n    # Create a distribution plot\n    plt.figure(figsize=(10, 6))\n    sns.distplot(grouped_df['col3'], kde=False, bins=30)\n    plt.xlabel('col3')\n    plt.title('Distribution of col3 grouped by col1 and col2')\n    plt.show()\n\n    return grouped_df, plt.gca()", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    # Create a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Group by 'col1' and 'col2' and create a new dataframe\n    grouped_df = df.groupby(['col1', 'col2']).mean().reset_index()\n\n    # Create a distribution plot\n    plt.figure(figsize=(10, 6))\n    sns.distplot(grouped_df['col3'], kde=False, bins=30)\n    plt.xlabel('col3')\n    plt.title('Distribution of col3 grouped by col1 and col2')\n    plt.show()\n\n    return grouped_df, plt.gca()"}
{"task_id": "BigCodeBench/67", "solution": "import pandas as pd\nimport re\nimport os\ndef task_func(dir_path: str, pattern: str = '^EMP'):\n    \"\"\"\n    Looks for all ascendingly sorted files in a directory that start with a given pattern,\n    and returns the number of files against their size.\n    \"\"\"\n    # Initialize an empty list to store file names and sizes\n    file_names = []\n    file_sizes = []\n\n    # Iterate over all files in the directory\n    for file_name in os.listdir(dir_path):\n        # Check if the file name starts with the given pattern\n        if re.match(pattern, file_name):\n            # Get the file size\n            file_size = os.path.getsize(os.path.join(dir_path, file_name))\n            # Append the file name and size to the corresponding lists\n            file_names.append(file_name)\n            file_sizes.append(file_size)\n\n    # Create a pandas DataFrame from the lists\n    df = pd.DataFrame({'File': file_names, 'Size': file_sizes})\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nimport os\n\ndef task_func(dir_path: str, pattern: str = '^EMP'):\n    \"\"\"\n    Looks for all ascendingly sorted files in a directory that start with a given pattern,\n    and returns the number of files against their size.\n    \"\"\"\n    # Initialize an empty list to store file names and sizes\n    file_names = []\n    file_sizes = []\n\n    # Iterate over all files in the directory\n    for file_name in os.listdir(dir_path):\n        # Check if the file name starts with the given pattern\n        if re.match(pattern, file_name):\n            # Get the file size\n            file_size = os.path.getsize(os.path.join(dir_path, file_name))\n            # Append the file name and size to the corresponding lists\n            file_names.append(file_name)\n            file_sizes.append(file_size)\n\n    # Create a pandas DataFrame from the lists\n    df = pd.DataFrame({'File': file_names, 'Size': file_sizes})\n\n    return df"}
{"task_id": "BigCodeBench/68", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data='/path/to/data.csv', emp_prefix='EMP'):\n    # Load CSV file into a DataFrame\n    df = pd.read_csv(data)\n\n    # Filter lines where employee ID begins with a prefix\n    df = df[df['Employee ID'].str.startswith(emp_prefix)]\n\n    # Draw a histogram of its age\n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=df, x='Age', kde=True, color='skyblue')\n    plt.title(f'Histogram of Age for Employees with ID starting with {emp_prefix}')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return df, plt.gca()", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data='/path/to/data.csv', emp_prefix='EMP'):\n    # Load CSV file into a DataFrame\n    df = pd.read_csv(data)\n\n    # Filter lines where employee ID begins with a prefix\n    df = df[df['Employee ID'].str.startswith(emp_prefix)]\n\n    # Draw a histogram of its age\n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=df, x='Age', kde=True, color='skyblue')\n    plt.title(f'Histogram of Age for Employees with ID starting with {emp_prefix}')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/69", "solution": "import random\nimport matplotlib.pyplot as plt\nSALARY_RANGE = (20000, 100000)\ndef task_func(dict1):\n    \"\"\"\n    Analyze the salary distribution within the department with code 'EMPXX'. Generate random salaries for each employee and create a histogram.\n    \"\"\"\n    # Get the salaries for the department of interest\n    salaries = [random.randint(*SALARY_RANGE) for _ in range(dict1['EMPXX'])]\n\n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(salaries, bins='auto', color='#1f77b4')\n\n    # Set the title and labels\n    ax.set_title('Salary Distribution in EMPXX Department')\n    ax.set_xlabel('Salary')\n    ax.set_ylabel('Number of Employees')\n\n    return ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\n# Constants\nSALARY_RANGE = (20000, 100000)\n\ndef task_func(dict1):\n    \"\"\"\n    Analyze the salary distribution within the department with code 'EMPXX'. Generate random salaries for each employee and create a histogram.\n    \"\"\"\n    # Get the salaries for the department of interest\n    salaries = [random.randint(*SALARY_RANGE) for _ in range(dict1['EMPXX'])]\n\n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(salaries, bins='auto', color='#1f77b4')\n\n    # Set the title and labels\n    ax.set_title('Salary Distribution in EMPXX Department')\n    ax.set_xlabel('Salary')\n    ax.set_ylabel('Number of Employees')\n\n    return ax"}
{"task_id": "BigCodeBench/70", "solution": "import pandas as pd\nimport json\nimport matplotlib.pyplot as plt\nCOLUMNS = ['email', 'list', 'sum', 'mean']\ndef task_func(json_file):\n    \"\"\"\n    Loads e-mail data from a JSON file, converts it into a Pandas DataFrame, calculates the sum and mean of the list associated with each e-mail, and records those values.\n    Returns a DataFrame and a plot of the sum and mean values for each email.\n    \"\"\"\n    try:\n        with open(json_file, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        return pd.DataFrame(columns=COLUMNS), None\n\n    df = pd.DataFrame(data)\n    df = df[COLUMNS]\n\n    if df.empty:\n        return df, None\n\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(lambda x: sum(x)/len(x))\n\n    fig, ax = plt.subplots()\n    df.plot(x='email', y=['sum', 'mean'], ax=ax)\n    ax.set_title('Sum and Mean of Lists by Email')\n    ax.set_ylabel('Value')\n    ax.legend(loc='upper left')\n\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport json\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['email', 'list', 'sum', 'mean']\n\ndef task_func(json_file):\n    \"\"\"\n    Loads e-mail data from a JSON file, converts it into a Pandas DataFrame, calculates the sum and mean of the list associated with each e-mail, and records those values.\n    Returns a DataFrame and a plot of the sum and mean values for each email.\n    \"\"\"\n    try:\n        with open(json_file, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        return pd.DataFrame(columns=COLUMNS), None\n\n    df = pd.DataFrame(data)\n    df = df[COLUMNS]\n\n    if df.empty:\n        return df, None\n\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(lambda x: sum(x)/len(x))\n\n    fig, ax = plt.subplots()\n    df.plot(x='email', y=['sum', 'mean'], ax=ax)\n    ax.set_title('Sum and Mean of Lists by Email')\n    ax.set_ylabel('Value')\n    ax.legend(loc='upper left')\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/71", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport ast\ndef task_func(csv_file):\n    # Load e-mail data from a CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert the 'list' column from string to list\n    df['list'] = df['list'].apply(lambda x: ast.literal_eval(x))\n\n    # Calculate the sum, mean, and standard deviation of the list associated with each e-mail\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['std'] = df['list'].apply(np.std)\n\n    # Draw a histogram of the mean values\n    plot = sns.histplot(df['mean'], kde=False)\n\n    return (df, plot)", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport ast\n\ndef task_func(csv_file):\n    # Load e-mail data from a CSV file\n    df = pd.read_csv(csv_file)\n\n    # Convert the 'list' column from string to list\n    df['list'] = df['list'].apply(lambda x: ast.literal_eval(x))\n\n    # Calculate the sum, mean, and standard deviation of the list associated with each e-mail\n    df['sum'] = df['list'].apply(sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['std'] = df['list'].apply(np.std)\n\n    # Draw a histogram of the mean values\n    plot = sns.histplot(df['mean'], kde=False)\n\n    return (df, plot)"}
{"task_id": "BigCodeBench/72", "solution": "import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport ast\ndef task_func(directory):\n    # Initialize variables\n    max_len = 0\n    max_file = None\n    df_list = []\n\n    # Traverse directory for CSV files\n    for file in os.listdir(directory):\n        if file.endswith('.csv'):\n            if len(file) > max_len:\n                max_len = len(file)\n                max_file = file\n\n    # If CSV file exists, load data, calculate sum, mean, median and draw histogram\n    if max_file is not None:\n        df = pd.read_csv(os.path.join(directory, max_file))\n        df['list'] = df['list'].apply(ast.literal_eval)  # Convert string representation of list to actual list\n        df['sum'] = df['list'].apply(lambda x: sum(x))\n        df['mean'] = df['list'].apply(lambda x: np.mean(x))\n        df['median'] = df['list'].apply(lambda x: np.median(x))\n        df_list.append(df)\n\n        # Draw histogram of median\n        plt.figure(figsize=(10, 6))\n        df['median'].plot(kind='hist', bins=20)\n        plt.title('Histogram of Median')\n        plt.xlabel('Median')\n        plt.ylabel('Frequency')\n        hist_plot = plt.gca()\n\n    # If no CSV file exists, return empty dataframe and None for histogram\n    else:\n        df = pd.DataFrame(columns=['email', 'list', 'sum', 'mean', 'median'])\n        hist_plot = None\n\n    return pd.concat(df_list, ignore_index=True), hist_plot", "raw_solution": "import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport ast\n\ndef task_func(directory):\n    # Initialize variables\n    max_len = 0\n    max_file = None\n    df_list = []\n\n    # Traverse directory for CSV files\n    for file in os.listdir(directory):\n        if file.endswith('.csv'):\n            if len(file) > max_len:\n                max_len = len(file)\n                max_file = file\n\n    # If CSV file exists, load data, calculate sum, mean, median and draw histogram\n    if max_file is not None:\n        df = pd.read_csv(os.path.join(directory, max_file))\n        df['list'] = df['list'].apply(ast.literal_eval)  # Convert string representation of list to actual list\n        df['sum'] = df['list'].apply(lambda x: sum(x))\n        df['mean'] = df['list'].apply(lambda x: np.mean(x))\n        df['median'] = df['list'].apply(lambda x: np.median(x))\n        df_list.append(df)\n\n        # Draw histogram of median\n        plt.figure(figsize=(10, 6))\n        df['median'].plot(kind='hist', bins=20)\n        plt.title('Histogram of Median')\n        plt.xlabel('Median')\n        plt.ylabel('Frequency')\n        hist_plot = plt.gca()\n\n    # If no CSV file exists, return empty dataframe and None for histogram\n    else:\n        df = pd.DataFrame(columns=['email', 'list', 'sum', 'mean', 'median'])\n        hist_plot = None\n\n    return pd.concat(df_list, ignore_index=True), hist_plot"}
{"task_id": "BigCodeBench/73", "solution": "import pandas as pd\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\ndef task_func(db_file):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # Query the database\n    query = \"SELECT email, list FROM EmailData\"\n    df = pd.read_sql_query(query, conn)\n\n    # Convert the 'list' column from string to list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and variance\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['var'] = df['list'].apply(np.var)\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    df[['sum', 'mean', 'var']].plot(kind='bar', ax=ax)\n    ax.set_xlabel('Email')\n    ax.set_ylabel('Value')\n\n    # Close the connection\n    conn.close()\n\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport sqlite3\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\n\ndef task_func(db_file):\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # Query the database\n    query = \"SELECT email, list FROM EmailData\"\n    df = pd.read_sql_query(query, conn)\n\n    # Convert the 'list' column from string to list\n    df['list'] = df['list'].apply(ast.literal_eval)\n\n    # Calculate sum, mean, and variance\n    df['sum'] = df['list'].apply(np.sum)\n    df['mean'] = df['list'].apply(np.mean)\n    df['var'] = df['list'].apply(np.var)\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    df[['sum', 'mean', 'var']].plot(kind='bar', ax=ax)\n    ax.set_xlabel('Email')\n    ax.set_ylabel('Value')\n\n    # Close the connection\n    conn.close()\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/74", "solution": "import socket\nimport requests\nfrom requests.exceptions import ConnectionError\ndef task_func(host):\n    \"\"\"\n    This function resolves the IP address of the given host and then uses the IP address to fetch geolocation information from the ipinfo.io API.\n    \"\"\"\n    if host is None or host == '':\n        raise ValueError(\"'host' cannot be None or an empty string\")\n\n    try:\n        ip_address = socket.gethostbyname(host)\n    except socket.gaierror:\n        raise ConnectionError(f\"Failed to resolve host: {host}\")\n\n    try:\n        response = requests.get(f\"https://ipinfo.io/{ip_address}\")\n        response.raise_for_status()\n    except (requests.exceptions.RequestException, requests.exceptions.HTTPError):\n        raise ConnectionError(\"Failed to fetch geolocation information from ipinfo.io\")\n\n    geolocation_info = response.json()\n    return {'IP Address': ip_address, 'Geolocation Information': geolocation_info}", "raw_solution": "import socket\nimport requests\nfrom requests.exceptions import ConnectionError\n\ndef task_func(host):\n    \"\"\"\n    This function resolves the IP address of the given host and then uses the IP address to fetch geolocation information from the ipinfo.io API.\n    \"\"\"\n    if host is None or host == '':\n        raise ValueError(\"'host' cannot be None or an empty string\")\n\n    try:\n        ip_address = socket.gethostbyname(host)\n    except socket.gaierror:\n        raise ConnectionError(f\"Failed to resolve host: {host}\")\n\n    try:\n        response = requests.get(f\"https://ipinfo.io/{ip_address}\")\n        response.raise_for_status()\n    except (requests.exceptions.RequestException, requests.exceptions.HTTPError):\n        raise ConnectionError(\"Failed to fetch geolocation information from ipinfo.io\")\n\n    geolocation_info = response.json()\n    return {'IP Address': ip_address, 'Geolocation Information': geolocation_info}"}
{"task_id": "BigCodeBench/75", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\nfrom datetime import datetime, timedelta\nimport seaborn as sns\ndef task_func(df, fruits=None, days=None, seed=None, sales_lower_bound=1, sales_upper_bound=50):\n    \"\"\"\n    Appends randomly generated sales data for specified fruits over a given range of days to a DataFrame, and returns a seaborn boxplot of the sales.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"'df' should be a pandas DataFrame\")\n    if not df.empty:\n        raise ValueError(\"'df' should be empty\")\n    if not fruits:\n        fruits = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n    if not days:\n        days = 30\n    if not seed:\n        seed = np.random.randint(0, 100)\n    if not isinstance(sales_lower_bound, int) or not isinstance(sales_upper_bound, int):\n        raise TypeError(\"'sales_lower_bound' and 'sales_upper_bound' should be integers\")\n    if not sales_lower_bound < sales_upper_bound:\n        raise ValueError(\"'sales_lower_bound' should be less than 'sales_upper_bound'\")\n\n    np.random.seed(seed)\n    dates = pd.date_range(datetime.now(), periods=days)\n    data = {\n        'Date': dates,\n        'Fruit': np.random.choice(fruits, size=days),\n        'Sales': np.random.randint(sales_lower_bound, sales_upper_bound, size=days)\n    }\n    df = pd.DataFrame(data)\n    df.set_index('Date', inplace=True)\n\n    g = sns.boxplot(x='Fruit', y='Sales', data=df)\n\n    return df, g", "raw_solution": "import pandas as pd\nimport numpy as np\nimport itertools\nfrom datetime import datetime, timedelta\nimport seaborn as sns\n\ndef task_func(df, fruits=None, days=None, seed=None, sales_lower_bound=1, sales_upper_bound=50):\n    \"\"\"\n    Appends randomly generated sales data for specified fruits over a given range of days to a DataFrame, and returns a seaborn boxplot of the sales.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"'df' should be a pandas DataFrame\")\n    if not df.empty:\n        raise ValueError(\"'df' should be empty\")\n    if not fruits:\n        fruits = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry']\n    if not days:\n        days = 30\n    if not seed:\n        seed = np.random.randint(0, 100)\n    if not isinstance(sales_lower_bound, int) or not isinstance(sales_upper_bound, int):\n        raise TypeError(\"'sales_lower_bound' and 'sales_upper_bound' should be integers\")\n    if not sales_lower_bound < sales_upper_bound:\n        raise ValueError(\"'sales_lower_bound' should be less than 'sales_upper_bound'\")\n\n    np.random.seed(seed)\n    dates = pd.date_range(datetime.now(), periods=days)\n    data = {\n        'Date': dates,\n        'Fruit': np.random.choice(fruits, size=days),\n        'Sales': np.random.randint(sales_lower_bound, sales_upper_bound, size=days)\n    }\n    df = pd.DataFrame(data)\n    df.set_index('Date', inplace=True)\n\n    g = sns.boxplot(x='Fruit', y='Sales', data=df)\n\n    return df, g"}
{"task_id": "BigCodeBench/76", "solution": "import random\nimport string\nfrom django.http import HttpResponse\ndef task_func(request, session_expire_time):\n    \"\"\"\n    This function creates a random session key comprising letters and digits with a specific length of 20,\n    then sets this key in a cookie on an HttpResponse object with the specified expiration time.\n    \"\"\"\n    # Generate a random session key\n    session_key = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n\n    # Check if the session key contains both letters and digits\n    if not any(c.isdigit() for c in session_key) or not any(c.isalpha() for c in session_key):\n        raise ValueError(\"Session key must contain both letters and digits\")\n\n    # Check if the session key length is 20\n    if len(session_key) != 20:\n        raise ValueError(\"Session key length must be 20\")\n\n    # Set the session key in a cookie on the HttpResponse object\n    response = HttpResponse(\"Session key generated successfully.\")\n    response.set_cookie('session_key', session_key, max_age=session_expire_time)\n\n    return response", "raw_solution": "import random\nimport string\nfrom django.http import HttpResponse\n\ndef task_func(request, session_expire_time):\n    \"\"\"\n    This function creates a random session key comprising letters and digits with a specific length of 20,\n    then sets this key in a cookie on an HttpResponse object with the specified expiration time.\n    \"\"\"\n    # Generate a random session key\n    session_key = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n\n    # Check if the session key contains both letters and digits\n    if not any(c.isdigit() for c in session_key) or not any(c.isalpha() for c in session_key):\n        raise ValueError(\"Session key must contain both letters and digits\")\n\n    # Check if the session key length is 20\n    if len(session_key) != 20:\n        raise ValueError(\"Session key length must be 20\")\n\n    # Set the session key in a cookie on the HttpResponse object\n    response = HttpResponse(\"Session key generated successfully.\")\n    response.set_cookie('session_key', session_key, max_age=session_expire_time)\n\n    return response"}
{"task_id": "BigCodeBench/77", "solution": "import hashlib\nimport base64\nimport binascii\nfrom django.http import HttpResponseBadRequest, HttpResponse\ndef task_func(data):\n    try:\n        username = data['username']\n        password = data['password']\n    except KeyError:\n        return HttpResponseBadRequest('Bad Request.')\n\n    try:\n        decoded_password = base64.b64decode(password)\n    except (binascii.Error, UnicodeDecodeError, TypeError):\n        return HttpResponseBadRequest('Bad Request.')\n\n    hashed_password = hashlib.sha256(decoded_password).digest()\n    expected_password = base64.b64encode(hashed_password).decode()\n\n    if username == 'admin' and password == expected_password:\n        return HttpResponse('Login successful.', status=400)\n    else:\n        return HttpResponse('Login failed.', status=401)", "raw_solution": "import hashlib\nimport base64\nimport binascii\nfrom django.http import HttpResponseBadRequest, HttpResponse\n\ndef task_func(data):\n    try:\n        username = data['username']\n        password = data['password']\n    except KeyError:\n        return HttpResponseBadRequest('Bad Request.')\n\n    try:\n        decoded_password = base64.b64decode(password)\n    except (binascii.Error, UnicodeDecodeError, TypeError):\n        return HttpResponseBadRequest('Bad Request.')\n\n    hashed_password = hashlib.sha256(decoded_password).digest()\n    expected_password = base64.b64encode(hashed_password).decode()\n\n    if username == 'admin' and password == expected_password:\n        return HttpResponse('Login successful.', status=400)\n    else:\n        return HttpResponse('Login failed.', status=401)"}
{"task_id": "BigCodeBench/78", "solution": "import csv\nimport io\nfrom django.http import HttpRequest, FileResponse\ndef task_func(request, header, csv_data):\n    \"\"\"\n    Generates a CSV file response from a Django HttpRequest.\n    \"\"\"\n    # Create a StringIO object to hold the CSV data\n    csv_file = io.StringIO()\n\n    # Create a CSV writer object\n    writer = csv.writer(csv_file)\n\n    # Write the header\n    writer.writerow(header)\n\n    # Write the CSV data\n    for row in csv_data:\n        writer.writerow(row)\n\n    # Reset the file pointer to the beginning\n    csv_file.seek(0)\n\n    # Create a Django FileResponse object\n    response = FileResponse(csv_file, as_attachment=True, filename='output.csv')\n\n    return response", "raw_solution": "import csv\nimport io\nfrom django.http import HttpRequest, FileResponse\n\ndef task_func(request, header, csv_data):\n    \"\"\"\n    Generates a CSV file response from a Django HttpRequest.\n    \"\"\"\n    # Create a StringIO object to hold the CSV data\n    csv_file = io.StringIO()\n\n    # Create a CSV writer object\n    writer = csv.writer(csv_file)\n\n    # Write the header\n    writer.writerow(header)\n\n    # Write the CSV data\n    for row in csv_data:\n        writer.writerow(row)\n\n    # Reset the file pointer to the beginning\n    csv_file.seek(0)\n\n    # Create a Django FileResponse object\n    response = FileResponse(csv_file, as_attachment=True, filename='output.csv')\n\n    return response"}
{"task_id": "BigCodeBench/79", "solution": "import zipfile\nimport io\nfrom django.http import FileResponse, HttpRequest\nfrom django.conf import settings\ndef task_func(request, file_paths):\n    \"\"\"\n    Generates a ZIP file response for a Django HttpRequest, zipping the specified files.\n    This function is useful for scenarios where multiple file downloads are required in response to a web request.\n    The actual HttpRequest is not utilized within the function but is required for compatibility with Django view structures.\n    \"\"\"\n    # Create a file-like object to store the zip file\n    zip_buffer = io.BytesIO()\n\n    # Create a zip file object\n    with zipfile.ZipFile(zip_buffer, \"a\", zipfile.ZIP_DEFLATED, False) as zip_file:\n        # Loop through the file paths\n        for file_path in file_paths:\n            # Add the file to the zip file\n            zip_file.write(file_path, arcname=file_path.split('/')[-1])\n\n    # Reset the file pointer to the start of the zip file\n    zip_buffer.seek(0)\n\n    # Create a FileResponse object with the zip file as the content\n    response = FileResponse(zip_buffer, content_type='application/zip')\n    response['Content-Disposition'] = 'attachment; filename=\"files.zip\"'\n\n    return response", "raw_solution": "import zipfile\nimport io\nfrom django.http import FileResponse, HttpRequest\nfrom django.conf import settings\n\ndef task_func(request, file_paths):\n    \"\"\"\n    Generates a ZIP file response for a Django HttpRequest, zipping the specified files.\n    This function is useful for scenarios where multiple file downloads are required in response to a web request.\n    The actual HttpRequest is not utilized within the function but is required for compatibility with Django view structures.\n    \"\"\"\n    # Create a file-like object to store the zip file\n    zip_buffer = io.BytesIO()\n\n    # Create a zip file object\n    with zipfile.ZipFile(zip_buffer, \"a\", zipfile.ZIP_DEFLATED, False) as zip_file:\n        # Loop through the file paths\n        for file_path in file_paths:\n            # Add the file to the zip file\n            zip_file.write(file_path, arcname=file_path.split('/')[-1])\n\n    # Reset the file pointer to the start of the zip file\n    zip_buffer.seek(0)\n\n    # Create a FileResponse object with the zip file as the content\n    response = FileResponse(zip_buffer, content_type='application/zip')\n    response['Content-Disposition'] = 'attachment; filename=\"files.zip\"'\n\n    return response"}
{"task_id": "BigCodeBench/80", "solution": "from flask import Flask, render_template, request\nimport json\nimport logging\ndef task_func(template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n\n    @app.route('/', methods=['GET', 'POST'])\n    def root():\n        if request.method == 'POST':\n            data = request.get_json()\n            logging.info(json.dumps(data))\n            return render_template('index.html', data=data)\n        return render_template('index.html')\n\n    return app", "raw_solution": "from flask import Flask, render_template, request\nimport json\nimport logging\nlogging.basicConfig(filename=\"out.log\", level=logging.INFO)\n\ndef task_func(template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n\n    @app.route('/', methods=['GET', 'POST'])\n    def root():\n        if request.method == 'POST':\n            data = request.get_json()\n            logging.info(json.dumps(data))\n            return render_template('index.html', data=data)\n        return render_template('index.html')\n\n    return app\n\n# Test the function\napp = task_func('./templates')"}
{"task_id": "BigCodeBench/81", "solution": "from flask import Flask, jsonify\nfrom flask_restful import Resource, Api\nimport requests\ndef task_func(api_url, template_folder):\n    \"\"\"\n    Creates a Flask application with a RESTful API endpoint. The endpoint, when accessed, fetches data from an external API and returns the response as JSON.\n    It is configured to use a specified templates folder, which must be provided when calling this function. The URL for the external API must also be provided when initializing the app.\n    \"\"\"\n    app = Flask(__name__, template_folder=template_folder)\n    api = Api(app)\n\n    class ExternalApiResource(Resource):\n        def get(self):\n            response = requests.get(api_url)\n            return jsonify(response.json())\n\n    api.add_resource(ExternalApiResource, '/api')\n\n    return app", "raw_solution": "from flask import Flask, jsonify\nfrom flask_restful import Resource, Api\nimport requests\n\ndef task_func(api_url, template_folder):\n    \"\"\"\n    Creates a Flask application with a RESTful API endpoint. The endpoint, when accessed, fetches data from an external API and returns the response as JSON.\n    It is configured to use a specified templates folder, which must be provided when calling this function. The URL for the external API must also be provided when initializing the app.\n    \"\"\"\n    app = Flask(__name__, template_folder=template_folder)\n    api = Api(app)\n\n    class ExternalApiResource(Resource):\n        def get(self):\n            response = requests.get(api_url)\n            return jsonify(response.json())\n\n    api.add_resource(ExternalApiResource, '/api')\n\n    return app"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash", "raw_solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nclass User(UserMixin):\n    def __init__(self, username, password):\n        self.username = username\n        self.password_hash = generate_password_hash(password)\n\n    def check_password(self, password):\n        return check_password_hash(self.password_hash, password)\n\nlogin_manager = LoginManager()\n\n@login_manager.user_loader\ndef load_user(user_id):\n    # This is where you would normally load a user from a database\n    return User('admin', 'admin')\n\ndef create_app(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = User(form.username.data, form.password.data)\n            if user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('protected'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return render_template('protected.html')\n\n    return app\n"}
{"task_id": "BigCodeBench/83", "solution": "from flask import Flask\nfrom flask_mail import Mail, Message\ndef task_func(smtp_server, smtp_port, smtp_user, smtp_password, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    mail = Mail(app)\n\n    app.config['MAIL_SERVER'] = smtp_server\n    app.config['MAIL_PORT'] = smtp_port\n    app.config['MAIL_USERNAME'] = smtp_user\n    app.config['MAIL_PASSWORD'] = smtp_password\n    app.config['MAIL_USE_TLS'] = False\n    app.config['MAIL_USE_SSL'] = True\n\n    mail = Mail(app)\n\n    @app.route('/send_email')\n    def send_email():\n        msg = Message('Hello',\n                      sender=smtp_user,\n                      recipients=['to@example.com'])\n        msg.body = 'This is a test email sent from a Flask application.'\n        mail.send(msg)\n        return 'Email sent!'\n\n    return app", "raw_solution": "from flask import Flask\nfrom flask_mail import Mail, Message\n\ndef task_func(smtp_server, smtp_port, smtp_user, smtp_password, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    mail = Mail(app)\n\n    app.config['MAIL_SERVER'] = smtp_server\n    app.config['MAIL_PORT'] = smtp_port\n    app.config['MAIL_USERNAME'] = smtp_user\n    app.config['MAIL_PASSWORD'] = smtp_password\n    app.config['MAIL_USE_TLS'] = False\n    app.config['MAIL_USE_SSL'] = True\n\n    mail = Mail(app)\n\n    @app.route('/send_email')\n    def send_email():\n        msg = Message('Hello',\n                      sender=smtp_user,\n                      recipients=['to@example.com'])\n        msg.body = 'This is a test email sent from a Flask application.'\n        mail.send(msg)\n        return 'Email sent!'\n\n    return app\n\napp = task_func('smtp.example.com', 465, 'user@example.com', 'password', 'templates')\n"}
{"task_id": "BigCodeBench/84", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(products, n_samples=100, sales_lower=50, sales_upper=200, profit_margin_min=0.1, profit_margin_max=0.5, random_seed=42):\n    # Validate inputs\n    if not isinstance(n_samples, int) or n_samples <= 0:\n        raise ValueError(\"n_samples must be a positive integer\")\n    if not isinstance(products, list) or not all(isinstance(product, str) for product in products):\n        raise TypeError(\"products must be a list of strings\")\n    if not isinstance(sales_lower, (int, float)) or not isinstance(sales_upper, (int, float)):\n        raise TypeError(\"sales_lower and sales_upper must be numeric\")\n    if sales_lower > sales_upper:\n        raise ValueError(\"sales_lower cannot be greater than sales_upper\")\n    if not isinstance(profit_margin_min, (int, float)) or not isinstance(profit_margin_max, (int, float)):\n        raise TypeError(\"profit_margin_min and profit_margin_max must be numeric\")\n    if profit_margin_min > profit_margin_max:\n        raise ValueError(\"profit_margin_min cannot be greater than profit_margin_max\")\n\n    # Set random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Generate sales and profit data\n    sales = np.random.randint(sales_lower, sales_upper, n_samples)\n    profit_margins = np.random.uniform(profit_margin_min, profit_margin_max, n_samples)\n    profits = sales * profit_margins\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Product\": np.random.choice(products, n_samples),\n        \"Sales\": sales,\n        \"Profit Margin\": profit_margins,\n        \"Profit\": profits\n    })\n\n    # Aggregate by product and sort by total profit\n    report = df.groupby(\"Product\").sum().sort_values(\"Profit\", ascending=False)\n\n    return report", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(products, n_samples=100, sales_lower=50, sales_upper=200, profit_margin_min=0.1, profit_margin_max=0.5, random_seed=42):\n    # Validate inputs\n    if not isinstance(n_samples, int) or n_samples <= 0:\n        raise ValueError(\"n_samples must be a positive integer\")\n    if not isinstance(products, list) or not all(isinstance(product, str) for product in products):\n        raise TypeError(\"products must be a list of strings\")\n    if not isinstance(sales_lower, (int, float)) or not isinstance(sales_upper, (int, float)):\n        raise TypeError(\"sales_lower and sales_upper must be numeric\")\n    if sales_lower > sales_upper:\n        raise ValueError(\"sales_lower cannot be greater than sales_upper\")\n    if not isinstance(profit_margin_min, (int, float)) or not isinstance(profit_margin_max, (int, float)):\n        raise TypeError(\"profit_margin_min and profit_margin_max must be numeric\")\n    if profit_margin_min > profit_margin_max:\n        raise ValueError(\"profit_margin_min cannot be greater than profit_margin_max\")\n\n    # Set random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Generate sales and profit data\n    sales = np.random.randint(sales_lower, sales_upper, n_samples)\n    profit_margins = np.random.uniform(profit_margin_min, profit_margin_max, n_samples)\n    profits = sales * profit_margins\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Product\": np.random.choice(products, n_samples),\n        \"Sales\": sales,\n        \"Profit Margin\": profit_margins,\n        \"Profit\": profits\n    })\n\n    # Aggregate by product and sort by total profit\n    report = df.groupby(\"Product\").sum().sort_values(\"Profit\", ascending=False)\n\n    return report"}
{"task_id": "BigCodeBench/85", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\ndef task_func(start_date, end_date, random_seed=42):\n    \"\"\"\n    Generates and plots weather data for a specified date range.\n    \"\"\"\n    if end_date < start_date:\n        raise ValueError(\"'end_date' should be later than 'start_date'\")\n\n    np.random.seed(random_seed)\n    date_range = pd.date_range(start=start_date, end=end_date)\n\n    temp_range = np.random.uniform(-10, 40, size=len(date_range))\n    humidity_range = np.random.uniform(20, 100, size=len(date_range))\n    wind_speed_range = np.random.uniform(0, 20, size=len(date_range))\n\n    df = pd.DataFrame({\n        'Date': date_range,\n        'Temperature': temp_range,\n        'Humidity': humidity_range,\n        'Wind Speed': wind_speed_range\n    })\n\n    fig, ax = plt.subplots(4, 1, figsize=(10, 10))\n\n    df['Temperature'].plot(ax=ax[0], title='Temperature over time')\n    df['Humidity'].plot(ax=ax[1], title='Humidity over time')\n    df['Wind Speed'].plot(ax=ax[2], title='Wind Speed over time')\n\n    ax[3].remove()\n    fig.tight_layout()\n\n    return df, fig", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\ndef task_func(start_date, end_date, random_seed=42):\n    \"\"\"\n    Generates and plots weather data for a specified date range.\n    \"\"\"\n    if end_date < start_date:\n        raise ValueError(\"'end_date' should be later than 'start_date'\")\n\n    np.random.seed(random_seed)\n    date_range = pd.date_range(start=start_date, end=end_date)\n\n    temp_range = np.random.uniform(-10, 40, size=len(date_range))\n    humidity_range = np.random.uniform(20, 100, size=len(date_range))\n    wind_speed_range = np.random.uniform(0, 20, size=len(date_range))\n\n    df = pd.DataFrame({\n        'Date': date_range,\n        'Temperature': temp_range,\n        'Humidity': humidity_range,\n        'Wind Speed': wind_speed_range\n    })\n\n    fig, ax = plt.subplots(4, 1, figsize=(10, 10))\n\n    df['Temperature'].plot(ax=ax[0], title='Temperature over time')\n    df['Humidity'].plot(ax=ax[1], title='Humidity over time')\n    df['Wind Speed'].plot(ax=ax[2], title='Wind Speed over time')\n\n    ax[3].remove()\n    fig.tight_layout()\n\n    return df, fig"}
{"task_id": "BigCodeBench/86", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(students=[\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"], seed=42):\n    np.random.seed(seed)\n    scores = np.random.randint(0, 100, len(students))\n    df = pd.DataFrame({\"Student\": students, \"Score\": scores})\n    df = df.sort_values(\"Score\")\n\n    fig, ax = plt.subplots()\n    ax.bar(df['Student'], df['Score'])\n    ax.set_xlabel('Student')\n    ax.set_ylabel('Score')\n    ax.set_title('Scores of Students')\n\n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(students=[\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"], seed=42):\n    np.random.seed(seed)\n    scores = np.random.randint(0, 100, len(students))\n    df = pd.DataFrame({\"Student\": students, \"Score\": scores})\n    df = df.sort_values(\"Score\")\n\n    fig, ax = plt.subplots()\n    ax.bar(df['Student'], df['Score'])\n    ax.set_xlabel('Student')\n    ax.set_ylabel('Score')\n    ax.set_title('Scores of Students')\n\n    return df, ax"}
{"task_id": "BigCodeBench/87", "solution": "import pandas as pd\nfrom random import choices, seed\ndef task_func(products, ratings, weights, random_seed=42):\n    \"\"\"\n    Generates a DataFrame containing ratings for a given list of products. Ratings are generated randomly based on the provided weights. The DataFrame is sorted by ratings in descending order.\n    \"\"\"\n    # Set the random seed for reproducibility\n    seed(random_seed)\n\n    # Generate ratings using the provided weights\n    generated_ratings = choices(ratings, weights, k=len(products))\n\n    # Create a DataFrame\n    df = pd.DataFrame(list(zip(products, generated_ratings)), columns=['Product', 'Rating'])\n\n    # Sort the DataFrame by 'Rating' in descending order\n    df = df.sort_values('Rating', ascending=False)\n\n    return df\nproducts = ['Product 1', 'Product 2', 'Product 3', 'Product 4', 'Product 5']\nratings = [1, 2, 3, 4, 5]\nweights = [0.1, 0.2, 0.3, 0.2, 0.2]", "raw_solution": "import pandas as pd\nfrom random import choices, seed\n\ndef task_func(products, ratings, weights, random_seed=42):\n    \"\"\"\n    Generates a DataFrame containing ratings for a given list of products. Ratings are generated randomly based on the provided weights. The DataFrame is sorted by ratings in descending order.\n    \"\"\"\n    # Set the random seed for reproducibility\n    seed(random_seed)\n\n    # Generate ratings using the provided weights\n    generated_ratings = choices(ratings, weights, k=len(products))\n\n    # Create a DataFrame\n    df = pd.DataFrame(list(zip(products, generated_ratings)), columns=['Product', 'Rating'])\n\n    # Sort the DataFrame by 'Rating' in descending order\n    df = df.sort_values('Rating', ascending=False)\n\n    return df\n\n# Test the function\nproducts = ['Product 1', 'Product 2', 'Product 3', 'Product 4', 'Product 5']\nratings = [1, 2, 3, 4, 5]\nweights = [0.1, 0.2, 0.3, 0.2, 0.2]\n\ndf = task_func(products, ratings, weights)"}
{"task_id": "BigCodeBench/88", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\ndef task_func(start_date, end_date, seed=42):\n    np.random.seed(seed)\n    n_days = (end_date - start_date).days + 1\n    dates = pd.date_range(start=start_date, end=end_date)\n    sales = np.random.randint(0, 501, n_days)\n    df = pd.DataFrame({'Date': dates, 'Sales': sales})\n    df.set_index('Date', inplace=True)\n\n    ax = df.plot(kind='line', title='Sales over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n\n    return df, ax.figure\nstart_date = datetime.now() - timedelta(days=30)\nend_date = datetime.now()", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\ndef task_func(start_date, end_date, seed=42):\n    np.random.seed(seed)\n    n_days = (end_date - start_date).days + 1\n    dates = pd.date_range(start=start_date, end=end_date)\n    sales = np.random.randint(0, 501, n_days)\n    df = pd.DataFrame({'Date': dates, 'Sales': sales})\n    df.set_index('Date', inplace=True)\n\n    ax = df.plot(kind='line', title='Sales over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n\n    return df, ax.figure\n\n# Test the function\nstart_date = datetime.now() - timedelta(days=30)\nend_date = datetime.now()\ndf, fig = task_func(start_date, end_date)\nplt.show()"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    It standardizes the column, calculates Z-scores, and removes data points where the Z-score exceeds a threshold.\n    The function also visualizes the data before and after outlier removal.\n\n    Parameters:\n    data (pandas.DataFrame): The dataset to be processed.\n    column (str): The column of the dataset to be processed.\n    outlier_z_score (float): The Z-score threshold for identifying outliers.\n\n    Returns:\n    tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Standardize the column\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[column].values.reshape(-1, 1))\n\n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data[column]))\n\n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove outliers\n    data_without_outliers = data.drop(outliers)\n\n    # Visualize data before and after outlier removal\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.scatter(range(len(data[column])), data[column])\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(range(len(data_without_outliers[column])), data_without_outliers[column])\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.tight_layout()\n    plt.show()\n\n    return data, data_without_outliers, outliers", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    It standardizes the column, calculates Z-scores, and removes data points where the Z-score exceeds a threshold.\n    The function also visualizes the data before and after outlier removal.\n\n    Parameters:\n    data (pandas.DataFrame): The dataset to be processed.\n    column (str): The column of the dataset to be processed.\n    outlier_z_score (float): The Z-score threshold for identifying outliers.\n\n    Returns:\n    tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Standardize the column\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[column].values.reshape(-1, 1))\n\n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data[column]))\n\n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove outliers\n    data_without_outliers = data.drop(outliers)\n\n    # Visualize data before and after outlier removal\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.scatter(range(len(data[column])), data[column])\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(range(len(data_without_outliers[column])), data_without_outliers[column])\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n\n    plt.tight_layout()\n    plt.show()\n\n    return data, data_without_outliers, outliers"}
{"task_id": "BigCodeBench/90", "solution": "import numpy as np\nimport math\ndef task_func(data, target, k):\n    \"\"\"\n    Calculates the 'k' nearest neighbors by geographic coordinates using a dataset and a target data point.\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"'k' should be a non-negative integer.\")\n\n    # Constants\n    R = 6371  # radius of the Earth in kilometers\n    lat1, lon1 = math.radians(target[0]), math.radians(target[1])\n\n    # Calculate distances\n    distances = []\n    for lat2, lon2 in data:\n        lat2, lon2 = math.radians(lat2), math.radians(lon2)\n        dlat, dlon = lat2 - lat1, lon2 - lon1\n        a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n        distance = R * c\n        distances.append(distance)\n\n    # Get 'k' nearest neighbors\n    nearest_neighbors = sorted(range(len(distances)), key=lambda i: distances[i])[:k]\n\n    # Convert to [Latitude, Longitude] format\n    nearest_neighbors = [[data[i][0], data[i][1]] for i in nearest_neighbors]\n\n    return nearest_neighbors", "raw_solution": "import numpy as np\nimport math\n\ndef task_func(data, target, k):\n    \"\"\"\n    Calculates the 'k' nearest neighbors by geographic coordinates using a dataset and a target data point.\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"'k' should be a non-negative integer.\")\n\n    # Constants\n    R = 6371  # radius of the Earth in kilometers\n    lat1, lon1 = math.radians(target[0]), math.radians(target[1])\n\n    # Calculate distances\n    distances = []\n    for lat2, lon2 in data:\n        lat2, lon2 = math.radians(lat2), math.radians(lon2)\n        dlat, dlon = lat2 - lat1, lon2 - lon1\n        a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n        distance = R * c\n        distances.append(distance)\n\n    # Get 'k' nearest neighbors\n    nearest_neighbors = sorted(range(len(distances)), key=lambda i: distances[i])[:k]\n\n    # Convert to [Latitude, Longitude] format\n    nearest_neighbors = [[data[i][0], data[i][1]] for i in nearest_neighbors]\n\n    return nearest_neighbors"}
{"task_id": "BigCodeBench/91", "solution": "from scipy.stats import linregress\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data, column1, column2):\n    if column1 not in data.columns or column2 not in data.columns:\n        raise ValueError(\"Specified columns do not exist in the DataFrame.\")\n\n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = linregress(data[column1], data[column2])\n\n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.scatter(data[column1], data[column2])\n    ax.plot(data[column1], slope * data[column1] + intercept, color='red')\n    ax.set_xlabel(column1)\n    ax.set_ylabel(column2)\n    ax.set_title(f'Linear Regression: {column1} vs {column2}')\n\n    return (slope, intercept, r_value, p_value, std_err), ax", "raw_solution": "from scipy.stats import linregress\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data, column1, column2):\n    if column1 not in data.columns or column2 not in data.columns:\n        raise ValueError(\"Specified columns do not exist in the DataFrame.\")\n\n    # Perform linear regression\n    slope, intercept, r_value, p_value, std_err = linregress(data[column1], data[column2])\n\n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.scatter(data[column1], data[column2])\n    ax.plot(data[column1], slope * data[column1] + intercept, color='red')\n    ax.set_xlabel(column1)\n    ax.set_ylabel(column2)\n    ax.set_title(f'Linear Regression: {column1} vs {column2}')\n\n    return (slope, intercept, r_value, p_value, std_err), ax"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' should be an integer greater than 1\")\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels)\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker='X', color='red')\n\n    return labels, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' should be an integer greater than 1\")\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(data)\n\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels)\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker='X', color='red')\n\n    return labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    transformed_df = pd.DataFrame(transformed_data, columns=[f'PC{i+1}' for i in range(n_components)])\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n\n    return transformed_df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    transformed_df = pd.DataFrame(transformed_data, columns=[f'PC{i+1}' for i in range(n_components)])\n\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n\n    return transformed_df, ax"}
{"task_id": "BigCodeBench/94", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(mean, std_dev, num_samples):\n    \"\"\"\n    Generates a histogram of samples drawn from a normal distribution and overlays the probability density function (PDF) of the normal distribution.\n    The plot is titled with the fit results, showing the mean and standard deviation used in the generation.\n    The function returns both the plot and the samples generated.\n    \"\"\"\n    # Generate samples from a normal distribution\n    samples = np.random.normal(loc=mean, scale=std_dev, size=num_samples)\n\n    # Create a histogram of the samples\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n\n    # Generate x values for the PDF\n    x = np.linspace(samples.min(), samples.max(), 100)\n\n    # Calculate the PDF values\n    pdf = norm.pdf(x, loc=mean, scale=std_dev)\n\n    # Plot the PDF\n    plt.plot(x, pdf, color='r', label='PDF')\n\n    # Set plot title\n    plt.title(f\"Fit results: mean = {mean:.2f}, std = {std_dev:.2f}\")\n\n    # Add legend\n    plt.legend()\n\n    # Return the plot and the samples\n    return plt.gcf(), samples", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, num_samples):\n    \"\"\"\n    Generates a histogram of samples drawn from a normal distribution and overlays the probability density function (PDF) of the normal distribution.\n    The plot is titled with the fit results, showing the mean and standard deviation used in the generation.\n    The function returns both the plot and the samples generated.\n    \"\"\"\n    # Generate samples from a normal distribution\n    samples = np.random.normal(loc=mean, scale=std_dev, size=num_samples)\n\n    # Create a histogram of the samples\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n\n    # Generate x values for the PDF\n    x = np.linspace(samples.min(), samples.max(), 100)\n\n    # Calculate the PDF values\n    pdf = norm.pdf(x, loc=mean, scale=std_dev)\n\n    # Plot the PDF\n    plt.plot(x, pdf, color='r', label='PDF')\n\n    # Set plot title\n    plt.title(f\"Fit results: mean = {mean:.2f}, std = {std_dev:.2f}\")\n\n    # Add legend\n    plt.legend()\n\n    # Return the plot and the samples\n    return plt.gcf(), samples"}
{"task_id": "BigCodeBench/95", "solution": "import pandas as pd\nfrom random import randint, uniform, seed\ndef task_func(categories=None, months=None, random_seed=42):\n    \"\"\"\n    Generates a DataFrame with simulated monthly sales data for various product categories.\n    \"\"\"\n    if not isinstance(categories, list) or not isinstance(months, list):\n        raise ValueError(\"Both 'categories' and 'months' should be provided as lists.\")\n    if not categories or not months:\n        raise ValueError(\"Both 'categories' and 'months' should not be empty lists.\")\n\n    seed(random_seed)\n\n    data = []\n    for category in categories:\n        for month in months:\n            sales = randint(100, 500) + uniform(0, 1)\n            data.append([month, category, sales])\n\n    df = pd.DataFrame(data, columns=['Month', 'Category', 'Sales'])\n    return df", "raw_solution": "import pandas as pd\nfrom random import randint, uniform, seed\n\ndef task_func(categories=None, months=None, random_seed=42):\n    \"\"\"\n    Generates a DataFrame with simulated monthly sales data for various product categories.\n    \"\"\"\n    if not isinstance(categories, list) or not isinstance(months, list):\n        raise ValueError(\"Both 'categories' and 'months' should be provided as lists.\")\n    if not categories or not months:\n        raise ValueError(\"Both 'categories' and 'months' should not be empty lists.\")\n\n    seed(random_seed)\n\n    data = []\n    for category in categories:\n        for month in months:\n            sales = randint(100, 500) + uniform(0, 1)\n            data.append([month, category, sales])\n\n    df = pd.DataFrame(data, columns=['Month', 'Category', 'Sales'])\n    return df"}
{"task_id": "BigCodeBench/96", "solution": "import csv\nfrom collections import Counter\nimport operator\ndef task_func(csv_file, csv_delimiter):\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file, delimiter=csv_delimiter)\n        words = [word for row in reader for word in row]\n    word_counts = Counter(words)\n    sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n    return sorted_word_counts", "raw_solution": "import csv\nfrom collections import Counter\nimport operator\n\ndef task_func(csv_file, csv_delimiter):\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file, delimiter=csv_delimiter)\n        words = [word for row in reader for word in row]\n    word_counts = Counter(words)\n    sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n    return sorted_word_counts"}
{"task_id": "BigCodeBench/97", "solution": "import math\nimport itertools\nfrom functools import reduce\nfrom operator import mul\ndef task_func(numbers):\n    \"\"\"\n    Generates all possible combinations of the provided numbers in a given list for each possible length.\n    For each combination, it computes the product of the numbers in the combination.\n    It then computes the logarithm of each product and sums these logarithms to produce the final result.\n    \"\"\"\n    # Initialize the sum of logarithms\n    sum_logs = 0\n\n    # Generate all possible combinations of the numbers\n    for r in range(1, len(numbers) + 1):\n        # Generate all combinations of length r\n        combinations = list(itertools.combinations(numbers, r))\n\n        # For each combination, compute the product of the numbers\n        for comb in combinations:\n            product = reduce(mul, comb)\n\n            # Compute the logarithm of the product and add it to the sum\n            sum_logs += math.log(product)\n\n    return sum_logs", "raw_solution": "import math\nimport itertools\nfrom functools import reduce\nfrom operator import mul\n\ndef task_func(numbers):\n    \"\"\"\n    Generates all possible combinations of the provided numbers in a given list for each possible length.\n    For each combination, it computes the product of the numbers in the combination.\n    It then computes the logarithm of each product and sums these logarithms to produce the final result.\n    \"\"\"\n    # Initialize the sum of logarithms\n    sum_logs = 0\n\n    # Generate all possible combinations of the numbers\n    for r in range(1, len(numbers) + 1):\n        # Generate all combinations of length r\n        combinations = list(itertools.combinations(numbers, r))\n\n        # For each combination, compute the product of the numbers\n        for comb in combinations:\n            product = reduce(mul, comb)\n\n            # Compute the logarithm of the product and add it to the sum\n            sum_logs += math.log(product)\n\n    return sum_logs"}
{"task_id": "BigCodeBench/98", "solution": "import random\nimport string\nfrom collections import Counter\ndef task_func(num_strings, string_length):\n    # Generate a list of random strings\n    strings = [''.join(random.choices(string.ascii_lowercase, k=string_length)) for _ in range(num_strings)]\n\n    # Flatten the list of strings and count the frequency of each character\n    freq_counter = Counter(''.join(strings))\n\n    # Return the characters and their frequencies sorted by frequency in descending order\n    return sorted(freq_counter.items(), key=lambda x: x[1], reverse=True)", "raw_solution": "import random\nimport string\nfrom collections import Counter\n\ndef task_func(num_strings, string_length):\n    # Generate a list of random strings\n    strings = [''.join(random.choices(string.ascii_lowercase, k=string_length)) for _ in range(num_strings)]\n\n    # Flatten the list of strings and count the frequency of each character\n    freq_counter = Counter(''.join(strings))\n\n    # Return the characters and their frequencies sorted by frequency in descending order\n    return sorted(freq_counter.items(), key=lambda x: x[1], reverse=True)\n\n# Test the function"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target\n\n    # Set global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create pair plot\n    g = sns.pairplot(iris_df, hue='species', diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'k'})\n    g.fig.suptitle(\"Iris Dataset Pair Plot\", y=1.08)\n\n    return g.fig", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Load iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target\n\n    # Set global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create pair plot\n    g = sns.pairplot(iris_df, hue='species', diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'k'})\n    g.fig.suptitle(\"Iris Dataset Pair Plot\", y=1.08)\n\n    return g.fig"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\nfrom typing import Tuple\ndef task_func(seed=42) -> Tuple[plt.axes.Axes, pd.DataFrame]:\n    \"\"\"\n    Generates a plot of random time series data for the past 30 days with reproducibility controlled by an optional seed parameter.\n    The plot is styled with Arial font for better readability.\n    \"\"\"\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate a DataFrame with random data\n        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]\n        values = [random.randint(0, 100) for _ in range(30)]\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        df['Date'] = pd.to_datetime(df['Date'])\n\n        # Create the plot\n        plt.figure(figsize=(10, 6))\n        plt.plot(df['Date'], df['Value'])\n        plt.title('Random Time Series Data', fontname=\"Arial\")\n        plt.xlabel('Date', fontname=\"Arial\")\n        plt.ylabel('Value', fontname=\"Arial\")\n        plt.xticks(rotation=45, fontname=\"Arial\")\n        plt.yticks(fontname=\"Arial\")\n        plt.grid(True)\n\n        return plt.gca(), df\n\n    except Exception as e:\n        raise ValueError(f\"There was an issue generating the data or plot: {e}\")", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\nfrom typing import Tuple\n\ndef task_func(seed=42) -> Tuple[plt.axes.Axes, pd.DataFrame]:\n    \"\"\"\n    Generates a plot of random time series data for the past 30 days with reproducibility controlled by an optional seed parameter.\n    The plot is styled with Arial font for better readability.\n    \"\"\"\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate a DataFrame with random data\n        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]\n        values = [random.randint(0, 100) for _ in range(30)]\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        df['Date'] = pd.to_datetime(df['Date'])\n\n        # Create the plot\n        plt.figure(figsize=(10, 6))\n        plt.plot(df['Date'], df['Value'])\n        plt.title('Random Time Series Data', fontname=\"Arial\")\n        plt.xlabel('Date', fontname=\"Arial\")\n        plt.ylabel('Value', fontname=\"Arial\")\n        plt.xticks(rotation=45, fontname=\"Arial\")\n        plt.yticks(fontname=\"Arial\")\n        plt.grid(True)\n\n        return plt.gca(), df\n\n    except Exception as e:\n        raise ValueError(f\"There was an issue generating the data or plot: {e}\")"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.datasets import load_boston\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_to=None):\n    \"\"\"\n    Draws the correlation heatmap of the Boston Housing dataset using Seaborn.\n    If `save_to` is provided, saves the plot to the specified file.\n    \"\"\"\n    try:\n        # Load the dataset\n        boston = load_boston()\n        df = pd.DataFrame(boston.data, columns=boston.feature_names)\n\n        # Compute the correlation matrix\n        corr = df.corr()\n\n        # Generate a mask for the upper triangle\n        mask = np.triu(np.ones_like(corr, dtype=bool))\n\n        # Set up the matplotlib figure\n        f, ax = plt.subplots(figsize=(11, 9))\n\n        # Generate a custom diverging colormap\n        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n        if save_to:\n            plt.savefig(save_to)\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"Error in generating or saving the plot: {e}\")", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.datasets import load_boston\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_to=None):\n    \"\"\"\n    Draws the correlation heatmap of the Boston Housing dataset using Seaborn.\n    If `save_to` is provided, saves the plot to the specified file.\n    \"\"\"\n    try:\n        # Load the dataset\n        boston = load_boston()\n        df = pd.DataFrame(boston.data, columns=boston.feature_names)\n\n        # Compute the correlation matrix\n        corr = df.corr()\n\n        # Generate a mask for the upper triangle\n        mask = np.triu(np.ones_like(corr, dtype=bool))\n\n        # Set up the matplotlib figure\n        f, ax = plt.subplots(figsize=(11, 9))\n\n        # Generate a custom diverging colormap\n        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n        if save_to:\n            plt.savefig(save_to)\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"Error in generating or saving the plot: {e}\")"}
{"task_id": "BigCodeBench/102", "solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\ndef task_func():\n    # Load the diabetes dataset\n    diabetes = load_diabetes()\n\n    # Create a DataFrame from the dataset\n    df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n    df['target'] = diabetes.target\n\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pairplot\n    pairplot = sns.pairplot(df)\n\n    return pairplot, df", "raw_solution": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.datasets import load_diabetes\n\ndef task_func():\n    # Load the diabetes dataset\n    diabetes = load_diabetes()\n\n    # Create a DataFrame from the dataset\n    df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n    df['target'] = diabetes.target\n\n    # Set the font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pairplot\n    pairplot = sns.pairplot(df)\n\n    return pairplot, df"}
{"task_id": "BigCodeBench/103", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(temperatures):\n    \"\"\"\n    Calculate and plot the daytime temperatures for New York over a given period.\n    The plot uses Arial font for display.\n    \"\"\"\n    if not isinstance(temperatures, pd.DataFrame):\n        raise ValueError(\"Input DataFrame is not in the expected format.\")\n    if temperatures.empty:\n        raise ValueError(\"Input DataFrame is empty.\")\n    if 'Date' not in temperatures.columns or 'Temperature' not in temperatures.columns:\n        raise ValueError(\"Input DataFrame does not contain the expected columns.\")\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(temperatures['Date'], temperatures['Temperature'])\n    plt.xlabel('Date', fontsize=14, fontname='Arial')\n    plt.ylabel('Temperature (\u00b0C)', fontsize=14, fontname='Arial')\n    plt.title('Daily Temperatures in New York', fontsize=16, fontname='Arial')\n    plt.xticks(fontsize=12, fontname='Arial')\n    plt.yticks(fontsize=12, fontname='Arial')\n    plt.grid(True)\n    return plt.gca()", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(temperatures):\n    \"\"\"\n    Calculate and plot the daytime temperatures for New York over a given period.\n    The plot uses Arial font for display.\n    \"\"\"\n    if not isinstance(temperatures, pd.DataFrame):\n        raise ValueError(\"Input DataFrame is not in the expected format.\")\n    if temperatures.empty:\n        raise ValueError(\"Input DataFrame is empty.\")\n    if 'Date' not in temperatures.columns or 'Temperature' not in temperatures.columns:\n        raise ValueError(\"Input DataFrame does not contain the expected columns.\")\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(temperatures['Date'], temperatures['Temperature'])\n    plt.xlabel('Date', fontsize=14, fontname='Arial')\n    plt.ylabel('Temperature (\u00b0C)', fontsize=14, fontname='Arial')\n    plt.title('Daily Temperatures in New York', fontsize=16, fontname='Arial')\n    plt.xticks(fontsize=12, fontname='Arial')\n    plt.yticks(fontsize=12, fontname='Arial')\n    plt.grid(True)\n    return plt.gca()"}
{"task_id": "BigCodeBench/104", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\ndef task_func(df, groups=['A', 'B', 'C', 'D', 'E']):\n    \"\"\"\n    Analyzes the groups in a DataFrame by plotting a scatter plot of the ordinals against the values for each group.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if not all(col in df.columns for col in ['Date', 'Value', 'Group']):\n        raise ValueError(\"DataFrame lacks required columns: 'Date', 'Value', 'Group'\")\n\n    colors = cycle('rgbcmy')\n    fig, ax = plt.subplots()\n\n    for group, color in zip(groups, colors):\n        group_df = df[df['Group'] == group]\n        ax.scatter(group_df['Date'], group_df['Value'], color=color, label=group)\n\n    ax.set_title('Scatterplot of Values for Each Group Over Time')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\ndef task_func(df, groups=['A', 'B', 'C', 'D', 'E']):\n    \"\"\"\n    Analyzes the groups in a DataFrame by plotting a scatter plot of the ordinals against the values for each group.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if not all(col in df.columns for col in ['Date', 'Value', 'Group']):\n        raise ValueError(\"DataFrame lacks required columns: 'Date', 'Value', 'Group'\")\n\n    colors = cycle('rgbcmy')\n    fig, ax = plt.subplots()\n\n    for group, color in zip(groups, colors):\n        group_df = df[df['Group'] == group]\n        ax.scatter(group_df['Date'], group_df['Value'], color=color, label=group)\n\n    ax.set_title('Scatterplot of Values for Each Group Over Time')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/105", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\ndef task_func(df):\n    \"\"\"\n    Performs exploratory data analysis on a dataframe.\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"Dataframe is empty\")\n    if not all(col in df.columns for col in ['date']):\n        raise ValueError(\"Required columns are missing\")\n    if not is_datetime(df['date']):\n        raise ValueError(\"'date' column is not in datetime format\")\n\n    # Convert 'date' column to ordinal format\n    df['date'] = df['date'].map(dt.datetime.toordinal)\n\n    # Create a correlation matrix\n    corr = df.corr()\n\n    # Generate a pair plot\n    pair_grid = sns.pairplot(df, diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'k'})\n\n    # Create a heatmap for the correlation matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Correlation Matrix')\n    plt.show()\n\n    return pair_grid", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\n\ndef task_func(df):\n    \"\"\"\n    Performs exploratory data analysis on a dataframe.\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"Dataframe is empty\")\n    if not all(col in df.columns for col in ['date']):\n        raise ValueError(\"Required columns are missing\")\n    if not is_datetime(df['date']):\n        raise ValueError(\"'date' column is not in datetime format\")\n\n    # Convert 'date' column to ordinal format\n    df['date'] = df['date'].map(dt.datetime.toordinal)\n\n    # Create a correlation matrix\n    corr = df.corr()\n\n    # Generate a pair plot\n    pair_grid = sns.pairplot(df, diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'k'})\n\n    # Create a heatmap for the correlation matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Correlation Matrix')\n    plt.show()\n\n    return pair_grid"}
{"task_id": "BigCodeBench/106", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a valid DataFrame\")\n    if 'date' not in df.columns or 'value' not in df.columns:\n        raise ValueError(\"'df' lacks the required columns\")\n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"'date' column is not in datetime format\")\n\n    df['date'] = df['date'].map(dt.datetime.toordinal)\n    X = df['date'].values.reshape(-1, 1)\n    y = df['value'].values.reshape(-1, 1)\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    predictions = model.predict(X)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X, y, color='blue')\n    ax.plot(X, predictions, color='red')\n    ax.set_title('Value vs Date (Linear Regression Prediction)')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n\n    return model, predictions, ax", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a valid DataFrame\")\n    if 'date' not in df.columns or 'value' not in df.columns:\n        raise ValueError(\"'df' lacks the required columns\")\n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"'date' column is not in datetime format\")\n\n    df['date'] = df['date'].map(dt.datetime.toordinal)\n    X = df['date'].values.reshape(-1, 1)\n    y = df['value'].values.reshape(-1, 1)\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    predictions = model.predict(X)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X, y, color='blue')\n    ax.plot(X, predictions, color='red')\n    ax.set_title('Value vs Date (Linear Regression Prediction)')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n\n    return model, predictions, ax"}
{"task_id": "BigCodeBench/107", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(df, n_clusters=3, random_state=0):\n    \"\"\"\n    Converts the 'date' column of a DataFrame to ordinal, performs KMeans clustering on 'date' and 'value' columns, and plots the clusters.\n    \"\"\"\n    if df.empty or 'date' not in df.columns or 'value' not in df.columns:\n        raise ValueError(\"DataFrame is empty or lacks required columns.\")\n\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].map(dt.datetime.toordinal)\n\n    X = df[['date', 'value']].values\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(X)\n\n    labels = kmeans.labels_\n\n    fig, ax = plt.subplots()\n    ax.scatter(df['date'], df['value'], c=labels)\n\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('KMeans Clustering of Value vs Date')\n\n    return ax", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(df, n_clusters=3, random_state=0):\n    \"\"\"\n    Converts the 'date' column of a DataFrame to ordinal, performs KMeans clustering on 'date' and 'value' columns, and plots the clusters.\n    \"\"\"\n    if df.empty or 'date' not in df.columns or 'value' not in df.columns:\n        raise ValueError(\"DataFrame is empty or lacks required columns.\")\n\n    df['date'] = pd.to_datetime(df['date'])\n    df['date'] = df['date'].map(dt.datetime.toordinal)\n\n    X = df[['date', 'value']].values\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(X)\n\n    labels = kmeans.labels_\n\n    fig, ax = plt.subplots()\n    ax.scatter(df['date'], df['value'], c=labels)\n\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n    ax.set_title('KMeans Clustering of Value vs Date')\n\n    return ax"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n\n    # Check if df has 'value' column\n    if 'value' not in df.columns:\n        raise ValueError(\"'df' should have a 'value' column\")\n\n    # Check if 'freq' is a valid frequency string\n    try:\n        pd.to_datetime(df.index, freq=freq)\n    except ValueError:\n        raise ValueError(\"'freq' should be a valid frequency string\")\n\n    # Check if 'decomposition_model' is 'additive' or 'multiplicative'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' should be 'additive' or 'multiplicative'\")\n\n    # Decompose the time series\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, freq=freq)\n\n    # Plot the decomposition\n    fig, ax = plt.subplots(4, 1, figsize=(10, 10))\n    decomposition.trend.plot(ax=ax[0], legend=False)\n    ax[0].set_title('Trend')\n    decomposition.seasonal.plot(ax=ax[1], legend=False)\n    ax[1].set_title('Seasonality')\n    decomposition.resid.plot(ax=ax[2], legend=False)\n    ax[2].set_title('Residuals')\n    plt.tight_layout()\n\n    return (decomposition, ax[-1])", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n\n    # Check if df has 'value' column\n    if 'value' not in df.columns:\n        raise ValueError(\"'df' should have a 'value' column\")\n\n    # Check if 'freq' is a valid frequency string\n    try:\n        pd.to_datetime(df.index, freq=freq)\n    except ValueError:\n        raise ValueError(\"'freq' should be a valid frequency string\")\n\n    # Check if 'decomposition_model' is 'additive' or 'multiplicative'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' should be 'additive' or 'multiplicative'\")\n\n    # Decompose the time series\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, freq=freq)\n\n    # Plot the decomposition\n    fig, ax = plt.subplots(4, 1, figsize=(10, 10))\n    decomposition.trend.plot(ax=ax[0], legend=False)\n    ax[0].set_title('Trend')\n    decomposition.seasonal.plot(ax=ax[1], legend=False)\n    ax[1].set_title('Seasonality')\n    decomposition.resid.plot(ax=ax[2], legend=False)\n    ax[2].set_title('Residuals')\n    plt.tight_layout()\n\n    return (decomposition, ax[-1])"}
{"task_id": "BigCodeBench/109", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, items=None, locations=None):\n    \"\"\"\n    Generates a bar chart representing the distribution of specified items across given locations.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n    if 'Item' not in df.columns or 'Location' not in df.columns:\n        raise ValueError(\"'df' should have 'Item' and 'Location' columns\")\n\n    if items is not None:\n        df = df[df['Item'].isin(items)]\n    if locations is not None:\n        df = df[df['Location'].isin(locations)]\n\n    item_counts = df['Item'].value_counts()\n    location_counts = df['Location'].value_counts()\n\n    fig, ax = plt.subplots()\n    ax.bar(item_counts.index, item_counts.values, label='Items')\n    ax.bar(location_counts.index, location_counts.values, label='Locations')\n    ax.legend()\n    plt.xlabel('Item/Location')\n    plt.ylabel('Count')\n    plt.title('Distribution of Items and Locations')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, items=None, locations=None):\n    \"\"\"\n    Generates a bar chart representing the distribution of specified items across given locations.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n    if 'Item' not in df.columns or 'Location' not in df.columns:\n        raise ValueError(\"'df' should have 'Item' and 'Location' columns\")\n\n    if items is not None:\n        df = df[df['Item'].isin(items)]\n    if locations is not None:\n        df = df[df['Location'].isin(locations)]\n\n    item_counts = df['Item'].value_counts()\n    location_counts = df['Location'].value_counts()\n\n    fig, ax = plt.subplots()\n    ax.bar(item_counts.index, item_counts.values, label='Items')\n    ax.bar(location_counts.index, location_counts.values, label='Locations')\n    ax.legend()\n    plt.xlabel('Item/Location')\n    plt.ylabel('Count')\n    plt.title('Distribution of Items and Locations')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    return ax"}
{"task_id": "BigCodeBench/110", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    \"\"\"\n    Draws and returns the daily turnover line chart from a pandas DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if 'Date' not in df.columns or 'Sales' not in df.columns:\n        raise ValueError(\"'df' lacks 'Date' or 'Sales' columns\")\n    if df.empty:\n        raise ValueError(\"'df' has no data to plot\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    df['Sales'].plot(ax=ax, kind='line', title='Daily Turnover')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    Draws and returns the daily turnover line chart from a pandas DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if 'Date' not in df.columns or 'Sales' not in df.columns:\n        raise ValueError(\"'df' lacks 'Date' or 'Sales' columns\")\n    if df.empty:\n        raise ValueError(\"'df' has no data to plot\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    df['Sales'].plot(ax=ax, kind='line', title='Daily Turnover')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n\n    return ax"}
{"task_id": "BigCodeBench/111", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    \"\"\"\n    Draws and returns a heat map with temperature data from a pandas DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if not all(col in df.columns for col in ['Date', 'Time', 'Temperature']):\n        raise ValueError(\"'df' lacks 'Date', 'Time', or 'Temperature' columns\")\n\n    df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n    df.set_index('DateTime', inplace=True)\n    df.drop(['Date', 'Time'], axis=1, inplace=True)\n\n    df_pivot = df.pivot_table(values='Temperature', index=df.index.hour, columns=df.index.dayofweek, aggfunc='mean')\n\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(df_pivot, cmap='coolwarm', cbar_kws={'label': 'Temperature'}, fmt=\".2f\")\n    heatmap.set_xlabel('Day of Week')\n    heatmap.set_ylabel('Hour of Day')\n    heatmap.set_title('Temperature Heatmap')\n\n    return heatmap", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    Draws and returns a heat map with temperature data from a pandas DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if not all(col in df.columns for col in ['Date', 'Time', 'Temperature']):\n        raise ValueError(\"'df' lacks 'Date', 'Time', or 'Temperature' columns\")\n\n    df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n    df.set_index('DateTime', inplace=True)\n    df.drop(['Date', 'Time'], axis=1, inplace=True)\n\n    df_pivot = df.pivot_table(values='Temperature', index=df.index.hour, columns=df.index.dayofweek, aggfunc='mean')\n\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(df_pivot, cmap='coolwarm', cbar_kws={'label': 'Temperature'}, fmt=\".2f\")\n    heatmap.set_xlabel('Day of Week')\n    heatmap.set_ylabel('Hour of Day')\n    heatmap.set_title('Temperature Heatmap')\n\n    return heatmap"}
{"task_id": "BigCodeBench/112", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    \"\"\"\n    Draws a pie chart of the status distribution from a pandas DataFrame with a 'Status' column.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a pandas DataFrame\")\n    if 'Status' not in df.columns:\n        raise ValueError(\"DataFrame does not contain the 'Status' column\")\n\n    status_counts = df['Status'].value_counts()\n    fig, ax = plt.subplots()\n    ax.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%')\n    ax.set_title('Status Distribution')\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    Draws a pie chart of the status distribution from a pandas DataFrame with a 'Status' column.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a pandas DataFrame\")\n    if 'Status' not in df.columns:\n        raise ValueError(\"DataFrame does not contain the 'Status' column\")\n\n    status_counts = df['Status'].value_counts()\n    fig, ax = plt.subplots()\n    ax.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%')\n    ax.set_title('Status Distribution')\n    return ax"}
{"task_id": "BigCodeBench/113", "solution": "import json\nfrom collections import Counter\nimport random\nimport os\ndef task_func(my_dict, keys):\n    if len(set(keys)) != 10:\n        raise ValueError(\"'keys' must contain exactly 10 unique elements\")\n\n    for key in keys:\n        my_dict[key] = random.randint(1, 100)\n\n    json_file = 'updated_dictionary.json'\n    with open(json_file, 'w') as f:\n        json.dump(my_dict, f)\n\n    counter = Counter(my_dict)\n    txt_file = 'key_frequencies.txt'\n    with open(txt_file, 'w') as f:\n        for key, count in counter.items():\n            f.write(f\"{key}: {count}\\n\")\n\n    return (my_dict, json_file, txt_file)", "raw_solution": "import json\nfrom collections import Counter\nimport random\nimport os\n\ndef task_func(my_dict, keys):\n    if len(set(keys)) != 10:\n        raise ValueError(\"'keys' must contain exactly 10 unique elements\")\n\n    for key in keys:\n        my_dict[key] = random.randint(1, 100)\n\n    json_file = 'updated_dictionary.json'\n    with open(json_file, 'w') as f:\n        json.dump(my_dict, f)\n\n    counter = Counter(my_dict)\n    txt_file = 'key_frequencies.txt'\n    with open(txt_file, 'w') as f:\n        for key, count in counter.items():\n            f.write(f\"{key}: {count}\\n\")\n\n    return (my_dict, json_file, txt_file)"}
{"task_id": "BigCodeBench/114", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(my_dict):\n    \"\"\"\n    Updates a dictionary by adding a normalized version of a numpy array found under the 'array' key.\n    The normalization is performed using MinMaxScaler, scaling each value to fall between 0 and 1.\n    \"\"\"\n    if 'array' not in my_dict:\n        raise KeyError(\"The 'array' key is not found in the dictionary.\")\n    if not isinstance(my_dict['array'], np.ndarray):\n        raise TypeError(\"The value of the 'array' key is not a numpy array.\")\n\n    array = my_dict['array']\n    if array.ndim != 1:\n        raise ValueError(\"The 'array' key's value should be a 1-dimensional numpy array.\")\n\n    scaler = MinMaxScaler()\n    normalized_array = scaler.fit_transform(array.reshape(-1, 1))\n    my_dict['normalized_array'] = normalized_array.flatten()", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(my_dict):\n    \"\"\"\n    Updates a dictionary by adding a normalized version of a numpy array found under the 'array' key.\n    The normalization is performed using MinMaxScaler, scaling each value to fall between 0 and 1.\n    \"\"\"\n    if 'array' not in my_dict:\n        raise KeyError(\"The 'array' key is not found in the dictionary.\")\n    if not isinstance(my_dict['array'], np.ndarray):\n        raise TypeError(\"The value of the 'array' key is not a numpy array.\")\n\n    array = my_dict['array']\n    if array.ndim != 1:\n        raise ValueError(\"The 'array' key's value should be a 1-dimensional numpy array.\")\n\n    scaler = MinMaxScaler()\n    normalized_array = scaler.fit_transform(array.reshape(-1, 1))\n    my_dict['normalized_array'] = normalized_array.flatten()"}
{"task_id": "BigCodeBench/115", "solution": "import numpy as np\nfrom scipy.stats import mode\nfrom scipy.stats import entropy\ndef task_func(numbers):\n    if not numbers:\n        raise ValueError(\"Input list 'numbers' cannot be empty\")\n    arr = np.array(numbers)\n    result = {}\n    result['mode'] = mode(arr)[0][0]\n    result['entropy'] = entropy(arr, base=2)\n    return result", "raw_solution": "import numpy as np\nfrom scipy.stats import mode\nfrom scipy.stats import entropy\n\ndef task_func(numbers):\n    if not numbers:\n        raise ValueError(\"Input list 'numbers' cannot be empty\")\n    arr = np.array(numbers)\n    result = {}\n    result['mode'] = mode(arr)[0][0]\n    result['entropy'] = entropy(arr, base=2)\n    return result"}
{"task_id": "BigCodeBench/116", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(mu, sigma, sample_size):\n    \"\"\"\n    Generates a numpy array of random samples drawn from a normal distribution and plots the histogram of these samples.\n    \"\"\"\n    # Generate samples\n    samples = np.random.normal(mu, sigma, sample_size)\n\n    # Plot histogram\n    plt.hist(samples, bins=30, alpha=0.75, color='blue')\n    plt.xlabel(\"Sample values\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Histogram of Generated Samples\")\n    plt.show()\n\n    return samples", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(mu, sigma, sample_size):\n    \"\"\"\n    Generates a numpy array of random samples drawn from a normal distribution and plots the histogram of these samples.\n    \"\"\"\n    # Generate samples\n    samples = np.random.normal(mu, sigma, sample_size)\n\n    # Plot histogram\n    plt.hist(samples, bins=30, alpha=0.75, color='blue')\n    plt.xlabel(\"Sample values\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Histogram of Generated Samples\")\n    plt.show()\n\n    return samples"}
{"task_id": "BigCodeBench/117", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, seed as set_seed\ndef task_func(num_of_students, seed=42, name_list=None, gender_list=None, age_range=(15, 20), score_range=(50, 100)):\n    if num_of_students <= 0:\n        raise ValueError(\"num_of_students must be positive\")\n\n    if name_list is None:\n        name_list = [\"John\", \"Jane\", \"Mary\", \"James\", \"Jennifer\", \"Maria\", \"Michael\", \"Jessica\", \"Sarah\", \"Jacob\"]\n    if gender_list is None:\n        gender_list = [\"Male\", \"Female\"]\n\n    set_seed(seed)\n    data = {\n        \"Name\": [choice(name_list) for _ in range(num_of_students)],\n        \"Age\": [np.random.randint(*age_range) for _ in range(num_of_students)],\n        \"Gender\": [choice(gender_list) for _ in range(num_of_students)],\n        \"Score\": [np.random.randint(*score_range) for _ in range(num_of_students)],\n    }\n    return pd.DataFrame(data)", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, seed as set_seed\n\ndef task_func(num_of_students, seed=42, name_list=None, gender_list=None, age_range=(15, 20), score_range=(50, 100)):\n    if num_of_students <= 0:\n        raise ValueError(\"num_of_students must be positive\")\n\n    if name_list is None:\n        name_list = [\"John\", \"Jane\", \"Mary\", \"James\", \"Jennifer\", \"Maria\", \"Michael\", \"Jessica\", \"Sarah\", \"Jacob\"]\n    if gender_list is None:\n        gender_list = [\"Male\", \"Female\"]\n\n    set_seed(seed)\n    data = {\n        \"Name\": [choice(name_list) for _ in range(num_of_students)],\n        \"Age\": [np.random.randint(*age_range) for _ in range(num_of_students)],\n        \"Gender\": [choice(gender_list) for _ in range(num_of_students)],\n        \"Score\": [np.random.randint(*score_range) for _ in range(num_of_students)],\n    }\n    return pd.DataFrame(data)\n\n# Test the function"}
{"task_id": "BigCodeBench/118", "solution": "import os\nimport shutil\ndef task_func(directory, backup_directory):\n    \"\"\"\n    Scans a specified directory for JSON files and copies them to a backup directory.\n    If the backup directory does not exist, it is created.\n    The function returns a list of paths to the copied files in the backup directory.\n    \"\"\"\n    # Check if backup directory exists, if not create it\n    if not os.path.exists(backup_directory):\n        os.makedirs(backup_directory)\n\n    # List to store paths to copied files\n    copied_files = []\n\n    # Iterate over files in the directory\n    for filename in os.listdir(directory):\n        # Check if file is a JSON file\n        if filename.endswith('.json'):\n            # Construct full file path\n            source_path = os.path.join(directory, filename)\n            # Construct destination path\n            destination_path = os.path.join(backup_directory, filename)\n            # Copy file to backup directory\n            shutil.copy2(source_path, destination_path)\n            # Add path to copied files list\n            copied_files.append(destination_path)\n\n    # Return list of paths to copied files\n    return copied_files", "raw_solution": "import os\nimport shutil\n\ndef task_func(directory, backup_directory):\n    \"\"\"\n    Scans a specified directory for JSON files and copies them to a backup directory.\n    If the backup directory does not exist, it is created.\n    The function returns a list of paths to the copied files in the backup directory.\n    \"\"\"\n    # Check if backup directory exists, if not create it\n    if not os.path.exists(backup_directory):\n        os.makedirs(backup_directory)\n\n    # List to store paths to copied files\n    copied_files = []\n\n    # Iterate over files in the directory\n    for filename in os.listdir(directory):\n        # Check if file is a JSON file\n        if filename.endswith('.json'):\n            # Construct full file path\n            source_path = os.path.join(directory, filename)\n            # Construct destination path\n            destination_path = os.path.join(backup_directory, filename)\n            # Copy file to backup directory\n            shutil.copy2(source_path, destination_path)\n            # Add path to copied files list\n            copied_files.append(destination_path)\n\n    # Return list of paths to copied files\n    return copied_files"}
{"task_id": "BigCodeBench/119", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    x = np.linspace(-10, 10, 400)\n    y = x**2\n\n    plt.plot(x, y)\n    plt.title('y = x^2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.show()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    x = np.linspace(-10, 10, 400)\n    y = x**2\n\n    plt.plot(x, y)\n    plt.title('y = x^2')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.grid(True)\n    plt.show()\n\n# Test the function\ntask_func()"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generates a pandas Series of random dates within a specified date range, including both start_date and end_date, with an optional seed for reproducibility.\n    \"\"\"\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"'start_date' and 'end_date' must be datetime.datetime instances\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' must be earlier than 'end_date'\")\n\n    random_seed(seed)\n    time_delta = end_date - start_date\n    days_range = [start_date + timedelta(days=i) for i in range(time_delta.days + 1)]\n\n    random_dates = pd.Series(days_range).sample(frac=1, random_state=seed)\n    return random_dates\nstart_date = datetime(2020, 1, 1)\nend_date = datetime(2020, 12, 31)\nseed = 42", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generates a pandas Series of random dates within a specified date range, including both start_date and end_date, with an optional seed for reproducibility.\n    \"\"\"\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"'start_date' and 'end_date' must be datetime.datetime instances\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' must be earlier than 'end_date'\")\n\n    random_seed(seed)\n    time_delta = end_date - start_date\n    days_range = [start_date + timedelta(days=i) for i in range(time_delta.days + 1)]\n\n    random_dates = pd.Series(days_range).sample(frac=1, random_state=seed)\n    return random_dates\n\n# Testing the function\nstart_date = datetime(2020, 1, 1)\nend_date = datetime(2020, 12, 31)\nseed = 42\nrandom_dates = task_func(start_date, end_date, seed)"}
{"task_id": "BigCodeBench/121", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(my_list, seed=42):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n\n    np.random.seed(seed)\n\n    categories = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n    sales_data = pd.DataFrame(np.random.randint(100, 500, size=(12, len(categories))), columns=categories)\n    sales_data.index = pd.date_range(start='01-01-2022', periods=12, freq='M')\n\n    my_list.append(\"12\")\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sales_data.plot(kind='bar', ax=ax)\n    ax.set_title('Monthly Sales Data')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n\n    return (sales_data, ax)", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, seed=42):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n\n    np.random.seed(seed)\n\n    categories = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n    sales_data = pd.DataFrame(np.random.randint(100, 500, size=(12, len(categories))), columns=categories)\n    sales_data.index = pd.date_range(start='01-01-2022', periods=12, freq='M')\n\n    my_list.append(\"12\")\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sales_data.plot(kind='bar', ax=ax)\n    ax.set_title('Monthly Sales Data')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n\n    return (sales_data, ax)"}
{"task_id": "BigCodeBench/122", "solution": "import numpy as np\nimport random\ndef task_func(my_list):\n    my_list.append(random.randint(0, 100))\n    return np.random.random(sum(my_list))\nmy_list = [1, 2, 3]", "raw_solution": "import numpy as np\nimport random\n\ndef task_func(my_list):\n    my_list.append(random.randint(0, 100))\n    return np.random.random(sum(my_list))\n\n# Testing the function\nmy_list = [1, 2, 3]"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    Modifies a list by adding the element '12', then concatenates a number of CSV files from a directory into a single DataFrame.\n    The number of files concatenated is determined by the sum of the numbers in the list.\n    \"\"\"\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list.\")\n    my_list.append('12')\n\n    # Get a list of all CSV files in the directory\n    file_list = glob.glob(os.path.join(file_dir, f\"*{file_ext}\"))\n    if not file_list:\n        raise FileNotFoundError(f\"No files found in the directory {file_dir} with extension {file_ext}.\")\n\n    # Concatenate the CSV files\n    df_list = []\n    for file in file_list[:sum(my_list)]:\n        df = pd.read_csv(file)\n        df_list.append(df)\n    df_concat = pd.concat(df_list, ignore_index=True)\n\n    return df_concat", "raw_solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    Modifies a list by adding the element '12', then concatenates a number of CSV files from a directory into a single DataFrame.\n    The number of files concatenated is determined by the sum of the numbers in the list.\n    \"\"\"\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list.\")\n    my_list.append('12')\n\n    # Get a list of all CSV files in the directory\n    file_list = glob.glob(os.path.join(file_dir, f\"*{file_ext}\"))\n    if not file_list:\n        raise FileNotFoundError(f\"No files found in the directory {file_dir} with extension {file_ext}.\")\n\n    # Concatenate the CSV files\n    df_list = []\n    for file in file_list[:sum(my_list)]:\n        df = pd.read_csv(file)\n        df_list.append(df)\n    df_concat = pd.concat(df_list, ignore_index=True)\n\n    return df_concat"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"'my_list' should contain only numeric elements\")\n\n    my_list.append(12)\n    total_sum = sum(my_list)\n    if total_sum > size:\n        raise ValueError(\"Sum of elements in 'my_list' exceeds 'size'\")\n\n    random_seed(seed)\n    start_time = time.time()\n    random_list = [randint(1, 100) for _ in range(total_sum)]\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 101), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return (time_taken, ax)", "raw_solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' should be a list\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"'my_list' should contain only numeric elements\")\n\n    my_list.append(12)\n    total_sum = sum(my_list)\n    if total_sum > size:\n        raise ValueError(\"Sum of elements in 'my_list' exceeds 'size'\")\n\n    random_seed(seed)\n    start_time = time.time()\n    random_list = [randint(1, 100) for _ in range(total_sum)]\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 101), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return (time_taken, ax)"}
{"task_id": "BigCodeBench/125", "solution": "from collections import defaultdict\nimport itertools\nimport json\nimport random\nimport string\ndef task_func(LETTERS, n):\n    # Generate all possible combinations of a given set of letters of length 'n'\n    combinations = [''.join(p) for p in itertools.product(LETTERS, repeat=n)]\n\n    # Count the occurrences of each letter in these combinations\n    letter_counts = defaultdict(int)\n    for combination in combinations:\n        for letter in combination:\n            letter_counts[letter] += 1\n\n    # Save the results in a JSON file\n    filename = f\"prefix_{random.randint(0, 100)}.json\"\n    with open(filename, 'w') as f:\n        json.dump(letter_counts, f)\n\n    return filename", "raw_solution": "from collections import defaultdict\nimport itertools\nimport json\nimport random\nimport string\n\ndef task_func(LETTERS, n):\n    # Generate all possible combinations of a given set of letters of length 'n'\n    combinations = [''.join(p) for p in itertools.product(LETTERS, repeat=n)]\n\n    # Count the occurrences of each letter in these combinations\n    letter_counts = defaultdict(int)\n    for combination in combinations:\n        for letter in combination:\n            letter_counts[letter] += 1\n\n    # Save the results in a JSON file\n    filename = f\"prefix_{random.randint(0, 100)}.json\"\n    with open(filename, 'w') as f:\n        json.dump(letter_counts, f)\n\n    return filename"}
{"task_id": "BigCodeBench/126", "solution": "import pandas as pd\nfrom random import randint, seed as random_seed\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(animals=None, seed=42):\n    \"\"\"\n    Generates a report on the number of animals in a zoo.\n    For each animal, generates a random count within a specified range,\n    calculates the mean, median, and standard deviation of these counts,\n    and returns a DataFrame with these statistics.\n    \"\"\"\n    if animals is None:\n        animals = ['Lion', 'Tiger', 'Bear', 'Monkey', 'Elephant', 'Giraffe', 'Hippo', 'Rhino', 'Leopard', 'Cheetah']\n\n    random_seed(seed)\n\n    data = {animal: [randint(1, 100) for _ in range(10)] for animal in animals}\n\n    df = pd.DataFrame(data)\n\n    statistics_df = pd.DataFrame(columns=['Animal', 'Mean', 'Median', 'Standard Deviation'])\n\n    for animal in animals:\n        counts = data[animal]\n        statistics_df = statistics_df.append({\n            'Animal': animal,\n            'Mean': statistics.mean(counts),\n            'Median': statistics.median(counts),\n            'Standard Deviation': statistics.stdev(counts)\n        }, ignore_index=True)\n\n    # Plotting the counts\n    df.plot(kind='bar', title='Animal Counts')\n    plt.ylabel('Count')\n    plt.xlabel('Animal')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return statistics_df", "raw_solution": "import pandas as pd\nfrom random import randint, seed as random_seed\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef task_func(animals=None, seed=42):\n    \"\"\"\n    Generates a report on the number of animals in a zoo.\n    For each animal, generates a random count within a specified range,\n    calculates the mean, median, and standard deviation of these counts,\n    and returns a DataFrame with these statistics.\n    \"\"\"\n    if animals is None:\n        animals = ['Lion', 'Tiger', 'Bear', 'Monkey', 'Elephant', 'Giraffe', 'Hippo', 'Rhino', 'Leopard', 'Cheetah']\n\n    random_seed(seed)\n\n    data = {animal: [randint(1, 100) for _ in range(10)] for animal in animals}\n\n    df = pd.DataFrame(data)\n\n    statistics_df = pd.DataFrame(columns=['Animal', 'Mean', 'Median', 'Standard Deviation'])\n\n    for animal in animals:\n        counts = data[animal]\n        statistics_df = statistics_df.append({\n            'Animal': animal,\n            'Mean': statistics.mean(counts),\n            'Median': statistics.median(counts),\n            'Standard Deviation': statistics.stdev(counts)\n        }, ignore_index=True)\n\n    # Plotting the counts\n    df.plot(kind='bar', title='Animal Counts')\n    plt.ylabel('Count')\n    plt.xlabel('Animal')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return statistics_df"}
{"task_id": "BigCodeBench/127", "solution": "import os\nimport shutil\nimport hashlib\ndef task_func(ROOT_DIR, DEST_DIR, SPECIFIC_HASH):\n    if not os.path.exists(ROOT_DIR):\n        raise ValueError(f\"Root directory {ROOT_DIR} does not exist.\")\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    count = 0\n    for filename in os.listdir(ROOT_DIR):\n        file_path = os.path.join(ROOT_DIR, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'rb') as f:\n                file_hash = hashlib.md5(f.read()).hexdigest()\n            if file_hash == SPECIFIC_HASH:\n                shutil.move(file_path, DEST_DIR)\n                count += 1\n    return count", "raw_solution": "import os\nimport shutil\nimport hashlib\n\ndef task_func(ROOT_DIR, DEST_DIR, SPECIFIC_HASH):\n    if not os.path.exists(ROOT_DIR):\n        raise ValueError(f\"Root directory {ROOT_DIR} does not exist.\")\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    count = 0\n    for filename in os.listdir(ROOT_DIR):\n        file_path = os.path.join(ROOT_DIR, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'rb') as f:\n                file_hash = hashlib.md5(f.read()).hexdigest()\n            if file_hash == SPECIFIC_HASH:\n                shutil.move(file_path, DEST_DIR)\n                count += 1\n    return count"}
{"task_id": "BigCodeBench/128", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(POINTS=100):\n    # Initialize arrays to store x and y coordinates\n    x_coords = np.zeros(POINTS)\n    y_coords = np.zeros(POINTS)\n\n    # Define directions\n    directions = ['up', 'down', 'left', 'right']\n\n    # Simulate random walk\n    for i in range(1, POINTS):\n        direction = directions[randint(0, 3)]\n        if direction == 'up':\n            y_coords[i] = y_coords[i - 1] + 1\n            x_coords[i] = x_coords[i - 1]\n        elif direction == 'down':\n            y_coords[i] = y_coords[i - 1] - 1\n            x_coords[i] = x_coords[i - 1]\n        elif direction == 'left':\n            x_coords[i] = x_coords[i - 1] - 1\n            y_coords[i] = y_coords[i - 1]\n        elif direction == 'right':\n            x_coords[i] = x_coords[i - 1] + 1\n            y_coords[i] = y_coords[i - 1]\n\n    # Plot the path\n    plt.figure(figsize=(10, 10))\n    plt.plot(x_coords, y_coords)\n    plt.title('Random Walk')\n    plt.xlabel('X Coordinates')\n    plt.ylabel('Y Coordinates')\n    plt.grid(True)\n\n    return plt.gcf()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef task_func(POINTS=100):\n    # Initialize arrays to store x and y coordinates\n    x_coords = np.zeros(POINTS)\n    y_coords = np.zeros(POINTS)\n\n    # Define directions\n    directions = ['up', 'down', 'left', 'right']\n\n    # Simulate random walk\n    for i in range(1, POINTS):\n        direction = directions[randint(0, 3)]\n        if direction == 'up':\n            y_coords[i] = y_coords[i - 1] + 1\n            x_coords[i] = x_coords[i - 1]\n        elif direction == 'down':\n            y_coords[i] = y_coords[i - 1] - 1\n            x_coords[i] = x_coords[i - 1]\n        elif direction == 'left':\n            x_coords[i] = x_coords[i - 1] - 1\n            y_coords[i] = y_coords[i - 1]\n        elif direction == 'right':\n            x_coords[i] = x_coords[i - 1] + 1\n            y_coords[i] = y_coords[i - 1]\n\n    # Plot the path\n    plt.figure(figsize=(10, 10))\n    plt.plot(x_coords, y_coords)\n    plt.title('Random Walk')\n    plt.xlabel('X Coordinates')\n    plt.ylabel('Y Coordinates')\n    plt.grid(True)\n\n    return plt.gcf()"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom requests.exceptions import ConnectionError, HTTPError\ndef task_func(url='http://example.com'):\n    \"\"\"\n    Scrapes the first table from a web page and extracts data into a Pandas DataFrame.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except (ConnectionError, HTTPError) as e:\n        raise e\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find_all('table')\n\n    if not table:\n        raise ValueError(\"No table data found on the page\")\n\n    table = table[0]\n    rows = table.find_all('tr')\n\n    if not rows:\n        raise ValueError(\"No table data found on the page\")\n\n    header = [th.text.strip() for th in rows[0].find_all('th')]\n    data = []\n\n    for row in rows[1:]:\n        cols = row.find_all('td')\n        cols = [col.text.strip() for col in cols]\n        data.append(cols)\n\n    df = pd.DataFrame(data, columns=header)\n\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom requests.exceptions import ConnectionError, HTTPError\n\ndef task_func(url='http://example.com'):\n    \"\"\"\n    Scrapes the first table from a web page and extracts data into a Pandas DataFrame.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except (ConnectionError, HTTPError) as e:\n        raise e\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find_all('table')\n\n    if not table:\n        raise ValueError(\"No table data found on the page\")\n\n    table = table[0]\n    rows = table.find_all('tr')\n\n    if not rows:\n        raise ValueError(\"No table data found on the page\")\n\n    header = [th.text.strip() for th in rows[0].find_all('th')]\n    data = []\n\n    for row in rows[1:]:\n        cols = row.find_all('td')\n        cols = [col.text.strip() for col in cols]\n        data.append(cols)\n\n    df = pd.DataFrame(data, columns=header)\n\n    return df"}
{"task_id": "BigCodeBench/130", "solution": "import base64\nimport binascii\nimport os\nimport hashlib\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    hex_bytes = binascii.unhexlify(hex_str)\n\n    # Generate a random salt of the specified size\n    salt = os.urandom(salt_size)\n\n    # Append the salt to the byte representation of the hex string\n    salted_data = salt + hex_bytes\n\n    # Compute the SHA256 hash of the salted data\n    hash_obj = hashlib.sha256()\n    hash_obj.update(salted_data)\n    hash_value = hash_obj.digest()\n\n    # Return the salt and hash as a tuple\n    return (base64.b64encode(salt).decode(), base64.b64encode(hash_value).decode())", "raw_solution": "import base64\nimport binascii\nimport os\nimport hashlib\n\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    hex_bytes = binascii.unhexlify(hex_str)\n\n    # Generate a random salt of the specified size\n    salt = os.urandom(salt_size)\n\n    # Append the salt to the byte representation of the hex string\n    salted_data = salt + hex_bytes\n\n    # Compute the SHA256 hash of the salted data\n    hash_obj = hashlib.sha256()\n    hash_obj.update(salted_data)\n    hash_value = hash_obj.digest()\n\n    # Return the salt and hash as a tuple\n    return (base64.b64encode(salt).decode(), base64.b64encode(hash_value).decode())"}
{"task_id": "BigCodeBench/131", "solution": "import base64\nimport binascii\nimport os\nimport hashlib\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    hex_bytes = binascii.unhexlify(hex_str)\n\n    # Generate a random salt of the given size\n    salt = os.urandom(salt_size)\n\n    # Append the salt to the byte representation of the hex string\n    salted_data = salt + hex_bytes\n\n    # Compute the SHA256 hash of the salted data\n    hash_obj = hashlib.sha256()\n    hash_obj.update(salted_data)\n    hash_value = hash_obj.digest()\n\n    # Return the salt and hash as a tuple\n    return (base64.b64encode(salt).decode(), base64.b64encode(hash_value).decode())\nhex_str = \"deadbeef\"\nsalt_size = 16", "raw_solution": "import base64\nimport binascii\nimport os\nimport hashlib\n\ndef task_func(hex_str, salt_size):\n    # Convert hex string to bytes\n    hex_bytes = binascii.unhexlify(hex_str)\n\n    # Generate a random salt of the given size\n    salt = os.urandom(salt_size)\n\n    # Append the salt to the byte representation of the hex string\n    salted_data = salt + hex_bytes\n\n    # Compute the SHA256 hash of the salted data\n    hash_obj = hashlib.sha256()\n    hash_obj.update(salted_data)\n    hash_value = hash_obj.digest()\n\n    # Return the salt and hash as a tuple\n    return (base64.b64encode(salt).decode(), base64.b64encode(hash_value).decode())\n\n# Test the function\nhex_str = \"deadbeef\"\nsalt_size = 16"}
{"task_id": "BigCodeBench/132", "solution": "import binascii\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom collections import Counter\ndef task_func(hex_str):\n    if not isinstance(hex_str, str):\n        raise ValueError(\"'hex_str' must be a string\")\n\n    # Remove '\\\\x' prefix if present\n    if hex_str.startswith('\\\\x'):\n        hex_str = hex_str[2:]\n\n    # Check if hex_str is a valid hex string\n    try:\n        binascii.unhexlify(hex_str)\n    except binascii.Error:\n        raise ValueError(\"'hex_str' is not a valid hex string\")\n\n    # Convert hex string to bytes\n    bytes_obj = binascii.unhexlify(hex_str)\n\n    # Count frequency of each byte value\n    byte_freq = Counter(bytes_obj)\n\n    # Create DataFrame\n    df = pd.DataFrame(list(byte_freq.items()), columns=['Byte Value', 'Frequency'])\n\n    # Create plot\n    fig, ax = plt.subplots()\n    df.plot(x='Byte Value', y='Frequency', kind='bar', ax=ax)\n    ax.set_xlabel('Byte Value')\n    ax.set_ylabel('Frequency')\n\n    return (df, ax.figure)", "raw_solution": "import binascii\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom collections import Counter\n\ndef task_func(hex_str):\n    if not isinstance(hex_str, str):\n        raise ValueError(\"'hex_str' must be a string\")\n\n    # Remove '\\\\x' prefix if present\n    if hex_str.startswith('\\\\x'):\n        hex_str = hex_str[2:]\n\n    # Check if hex_str is a valid hex string\n    try:\n        binascii.unhexlify(hex_str)\n    except binascii.Error:\n        raise ValueError(\"'hex_str' is not a valid hex string\")\n\n    # Convert hex string to bytes\n    bytes_obj = binascii.unhexlify(hex_str)\n\n    # Count frequency of each byte value\n    byte_freq = Counter(bytes_obj)\n\n    # Create DataFrame\n    df = pd.DataFrame(list(byte_freq.items()), columns=['Byte Value', 'Frequency'])\n\n    # Create plot\n    fig, ax = plt.subplots()\n    df.plot(x='Byte Value', y='Frequency', kind='bar', ax=ax)\n    ax.set_xlabel('Byte Value')\n    ax.set_ylabel('Frequency')\n\n    return (df, ax.figure)"}
{"task_id": "BigCodeBench/133", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(df):\n    \"\"\"\n    Normalizes the last column of the DataFrame using MinMaxScaler from sklearn and plots the normalized data.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame.\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n\n    # Get the last column name\n    last_column = df.columns[-1]\n\n    # Normalize the last column\n    scaler = MinMaxScaler()\n    df[last_column] = scaler.fit_transform(df[[last_column]])\n\n    # Plot the normalized data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df[last_column])\n    plt.title(f'Normalized Data of {last_column}')\n    plt.xlabel('Index')\n    plt.ylabel('Normalized Value')\n    plt.tight_layout()\n\n    return df, plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(df):\n    \"\"\"\n    Normalizes the last column of the DataFrame using MinMaxScaler from sklearn and plots the normalized data.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame.\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n\n    # Get the last column name\n    last_column = df.columns[-1]\n\n    # Normalize the last column\n    scaler = MinMaxScaler()\n    df[last_column] = scaler.fit_transform(df[[last_column]])\n\n    # Plot the normalized data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df[last_column])\n    plt.title(f'Normalized Data of {last_column}')\n    plt.xlabel('Index')\n    plt.ylabel('Normalized Value')\n    plt.tight_layout()\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/134", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=20):\n    \"\"\"\n    Draws a histogram of the last column of the DataFrame and returns the plot.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame.\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n\n    last_column = df.columns[-1]\n    histogram = df[last_column].plot(kind='hist', bins=bins)\n    plt.title(f'Histogram of {last_column}')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n\n    return histogram", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=20):\n    \"\"\"\n    Draws a histogram of the last column of the DataFrame and returns the plot.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame.\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n\n    last_column = df.columns[-1]\n    histogram = df[last_column].plot(kind='hist', bins=bins)\n    plt.title(f'Histogram of {last_column}')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n\n    return histogram"}
{"task_id": "BigCodeBench/135", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n    if df.shape[1] == 0:\n        raise ValueError(\"DataFrame has no columns\")\n\n    # Impute missing values in the last column using mean imputation\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n    df.iloc[:, -1] = imp.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))\n\n    # Create a box plot to visualize the distribution of data in the last column\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=df.iloc[:, -1])\n    plt.title('Box plot of the last column')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n\n    return df, plt.gca()", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n    if df.shape[1] == 0:\n        raise ValueError(\"DataFrame has no columns\")\n\n    # Impute missing values in the last column using mean imputation\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n    df.iloc[:, -1] = imp.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))\n\n    # Create a box plot to visualize the distribution of data in the last column\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=df.iloc[:, -1])\n    plt.title('Box plot of the last column')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/136", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame should not be empty\")\n\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df)\n\n    df_pca = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n\n    fig, ax = plt.subplots()\n    ax.scatter(df_pca['Principal Component 1'], df_pca['Principal Component 2'])\n    ax.set_title('2 Component PCA')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return df_pca, ax", "raw_solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame should not be empty\")\n\n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df)\n\n    df_pca = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n\n    fig, ax = plt.subplots()\n    ax.scatter(df_pca['Principal Component 1'], df_pca['Principal Component 2'])\n    ax.set_title('2 Component PCA')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n\n    return df_pca, ax"}
{"task_id": "BigCodeBench/137", "solution": "import pandas as pd\nfrom scipy.stats import skew\ndef task_func(df):\n    \"\"\"\n    Calculates the skewness of the last column of the dataframe.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.shape[1] == 0:\n        raise ValueError(\"DataFrame has no columns\")\n    return skew(df.iloc[:, -1])\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})", "raw_solution": "import pandas as pd\nfrom scipy.stats import skew\n\ndef task_func(df):\n    \"\"\"\n    Calculates the skewness of the last column of the dataframe.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.shape[1] == 0:\n        raise ValueError(\"DataFrame has no columns\")\n    return skew(df.iloc[:, -1])\n\n# Testing the function\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})\n"}
{"task_id": "BigCodeBench/138", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, letters=list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')):\n    \"\"\"\n    Create and return a bar chart of the frequency of letters in a DataFrame where the column 'Letters' contains English uppercase letters.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if 'Letters' not in df.columns:\n        raise ValueError(\"'df' lacks the 'Letters' column\")\n\n    letter_counts = df['Letters'].value_counts()\n    letter_counts = letter_counts[letters]  # filter to only include specified letters\n\n    fig, ax = plt.subplots()\n    letter_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, letters=list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')):\n    \"\"\"\n    Create and return a bar chart of the frequency of letters in a DataFrame where the column 'Letters' contains English uppercase letters.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' is not a DataFrame\")\n    if 'Letters' not in df.columns:\n        raise ValueError(\"'df' lacks the 'Letters' column\")\n\n    letter_counts = df['Letters'].value_counts()\n    letter_counts = letter_counts[letters]  # filter to only include specified letters\n\n    fig, ax = plt.subplots()\n    letter_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n\n    return ax"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input should be a non-empty DataFrame\")\n\n    numeric_cols = df.select_dtypes(include=['int', 'float']).columns\n    if not numeric_cols.any():\n        raise ValueError(\"No numeric columns in the DataFrame\")\n\n    hist_plots = []\n    for col in numeric_cols:\n        plt.figure()\n        df[col].hist(bins=10)\n        plt.title(col)\n        plt.xlabel('Value')\n        plt.ylabel('Frequency')\n        hist_plots.append(plt.gca())\n\n    return hist_plots", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input should be a non-empty DataFrame\")\n\n    numeric_cols = df.select_dtypes(include=['int', 'float']).columns\n    if not numeric_cols.any():\n        raise ValueError(\"No numeric columns in the DataFrame\")\n\n    hist_plots = []\n    for col in numeric_cols:\n        plt.figure()\n        df[col].hist(bins=10)\n        plt.title(col)\n        plt.xlabel('Value')\n        plt.ylabel('Frequency')\n        hist_plots.append(plt.gca())\n\n    return hist_plots"}
{"task_id": "BigCodeBench/140", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, cols):\n    \"\"\"\n    Standardize specified numeric columns in a dataframe.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n    if not isinstance(cols, list):\n        raise ValueError(\"'cols' should be a list\")\n    for col in cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' does not exist in 'df'\")\n\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, cols):\n    \"\"\"\n    Standardize specified numeric columns in a dataframe.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"'df' should be a pandas DataFrame\")\n    if not isinstance(cols, list):\n        raise ValueError(\"'cols' should be a list\")\n    for col in cols:\n        if col not in df.columns:\n            raise ValueError(f\"Column '{col}' does not exist in 'df'\")\n\n    scaler = StandardScaler()\n    df[cols] = scaler.fit_transform(df[cols])\n    return df"}
{"task_id": "BigCodeBench/141", "solution": "import numpy as np\nimport pandas as pd\nimport statistics\ndef task_func(rows, columns=['A', 'B', 'C', 'D', 'E', 'F'], seed=42):\n    if not isinstance(rows, int) or rows <= 0:\n        raise ValueError(\"'rows' should be a positive integer greater than 0\")\n\n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n\n    stats_dict = {}\n    for col in columns:\n        mean_val = df[col].mean()\n        median_val = df[col].median()\n        stats_dict[col] = {'mean': mean_val, 'median': median_val}\n\n    return df, stats_dict", "raw_solution": "import numpy as np\nimport pandas as pd\nimport statistics\n\ndef task_func(rows, columns=['A', 'B', 'C', 'D', 'E', 'F'], seed=42):\n    if not isinstance(rows, int) or rows <= 0:\n        raise ValueError(\"'rows' should be a positive integer greater than 0\")\n\n    np.random.seed(seed)\n    data = np.random.randint(1, 101, size=(rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n\n    stats_dict = {}\n    for col in columns:\n        mean_val = df[col].mean()\n        median_val = df[col].median()\n        stats_dict[col] = {'mean': mean_val, 'median': median_val}\n\n    return df, stats_dict"}
{"task_id": "BigCodeBench/142", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    x = np.linspace(0, 2*np.pi, 100)\n    y_sin = np.sin(x)\n    y_cos = np.cos(x)\n\n    fig, axs = plt.subplots(2)\n    fig.set_size_inches(10, 10)\n\n    axs[0].plot(x, y_sin)\n    axs[0].set_title('Sine function')\n    axs[0].set_xlabel('x')\n    axs[0].set_ylabel('sin(x)')\n\n    axs[1].plot(x, y_cos)\n    axs[1].set_title('Cosine function')\n    axs[1].set_xlabel('x')\n    axs[1].set_ylabel('cos(x)')\n\n    plt.tight_layout()\n\n    return fig, axs", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    x = np.linspace(0, 2*np.pi, 100)\n    y_sin = np.sin(x)\n    y_cos = np.cos(x)\n\n    fig, axs = plt.subplots(2)\n    fig.set_size_inches(10, 10)\n\n    axs[0].plot(x, y_sin)\n    axs[0].set_title('Sine function')\n    axs[0].set_xlabel('x')\n    axs[0].set_ylabel('sin(x)')\n\n    axs[1].plot(x, y_cos)\n    axs[1].set_title('Cosine function')\n    axs[1].set_xlabel('x')\n    axs[1].set_ylabel('cos(x)')\n\n    plt.tight_layout()\n\n    return fig, axs"}
{"task_id": "BigCodeBench/143", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func():\n    # Create an array of x values from -10 to 10\n    x = np.linspace(-10, 10, 400)\n\n    # Calculate corresponding y values\n    y = 2*x + 1\n\n    # Create the plot\n    fig, ax = plt.subplots()\n\n    # Plot the equation y = 2x + 1\n    ax.plot(x, y, label='y=2x+1', color='red')\n\n    # Mark the solution at x = 2, y = 5\n    ax.plot(2, 5, marker='o', markersize=10, color='green', label='solution')\n\n    # Set the title and labels\n    ax.set_title('Solution of the equation y=2x+1 at x=2')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    # Set the x-axis range\n    ax.set_xlim([-10, 10])\n\n    # Set the y-axis range automatically based on the equation\n    ax.set_ylim([-10, 30])\n\n    # Add a legend\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    # Create an array of x values from -10 to 10\n    x = np.linspace(-10, 10, 400)\n\n    # Calculate corresponding y values\n    y = 2*x + 1\n\n    # Create the plot\n    fig, ax = plt.subplots()\n\n    # Plot the equation y = 2x + 1\n    ax.plot(x, y, label='y=2x+1', color='red')\n\n    # Mark the solution at x = 2, y = 5\n    ax.plot(2, 5, marker='o', markersize=10, color='green', label='solution')\n\n    # Set the title and labels\n    ax.set_title('Solution of the equation y=2x+1 at x=2')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    # Set the x-axis range\n    ax.set_xlim([-10, 10])\n\n    # Set the y-axis range automatically based on the equation\n    ax.set_ylim([-10, 30])\n\n    # Add a legend\n    ax.legend()\n\n    return ax\n\n# Call the function\ntask_func()\n\n# Display the plot\nplt.show()"}
{"task_id": "BigCodeBench/144", "solution": "import ipaddress\nimport requests\ndef task_func(ip_range, timeout):\n    \"\"\"\n    Scans a specified IP address range and sends an HTTP GET request to each IP to verify if it is an active web server.\n    \"\"\"\n    try:\n        network = ipaddress.IPv4Network(ip_range)\n    except ValueError:\n        raise ValueError(\"Invalid IP range provided\")\n\n    active_ips = []\n\n    for ip in network.hosts():\n        url = f\"http://{ip}\"\n        try:\n            response = requests.get(url, timeout=timeout)\n            if response.status_code == 200:\n                active_ips.append(str(ip))\n        except requests.exceptions.RequestException:\n            continue\n\n    return active_ips", "raw_solution": "import ipaddress\nimport requests\n\ndef task_func(ip_range, timeout):\n    \"\"\"\n    Scans a specified IP address range and sends an HTTP GET request to each IP to verify if it is an active web server.\n    \"\"\"\n    try:\n        network = ipaddress.IPv4Network(ip_range)\n    except ValueError:\n        raise ValueError(\"Invalid IP range provided\")\n\n    active_ips = []\n\n    for ip in network.hosts():\n        url = f\"http://{ip}\"\n        try:\n            response = requests.get(url, timeout=timeout)\n            if response.status_code == 200:\n                active_ips.append(str(ip))\n        except requests.exceptions.RequestException:\n            continue\n\n    return active_ips"}
{"task_id": "BigCodeBench/145", "solution": "import csv\nfrom ipaddress import IPv4Network, IPv4Address\ndef task_func(ip_range, csv_path):\n    \"\"\"\n    Generates a CSV file listing all IP addresses in the specified IP range.\n    Each IP address is written as a row in the CSV file.\n    \"\"\"\n    # Parse the IP range\n    start_ip, end_ip = ip_range.split('-')\n    start_ip = int(IPv4Address(start_ip))\n    end_ip = int(IPv4Address(end_ip))\n\n    # Generate IP addresses\n    ip_addresses = [str(IPv4Address(ip)) for ip in range(start_ip, end_ip+1)]\n\n    # Write IP addresses to CSV file\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for ip in ip_addresses:\n            writer.writerow([ip])\n\n    return csv_path", "raw_solution": "import csv\nfrom ipaddress import IPv4Network, IPv4Address\n\ndef task_func(ip_range, csv_path):\n    \"\"\"\n    Generates a CSV file listing all IP addresses in the specified IP range.\n    Each IP address is written as a row in the CSV file.\n    \"\"\"\n    # Parse the IP range\n    start_ip, end_ip = ip_range.split('-')\n    start_ip = int(IPv4Address(start_ip))\n    end_ip = int(IPv4Address(end_ip))\n\n    # Generate IP addresses\n    ip_addresses = [str(IPv4Address(ip)) for ip in range(start_ip, end_ip+1)]\n\n    # Write IP addresses to CSV file\n    with open(csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for ip in ip_addresses:\n            writer.writerow([ip])\n\n    return csv_path"}
{"task_id": "BigCodeBench/146", "solution": "import subprocess\nfrom ipaddress import IPv4Network\ndef task_func(ip_range):\n    \"\"\"\n    Scans the specified IP address range and pings each IP to check if it is active.\n    Returns a dictionary with IP addresses as keys and a boolean value indicating their active status (True if the ping is successful, False otherwise).\n    Raises subprocess.CalledProcessError if a ping command fails due to a subprocess error.\n    \"\"\"\n    network = IPv4Network(ip_range)\n    result = {}\n    for ip in network.hosts():\n        ip_str = str(ip)\n        try:\n            # Use the 'ping' command to check if the IP is active\n            subprocess.run(['ping', '-c', '1', ip_str], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            result[ip_str] = True\n        except subprocess.CalledProcessError:\n            result[ip_str] = False\n    return result", "raw_solution": "import subprocess\nfrom ipaddress import IPv4Network\n\ndef task_func(ip_range):\n    \"\"\"\n    Scans the specified IP address range and pings each IP to check if it is active.\n    Returns a dictionary with IP addresses as keys and a boolean value indicating their active status (True if the ping is successful, False otherwise).\n    Raises subprocess.CalledProcessError if a ping command fails due to a subprocess error.\n    \"\"\"\n    network = IPv4Network(ip_range)\n    result = {}\n    for ip in network.hosts():\n        ip_str = str(ip)\n        try:\n            # Use the 'ping' command to check if the IP is active\n            subprocess.run(['ping', '-c', '1', ip_str], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            result[ip_str] = True\n        except subprocess.CalledProcessError:\n            result[ip_str] = False\n    return result"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef check_port(ip, port):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.settimeout(5)\n    result = sock.connect_ex((str(ip), port))\n    sock.close()\n    return result == 0\ndef task_func(ip_range, port):\n    ip_network = IPv4Network(ip_range)\n    ip_list = [str(ip) for ip in ip_network.hosts()]\n    results = {}\n\n    def worker(ip):\n        results[ip] = check_port(ip, port)\n\n    threads = [Thread(target=worker, args=(ip,)) for ip in ip_list]\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n\n    return results", "raw_solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef check_port(ip, port):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.settimeout(5)\n    result = sock.connect_ex((str(ip), port))\n    sock.close()\n    return result == 0\n\ndef task_func(ip_range, port):\n    ip_network = IPv4Network(ip_range)\n    ip_list = [str(ip) for ip in ip_network.hosts()]\n    results = {}\n\n    def worker(ip):\n        results[ip] = check_port(ip, port)\n\n    threads = [Thread(target=worker, args=(ip,)) for ip in ip_list]\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n\n    return results"}
{"task_id": "BigCodeBench/148", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Encrypt the categorical data in a specific column of a DataFrame using LabelEncoder.\n    \"\"\"\n    if column_name in df.columns:\n        le = LabelEncoder()\n        df[column_name] = le.fit_transform(df[column_name])\n    else:\n        raise ValueError(f\"Column {column_name} not found in DataFrame\")\n    return df\ndf = pd.DataFrame({\n    'color': ['blue', 'green', 'red', 'blue', 'green'],\n    'shape': ['circle', 'square', 'triangle', 'circle', 'square'],\n    'size': ['large', 'medium', 'large', 'small', 'medium']\n})", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Encrypt the categorical data in a specific column of a DataFrame using LabelEncoder.\n    \"\"\"\n    if column_name in df.columns:\n        le = LabelEncoder()\n        df[column_name] = le.fit_transform(df[column_name])\n    else:\n        raise ValueError(f\"Column {column_name} not found in DataFrame\")\n    return df\n\n# Test the function\ndf = pd.DataFrame({\n    'color': ['blue', 'green', 'red', 'blue', 'green'],\n    'shape': ['circle', 'square', 'triangle', 'circle', 'square'],\n    'size': ['large', 'medium', 'large', 'small', 'medium']\n})\n\ndf = task_func(df, 'color')"}
{"task_id": "BigCodeBench/149", "solution": "import pandas as pd\nimport numpy as np\nDEFAULT_COLUMNS = ['Element', 'Count']\ndef task_func(elements, include_index=False):\n    \"\"\"\n    Constructs a DataFrame that enumerates the character counts of each string in a provided list of elements.\n    This function can optionally include an index column for each row in the DataFrame.\n    \"\"\"\n    if include_index:\n        DEFAULT_COLUMNS.insert(0, 'Index')\n\n    df = pd.DataFrame(elements, columns=DEFAULT_COLUMNS[:1])\n    df[DEFAULT_COLUMNS[1]] = df[DEFAULT_COLUMNS[0]].apply(len)\n\n    if include_index:\n        df.reset_index(inplace=True)\n        df.rename(columns={'index': DEFAULT_COLUMNS[0]}, inplace=True)\n\n    return df\nelements = ['Hello', 'World', 'Python', 'Pandas']", "raw_solution": "import pandas as pd\nimport numpy as np\n\nDEFAULT_COLUMNS = ['Element', 'Count']\n\ndef task_func(elements, include_index=False):\n    \"\"\"\n    Constructs a DataFrame that enumerates the character counts of each string in a provided list of elements.\n    This function can optionally include an index column for each row in the DataFrame.\n    \"\"\"\n    if include_index:\n        DEFAULT_COLUMNS.insert(0, 'Index')\n\n    df = pd.DataFrame(elements, columns=DEFAULT_COLUMNS[:1])\n    df[DEFAULT_COLUMNS[1]] = df[DEFAULT_COLUMNS[0]].apply(len)\n\n    if include_index:\n        df.reset_index(inplace=True)\n        df.rename(columns={'index': DEFAULT_COLUMNS[0]}, inplace=True)\n\n    return df\n\n# Test the function\nelements = ['Hello', 'World', 'Python', 'Pandas']\ndf = task_func(elements, include_index=True)"}
{"task_id": "BigCodeBench/150", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(product_dict, product_keys):\n    if not product_dict or not product_keys:\n        return None, None\n\n    data = []\n    for key in product_keys:\n        if key in product_dict:\n            data.append([key] + list(product_dict[key].values()))\n\n    df = pd.DataFrame(data, columns=['Product', 'Quantity', 'Price', 'Profit'])\n    df['Average Price'] = df['Price'].mean()\n    df['Average Profit'] = df['Profit'].mean()\n\n    fig, ax = plt.subplots()\n    df.plot(x='Product', y='Profit', kind='bar', ax=ax)\n    ax.set_xlabel('Product')\n    ax.set_ylabel('Profit')\n    plt.tight_layout()\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(product_dict, product_keys):\n    if not product_dict or not product_keys:\n        return None, None\n\n    data = []\n    for key in product_keys:\n        if key in product_dict:\n            data.append([key] + list(product_dict[key].values()))\n\n    df = pd.DataFrame(data, columns=['Product', 'Quantity', 'Price', 'Profit'])\n    df['Average Price'] = df['Price'].mean()\n    df['Average Profit'] = df['Profit'].mean()\n\n    fig, ax = plt.subplots()\n    df.plot(x='Product', y='Profit', kind='bar', ax=ax)\n    ax.set_xlabel('Product')\n    ax.set_ylabel('Profit')\n    plt.tight_layout()\n\n    return df, ax"}
{"task_id": "BigCodeBench/151", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data_dict, data_keys):\n    \"\"\"\n    Normalize data specified by keys in a dictionary using MinMax scaling and plot the results.\n    \"\"\"\n    # Check if all keys are in data_dict\n    if not all(key in data_dict for key in data_keys):\n        raise ValueError(\"One or more keys in data_keys are not found in data_dict\")\n\n    # Create a DataFrame from the data_dict\n    df = pd.DataFrame(data_dict)\n\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    df[data_keys] = scaler.fit_transform(df[data_keys])\n\n    # Create a plot\n    fig, ax = plt.subplots()\n    df[data_keys].plot(ax=ax)\n\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data_dict, data_keys):\n    \"\"\"\n    Normalize data specified by keys in a dictionary using MinMax scaling and plot the results.\n    \"\"\"\n    # Check if all keys are in data_dict\n    if not all(key in data_dict for key in data_keys):\n        raise ValueError(\"One or more keys in data_keys are not found in data_dict\")\n\n    # Create a DataFrame from the data_dict\n    df = pd.DataFrame(data_dict)\n\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    df[data_keys] = scaler.fit_transform(df[data_keys])\n\n    # Create a plot\n    fig, ax = plt.subplots()\n    df[data_keys].plot(ax=ax)\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/152", "solution": "import pandas as pd\nimport numpy as np\nimport random\nSTUDENTS = ['Joe', 'Amy', 'Mark', 'Sara', 'John', 'Emily', 'Zoe', 'Matt']\nCOURSES = ['Math', 'Physics', 'Chemistry', 'Biology', 'English', 'History', 'Geography', 'Computer Science']\ndef task_func():\n    # Initialize an empty DataFrame\n    df = pd.DataFrame()\n\n    # For each student, generate random grades for each course and calculate their average\n    for student in STUDENTS:\n        # Generate random grades for each course using a uniform distribution between 0 and 100\n        grades = [random.uniform(0, 100) for _ in range(len(COURSES))]\n\n        # Add the grades to the DataFrame\n        df = df.append(pd.Series([student] + grades + [np.mean(grades)], index=['Student'] + COURSES + ['Average']), ignore_index=True)\n\n    # Set the column names\n    df.columns = ['Student'] + COURSES + ['Average']\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport random\n\n# Constants\nSTUDENTS = ['Joe', 'Amy', 'Mark', 'Sara', 'John', 'Emily', 'Zoe', 'Matt']\nCOURSES = ['Math', 'Physics', 'Chemistry', 'Biology', 'English', 'History', 'Geography', 'Computer Science']\n\ndef task_func():\n    # Initialize an empty DataFrame\n    df = pd.DataFrame()\n\n    # For each student, generate random grades for each course and calculate their average\n    for student in STUDENTS:\n        # Generate random grades for each course using a uniform distribution between 0 and 100\n        grades = [random.uniform(0, 100) for _ in range(len(COURSES))]\n\n        # Add the grades to the DataFrame\n        df = df.append(pd.Series([student] + grades + [np.mean(grades)], index=['Student'] + COURSES + ['Average']), ignore_index=True)\n\n    # Set the column names\n    df.columns = ['Student'] + COURSES + ['Average']\n\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/153", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(data):\n    \"\"\"\n    Transforms categorical data into a numerical format suitable for machine learning algorithms using sklearn's LabelEncoder.\n    This function generates a DataFrame that pairs original categorical values with their numerical encodings.\n    \"\"\"\n    # Initialize LabelEncoder\n    le = LabelEncoder()\n\n    # Fit and transform the data\n    encoded_data = le.fit_transform(data)\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Category': data,\n        'Encoded': encoded_data\n    })\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(data):\n    \"\"\"\n    Transforms categorical data into a numerical format suitable for machine learning algorithms using sklearn's LabelEncoder.\n    This function generates a DataFrame that pairs original categorical values with their numerical encodings.\n    \"\"\"\n    # Initialize LabelEncoder\n    le = LabelEncoder()\n\n    # Fit and transform the data\n    encoded_data = le.fit_transform(data)\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Category': data,\n        'Encoded': encoded_data\n    })\n\n    return df"}
{"task_id": "BigCodeBench/154", "solution": "import re\nimport os\nimport glob\nimport mimetypes\ndef task_func(directory, file_pattern, suffix):\n    \"\"\"\n    Scans a specified directory for files matching a given pattern and with a certain suffix, then determines their file types.\n    The function returns a dictionary with file names as keys and their corresponding MIME types as values.\n    \"\"\"\n    # Initialize an empty dictionary to store the file names and their MIME types\n    file_dict = {}\n\n    # Use glob to get a list of files matching the pattern and suffix in the directory\n    for file in glob.glob(os.path.join(directory, file_pattern + '*' + suffix)):\n        # Use mimetypes to get the MIME type of the file\n        mime_type = mimetypes.guess_type(file)[0]\n\n        # Add the file name and its MIME type to the dictionary\n        file_dict[os.path.basename(file)] = mime_type\n\n    # Return the dictionary\n    return file_dict", "raw_solution": "import re\nimport os\nimport glob\nimport mimetypes\n\ndef task_func(directory, file_pattern, suffix):\n    \"\"\"\n    Scans a specified directory for files matching a given pattern and with a certain suffix, then determines their file types.\n    The function returns a dictionary with file names as keys and their corresponding MIME types as values.\n    \"\"\"\n    # Initialize an empty dictionary to store the file names and their MIME types\n    file_dict = {}\n\n    # Use glob to get a list of files matching the pattern and suffix in the directory\n    for file in glob.glob(os.path.join(directory, file_pattern + '*' + suffix)):\n        # Use mimetypes to get the MIME type of the file\n        mime_type = mimetypes.guess_type(file)[0]\n\n        # Add the file name and its MIME type to the dictionary\n        file_dict[os.path.basename(file)] = mime_type\n\n    # Return the dictionary\n    return file_dict"}
{"task_id": "BigCodeBench/155", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\ndef task_func(data):\n    \"\"\"\n    Computes the average of each row in a provided 2D array and appends these averages as a new column.\n    Additionally, it plots the averages against their respective row indices.\n    \"\"\"\n    # Convert the 2D array to a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Compute the average of each row and append it as a new column\n    df['Average'] = df.mean(axis=1)\n\n    # Create a plot of the averages against their respective row indices\n    fig, ax = plt.subplots()\n    ax.plot(df['Average'])\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Average')\n    ax.set_title('Average of each row')\n\n    return (df, ax)", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef task_func(data):\n    \"\"\"\n    Computes the average of each row in a provided 2D array and appends these averages as a new column.\n    Additionally, it plots the averages against their respective row indices.\n    \"\"\"\n    # Convert the 2D array to a pandas DataFrame\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n\n    # Compute the average of each row and append it as a new column\n    df['Average'] = df.mean(axis=1)\n\n    # Create a plot of the averages against their respective row indices\n    fig, ax = plt.subplots()\n    ax.plot(df['Average'])\n    ax.set_xlabel('Row Index')\n    ax.set_ylabel('Average')\n    ax.set_title('Average of each row')\n\n    return (df, ax)"}
{"task_id": "BigCodeBench/156", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Normalizes a given dataset using MinMax scaling and calculates the average of each row.\n    This average is then added as a new column 'Average' to the resulting DataFrame.\n    The function also visualizes these averages in a plot.\n    \"\"\"\n    # Normalize the data\n    scaler = MinMaxScaler()\n    data_normalized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Calculate the average of each row\n    data_normalized['Average'] = data_normalized.mean(axis=1)\n\n    # Create a plot of the averages\n    fig, ax = plt.subplots()\n    data_normalized['Average'].plot(kind='bar', ax=ax)\n    ax.set_ylabel('Average')\n    ax.set_title('Average values across the dataset')\n\n    return data_normalized, ax.figure", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    Normalizes a given dataset using MinMax scaling and calculates the average of each row.\n    This average is then added as a new column 'Average' to the resulting DataFrame.\n    The function also visualizes these averages in a plot.\n    \"\"\"\n    # Normalize the data\n    scaler = MinMaxScaler()\n    data_normalized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Calculate the average of each row\n    data_normalized['Average'] = data_normalized.mean(axis=1)\n\n    # Create a plot of the averages\n    fig, ax = plt.subplots()\n    data_normalized['Average'].plot(kind='bar', ax=ax)\n    ax.set_ylabel('Average')\n    ax.set_title('Average values across the dataset')\n\n    return data_normalized, ax.figure"}
{"task_id": "BigCodeBench/157", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(data):\n    \"\"\"\n    Analyze a dataset by calculating the average of values across each row and visualizing the correlation matrix as a heatmap.\n    \"\"\"\n    # Check if data is a 2D array\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a 2D array.\")\n\n    # Check if data contains non-numeric data\n    if pd.api.types.is_numeric_dtype(data) == False:\n        raise ValueError(\"Input data should contain only numeric values.\")\n\n    # Calculate average across each row\n    data['Average'] = data.mean(axis=1)\n\n    # Calculate correlation matrix\n    corr = data.corr()\n\n    # Create a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n    return data, plt.gca()", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(data):\n    \"\"\"\n    Analyze a dataset by calculating the average of values across each row and visualizing the correlation matrix as a heatmap.\n    \"\"\"\n    # Check if data is a 2D array\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a 2D array.\")\n\n    # Check if data contains non-numeric data\n    if pd.api.types.is_numeric_dtype(data) == False:\n        raise ValueError(\"Input data should contain only numeric values.\")\n\n    # Calculate average across each row\n    data['Average'] = data.mean(axis=1)\n\n    # Calculate correlation matrix\n    corr = data.corr()\n\n    # Create a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n    return data, plt.gca()"}
{"task_id": "BigCodeBench/158", "solution": "import json\nimport urllib.request\nimport urllib.parse\nimport gzip\nimport os\ndef task_func(url_str, file_path):\n    \"\"\"\n    Fetches JSON data from a given URL, decodes the json-formatted data, and compresses it into a gzip file.\n    \"\"\"\n    # Fetch JSON data from URL\n    with urllib.request.urlopen(url_str) as url:\n        data = url.read()\n\n    # Decode JSON data\n    json_data = json.loads(data)\n\n    # Compress JSON data into gzip file\n    with gzip.open(file_path, 'wb') as f:\n        f.write(json.dumps(json_data).encode('utf-8'))\n\n    # Return path to the compressed gzip file\n    return file_path", "raw_solution": "import json\nimport urllib.request\nimport urllib.parse\nimport gzip\nimport os\n\ndef task_func(url_str, file_path):\n    \"\"\"\n    Fetches JSON data from a given URL, decodes the json-formatted data, and compresses it into a gzip file.\n    \"\"\"\n    # Fetch JSON data from URL\n    with urllib.request.urlopen(url_str) as url:\n        data = url.read()\n\n    # Decode JSON data\n    json_data = json.loads(data)\n\n    # Compress JSON data into gzip file\n    with gzip.open(file_path, 'wb') as f:\n        f.write(json.dumps(json_data).encode('utf-8'))\n\n    # Return path to the compressed gzip file\n    return file_path"}
{"task_id": "BigCodeBench/159", "solution": "import struct\nimport io\nimport gzip\nimport numpy as np\ndef task_func(newArray):\n    \"\"\"\n    Compresses a given NumPy array using gzip compression and returns the compressed data.\n    \"\"\"\n    # Convert the NumPy array to bytes\n    bytes_data = newArray.tobytes()\n\n    # Compress the bytes data using gzip\n    compressed_data = gzip.compress(bytes_data)\n\n    return compressed_data", "raw_solution": "import struct\nimport io\nimport gzip\nimport numpy as np\n\ndef task_func(newArray):\n    \"\"\"\n    Compresses a given NumPy array using gzip compression and returns the compressed data.\n    \"\"\"\n    # Convert the NumPy array to bytes\n    bytes_data = newArray.tobytes()\n\n    # Compress the bytes data using gzip\n    compressed_data = gzip.compress(bytes_data)\n\n    return compressed_data"}
{"task_id": "BigCodeBench/160", "solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\ndef task_func(data):\n    \"\"\"\n    Processes a given dataset to compute the average of each row, plots the distribution of these averages, and evaluates their normality.\n    \"\"\"\n    if data.shape[1] != len(COLUMN_NAMES):\n        raise ValueError(\"The input data does not have exactly eight columns.\")\n\n    data['Average'] = data[COLUMN_NAMES].mean(axis=1)\n\n    plot = sns.distplot(data['Average'])\n\n    p_value = None\n    if len(data) > 20:\n        _, p_value = stats.normaltest(data['Average'])\n\n    return data, plot, p_value", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\n# Constants\nCOLUMN_NAMES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\ndef task_func(data):\n    \"\"\"\n    Processes a given dataset to compute the average of each row, plots the distribution of these averages, and evaluates their normality.\n    \"\"\"\n    if data.shape[1] != len(COLUMN_NAMES):\n        raise ValueError(\"The input data does not have exactly eight columns.\")\n\n    data['Average'] = data[COLUMN_NAMES].mean(axis=1)\n\n    plot = sns.distplot(data['Average'])\n\n    p_value = None\n    if len(data) > 20:\n        _, p_value = stats.normaltest(data['Average'])\n\n    return data, plot, p_value"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information such as message type, timestamp, and the message itself from a log file and stores the data in a CSV format.\n    \"\"\"\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(.*?)\\] - (.*)'\n\n    # Initialize lists to store the extracted data\n    types = []\n    timestamps = []\n    messages = []\n\n    # Open the log file and read each line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Attempt to extract the log entry data\n            match = re.search(pattern, line)\n            if match:\n                types.append(match.group(1))\n                try:\n                    # Attempt to parse the timestamp\n                    timestamp = datetime.strptime(match.group(2), '%Y-%m-%d %H:%M:%S')\n                    timestamps.append(timestamp)\n                except ValueError:\n                    # Raise an exception if the timestamp is invalid\n                    raise ValueError(f'Invalid timestamp in log entry: {line}')\n                messages.append(match.group(3))\n            else:\n                # If the line does not match the pattern, skip it\n                continue\n\n    # If no valid log entries were found, raise an exception\n    if not types or not timestamps or not messages:\n        raise ValueError('No valid log entries found')\n\n    # Create a pandas DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Type': types,\n        'Timestamp': timestamps,\n        'Message': messages\n    })\n\n    # Save the DataFrame to a CSV file\n    csv_file = log_file.replace('.log', '.csv')\n    df.to_csv(csv_file, index=False)\n\n    return csv_file", "raw_solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information such as message type, timestamp, and the message itself from a log file and stores the data in a CSV format.\n    \"\"\"\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(.*?)\\] - (.*)'\n\n    # Initialize lists to store the extracted data\n    types = []\n    timestamps = []\n    messages = []\n\n    # Open the log file and read each line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Attempt to extract the log entry data\n            match = re.search(pattern, line)\n            if match:\n                types.append(match.group(1))\n                try:\n                    # Attempt to parse the timestamp\n                    timestamp = datetime.strptime(match.group(2), '%Y-%m-%d %H:%M:%S')\n                    timestamps.append(timestamp)\n                except ValueError:\n                    # Raise an exception if the timestamp is invalid\n                    raise ValueError(f'Invalid timestamp in log entry: {line}')\n                messages.append(match.group(3))\n            else:\n                # If the line does not match the pattern, skip it\n                continue\n\n    # If no valid log entries were found, raise an exception\n    if not types or not timestamps or not messages:\n        raise ValueError('No valid log entries found')\n\n    # Create a pandas DataFrame from the extracted data\n    df = pd.DataFrame({\n        'Type': types,\n        'Timestamp': timestamps,\n        'Message': messages\n    })\n\n    # Save the DataFrame to a CSV file\n    csv_file = log_file.replace('.log', '.csv')\n    df.to_csv(csv_file, index=False)\n\n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a text.\n    The function generates a histogram subplot, which facilitates the understanding of how word lengths vary within the provided text.\n    \"\"\"\n    # Filter out non-word characters and split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n\n    # Create a histogram of word lengths\n    bins = np.arange(min(word_lengths), max(word_lengths) + 2, 1) - 0.5\n    ax = plt.hist(word_lengths, bins=bins, rwidth=rwidth, color='g', zorder=2)\n\n    # Set the x-axis label and title\n    plt.xlabel('Word Length')\n    plt.title('Distribution of Word Lengths')\n\n    return ax", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a text.\n    The function generates a histogram subplot, which facilitates the understanding of how word lengths vary within the provided text.\n    \"\"\"\n    # Filter out non-word characters and split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n\n    # Create a histogram of word lengths\n    bins = np.arange(min(word_lengths), max(word_lengths) + 2, 1) - 0.5\n    ax = plt.hist(word_lengths, bins=bins, rwidth=rwidth, color='g', zorder=2)\n\n    # Set the x-axis label and title\n    plt.xlabel('Word Length')\n    plt.title('Distribution of Word Lengths')\n\n    return ax"}
{"task_id": "BigCodeBench/163", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(rows=5, cols=5):\n    if cols > 26:\n        raise ValueError(\"The number of columns cannot exceed the number of available categories (26).\")\n\n    # Generate random numerical data\n    data = np.random.randint(1, 100, size=(rows, cols))\n    df = pd.DataFrame(data, columns=[chr(65 + i) for i in range(cols)])\n\n    # Plot stacked bar chart\n    ax = df.plot(kind='bar', stacked=True)\n    plt.title('Stacked Bar Chart')\n    plt.xlabel('Categories')\n    plt.ylabel('Values')\n    plt.legend(title='Columns')\n\n    return ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(rows=5, cols=5):\n    if cols > 26:\n        raise ValueError(\"The number of columns cannot exceed the number of available categories (26).\")\n\n    # Generate random numerical data\n    data = np.random.randint(1, 100, size=(rows, cols))\n    df = pd.DataFrame(data, columns=[chr(65 + i) for i in range(cols)])\n\n    # Plot stacked bar chart\n    ax = df.plot(kind='bar', stacked=True)\n    plt.title('Stacked Bar Chart')\n    plt.xlabel('Categories')\n    plt.ylabel('Values')\n    plt.legend(title='Columns')\n\n    return ax"}
{"task_id": "BigCodeBench/164", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(num_labels=5, data_range=(0, 1)):\n    \"\"\"\n    Generates random numeric data across a specified range for a given number of categories and visualizes it with a stacked bar chart.\n    \"\"\"\n    # Generate random data\n    data = np.random.rand(num_labels, num_labels)\n    data = data / data.sum(axis=0)\n    data = data * (data_range[1] - data_range[0]) + data_range[0]\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[f'Category {i+1}' for i in range(num_labels)], index=[f'Data {i+1}' for i in range(num_labels)])\n\n    # Create stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Category')\n    ax.set_title('Stacked Bar Chart of Generated Data')\n\n    return fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(num_labels=5, data_range=(0, 1)):\n    \"\"\"\n    Generates random numeric data across a specified range for a given number of categories and visualizes it with a stacked bar chart.\n    \"\"\"\n    # Generate random data\n    data = np.random.rand(num_labels, num_labels)\n    data = data / data.sum(axis=0)\n    data = data * (data_range[1] - data_range[0]) + data_range[0]\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[f'Category {i+1}' for i in range(num_labels)], index=[f'Data {i+1}' for i in range(num_labels)])\n\n    # Create stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Category')\n    ax.set_title('Stacked Bar Chart of Generated Data')\n\n    return fig\n\n# Test the function\nfig = task_func(num_labels=3, data_range=(1, 10))\nfig.show()"}
{"task_id": "BigCodeBench/165", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(num_rows=5, rand_range=(0, 100)):\n    \"\"\"\n    Creates a DataFrame with random integer values within a specified range for categories 'A' through 'E',\n    and visualizes this data with a stacked bar chart.\n    \"\"\"\n    # Create DataFrame with random integer values\n    df = pd.DataFrame({\n        'A': [randint(*rand_range) for _ in range(num_rows)],\n        'B': [randint(*rand_range) for _ in range(num_rows)],\n        'C': [randint(*rand_range) for _ in range(num_rows)],\n        'D': [randint(*rand_range) for _ in range(num_rows)],\n        'E': [randint(*rand_range) for _ in range(num_rows)],\n    }, index=[chr(65 + i) for i in range(num_rows)])\n\n    # Create stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_xlabel('Categories')\n    ax.set_ylabel('Values')\n    ax.set_title('Stacked Bar Chart')\n\n    return fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef task_func(num_rows=5, rand_range=(0, 100)):\n    \"\"\"\n    Creates a DataFrame with random integer values within a specified range for categories 'A' through 'E',\n    and visualizes this data with a stacked bar chart.\n    \"\"\"\n    # Create DataFrame with random integer values\n    df = pd.DataFrame({\n        'A': [randint(*rand_range) for _ in range(num_rows)],\n        'B': [randint(*rand_range) for _ in range(num_rows)],\n        'C': [randint(*rand_range) for _ in range(num_rows)],\n        'D': [randint(*rand_range) for _ in range(num_rows)],\n        'E': [randint(*rand_range) for _ in range(num_rows)],\n    }, index=[chr(65 + i) for i in range(num_rows)])\n\n    # Create stacked bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', stacked=True, ax=ax)\n    ax.set_xlabel('Categories')\n    ax.set_ylabel('Values')\n    ax.set_title('Stacked Bar Chart')\n\n    return fig"}
{"task_id": "BigCodeBench/166", "solution": "import pandas as pd\nfrom datetime import datetime\nimport holidays\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    \"\"\"\n    Create a list of business days between two dates, excluding weekends and specified country's public holidays.\n    \"\"\"\n    if not isinstance(start_date, datetime) or start_date > end_date:\n        raise ValueError(\"start_date should be a datetime object and should be before end_date\")\n    if not isinstance(end_date, datetime) or end_date < start_date:\n        raise ValueError(\"end_date should be a datetime object and should be after start_date\")\n\n    country_holidays = holidays.CountryHoliday(country)\n    all_dates = pd.date_range(start=start_date, end=end_date, freq='B')\n    business_days = all_dates[~all_dates.isin(country_holidays)]\n\n    return business_days.tolist()", "raw_solution": "import pandas as pd\nfrom datetime import datetime\nimport holidays\n\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    \"\"\"\n    Create a list of business days between two dates, excluding weekends and specified country's public holidays.\n    \"\"\"\n    if not isinstance(start_date, datetime) or start_date > end_date:\n        raise ValueError(\"start_date should be a datetime object and should be before end_date\")\n    if not isinstance(end_date, datetime) or end_date < start_date:\n        raise ValueError(\"end_date should be a datetime object and should be after start_date\")\n\n    country_holidays = holidays.CountryHoliday(country)\n    all_dates = pd.date_range(start=start_date, end=end_date, freq='B')\n    business_days = all_dates[~all_dates.isin(country_holidays)]\n\n    return business_days.tolist()\n\n# Test the function"}
{"task_id": "BigCodeBench/167", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\ndef task_func(num_types=5, integer_range=(0, 100)):\n    \"\"\"\n    Generates a DataFrame containing random integer values across a specified number of categories,\n    and visualizes these data as a horizontal stacked bar chart.\n    \"\"\"\n    # Generate random data\n    data = {f'Category {i+1}': [randint(*integer_range) for _ in range(num_types)] for i in range(num_types)}\n    df = pd.DataFrame(data)\n\n    # Create the plot\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Categories')\n    ax.set_title('Horizontal Stacked Bar Chart')\n\n    return fig, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef task_func(num_types=5, integer_range=(0, 100)):\n    \"\"\"\n    Generates a DataFrame containing random integer values across a specified number of categories,\n    and visualizes these data as a horizontal stacked bar chart.\n    \"\"\"\n    # Generate random data\n    data = {f'Category {i+1}': [randint(*integer_range) for _ in range(num_types)] for i in range(num_types)}\n    df = pd.DataFrame(data)\n\n    # Create the plot\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Categories')\n    ax.set_title('Horizontal Stacked Bar Chart')\n\n    return fig, ax\n\n# Test the function\nfig, ax = task_func(5, (0, 100))\nplt.show()"}
{"task_id": "BigCodeBench/168", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(num_groups=5, data_size=5, labels=None):\n    \"\"\"\n    Generate random data and visualize it with a stacked bar chart.\n    \"\"\"\n    # Generate random data\n    np.random.seed(0)\n    data = np.random.randint(0, 100, (data_size, num_groups))\n    df = pd.DataFrame(data, columns=labels)\n\n    # Create stacked bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(data_size), df.iloc[:, 0], label=labels[0])\n    for i in range(1, num_groups):\n        ax.bar(range(data_size), df.iloc[:, i], bottom=df.iloc[:, :i].sum(axis=1), label=labels[i])\n\n    ax.legend()\n    plt.savefig('test_plot.png')\n\n    return (fig, df, 'test_plot.png')\nlabels = ['Group 1', 'Group 2', 'Group 3', 'Group 4', 'Group 5']", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(num_groups=5, data_size=5, labels=None):\n    \"\"\"\n    Generate random data and visualize it with a stacked bar chart.\n    \"\"\"\n    # Generate random data\n    np.random.seed(0)\n    data = np.random.randint(0, 100, (data_size, num_groups))\n    df = pd.DataFrame(data, columns=labels)\n\n    # Create stacked bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(data_size), df.iloc[:, 0], label=labels[0])\n    for i in range(1, num_groups):\n        ax.bar(range(data_size), df.iloc[:, i], bottom=df.iloc[:, :i].sum(axis=1), label=labels[i])\n\n    ax.legend()\n    plt.savefig('test_plot.png')\n\n    return (fig, df, 'test_plot.png')\n\n# Test the function\nlabels = ['Group 1', 'Group 2', 'Group 3', 'Group 4', 'Group 5']\nfig, df, filename = task_func(num_groups=5, data_size=5, labels=labels)"}
{"task_id": "BigCodeBench/169", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\ndef task_func(image, sigma=2):\n    \"\"\"\n    Applies a Gaussian filter to the given image and returns the filtered image along with the plot of the original and filtered images side by side.\n    \"\"\"\n    if not isinstance(image, np.ndarray):\n        raise TypeError(\"Input image should be a numpy array.\")\n    if not isinstance(sigma, (int, float)) or sigma <= 0:\n        raise ValueError(\"Sigma should be a positive number.\")\n\n    filtered_image = gaussian_filter(image, sigma=sigma)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].imshow(image, cmap='gray', interpolation='nearest')\n    ax[0].set_title('Original')\n    ax[1].imshow(filtered_image, cmap='gray', interpolation='nearest')\n    ax[1].set_title('Filtered')\n    plt.tight_layout()\n\n    return ax, filtered_image", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\n\ndef task_func(image, sigma=2):\n    \"\"\"\n    Applies a Gaussian filter to the given image and returns the filtered image along with the plot of the original and filtered images side by side.\n    \"\"\"\n    if not isinstance(image, np.ndarray):\n        raise TypeError(\"Input image should be a numpy array.\")\n    if not isinstance(sigma, (int, float)) or sigma <= 0:\n        raise ValueError(\"Sigma should be a positive number.\")\n\n    filtered_image = gaussian_filter(image, sigma=sigma)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].imshow(image, cmap='gray', interpolation='nearest')\n    ax[0].set_title('Original')\n    ax[1].imshow(filtered_image, cmap='gray', interpolation='nearest')\n    ax[1].set_title('Filtered')\n    plt.tight_layout()\n\n    return ax, filtered_image"}
{"task_id": "BigCodeBench/170", "solution": "import pandas as pd\nimport requests\nfrom io import StringIO\ndef task_func(csv_url, sort_by_column=\"title\"):\n    \"\"\"\n    Fetches data from a given CSV URL and returns a pandas DataFrame sorted based on the specified column.\n    \"\"\"\n    response = requests.get(csv_url)\n    if response.status_code != 200:\n        raise Exception(f\"Response status code is not 200, but {response.status_code}\")\n\n    data = pd.read_csv(StringIO(response.text))\n    sorted_data = data.sort_values(by=[sort_by_column])\n\n    return sorted_data", "raw_solution": "import pandas as pd\nimport requests\nfrom io import StringIO\n\ndef task_func(csv_url, sort_by_column=\"title\"):\n    \"\"\"\n    Fetches data from a given CSV URL and returns a pandas DataFrame sorted based on the specified column.\n    \"\"\"\n    response = requests.get(csv_url)\n    if response.status_code != 200:\n        raise Exception(f\"Response status code is not 200, but {response.status_code}\")\n\n    data = pd.read_csv(StringIO(response.text))\n    sorted_data = data.sort_values(by=[sort_by_column])\n\n    return sorted_data"}
{"task_id": "BigCodeBench/171", "solution": "import random\nimport pandas as pd\nfrom collections import Counter\ndef task_func(vegetable_dict, seed=0):\n    random.seed(seed)\n\n    # Reverse the dictionary to map vegetables to people\n    rev_dict = {v: k for k, vs in vegetable_dict.items() for v in vs}\n\n    # Assign random counts to these vegetables\n    for veg in rev_dict:\n        rev_dict[veg] = random.randint(1, 10)\n\n    # Calculate the total counts\n    total_counts = sum(rev_dict.values())\n\n    # Calculate the occurrences of each vegetable as a percentage of the total counts\n    veg_counts = Counter(rev_dict)\n    percentage_occurrence = {veg: (count/total_counts)*100 for veg, count in veg_counts.items()}\n\n    # Create a DataFrame\n    df = pd.DataFrame(list(rev_dict.items()), columns=['Vegetable', 'Count'])\n    df['Percentage'] = df['Vegetable'].map(percentage_occurrence)\n\n    return df", "raw_solution": "import random\nimport pandas as pd\nfrom collections import Counter\n\n# Constants\nVEGETABLES = ['Carrot', 'Potato', 'Tomato', 'Cabbage', 'Spinach']\n\ndef task_func(vegetable_dict, seed=0):\n    random.seed(seed)\n\n    # Reverse the dictionary to map vegetables to people\n    rev_dict = {v: k for k, vs in vegetable_dict.items() for v in vs}\n\n    # Assign random counts to these vegetables\n    for veg in rev_dict:\n        rev_dict[veg] = random.randint(1, 10)\n\n    # Calculate the total counts\n    total_counts = sum(rev_dict.values())\n\n    # Calculate the occurrences of each vegetable as a percentage of the total counts\n    veg_counts = Counter(rev_dict)\n    percentage_occurrence = {veg: (count/total_counts)*100 for veg, count in veg_counts.items()}\n\n    # Create a DataFrame\n    df = pd.DataFrame(list(rev_dict.items()), columns=['Vegetable', 'Count'])\n    df['Percentage'] = df['Vegetable'].map(percentage_occurrence)\n\n    return df\n\n# Test the function\nveg_dict = {'John': ['Carrot', 'Potato'], 'Jane': ['Tomato', 'Cabbage', 'Spinach'], 'Jim': ['Potato', 'Cabbage']}"}
{"task_id": "BigCodeBench/172", "solution": "import json\nfrom datetime import datetime\ndef task_func(json_data):\n    \"\"\"\n    Determines if the given datetime is a weekend.\n    \"\"\"\n    # Parse the JSON data\n    data = json.loads(json_data)\n\n    # Extract the datetime\n    utc_datetime = datetime.strptime(data['utc_datetime'], '%Y-%m-%d %H:%M:%S')\n\n    # Determine if the date is a weekend\n    return utc_datetime.weekday() > 4", "raw_solution": "import json\nfrom datetime import datetime\n\ndef task_func(json_data):\n    \"\"\"\n    Determines if the given datetime is a weekend.\n    \"\"\"\n    # Parse the JSON data\n    data = json.loads(json_data)\n\n    # Extract the datetime\n    utc_datetime = datetime.strptime(data['utc_datetime'], '%Y-%m-%d %H:%M:%S')\n\n    # Determine if the date is a weekend\n    return utc_datetime.weekday() > 4"}
{"task_id": "BigCodeBench/173", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(country_dict):\n    \"\"\"\n    Generates a DataFrame representing the GDP for a predefined set of countries based on their presence in the provided dictionary.\n    The GDP values are simulated with random integers to model economic data.\n    \"\"\"\n    gdp_values = np.random.randint(1000000000, 100000000000, len(country_dict))\n    df = pd.DataFrame(list(country_dict.keys()), columns=['Country'])\n    df['GDP'] = gdp_values\n    df.set_index('Country', inplace=True)\n    return df\ncountry_dict = {'USA': 1, 'China': 2, 'Japan': 3, 'Germany': 4, 'UK': 5}", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(country_dict):\n    \"\"\"\n    Generates a DataFrame representing the GDP for a predefined set of countries based on their presence in the provided dictionary.\n    The GDP values are simulated with random integers to model economic data.\n    \"\"\"\n    gdp_values = np.random.randint(1000000000, 100000000000, len(country_dict))\n    df = pd.DataFrame(list(country_dict.keys()), columns=['Country'])\n    df['GDP'] = gdp_values\n    df.set_index('Country', inplace=True)\n    return df\n\n# Test the function\ncountry_dict = {'USA': 1, 'China': 2, 'Japan': 3, 'Germany': 4, 'UK': 5}"}
{"task_id": "BigCodeBench/174", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data, key, min_value, max_value):\n    \"\"\"\n    Adds a new column with random values to the 'data' DataFrame.\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n\n    data[key] = np.random.randint(min_value, max_value, data.shape[0])\n\n    return data\ndata = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50]})\nkey = 'C'\nmin_value = 1\nmax_value = 100", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data, key, min_value, max_value):\n    \"\"\"\n    Adds a new column with random values to the 'data' DataFrame.\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n\n    data[key] = np.random.randint(min_value, max_value, data.shape[0])\n\n    return data\n\n# Testing the function\ndata = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50]})\nkey = 'C'\nmin_value = 1\nmax_value = 100\n\ntry:\n    new_data = task_func(data, key, min_value, max_value)\n    print(new_data)\nexcept ValueError as e:\n    print(e)"}
{"task_id": "BigCodeBench/175", "solution": "import re\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df):\n    \"\"\"\n    Analyzes a DataFrame to find videos with titles containing \"how\" or \"what\" and visualizes their like ratios.\n    The like ratio for each video is calculated by dividing the number of likes by the number of views.\n    This function generates a bar plot of the like ratios for these specific videos.\n    If the DataFrame is empty, lacks the required columns, or contains no titles matching the criteria, an empty subplot is returned.\n    \"\"\"\n    # Check for the presence of the necessary data columns\n    if not all(col in df.columns for col in ['Title', 'Views', 'Likes']):\n        print(\"DataFrame does not contain the required columns.\")\n        return plt.subplot(111)\n\n    # Filter videos with titles containing \"how\" or \"what\"\n    filtered_df = df[df['Title'].apply(lambda x: re.search(r'\\b(how|what)\\b', x, re.I) is not None)]\n\n    # Check if there are any entries matching the search criteria\n    if filtered_df.empty:\n        print(\"No video titles match the search criteria.\")\n        return plt.subplot(111)\n\n    # Calculate like ratios\n    filtered_df['Like Ratio'] = filtered_df['Likes'] / filtered_df['Views']\n\n    # Generate bar plot\n    ax = filtered_df.plot(x='Title', y='Like Ratio', kind='bar', legend=False)\n    ax.set_xlabel('Video Title')\n    ax.set_ylabel('Like Ratio')\n    plt.tight_layout()\n\n    return ax", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df):\n    \"\"\"\n    Analyzes a DataFrame to find videos with titles containing \"how\" or \"what\" and visualizes their like ratios.\n    The like ratio for each video is calculated by dividing the number of likes by the number of views.\n    This function generates a bar plot of the like ratios for these specific videos.\n    If the DataFrame is empty, lacks the required columns, or contains no titles matching the criteria, an empty subplot is returned.\n    \"\"\"\n    # Check for the presence of the necessary data columns\n    if not all(col in df.columns for col in ['Title', 'Views', 'Likes']):\n        print(\"DataFrame does not contain the required columns.\")\n        return plt.subplot(111)\n\n    # Filter videos with titles containing \"how\" or \"what\"\n    filtered_df = df[df['Title'].apply(lambda x: re.search(r'\\b(how|what)\\b', x, re.I) is not None)]\n\n    # Check if there are any entries matching the search criteria\n    if filtered_df.empty:\n        print(\"No video titles match the search criteria.\")\n        return plt.subplot(111)\n\n    # Calculate like ratios\n    filtered_df['Like Ratio'] = filtered_df['Likes'] / filtered_df['Views']\n\n    # Generate bar plot\n    ax = filtered_df.plot(x='Title', y='Like Ratio', kind='bar', legend=False)\n    ax.set_xlabel('Video Title')\n    ax.set_ylabel('Like Ratio')\n    plt.tight_layout()\n\n    return ax"}
{"task_id": "BigCodeBench/176", "solution": "import re\nimport socket\ndef task_func(ip_addresses: list) -> dict:\n    \"\"\"\n    Given a list of IP addresses, this function returns a dictionary mapping each valid IP address to its respective hostname.\n    If the hostname cannot be determined, the value will be None.\n    \"\"\"\n    result = {}\n    for ip in ip_addresses:\n        try:\n            result[ip] = socket.gethostbyaddr(ip)[0]\n        except socket.herror:\n            result[ip] = None\n    return result\nip_addresses = ['127.0.0.1', '8.8.8.8', '2404:6800:4008:80f::200e']", "raw_solution": "import re\nimport socket\n\ndef task_func(ip_addresses: list) -> dict:\n    \"\"\"\n    Given a list of IP addresses, this function returns a dictionary mapping each valid IP address to its respective hostname.\n    If the hostname cannot be determined, the value will be None.\n    \"\"\"\n    result = {}\n    for ip in ip_addresses:\n        try:\n            result[ip] = socket.gethostbyaddr(ip)[0]\n        except socket.herror:\n            result[ip] = None\n    return result\n\n# Testing the function\nip_addresses = ['127.0.0.1', '8.8.8.8', '2404:6800:4008:80f::200e']"}
{"task_id": "BigCodeBench/177", "solution": "import pandas as pd\nimport re\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize\nfrom string import punctuation\ndef task_func(df):\n    \"\"\"\n    Extracts articles whose titles contain specific case-insensitive keywords (\"like\" or \"what\") from a DataFrame and analyzes the frequency of each word in the content of these articles, excluding punctuation.\n    \"\"\"\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"The DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    # Extract articles with titles containing \"like\" or \"what\"\n    articles = df[df['Title'].str.contains('like', case=False) | df['Title'].str.contains('what', case=False)]\n\n    # Extract words from the content of the articles\n    words = word_tokenize(articles['Content'].str.cat(sep=' '))\n\n    # Remove punctuation\n    words = [word.lower() for word in words if word not in punctuation]\n\n    # Count the frequency of each word\n    word_freq = dict(Counter(words))\n\n    return word_freq", "raw_solution": "import pandas as pd\nimport re\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize\nfrom string import punctuation\n\ndef task_func(df):\n    \"\"\"\n    Extracts articles whose titles contain specific case-insensitive keywords (\"like\" or \"what\") from a DataFrame and analyzes the frequency of each word in the content of these articles, excluding punctuation.\n    \"\"\"\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"The DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    # Extract articles with titles containing \"like\" or \"what\"\n    articles = df[df['Title'].str.contains('like', case=False) | df['Title'].str.contains('what', case=False)]\n\n    # Extract words from the content of the articles\n    words = word_tokenize(articles['Content'].str.cat(sep=' '))\n\n    # Remove punctuation\n    words = [word.lower() for word in words if word not in punctuation]\n\n    # Count the frequency of each word\n    word_freq = dict(Counter(words))\n\n    return word_freq"}
{"task_id": "BigCodeBench/178", "solution": "import re\nimport json\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\ndef task_func(ip_address):\n    \"\"\"\n    This function takes a JSON response containing the IP address, checks if the IP address is valid,\n    and returns the IP address if it is valid. If the IP address is not valid, it returns 'Invalid IP address received'.\n    \"\"\"\n    # Check if the IP address is valid\n    if re.fullmatch(IP_REGEX, ip_address):\n        return ip_address\n    else:\n        return 'Invalid IP address received'", "raw_solution": "import re\nimport json\n\n# Constants\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef task_func(ip_address):\n    \"\"\"\n    This function takes a JSON response containing the IP address, checks if the IP address is valid,\n    and returns the IP address if it is valid. If the IP address is not valid, it returns 'Invalid IP address received'.\n    \"\"\"\n    # Check if the IP address is valid\n    if re.fullmatch(IP_REGEX, ip_address):\n        return ip_address\n    else:\n        return 'Invalid IP address received'\n\n# Test the function"}
{"task_id": "BigCodeBench/179", "solution": "import re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\ndef task_func(df):\n    \"\"\"\n    Analyzes a given DataFrame containing article titles and content to identify articles with titles that include the words \"how\" or \"what\".\n    It calculates the TF-IDF scores for the words in the content of these articles and visualizes these scores in a bar plot.\n    \"\"\"\n    # Check if 'Title' and 'Content' columns exist in the DataFrame\n    if 'Title' not in df.columns or 'Content' not in df.columns:\n        return plt.figure()\n\n    # Filter articles based on the title\n    df = df[df['Title'].str.contains('how|what', case=False)]\n\n    # Check if there are any articles left\n    if df.empty:\n        return plt.figure()\n\n    # Calculate TF-IDF scores\n    vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(df['Content'])\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Calculate average TF-IDF scores for each article\n    avg_tfidf_scores = np.mean(tfidf_matrix, axis=0).tolist()[0]\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(feature_names, avg_tfidf_scores)\n    ax.set_ylabel('TF-IDF Score')\n    ax.set_xticklabels(feature_names, rotation=90)\n\n    return ax", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\ndef task_func(df):\n    \"\"\"\n    Analyzes a given DataFrame containing article titles and content to identify articles with titles that include the words \"how\" or \"what\".\n    It calculates the TF-IDF scores for the words in the content of these articles and visualizes these scores in a bar plot.\n    \"\"\"\n    # Check if 'Title' and 'Content' columns exist in the DataFrame\n    if 'Title' not in df.columns or 'Content' not in df.columns:\n        return plt.figure()\n\n    # Filter articles based on the title\n    df = df[df['Title'].str.contains('how|what', case=False)]\n\n    # Check if there are any articles left\n    if df.empty:\n        return plt.figure()\n\n    # Calculate TF-IDF scores\n    vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(df['Content'])\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Calculate average TF-IDF scores for each article\n    avg_tfidf_scores = np.mean(tfidf_matrix, axis=0).tolist()[0]\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(feature_names, avg_tfidf_scores)\n    ax.set_ylabel('TF-IDF Score')\n    ax.set_xticklabels(feature_names, rotation=90)\n\n    return ax"}
{"task_id": "BigCodeBench/180", "solution": "from PIL import Image\nimport numpy as np\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(img_path, scale_factors=[0.5, 0.75, 1.5, 2.0]):\n    \"\"\"\n    Open an image file and scale it by different scaling factors. Display each scaled image using matplotlib and return the scaled images with their Axes.\n    \"\"\"\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"Image file {img_path} not found\")\n\n    img = Image.open(img_path)\n    img_array = np.array(img)\n\n    result = []\n    for scale in scale_factors:\n        scaled_img_array = resize(img_array, (int(img_array.shape[0] * scale), int(img_array.shape[1] * scale)))\n        fig, ax = plt.subplots(1, 1)\n        ax.imshow(scaled_img_array)\n        ax.axis('off')\n        result.append((ax, scaled_img_array))\n\n    return result", "raw_solution": "from PIL import Image\nimport numpy as np\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(img_path, scale_factors=[0.5, 0.75, 1.5, 2.0]):\n    \"\"\"\n    Open an image file and scale it by different scaling factors. Display each scaled image using matplotlib and return the scaled images with their Axes.\n    \"\"\"\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"Image file {img_path} not found\")\n\n    img = Image.open(img_path)\n    img_array = np.array(img)\n\n    result = []\n    for scale in scale_factors:\n        scaled_img_array = resize(img_array, (int(img_array.shape[0] * scale), int(img_array.shape[1] * scale)))\n        fig, ax = plt.subplots(1, 1)\n        ax.imshow(scaled_img_array)\n        ax.axis('off')\n        result.append((ax, scaled_img_array))\n\n    return result"}
{"task_id": "BigCodeBench/181", "solution": "from django.http import HttpResponse\nfrom django.conf import settings\nimport random\nimport time\nimport json\ndef task_func(data, min_delay, max_delay):\n    \"\"\"\n    After a random delay, generate a Django HttpResponse with JSON data to simulate the latency of the network.\n    \"\"\"\n    delay = random.randint(min_delay, max_delay)\n    time.sleep(delay)\n    json_data = json.dumps(data)\n    return HttpResponse(json_data, content_type='application/json')", "raw_solution": "from django.http import HttpResponse\nfrom django.conf import settings\nimport random\nimport time\nimport json\n\ndef task_func(data, min_delay, max_delay):\n    \"\"\"\n    After a random delay, generate a Django HttpResponse with JSON data to simulate the latency of the network.\n    \"\"\"\n    delay = random.randint(min_delay, max_delay)\n    time.sleep(delay)\n    json_data = json.dumps(data)\n    return HttpResponse(json_data, content_type='application/json')"}
{"task_id": "BigCodeBench/182", "solution": "import re\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef task_func(df):\n    \"\"\"\n    Analyzes articles by their titles for specific case-insensitive keywords (\"how\" or \"what\"),\n    vectorizes the content using CountVectorizer, and groups them into clusters using KMeans clustering.\n    \"\"\"\n    # Filter articles that start with \"how\" or \"what\"\n    df = df[df['title'].str.lower().str.startswith('how') | df['title'].str.lower().str.startswith('what')]\n\n    # Vectorize the content using CountVectorizer\n    vectorizer = CountVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(df['content'])\n\n    # Group articles into clusters using KMeans clustering\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(X)\n\n    # Get cluster labels for the filtered articles\n    labels = kmeans.labels_\n\n    return labels.tolist()\ndf = pd.DataFrame({\n    'title': ['How to code', 'What is AI', 'How to learn Python', 'What is Data Science', 'How to cook'],\n    'content': ['This is a guide to coding', 'This is an introduction to AI', 'This is a guide to Python', 'This is an introduction to Data Science', 'This is a guide to cooking']\n})", "raw_solution": "import re\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef task_func(df):\n    \"\"\"\n    Analyzes articles by their titles for specific case-insensitive keywords (\"how\" or \"what\"),\n    vectorizes the content using CountVectorizer, and groups them into clusters using KMeans clustering.\n    \"\"\"\n    # Filter articles that start with \"how\" or \"what\"\n    df = df[df['title'].str.lower().str.startswith('how') | df['title'].str.lower().str.startswith('what')]\n\n    # Vectorize the content using CountVectorizer\n    vectorizer = CountVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(df['content'])\n\n    # Group articles into clusters using KMeans clustering\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(X)\n\n    # Get cluster labels for the filtered articles\n    labels = kmeans.labels_\n\n    return labels.tolist()\n\n# Test the function\ndf = pd.DataFrame({\n    'title': ['How to code', 'What is AI', 'How to learn Python', 'What is Data Science', 'How to cook'],\n    'content': ['This is a guide to coding', 'This is an introduction to AI', 'This is a guide to Python', 'This is an introduction to Data Science', 'This is a guide to cooking']\n})"}
{"task_id": "BigCodeBench/183", "solution": "from django.http import HttpResponse\nimport uuid\nimport json\ndef task_func(data):\n    \"\"\"\n    Create a Django HttpResponse with JSON data, and include a UUID in the HTTP headers to track requests.\n    \"\"\"\n    # Generate a UUID\n    request_id = uuid.uuid4()\n\n    # Convert the data to JSON\n    json_data = json.dumps(data)\n\n    # Create the HttpResponse with JSON data\n    response = HttpResponse(json_data, content_type='application/json')\n\n    # Add the UUID to the headers\n    response['X-Request-ID'] = str(request_id)\n\n    return response", "raw_solution": "from django.http import HttpResponse\nimport uuid\nimport json\n\ndef task_func(data):\n    \"\"\"\n    Create a Django HttpResponse with JSON data, and include a UUID in the HTTP headers to track requests.\n    \"\"\"\n    # Generate a UUID\n    request_id = uuid.uuid4()\n\n    # Convert the data to JSON\n    json_data = json.dumps(data)\n\n    # Create the HttpResponse with JSON data\n    response = HttpResponse(json_data, content_type='application/json')\n\n    # Add the UUID to the headers\n    response['X-Request-ID'] = str(request_id)\n\n    return response"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    \"\"\"\n    Prepares and transforms text data from a specified column in a DataFrame by removing stopwords, numbers, and punctuation,\n    and subsequently applying a vectorization process to convert text into a numeric format suitable for analysis.\n    \"\"\"\n    # Remove stopwords, numbers, and punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join(\n        [word for word in x.split() if word not in STOPWORDS and not word.isdigit() and word.isalpha()]))\n\n    # Apply vectorization\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert to DataFrame\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    \"\"\"\n    Prepares and transforms text data from a specified column in a DataFrame by removing stopwords, numbers, and punctuation,\n    and subsequently applying a vectorization process to convert text into a numeric format suitable for analysis.\n    \"\"\"\n    # Remove stopwords, numbers, and punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join(\n        [word for word in x.split() if word not in STOPWORDS and not word.isdigit() and word.isalpha()]))\n\n    # Apply vectorization\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert to DataFrame\n    df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n\n    return df"}
{"task_id": "BigCodeBench/185", "solution": "import pandas as pd\nimport numpy as np\nimport folium\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not isinstance(dic, dict):\n        raise ValueError(\"Input should be a dictionary\")\n    if not isinstance(cities, list):\n        raise ValueError(\"Cities should be a list\")\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"All cities should be strings\")\n    if not all(isinstance(coord, tuple) and len(coord) == 2 for coord in dic.values()):\n        raise ValueError(\"Coordinates should be tuples of length 2\")\n    if not all(key in dic for key in ['Lon', 'Lat']):\n        raise ValueError(\"'Lon' and 'Lat' keys are missing in the dictionary\")\n\n    lon_range, lat_range = dic['Lon'], dic['Lat']\n    df = pd.DataFrame(cities, columns=['City'])\n    df['Longitude'] = np.random.uniform(lon_range[0], lon_range[1], len(cities))\n    df['Latitude'] = np.random.uniform(lat_range[0], lat_range[1], len(cities))\n\n    m = folium.Map(location=[np.mean(lat_range), np.mean(lon_range)], zoom_start=2)\n    for _, row in df.iterrows():\n        folium.Marker([row['Latitude'], row['Longitude']], popup=row['City']).add_to(m)\n\n    return (m, df)", "raw_solution": "import pandas as pd\nimport numpy as np\nimport folium\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not isinstance(dic, dict):\n        raise ValueError(\"Input should be a dictionary\")\n    if not isinstance(cities, list):\n        raise ValueError(\"Cities should be a list\")\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"All cities should be strings\")\n    if not all(isinstance(coord, tuple) and len(coord) == 2 for coord in dic.values()):\n        raise ValueError(\"Coordinates should be tuples of length 2\")\n    if not all(key in dic for key in ['Lon', 'Lat']):\n        raise ValueError(\"'Lon' and 'Lat' keys are missing in the dictionary\")\n\n    lon_range, lat_range = dic['Lon'], dic['Lat']\n    df = pd.DataFrame(cities, columns=['City'])\n    df['Longitude'] = np.random.uniform(lon_range[0], lon_range[1], len(cities))\n    df['Latitude'] = np.random.uniform(lat_range[0], lat_range[1], len(cities))\n\n    m = folium.Map(location=[np.mean(lat_range), np.mean(lon_range)], zoom_start=2)\n    for _, row in df.iterrows():\n        folium.Marker([row['Latitude'], row['Longitude']], popup=row['City']).add_to(m)\n\n    return (m, df)"}
{"task_id": "BigCodeBench/186", "solution": "from geopy.distance import geodesic\nimport folium\ndef task_func(dic):\n    \"\"\"\n    Generates a Folium map with markers for specified locations and calculates the geodesic distances between each pair of locations.\n    \"\"\"\n    if not dic:\n        raise ValueError(\"Input dictionary is empty\")\n\n    # Create a map centered at the first location\n    m = folium.Map(location=list(dic.values())[0], zoom_start=13)\n\n    # Add markers to the map for each location\n    for name, coords in dic.items():\n        folium.Marker(location=coords, popup=name).add_to(m)\n\n    # Calculate distances between each pair of locations\n    distances = {}\n    for name1, coords1 in dic.items():\n        for name2, coords2 in dic.items():\n            if name1 != name2:\n                distance = geodesic(coords1, coords2).km\n                distances[(name1, name2)] = distance\n\n    return m, distances", "raw_solution": "from geopy.distance import geodesic\nimport folium\n\ndef task_func(dic):\n    \"\"\"\n    Generates a Folium map with markers for specified locations and calculates the geodesic distances between each pair of locations.\n    \"\"\"\n    if not dic:\n        raise ValueError(\"Input dictionary is empty\")\n\n    # Create a map centered at the first location\n    m = folium.Map(location=list(dic.values())[0], zoom_start=13)\n\n    # Add markers to the map for each location\n    for name, coords in dic.items():\n        folium.Marker(location=coords, popup=name).add_to(m)\n\n    # Calculate distances between each pair of locations\n    distances = {}\n    for name1, coords1 in dic.items():\n        for name2, coords2 in dic.items():\n            if name1 != name2:\n                distance = geodesic(coords1, coords2).km\n                distances[(name1, name2)] = distance\n\n    return m, distances"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not isinstance(dic, dict):\n        raise ValueError(\"Input 'dic' should be a dictionary.\")\n    if not isinstance(cities, list):\n        raise ValueError(\"Input 'cities' should be a list.\")\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"'Lon' and 'Lat' keys are required in the dictionary.\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"'Lon' and 'Lat' values should be tuples.\")\n\n    lon_range, lat_range = dic['Lon'], dic['Lat']\n    coordinates = [(np.random.uniform(lon_range[0], lon_range[1]), np.random.uniform(lat_range[0], lat_range[1])) for _ in range(len(cities))]\n\n    geometry = [Point(xy) for xy in coordinates]\n    df = gpd.GeoDataFrame(cities, geometry=geometry, crs=4326)\n    df.columns = ['City', 'Coordinates']\n\n    return df", "raw_solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    if not isinstance(dic, dict):\n        raise ValueError(\"Input 'dic' should be a dictionary.\")\n    if not isinstance(cities, list):\n        raise ValueError(\"Input 'cities' should be a list.\")\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"'Lon' and 'Lat' keys are required in the dictionary.\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"'Lon' and 'Lat' values should be tuples.\")\n\n    lon_range, lat_range = dic['Lon'], dic['Lat']\n    coordinates = [(np.random.uniform(lon_range[0], lon_range[1]), np.random.uniform(lat_range[0], lat_range[1])) for _ in range(len(cities))]\n\n    geometry = [Point(xy) for xy in coordinates]\n    df = gpd.GeoDataFrame(cities, geometry=geometry, crs=4326)\n    df.columns = ['City', 'Coordinates']\n\n    return df"}
{"task_id": "BigCodeBench/188", "solution": "import pandas as pd\nimport folium\nfrom geopy.geocoders import Photon\ndef task_func(dic):\n    \"\"\"\n    Generates a Folium map with markers for specified locations.\n    It preprocesses the input to handle both direct geographical coordinates and address strings.\n    For address strings, it dynamically resolves their latitude and longitude using the Photon geolocation service.\n    This flexible input handling allows for easy mapping of various location types.\n    \"\"\"\n    geolocator = Photon(user_agent=\"geoapiExercises\")\n\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(dic)\n\n    # Initialize an empty list to store the coordinates\n    coordinates = []\n\n    # Iterate over each row in the DataFrame\n    for index, row in df.iterrows():\n        location = row['location']\n\n        # Check if the location is a string (address) or a tuple (geographical coordinates)\n        if isinstance(location, str):\n            # Use Photon geolocator to get the latitude and longitude\n            geolocation = geolocator.geocode(location)\n            latitude, longitude = geolocation.latitude, geolocation.longitude\n        else:\n            latitude, longitude = location\n\n        # Append the latitude and longitude to the list\n        coordinates.append((latitude, longitude))\n\n    # Create a Folium map\n    m = folium.Map(location=[df['latitude'].mean(), df['longitude'].mean()], zoom_start=13)\n\n    # Add markers to the map for each location\n    for lat, lon in coordinates:\n        folium.Marker([lat, lon]).add_to(m)\n\n    return m", "raw_solution": "import pandas as pd\nimport folium\nfrom geopy.geocoders import Photon\n\ndef task_func(dic):\n    \"\"\"\n    Generates a Folium map with markers for specified locations.\n    It preprocesses the input to handle both direct geographical coordinates and address strings.\n    For address strings, it dynamically resolves their latitude and longitude using the Photon geolocation service.\n    This flexible input handling allows for easy mapping of various location types.\n    \"\"\"\n    geolocator = Photon(user_agent=\"geoapiExercises\")\n\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(dic)\n\n    # Initialize an empty list to store the coordinates\n    coordinates = []\n\n    # Iterate over each row in the DataFrame\n    for index, row in df.iterrows():\n        location = row['location']\n\n        # Check if the location is a string (address) or a tuple (geographical coordinates)\n        if isinstance(location, str):\n            # Use Photon geolocator to get the latitude and longitude\n            geolocation = geolocator.geocode(location)\n            latitude, longitude = geolocation.latitude, geolocation.longitude\n        else:\n            latitude, longitude = location\n\n        # Append the latitude and longitude to the list\n        coordinates.append((latitude, longitude))\n\n    # Create a Folium map\n    m = folium.Map(location=[df['latitude'].mean(), df['longitude'].mean()], zoom_start=13)\n\n    # Add markers to the map for each location\n    for lat, lon in coordinates:\n        folium.Marker([lat, lon]).add_to(m)\n\n    return m"}
{"task_id": "BigCodeBench/189", "solution": "import re\nimport json\nimport requests\nfrom typing import List\ndef task_func(data_url: str) -> List[str]:\n    \"\"\"\n    Fetches data from a specific URL and extracts all names from the JSON-formatted data that are not enclosed by square brackets.\n    \"\"\"\n    try:\n        response = requests.get(data_url)\n        data = json.loads(response.text)\n        names = re.findall(r'\"name\":\\s*\"([^\"]+)\"', json.dumps(data))\n        return names\n    except Exception:\n        return [\"Invalid url input\"]", "raw_solution": "import re\nimport json\nimport requests\nfrom typing import List\n\ndef task_func(data_url: str) -> List[str]:\n    \"\"\"\n    Fetches data from a specific URL and extracts all names from the JSON-formatted data that are not enclosed by square brackets.\n    \"\"\"\n    try:\n        response = requests.get(data_url)\n        data = json.loads(response.text)\n        names = re.findall(r'\"name\":\\s*\"([^\"]+)\"', json.dumps(data))\n        return names\n    except Exception:\n        return [\"Invalid url input\"]"}
{"task_id": "BigCodeBench/190", "solution": "import sqlite3\nimport pandas as pd\nimport csv\nfrom io import StringIO\nDATABASE_NAME = 'test.db'\nTABLE_NAME = 'test_table'\ndef task_func(csv_input):\n    # Connect to SQLite database\n    conn = sqlite3.connect(DATABASE_NAME)\n    cursor = conn.cursor()\n\n    # Create table if it doesn't exist\n    cursor.execute(f'CREATE TABLE IF NOT EXISTS {TABLE_NAME} (id INTEGER PRIMARY KEY, data TEXT)')\n\n    # Delete existing data from table\n    cursor.execute(f'DELETE FROM {TABLE_NAME}')\n\n    # Insert data from CSV into table\n    if isinstance(csv_input, str):\n        with open(csv_input, 'r') as file:\n            csv_data = csv.reader(file)\n            next(csv_data)  # Skip header\n            cursor.executemany(f'INSERT INTO {TABLE_NAME} (data) VALUES (?)', csv_data)\n    elif isinstance(csv_input, StringIO):\n        csv_data = csv.reader(csv_input)\n        next(csv_data)  # Skip header\n        cursor.executemany(f'INSERT INTO {TABLE_NAME} (data) VALUES (?)', csv_data)\n    else:\n        raise TypeError('csv_input must be a file path or StringIO object')\n\n    # Commit changes and close connection\n    conn.commit()\n    conn.close()\n\n    # Query data from table and return as DataFrame\n    conn = sqlite3.connect(DATABASE_NAME)\n    df = pd.read_sql_query(f'SELECT * FROM {TABLE_NAME}', conn)\n    conn.close()\n\n    return df", "raw_solution": "import sqlite3\nimport pandas as pd\nimport csv\nfrom io import StringIO\n\n# Constants\nDATABASE_NAME = 'test.db'\nTABLE_NAME = 'test_table'\n\ndef task_func(csv_input):\n    # Connect to SQLite database\n    conn = sqlite3.connect(DATABASE_NAME)\n    cursor = conn.cursor()\n\n    # Create table if it doesn't exist\n    cursor.execute(f'CREATE TABLE IF NOT EXISTS {TABLE_NAME} (id INTEGER PRIMARY KEY, data TEXT)')\n\n    # Delete existing data from table\n    cursor.execute(f'DELETE FROM {TABLE_NAME}')\n\n    # Insert data from CSV into table\n    if isinstance(csv_input, str):\n        with open(csv_input, 'r') as file:\n            csv_data = csv.reader(file)\n            next(csv_data)  # Skip header\n            cursor.executemany(f'INSERT INTO {TABLE_NAME} (data) VALUES (?)', csv_data)\n    elif isinstance(csv_input, StringIO):\n        csv_data = csv.reader(csv_input)\n        next(csv_data)  # Skip header\n        cursor.executemany(f'INSERT INTO {TABLE_NAME} (data) VALUES (?)', csv_data)\n    else:\n        raise TypeError('csv_input must be a file path or StringIO object')\n\n    # Commit changes and close connection\n    conn.commit()\n    conn.close()\n\n    # Query data from table and return as DataFrame\n    conn = sqlite3.connect(DATABASE_NAME)\n    df = pd.read_sql_query(f'SELECT * FROM {TABLE_NAME}', conn)\n    conn.close()\n\n    return df"}
{"task_id": "BigCodeBench/191", "solution": "import random\nfrom scipy import stats\ndef task_func(animals, mean):\n    # Generate a Poisson distributed number of customers\n    num_customers = stats.poisson.rvs(mean)\n\n    # Initialize a dictionary to store sales\n    sales = {animal: 0 for animal in animals}\n\n    # Simulate sales for each customer\n    for _ in range(num_customers):\n        # Randomly select an animal for the customer to buy\n        animal = random.choice(animals)\n        # Increment the count of the selected animal in the sales dictionary\n        sales[animal] += 1\n\n    return sales\nanimals = ['dog', 'cat', 'bird', 'fish']\nmean = 5", "raw_solution": "import random\nfrom scipy import stats\n\ndef task_func(animals, mean):\n    # Generate a Poisson distributed number of customers\n    num_customers = stats.poisson.rvs(mean)\n\n    # Initialize a dictionary to store sales\n    sales = {animal: 0 for animal in animals}\n\n    # Simulate sales for each customer\n    for _ in range(num_customers):\n        # Randomly select an animal for the customer to buy\n        animal = random.choice(animals)\n        # Increment the count of the selected animal in the sales dictionary\n        sales[animal] += 1\n\n    return sales\n\n# Test the function\nanimals = ['dog', 'cat', 'bird', 'fish']\nmean = 5"}
{"task_id": "BigCodeBench/192", "solution": "import re\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\nRECEPIENT_ADDRESS = \"names@gmail.com\"\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\ndef task_func(text=TEXT, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, recepient_address=RECEPIENT_ADDRESS, smtp=None):\n    # Extract names\n    names = re.findall(r'\\[.*?\\]', text)\n    names = [name.split(' ')[0] for name in names]\n\n    # Send email\n    if smtp is None:\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        smtp.starttls()\n        smtp.login(email_address, email_password)\n\n    msg = MIMEMultipart()\n    msg['From'] = email_address\n    msg['To'] = recepient_address\n    msg['Subject'] = 'Extracted Names'\n    msg.attach(MIMEText('\\n'.join(names)))\n\n    smtp.send_message(msg)\n    smtp.quit()\n\n    return names", "raw_solution": "import re\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\n\n# Constants\nTEXT = \"Josie Smith [3996 COLLEGE AVENUE, SOMETOWN, MD 21003]Mugsy Dog Smith [2560 OAK ST, GLENMEADE, WI 14098]\"\nRECEPIENT_ADDRESS = \"names@gmail.com\"\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\n\ndef task_func(text=TEXT, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, recepient_address=RECEPIENT_ADDRESS, smtp=None):\n    # Extract names\n    names = re.findall(r'\\[.*?\\]', text)\n    names = [name.split(' ')[0] for name in names]\n\n    # Send email\n    if smtp is None:\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        smtp.starttls()\n        smtp.login(email_address, email_password)\n\n    msg = MIMEMultipart()\n    msg['From'] = email_address\n    msg['To'] = recepient_address\n    msg['Subject'] = 'Extracted Names'\n    msg.attach(MIMEText('\\n'.join(names)))\n\n    smtp.send_message(msg)\n    smtp.quit()\n\n    return names"}
{"task_id": "BigCodeBench/193", "solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, randint, sample\nDATA_TYPES = [str, int, float, list, tuple, dict, set]\ndef generate_random_data(data_type):\n    if data_type == str:\n        return ''.join(choice('abcdefghijklmnopqrstuvwxyz') for _ in range(5))\n    elif data_type == int:\n        return randint(0, 9)\n    elif data_type == float:\n        return float(randint(0, 9))\n    elif data_type == list:\n        return sample(range(0, 10), randint(1, 5))\n    elif data_type == tuple:\n        return tuple(sample(range(0, 10), randint(1, 5)))\n    elif data_type == dict:\n        return {i: randint(0, 9) for i in range(randint(1, 5))}\n    elif data_type == set:\n        return set(sample(range(0, 10), randint(1, 5)))\ndef task_func(rows, columns):\n    data = {f'col{i}': [generate_random_data(choice(DATA_TYPES)) for _ in range(rows)] for i in range(columns)}\n    return pd.DataFrame(data)", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom random import choice, randint, sample\n\n# Constants\nDATA_TYPES = [str, int, float, list, tuple, dict, set]\n\ndef generate_random_data(data_type):\n    if data_type == str:\n        return ''.join(choice('abcdefghijklmnopqrstuvwxyz') for _ in range(5))\n    elif data_type == int:\n        return randint(0, 9)\n    elif data_type == float:\n        return float(randint(0, 9))\n    elif data_type == list:\n        return sample(range(0, 10), randint(1, 5))\n    elif data_type == tuple:\n        return tuple(sample(range(0, 10), randint(1, 5)))\n    elif data_type == dict:\n        return {i: randint(0, 9) for i in range(randint(1, 5))}\n    elif data_type == set:\n        return set(sample(range(0, 10), randint(1, 5)))\n\ndef task_func(rows, columns):\n    data = {f'col{i}': [generate_random_data(choice(DATA_TYPES)) for _ in range(rows)] for i in range(columns)}\n    return pd.DataFrame(data)\n\n# Test the function\ndf = task_func(5, 3)"}
{"task_id": "BigCodeBench/194", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nBAR_COLOR = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\ndef task_func(data_size):\n    \"\"\"\n    Generates random numeric data and creates a histogram of the data. The color of the histogram bars is randomly selected from a predefined list.\n    \"\"\"\n    # Generate random data\n    data = np.random.randn(data_size)\n\n    # Select a random color from the list\n    bar_color = np.random.choice(BAR_COLOR)\n\n    # Create histogram\n    plt.hist(data, color=bar_color)\n    plt.show()\n\n    return data, bar_color", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nBAR_COLOR = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']\n\ndef task_func(data_size):\n    \"\"\"\n    Generates random numeric data and creates a histogram of the data. The color of the histogram bars is randomly selected from a predefined list.\n    \"\"\"\n    # Generate random data\n    data = np.random.randn(data_size)\n\n    # Select a random color from the list\n    bar_color = np.random.choice(BAR_COLOR)\n\n    # Create histogram\n    plt.hist(data, color=bar_color)\n    plt.show()\n\n    return data, bar_color"}
{"task_id": "BigCodeBench/195", "solution": "import subprocess\nimport platform\nimport time\ndef task_func(url):\n    \"\"\"\n    Open a web page in the default web browser in a background process.\n    \"\"\"\n    # Determine the command to open the URL based on the operating system\n    if platform.system() == \"Windows\":\n        command = [\"start\", url]\n    elif platform.system() == \"Darwin\":  # MacOS\n        command = [\"open\", url]\n    else:  # Linux\n        command = [\"xdg-open\", url]\n\n    # Start the subprocess\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Wait for the process to finish\n    time.sleep(1)\n\n    # Get the return code\n    return_code = process.poll()\n\n    return return_code", "raw_solution": "import subprocess\nimport platform\nimport time\n\ndef task_func(url):\n    \"\"\"\n    Open a web page in the default web browser in a background process.\n    \"\"\"\n    # Determine the command to open the URL based on the operating system\n    if platform.system() == \"Windows\":\n        command = [\"start\", url]\n    elif platform.system() == \"Darwin\":  # MacOS\n        command = [\"open\", url]\n    else:  # Linux\n        command = [\"xdg-open\", url]\n\n    # Start the subprocess\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Wait for the process to finish\n    time.sleep(1)\n\n    # Get the return code\n    return_code = process.poll()\n\n    return return_code"}
{"task_id": "BigCodeBench/196", "solution": "import random\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom typing import Tuple, List\ndef task_func(length: int, range_limit: int = 100, seed: int = 0) -> Tuple[plt.Axes, List[int]]:\n    \"\"\"\n    Generates a list of random numbers, sorts them, and plots a histogram of the distribution.\n    Returns the axes object and the list of random numbers.\n    \"\"\"\n    if range_limit <= 1:\n        raise ValueError(\"range_limit must be greater than 1\")\n\n    random.seed(seed)\n    numbers = [random.randint(0, range_limit) for _ in range(length)]\n    numbers.sort()\n\n    ax = sns.histplot(numbers, bins=range_limit, kde=False)\n    plt.title(\"Histogram of Sorted Random Numbers\")\n    plt.xlabel(\"Number\")\n    plt.ylabel(\"Frequency\")\n\n    return ax, numbers", "raw_solution": "import random\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom typing import Tuple, List\n\ndef task_func(length: int, range_limit: int = 100, seed: int = 0) -> Tuple[plt.Axes, List[int]]:\n    \"\"\"\n    Generates a list of random numbers, sorts them, and plots a histogram of the distribution.\n    Returns the axes object and the list of random numbers.\n    \"\"\"\n    if range_limit <= 1:\n        raise ValueError(\"range_limit must be greater than 1\")\n\n    random.seed(seed)\n    numbers = [random.randint(0, range_limit) for _ in range(length)]\n    numbers.sort()\n\n    ax = sns.histplot(numbers, bins=range_limit, kde=False)\n    plt.title(\"Histogram of Sorted Random Numbers\")\n    plt.xlabel(\"Number\")\n    plt.ylabel(\"Frequency\")\n\n    return ax, numbers"}
{"task_id": "BigCodeBench/197", "solution": "import heapq\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(l1, l2, N=10):\n    # Calculate the differences\n    diffs = [abs(a - b) for a, b in zip(l1, l2)]\n    # Use heapq to get the N largest differences\n    largest_diffs = heapq.nlargest(N, diffs)\n    # Square the differences, take the square root, and plot\n    values = [math.sqrt(d) for d in largest_diffs]\n    ax = plt.bar(range(N), values)\n    plt.xlabel('Index')\n    plt.ylabel('Square Root of Difference')\n    plt.title('N Largest Differences')\n    plt.tight_layout()\n    return ax", "raw_solution": "import heapq\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(l1, l2, N=10):\n    # Calculate the differences\n    diffs = [abs(a - b) for a, b in zip(l1, l2)]\n    # Use heapq to get the N largest differences\n    largest_diffs = heapq.nlargest(N, diffs)\n    # Square the differences, take the square root, and plot\n    values = [math.sqrt(d) for d in largest_diffs]\n    ax = plt.bar(range(N), values)\n    plt.xlabel('Index')\n    plt.ylabel('Square Root of Difference')\n    plt.title('N Largest Differences')\n    plt.tight_layout()\n    return ax"}
{"task_id": "BigCodeBench/198", "solution": "import numpy as np\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(data, value):\n    if not data:\n        return np.array([]), 0\n\n    avg = statistics.mean(data)\n    data_above_avg = [x for x in data if x > avg]\n    count = bisect.bisect_right(data, value)\n\n    plt.hist(data, bins='auto', alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the Sorted Numbers')\n    plt.show()\n\n    return np.array(data_above_avg), count", "raw_solution": "import numpy as np\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef task_func(data, value):\n    if not data:\n        return np.array([]), 0\n\n    avg = statistics.mean(data)\n    data_above_avg = [x for x in data if x > avg]\n    count = bisect.bisect_right(data, value)\n\n    plt.hist(data, bins='auto', alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the Sorted Numbers')\n    plt.show()\n\n    return np.array(data_above_avg), count"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    Generate a weather report for specified cities at a given UTC datetime.\n    \"\"\"\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n\n    set_seed(seed)\n\n    data = []\n    for city in cities:\n        tz = pytz.timezone(timezones[city])\n        local_datetime = utc_datetime.astimezone(tz)\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        data.append([city, local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z'), weather_condition])\n\n    df = pd.DataFrame(data, columns=['City', 'Local Time', 'Weather Condition'])\n    return df", "raw_solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    Generate a weather report for specified cities at a given UTC datetime.\n    \"\"\"\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n\n    set_seed(seed)\n\n    data = []\n    for city in cities:\n        tz = pytz.timezone(timezones[city])\n        local_datetime = utc_datetime.astimezone(tz)\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        data.append([city, local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z'), weather_condition])\n\n    df = pd.DataFrame(data, columns=['City', 'Local Time', 'Weather Condition'])\n    return df"}
{"task_id": "BigCodeBench/200", "solution": "import random\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\ndef task_func(n, value):\n    # Generate 'n' random numbers between 0 and 1\n    numbers = [random.random() for _ in range(n)]\n\n    # Calculate the average of the generated numbers\n    avg = statistics.mean(numbers)\n\n    # Find numbers greater than the average\n    numbers_greater_than_avg = [num for num in numbers if num > avg]\n\n    # Count numbers greater than or equal to the specified value\n    count = sum(1 for num in numbers_greater_than_avg if num >= value)\n\n    # Sort the numbers\n    numbers_greater_than_avg.sort()\n\n    # Plot the sorted numbers\n    plt.plot(numbers_greater_than_avg)\n    plt.show()\n\n    return numbers_greater_than_avg, count", "raw_solution": "import random\nimport bisect\nimport statistics\nimport matplotlib.pyplot as plt\n\ndef task_func(n, value):\n    # Generate 'n' random numbers between 0 and 1\n    numbers = [random.random() for _ in range(n)]\n\n    # Calculate the average of the generated numbers\n    avg = statistics.mean(numbers)\n\n    # Find numbers greater than the average\n    numbers_greater_than_avg = [num for num in numbers if num > avg]\n\n    # Count numbers greater than or equal to the specified value\n    count = sum(1 for num in numbers_greater_than_avg if num >= value)\n\n    # Sort the numbers\n    numbers_greater_than_avg.sort()\n\n    # Plot the sorted numbers\n    plt.plot(numbers_greater_than_avg)\n    plt.show()\n\n    return numbers_greater_than_avg, count"}
{"task_id": "BigCodeBench/201", "solution": "import bisect\nimport statistics\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df, column, value):\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value must be a number\")\n    if column not in df.columns:\n        raise ValueError(\"Column does not exist in the DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[column]):\n        raise ValueError(\"Column must be numeric\")\n\n    avg = statistics.mean(df[column])\n    values_greater_avg = df[df[column] > avg]\n    values_greater_value = df[df[column] > value]\n\n    fig, ax = plt.subplots()\n    ax.hist(df[column], bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n\n    return (np.array(values_greater_avg[column]), len(values_greater_value), ax)", "raw_solution": "import bisect\nimport statistics\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df, column, value):\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"Value must be a number\")\n    if column not in df.columns:\n        raise ValueError(\"Column does not exist in the DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[column]):\n        raise ValueError(\"Column must be numeric\")\n\n    avg = statistics.mean(df[column])\n    values_greater_avg = df[df[column] > avg]\n    values_greater_value = df[df[column] > value]\n\n    fig, ax = plt.subplots()\n    ax.hist(df[column], bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n\n    return (np.array(values_greater_avg[column]), len(values_greater_value), ax)"}
{"task_id": "BigCodeBench/202", "solution": "import re\nimport json\nfrom collections import Counter\ndef task_func(json_str, top_n=10):\n    \"\"\"\n    Extracts all URLs from a string-serialized JSON dict using a specific URL pattern and returns a dict with the URLs as keys and the number of times they appear as values.\n    \"\"\"\n    # Define the URL pattern\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n\n    # Load the JSON string into a Python dict\n    json_dict = json.loads(json_str)\n\n    # Flatten the dict into a list of strings\n    flat_list = [str(item) for sublist in json_dict.values() for item in sublist]\n\n    # Find all URLs in the flattened list\n    urls = [match.group() for item in flat_list for match in re.finditer(url_pattern, item)]\n\n    # Count the occurrences of each URL\n    url_counts = dict(Counter(urls))\n\n    # Sort the URLs by their counts in descending order and return the top_n URLs\n    top_urls = dict(sorted(url_counts.items(), key=lambda item: item[1], reverse=True)[:top_n])\n\n    return top_urls", "raw_solution": "import re\nimport json\nfrom collections import Counter\n\ndef task_func(json_str, top_n=10):\n    \"\"\"\n    Extracts all URLs from a string-serialized JSON dict using a specific URL pattern and returns a dict with the URLs as keys and the number of times they appear as values.\n    \"\"\"\n    # Define the URL pattern\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n\n    # Load the JSON string into a Python dict\n    json_dict = json.loads(json_str)\n\n    # Flatten the dict into a list of strings\n    flat_list = [str(item) for sublist in json_dict.values() for item in sublist]\n\n    # Find all URLs in the flattened list\n    urls = [match.group() for item in flat_list for match in re.finditer(url_pattern, item)]\n\n    # Count the occurrences of each URL\n    url_counts = dict(Counter(urls))\n\n    # Sort the URLs by their counts in descending order and return the top_n URLs\n    top_urls = dict(sorted(url_counts.items(), key=lambda item: item[1], reverse=True)[:top_n])\n\n    return top_urls"}
{"task_id": "BigCodeBench/203", "solution": "import json\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\ndef task_func(input_data=None, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, smtp=None):\n    \"\"\"\n    Extracts names and email addresses from a JSON-formatted string, and sends the names in an email.\n    \"\"\"\n    # Parse the JSON data\n    data = json.loads(input_data)\n\n    # Extract names and email addresses\n    names = [item['name'] for item in data]\n    email_addresses = [item['email'] for item in data]\n\n    # Create the email message\n    msg = MIMEMultipart()\n    msg['From'] = email_address\n    msg['To'] = ', '.join(email_addresses)\n    msg['Subject'] = 'Extracted Names'\n    msg.attach(MIMEText('\\n'.join(names)))\n\n    # Send the email\n    if smtp is None:\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        smtp.starttls()\n        smtp.login(email_address, email_password)\n    smtp.send_message(msg)\n    smtp.quit()\n\n    return names", "raw_solution": "import json\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\n\n# Constants\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\nEMAIL_ADDRESS = \"your.email@gmail.com\"\nEMAIL_PASSWORD = \"your.password\"\n\ndef task_func(input_data=None, smtp_server=SMTP_SERVER, smtp_port=SMTP_PORT, email_address=EMAIL_ADDRESS, email_password=EMAIL_PASSWORD, smtp=None):\n    \"\"\"\n    Extracts names and email addresses from a JSON-formatted string, and sends the names in an email.\n    \"\"\"\n    # Parse the JSON data\n    data = json.loads(input_data)\n\n    # Extract names and email addresses\n    names = [item['name'] for item in data]\n    email_addresses = [item['email'] for item in data]\n\n    # Create the email message\n    msg = MIMEMultipart()\n    msg['From'] = email_address\n    msg['To'] = ', '.join(email_addresses)\n    msg['Subject'] = 'Extracted Names'\n    msg.attach(MIMEText('\\n'.join(names)))\n\n    # Send the email\n    if smtp is None:\n        smtp = smtplib.SMTP(smtp_server, smtp_port)\n        smtp.starttls()\n        smtp.login(email_address, email_password)\n    smtp.send_message(msg)\n    smtp.quit()\n\n    return names"}
{"task_id": "BigCodeBench/204", "solution": "import numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    mean = np.mean(L)\n    median = np.median(L)\n    mode = [item for item, count in Counter(L).items() if count == max(list(Counter(L).values()))]\n    std_dev = np.std(L)\n\n    fig, ax = plt.subplots()\n    ax.hist(L, bins='auto')\n    ax.set_title('Histogram of L')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'std_dev': std_dev,\n        'plot': ax\n    }", "raw_solution": "import numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    mean = np.mean(L)\n    median = np.median(L)\n    mode = [item for item, count in Counter(L).items() if count == max(list(Counter(L).values()))]\n    std_dev = np.std(L)\n\n    fig, ax = plt.subplots()\n    ax.hist(L, bins='auto')\n    ax.set_title('Histogram of L')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'std_dev': std_dev,\n        'plot': ax\n    }"}
{"task_id": "BigCodeBench/205", "solution": "import subprocess\nfrom multiprocessing import Pool\ndef execute_command(command):\n    \"\"\"\n    Executes a shell command and returns the output as a byte string.\n    \"\"\"\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    output, _ = process.communicate()\n    return output\ndef task_func(commands):\n    \"\"\"\n    Executes a list of shell commands in parallel and returns their outputs.\n    \"\"\"\n    if not commands:\n        return []\n\n    with Pool(len(commands)) as p:\n        outputs = p.map(execute_command, commands)\n\n    return outputs\ncommands = ['echo Hello, World!', 'ls', 'pwd']", "raw_solution": "import subprocess\nfrom multiprocessing import Pool\n\ndef execute_command(command):\n    \"\"\"\n    Executes a shell command and returns the output as a byte string.\n    \"\"\"\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    output, _ = process.communicate()\n    return output\n\ndef task_func(commands):\n    \"\"\"\n    Executes a list of shell commands in parallel and returns their outputs.\n    \"\"\"\n    if not commands:\n        return []\n\n    with Pool(len(commands)) as p:\n        outputs = p.map(execute_command, commands)\n\n    return outputs\n\n# Test the function\ncommands = ['echo Hello, World!', 'ls', 'pwd']\noutputs = task_func(commands)\nfor output in outputs:\n    print(output.decode())"}
{"task_id": "BigCodeBench/206", "solution": "import csv\nimport json\nimport os\ndef task_func(file_name):\n    try:\n        if not os.path.exists(file_name):\n            raise FileNotFoundError(f\"The file {file_name} does not exist.\")\n\n        with open(file_name, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            data = [row for row in csv_reader]\n\n        json_file_name = os.path.splitext(file_name)[0] + '.json'\n        with open(json_file_name, 'w') as json_file:\n            json.dump(data, json_file)\n\n        return json_file_name\n    except FileNotFoundError as fnf_error:\n        print(fnf_error)", "raw_solution": "import csv\nimport json\nimport os\n\ndef task_func(file_name):\n    try:\n        if not os.path.exists(file_name):\n            raise FileNotFoundError(f\"The file {file_name} does not exist.\")\n\n        with open(file_name, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            data = [row for row in csv_reader]\n\n        json_file_name = os.path.splitext(file_name)[0] + '.json'\n        with open(json_file_name, 'w') as json_file:\n            json.dump(data, json_file)\n\n        return json_file_name\n    except FileNotFoundError as fnf_error:\n        print(fnf_error)"}
{"task_id": "BigCodeBench/207", "solution": "import re\nimport requests\ndef task_func(input_str):\n    \"\"\"\n    Extracts an API endpoint from the input string, sends a GET request to the endpoint, and returns the response data in JSON format.\n    \"\"\"\n    # Extract the API endpoint using regex\n    api_endpoint = re.findall(r'https?://[^\\s]+', input_str)\n    if not api_endpoint:\n        return \"No API endpoint found in the input string.\"\n\n    # Send a GET request to the endpoint\n    response = requests.get(api_endpoint[0])\n\n    # Return the response data in JSON format\n    return response.json()", "raw_solution": "import re\nimport requests\n\ndef task_func(input_str):\n    \"\"\"\n    Extracts an API endpoint from the input string, sends a GET request to the endpoint, and returns the response data in JSON format.\n    \"\"\"\n    # Extract the API endpoint using regex\n    api_endpoint = re.findall(r'https?://[^\\s]+', input_str)\n    if not api_endpoint:\n        return \"No API endpoint found in the input string.\"\n\n    # Send a GET request to the endpoint\n    response = requests.get(api_endpoint[0])\n\n    # Return the response data in JSON format\n    return response.json()"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    np.random.seed(seed)\n    random_walk = np.random.choice([-1, 1], size=elements)\n    random_walk_cumulative = np.cumsum(random_walk)\n\n    stats = {\n        'count': elements,\n        'mean': np.mean(random_walk_cumulative),\n        'std': np.std(random_walk_cumulative),\n        'min': np.min(random_walk_cumulative),\n        '5th percentile': np.percentile(random_walk_cumulative, 5),\n        '25th percentile': np.percentile(random_walk_cumulative, 25),\n        'median': np.median(random_walk_cumulative),\n        '75th percentile': np.percentile(random_walk_cumulative, 75),\n        '95th percentile': np.percentile(random_walk_cumulative, 95),\n        'max': np.max(random_walk_cumulative),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(random_walk_cumulative)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Cumulative Sum')\n\n    return stats, ax.figure", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    np.random.seed(seed)\n    random_walk = np.random.choice([-1, 1], size=elements)\n    random_walk_cumulative = np.cumsum(random_walk)\n\n    stats = {\n        'count': elements,\n        'mean': np.mean(random_walk_cumulative),\n        'std': np.std(random_walk_cumulative),\n        'min': np.min(random_walk_cumulative),\n        '5th percentile': np.percentile(random_walk_cumulative, 5),\n        '25th percentile': np.percentile(random_walk_cumulative, 25),\n        'median': np.median(random_walk_cumulative),\n        '75th percentile': np.percentile(random_walk_cumulative, 75),\n        '95th percentile': np.percentile(random_walk_cumulative, 95),\n        'max': np.max(random_walk_cumulative),\n    }\n\n    fig, ax = plt.subplots()\n    ax.plot(random_walk_cumulative)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Cumulative Sum')\n\n    return stats, ax.figure"}
{"task_id": "BigCodeBench/209", "solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Plots a scatter graph of tuples and highlights the tuple with the maximum value at index 1.\n    \"\"\"\n    # Convert data to numpy array for easier manipulation\n    data = np.array(data)\n\n    # Find the tuple with the maximum value at index 1\n    max_tuple = max(data, key=itemgetter(1))\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(data[:, 0], data[:, 1])\n\n    # Highlight the maximum tuple\n    ax.scatter(max_tuple[0], max_tuple[1], color='r', label='Max Tuple')\n\n    # Set plot title and labels\n    ax.set_title('Max Tuple Highlighted')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    # Add legend\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    Plots a scatter graph of tuples and highlights the tuple with the maximum value at index 1.\n    \"\"\"\n    # Convert data to numpy array for easier manipulation\n    data = np.array(data)\n\n    # Find the tuple with the maximum value at index 1\n    max_tuple = max(data, key=itemgetter(1))\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(data[:, 0], data[:, 1])\n\n    # Highlight the maximum tuple\n    ax.scatter(max_tuple[0], max_tuple[1], color='r', label='Max Tuple')\n\n    # Set plot title and labels\n    ax.set_title('Max Tuple Highlighted')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    # Add legend\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/210", "solution": "import collections\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Count the frequency of each letter in the data\n    letter_counts = collections.Counter(data)\n\n    # Find the letter with the maximum count\n    max_count_letter = max(letter_counts.items(), key=itemgetter(1))[0]\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(letter_counts.keys(), letter_counts.values())\n\n    # Highlight the letter with the maximum count\n    rects = ax.patches\n    for rect in rects:\n        if rect.get_x() == max_count_letter:\n            rect.set_color('r')\n\n    # Set labels and title\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Count')\n    ax.set_title('Letter Counts with Max Value Letter Highlighted')\n    ax.legend(['Letter Counts', 'Max Value Letter'])\n\n    return ax", "raw_solution": "import collections\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Count the frequency of each letter in the data\n    letter_counts = collections.Counter(data)\n\n    # Find the letter with the maximum count\n    max_count_letter = max(letter_counts.items(), key=itemgetter(1))[0]\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(letter_counts.keys(), letter_counts.values())\n\n    # Highlight the letter with the maximum count\n    rects = ax.patches\n    for rect in rects:\n        if rect.get_x() == max_count_letter:\n            rect.set_color('r')\n\n    # Set labels and title\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Count')\n    ax.set_title('Letter Counts with Max Value Letter Highlighted')\n    ax.legend(['Letter Counts', 'Max Value Letter'])\n\n    return ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\nfrom io import BytesIO\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Downloads a zip file from a URL, extracts its contents to a specified directory,\n    and returns a list of the extracted files.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n\n    # Raise an exception if the GET request was unsuccessful\n    response.raise_for_status()\n\n    # Create a zipfile object from the response content\n    zip_file = zipfile.ZipFile(BytesIO(response.content))\n\n    # Extract all files in the zip file to the destination directory\n    zip_file.extractall(destination_directory)\n\n    # Get a list of all files in the destination directory\n    extracted_files = os.listdir(destination_directory)\n\n    return extracted_files", "raw_solution": "import requests\nimport os\nimport zipfile\nfrom io import BytesIO\n\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Downloads a zip file from a URL, extracts its contents to a specified directory,\n    and returns a list of the extracted files.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n\n    # Raise an exception if the GET request was unsuccessful\n    response.raise_for_status()\n\n    # Create a zipfile object from the response content\n    zip_file = zipfile.ZipFile(BytesIO(response.content))\n\n    # Extract all files in the zip file to the destination directory\n    zip_file.extractall(destination_directory)\n\n    # Get a list of all files in the destination directory\n    extracted_files = os.listdir(destination_directory)\n\n    return extracted_files"}
{"task_id": "BigCodeBench/212", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Draws a scatter plot of dots and marks the point with the maximum y-value.\n    Returns the axes object and the point with the maximum y-value.\n    \"\"\"\n    # Generate x and y values from the data\n    x, y = zip(*data)\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n\n    # Mark the point with the maximum y-value\n    max_y_point = max(data, key=lambda point: point[1])\n    ax.scatter(*max_y_point, color='r')\n\n    # Label the axes and set the title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Points with Max Y Point Highlighted')\n\n    return ax, max_y_point", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    Draws a scatter plot of dots and marks the point with the maximum y-value.\n    Returns the axes object and the point with the maximum y-value.\n    \"\"\"\n    # Generate x and y values from the data\n    x, y = zip(*data)\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n\n    # Mark the point with the maximum y-value\n    max_y_point = max(data, key=lambda point: point[1])\n    ax.scatter(*max_y_point, color='r')\n\n    # Label the axes and set the title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Points with Max Y Point Highlighted')\n\n    return ax, max_y_point"}
{"task_id": "BigCodeBench/213", "solution": "import time\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kurtosis\ndef task_func(intervals=100, seed=0):\n    random.seed(seed)\n    numbers = []\n    start_time = time.time()\n    for i in range(intervals):\n        numbers.append(random.random())\n        time.sleep(1)\n    elapsed_time = time.time() - start_time\n\n    fig, ax = plt.subplots()\n    ax.plot(range(intervals), numbers)\n    ax.set_xlabel('Elapsed Time (s)')\n    ax.set_ylabel('Random Number')\n    ax.set_title('Random Number Generation Over Time')\n    plt.show()\n\n    kurt = kurtosis(numbers)\n\n    return ax, kurt", "raw_solution": "import time\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kurtosis\n\ndef task_func(intervals=100, seed=0):\n    random.seed(seed)\n    numbers = []\n    start_time = time.time()\n    for i in range(intervals):\n        numbers.append(random.random())\n        time.sleep(1)\n    elapsed_time = time.time() - start_time\n\n    fig, ax = plt.subplots()\n    ax.plot(range(intervals), numbers)\n    ax.set_xlabel('Elapsed Time (s)')\n    ax.set_ylabel('Random Number')\n    ax.set_title('Random Number Generation Over Time')\n    plt.show()\n\n    kurt = kurtosis(numbers)\n\n    return ax, kurt"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low should be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size)\n\n    fig, ax = plt.subplots(1, 1)\n    ax.imshow(image)\n    ax.axis('off')\n    plt.show()\n\n    return ax, image", "raw_solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    if range_low >= range_high:\n        raise ValueError(\"range_low should be less than range_high\")\n\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size)\n\n    fig, ax = plt.subplots(1, 1)\n    ax.imshow(image)\n    ax.axis('off')\n    plt.show()\n\n    return ax, image"}
{"task_id": "BigCodeBench/215", "solution": "import requests\nimport json\nimport pandas as p\nimport seaborn as sns\nfrom pandas.api.types import is_numeric_dtype\nimport matplotlib.pyplot as plt\nHEADERS = {\n    'accept': 'application/json'\n}\ndef task_func(url, parameters):\n    \"\"\"\n    Retrieves data from a specific API endpoint with the provided parameters,\n    converts the data into a pandas dataframe, and draws a heatmap to show\n    the correlation between numerical characteristics.\n    \"\"\"\n    try:\n        response = requests.get(url, headers=HEADERS, params=parameters)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        raise Exception(\"HTTP Error:\", errh)\n    except requests.exceptions.ConnectionError as errc:\n        raise Exception(\"Error Connecting:\", errc)\n    except requests.exceptions.Timeout as errt:\n        raise Exception(\"Timeout Error:\", errt)\n    except requests.exceptions.RequestException as err:\n        raise Exception(\"Something went wrong\", err)\n\n    data = response.json()\n    if not data:\n        raise Exception(\"Empty data\")\n\n    df = pd.DataFrame(data)\n    if not df.empty:\n        num_cols = [col for col in df.columns if is_numeric_dtype(df[col])]\n        corr = df[num_cols].corr()\n        fig, ax = plt.subplots(figsize=(10, 8))\n        sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n        plt.show()\n        return df, ax\n    else:\n        raise Exception(\"Invalid data\")", "raw_solution": "import requests\nimport json\nimport pandas as p\nimport seaborn as sns\nfrom pandas.api.types import is_numeric_dtype\nimport matplotlib.pyplot as plt\n\n# Constants\nHEADERS = {\n    'accept': 'application/json'\n}\n\ndef task_func(url, parameters):\n    \"\"\"\n    Retrieves data from a specific API endpoint with the provided parameters,\n    converts the data into a pandas dataframe, and draws a heatmap to show\n    the correlation between numerical characteristics.\n    \"\"\"\n    try:\n        response = requests.get(url, headers=HEADERS, params=parameters)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        raise Exception(\"HTTP Error:\", errh)\n    except requests.exceptions.ConnectionError as errc:\n        raise Exception(\"Error Connecting:\", errc)\n    except requests.exceptions.Timeout as errt:\n        raise Exception(\"Timeout Error:\", errt)\n    except requests.exceptions.RequestException as err:\n        raise Exception(\"Something went wrong\", err)\n\n    data = response.json()\n    if not data:\n        raise Exception(\"Empty data\")\n\n    df = pd.DataFrame(data)\n    if not df.empty:\n        num_cols = [col for col in df.columns if is_numeric_dtype(df[col])]\n        corr = df[num_cols].corr()\n        fig, ax = plt.subplots(figsize=(10, 8))\n        sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', ax=ax)\n        plt.show()\n        return df, ax\n    else:\n        raise Exception(\"Invalid data\")"}
{"task_id": "BigCodeBench/216", "solution": "import pandas as pd\nimport os\nimport json\nfrom collections import Counter\nimport re\ndef task_func(json_dir_path, word_count):\n    \"\"\"\n    Analyze text content in JSON files from a given directory and find the most common words.\n    \"\"\"\n    # Initialize an empty list to store the text content\n    text_content = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(json_dir_path):\n        # Check if the file is a JSON file\n        if filename.endswith('.json'):\n            # Open the file and load its content\n            with open(os.path.join(json_dir_path, filename), 'r') as file:\n                data = json.load(file)\n\n            # Extract the text content from the JSON data\n            # This is a placeholder, you should replace it with the actual extraction logic\n            # For example, if the JSON data is a dictionary with a 'text' key:\n            if 'text' in data:\n                text_content.append(data['text'])\n\n    # Join all text content into a single string\n    text = ' '.join(text_content)\n\n    # Remove punctuation and convert to lower case\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Split the text into words\n    words = text.split()\n\n    # Count the occurrences of each word\n    word_counts = Counter(words)\n\n    # Return the most common words and their counts\n    return word_counts.most_common(word_count)", "raw_solution": "import pandas as pd\nimport os\nimport json\nfrom collections import Counter\nimport re\n\ndef task_func(json_dir_path, word_count):\n    \"\"\"\n    Analyze text content in JSON files from a given directory and find the most common words.\n    \"\"\"\n    # Initialize an empty list to store the text content\n    text_content = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(json_dir_path):\n        # Check if the file is a JSON file\n        if filename.endswith('.json'):\n            # Open the file and load its content\n            with open(os.path.join(json_dir_path, filename), 'r') as file:\n                data = json.load(file)\n\n            # Extract the text content from the JSON data\n            # This is a placeholder, you should replace it with the actual extraction logic\n            # For example, if the JSON data is a dictionary with a 'text' key:\n            if 'text' in data:\n                text_content.append(data['text'])\n\n    # Join all text content into a single string\n    text = ' '.join(text_content)\n\n    # Remove punctuation and convert to lower case\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Split the text into words\n    words = text.split()\n\n    # Count the occurrences of each word\n    word_counts = Counter(words)\n\n    # Return the most common words and their counts\n    return word_counts.most_common(word_count)"}
{"task_id": "BigCodeBench/217", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mu=0, sigma=1, sample_size=1000, seed=0):\n    np.random.seed(seed)\n    # Generate a sample from a normal distribution\n    sample = np.random.normal(mu, sigma, sample_size)\n\n    # Calculate empirical mean and standard deviation\n    empirical_mean = np.mean(sample)\n    empirical_std_dev = np.std(sample)\n\n    # Create a range of x values for the PDF\n    x = np.linspace(empirical_mean - 3*empirical_std_dev, empirical_mean + 3*empirical_std_dev, 100)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the histogram\n    ax.hist(sample, bins=30, density=True, alpha=0.5, label='Histogram')\n\n    # Plot the PDF\n    ax.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', label='PDF')\n\n    # Set the title\n    ax.set_title(f'Normal Distribution with $\\\\mu = {mu:.2f}, \\\\sigma = {sigma:.2f}$')\n\n    # Add legend\n    ax.legend()\n\n    return ax, empirical_mean, empirical_std_dev", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(mu=0, sigma=1, sample_size=1000, seed=0):\n    np.random.seed(seed)\n    # Generate a sample from a normal distribution\n    sample = np.random.normal(mu, sigma, sample_size)\n\n    # Calculate empirical mean and standard deviation\n    empirical_mean = np.mean(sample)\n    empirical_std_dev = np.std(sample)\n\n    # Create a range of x values for the PDF\n    x = np.linspace(empirical_mean - 3*empirical_std_dev, empirical_mean + 3*empirical_std_dev, 100)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the histogram\n    ax.hist(sample, bins=30, density=True, alpha=0.5, label='Histogram')\n\n    # Plot the PDF\n    ax.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', label='PDF')\n\n    # Set the title\n    ax.set_title(f'Normal Distribution with $\\\\mu = {mu:.2f}, \\\\sigma = {sigma:.2f}$')\n\n    # Add legend\n    ax.legend()\n\n    return ax, empirical_mean, empirical_std_dev"}
{"task_id": "BigCodeBench/218", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\nTARGET = 'target'\ndef task_func(df, dict_mapping, plot_histogram=False):\n    \"\"\"\n    Pre-processes a DataFrame by replacing values according to a dictionary mapping,\n    standardizing specified features, and optionally drawing a histogram of the target variable.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n    if TARGET not in df.columns or any(feature not in df.columns for feature in FEATURES):\n        raise ValueError(f\"{TARGET} or one of the features not in the DataFrame columns.\")\n\n    # Replace values according to dict_mapping\n    df.replace(dict_mapping, inplace=True)\n\n    # Standardize features\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n\n    histogram = None\n    if plot_histogram:\n        # Draw histogram of the target variable\n        histogram = df[TARGET].hist(bins=30)\n        plt.title('Histogram of the target variable')\n        plt.xlabel(TARGET)\n        plt.ylabel('Frequency')\n\n    return df, histogram", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\nTARGET = 'target'\n\ndef task_func(df, dict_mapping, plot_histogram=False):\n    \"\"\"\n    Pre-processes a DataFrame by replacing values according to a dictionary mapping,\n    standardizing specified features, and optionally drawing a histogram of the target variable.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n    if TARGET not in df.columns or any(feature not in df.columns for feature in FEATURES):\n        raise ValueError(f\"{TARGET} or one of the features not in the DataFrame columns.\")\n\n    # Replace values according to dict_mapping\n    df.replace(dict_mapping, inplace=True)\n\n    # Standardize features\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n\n    histogram = None\n    if plot_histogram:\n        # Draw histogram of the target variable\n        histogram = df[TARGET].hist(bins=30)\n        plt.title('Histogram of the target variable')\n        plt.xlabel(TARGET)\n        plt.ylabel('Frequency')\n\n    return df, histogram"}
{"task_id": "BigCodeBench/219", "solution": "import math\nimport statistics\nimport numpy as np\nfrom scipy.signal import periodogram\nfrom collections import Counter\ndef task_func(input_list):\n    # Sort the input list in ascending order based on the degree value of its elements\n    sorted_list = sorted(input_list, key=lambda x: x[1])\n\n    # Calculate the mean, median, and mode of the sorted list\n    degree_values = [x[1] for x in sorted_list]\n    mean_sorted = round(statistics.mean(degree_values), 2)\n    median_sorted = round(statistics.median(degree_values), 2)\n    mode_sorted = round(statistics.mode(degree_values), 2)\n\n    # Calculate the magnitude of the fast fourier transform of the degree values\n    fft_values = np.abs(np.fft.fft(degree_values).real)\n    fft_mean = round(np.mean(fft_values), 2)\n    fft_median = round(np.median(fft_values), 2)\n    fft_mode = round(statistics.mode(fft_values), 2)\n\n    return (mean_sorted, median_sorted, mode_sorted, fft_mean, fft_median, fft_mode)", "raw_solution": "import math\nimport statistics\nimport numpy as np\nfrom scipy.signal import periodogram\nfrom collections import Counter\n\ndef task_func(input_list):\n    # Sort the input list in ascending order based on the degree value of its elements\n    sorted_list = sorted(input_list, key=lambda x: x[1])\n\n    # Calculate the mean, median, and mode of the sorted list\n    degree_values = [x[1] for x in sorted_list]\n    mean_sorted = round(statistics.mean(degree_values), 2)\n    median_sorted = round(statistics.median(degree_values), 2)\n    mode_sorted = round(statistics.mode(degree_values), 2)\n\n    # Calculate the magnitude of the fast fourier transform of the degree values\n    fft_values = np.abs(np.fft.fft(degree_values).real)\n    fft_mean = round(np.mean(fft_values), 2)\n    fft_median = round(np.median(fft_values), 2)\n    fft_mode = round(statistics.mode(fft_values), 2)\n\n    return (mean_sorted, median_sorted, mode_sorted, fft_mean, fft_median, fft_mode)\n\n# Test the function"}
{"task_id": "BigCodeBench/220", "solution": "from random import choice\nimport turtle\nimport time\ndef task_func(colors):\n    \"\"\"\n    Draws five squares of random colors using Turtle Graphics. Each square is drawn sequentially with a 1-second pause between squares.\n    \"\"\"\n    # Set up the Turtle Graphics window\n    window = turtle.Screen()\n    window.setup(500, 500)\n    window.title(\"Random Squares\")\n\n    # Create a Turtle object\n    t = turtle.Turtle()\n\n    # Draw five squares of random colors\n    for _ in range(5):\n        # Choose a random color from the provided list\n        color = choice(colors)\n\n        # Set the color for the Turtle\n        t.color(color)\n\n        # Draw a square\n        for _ in range(4):\n            t.forward(100)\n            t.right(90)\n\n        # Pause for 1 second\n        time.sleep(1)\n\n        # Clear the drawing area for the next square\n        t.clear()\n\n    # Close the Turtle Graphics window\n    turtle.done()\ncolors = [\"red\", \"green\", \"blue\", \"yellow\", \"orange\", \"purple\"]", "raw_solution": "from random import choice\nimport turtle\nimport time\n\ndef task_func(colors):\n    \"\"\"\n    Draws five squares of random colors using Turtle Graphics. Each square is drawn sequentially with a 1-second pause between squares.\n    \"\"\"\n    # Set up the Turtle Graphics window\n    window = turtle.Screen()\n    window.setup(500, 500)\n    window.title(\"Random Squares\")\n\n    # Create a Turtle object\n    t = turtle.Turtle()\n\n    # Draw five squares of random colors\n    for _ in range(5):\n        # Choose a random color from the provided list\n        color = choice(colors)\n\n        # Set the color for the Turtle\n        t.color(color)\n\n        # Draw a square\n        for _ in range(4):\n            t.forward(100)\n            t.right(90)\n\n        # Pause for 1 second\n        time.sleep(1)\n\n        # Clear the drawing area for the next square\n        t.clear()\n\n    # Close the Turtle Graphics window\n    turtle.done()\n\n# Test the function\ncolors = [\"red\", \"green\", \"blue\", \"yellow\", \"orange\", \"purple\"]\ntask_func(colors)"}
{"task_id": "BigCodeBench/221", "solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\ndef task_func(df, dct):\n    \"\"\"\n    This function calculates and returns the mean, median, mode, and variance for specified features in a DataFrame.\n    It replaces certain values in the DataFrame based on a provided dictionary mapping before performing the calculations.\n    \"\"\"\n    # Check if the DataFrame contains the required features\n    for feature in FEATURES:\n        if feature not in df.columns:\n            return \"Invalid input: DataFrame does not contain the '{}' feature\".format(feature)\n\n    # Replace values in the DataFrame based on the provided dictionary\n    for key, value in dct.items():\n        df[key] = df[key].replace(key, value)\n\n    # Calculate statistics for each feature\n    stats_dict = {}\n    for feature in FEATURES:\n        try:\n            mean = df[feature].mean()\n            median = df[feature].median()\n            mode = stats.mode(df[feature])[0][0]\n            variance = df[feature].var()\n            stats_dict[feature] = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance}\n        except Exception as e:\n            return \"Error in calculation: {}\".format(str(e))\n\n    return stats_dict", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\n\n# Constants\nFEATURES = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n\ndef task_func(df, dct):\n    \"\"\"\n    This function calculates and returns the mean, median, mode, and variance for specified features in a DataFrame.\n    It replaces certain values in the DataFrame based on a provided dictionary mapping before performing the calculations.\n    \"\"\"\n    # Check if the DataFrame contains the required features\n    for feature in FEATURES:\n        if feature not in df.columns:\n            return \"Invalid input: DataFrame does not contain the '{}' feature\".format(feature)\n\n    # Replace values in the DataFrame based on the provided dictionary\n    for key, value in dct.items():\n        df[key] = df[key].replace(key, value)\n\n    # Calculate statistics for each feature\n    stats_dict = {}\n    for feature in FEATURES:\n        try:\n            mean = df[feature].mean()\n            median = df[feature].median()\n            mode = stats.mode(df[feature])[0][0]\n            variance = df[feature].var()\n            stats_dict[feature] = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance}\n        except Exception as e:\n            return \"Error in calculation: {}\".format(str(e))\n\n    return stats_dict"}
{"task_id": "BigCodeBench/222", "solution": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(list_input):\n    \"\"\"\n    Sorts the given list in ascending order based on the degree value of its elements,\n    calculates the cumulative sum of the sorted list, and draws a line chart of the cumulative sum.\n    \"\"\"\n    # Calculate the degree value of each element in the list\n    degrees = [math.degrees(x) for x in list_input]\n\n    # Sort the list based on the degree values\n    sorted_list = [x for _, x in sorted(zip(degrees, list_input))]\n\n    # Calculate the cumulative sum of the sorted list\n    cumulative_sum = np.cumsum(sorted_list)\n\n    # Create a line chart of the cumulative sum\n    fig, ax = plt.subplots()\n    ax.plot(cumulative_sum)\n\n    return cumulative_sum, ax", "raw_solution": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(list_input):\n    \"\"\"\n    Sorts the given list in ascending order based on the degree value of its elements,\n    calculates the cumulative sum of the sorted list, and draws a line chart of the cumulative sum.\n    \"\"\"\n    # Calculate the degree value of each element in the list\n    degrees = [math.degrees(x) for x in list_input]\n\n    # Sort the list based on the degree values\n    sorted_list = [x for _, x in sorted(zip(degrees, list_input))]\n\n    # Calculate the cumulative sum of the sorted list\n    cumulative_sum = np.cumsum(sorted_list)\n\n    # Create a line chart of the cumulative sum\n    fig, ax = plt.subplots()\n    ax.plot(cumulative_sum)\n\n    return cumulative_sum, ax"}
{"task_id": "BigCodeBench/223", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport numpy as np\ndef task_func(df, dct, columns=None):\n    \"\"\"\n    Preprocesses a pandas DataFrame by replacing specified values, encoding categorical attributes, and standardizing numerical attributes.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    # Replace specified values\n    if dct is not None:\n        for col, value_dict in dct.items():\n            df[col] = df[col].replace(value_dict)\n\n    # Encode categorical attributes\n    for col in df.columns:\n        if df[col].dtype == 'object' and columns is None or (columns is not None and col in columns):\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col])\n\n    # Standardize numerical attributes\n    for col in df.columns:\n        if df[col].dtype in ['int64', 'float64'] and columns is None or (columns is not None and col in columns):\n            scaler = StandardScaler()\n            df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport numpy as np\n\ndef task_func(df, dct, columns=None):\n    \"\"\"\n    Preprocesses a pandas DataFrame by replacing specified values, encoding categorical attributes, and standardizing numerical attributes.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    # Replace specified values\n    if dct is not None:\n        for col, value_dict in dct.items():\n            df[col] = df[col].replace(value_dict)\n\n    # Encode categorical attributes\n    for col in df.columns:\n        if df[col].dtype == 'object' and columns is None or (columns is not None and col in columns):\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col])\n\n    # Standardize numerical attributes\n    for col in df.columns:\n        if df[col].dtype in ['int64', 'float64'] and columns is None or (columns is not None and col in columns):\n            scaler = StandardScaler()\n            df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n\n    return df"}
{"task_id": "BigCodeBench/224", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    x_values = np.arange(range_start, range_end, step)\n    sin_values = np.sin(x_values)\n    cos_values = np.cos(x_values)\n    abs_diff_values = np.abs(sin_values - cos_values)\n\n    # Generator\n    def generator():\n        for x, sin_x, cos_x, abs_diff in zip(x_values, sin_values, cos_values, abs_diff_values):\n            yield (x, sin_x, cos_x, abs_diff)\n\n    # Plot\n    fig, ax = plt.subplots()\n    ax.plot(x_values, sin_values, label='sin(x)')\n    ax.plot(x_values, cos_values, label='cos(x)')\n    ax.plot(x_values, abs_diff_values, label='abs(sin(x) - cos(x))')\n    ax.legend()\n\n    # FFT\n    fft_values = fft(abs_diff_values)\n    fft_magnitude = np.abs(fft_values)\n    mean_fft_magnitude = np.mean(fft_magnitude)\n    median_fft_magnitude = np.median(fft_magnitude)\n\n    return generator(), ax, mean_fft_magnitude, median_fft_magnitude", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(range_start=-10, range_end=10, step=0.1):\n    x_values = np.arange(range_start, range_end, step)\n    sin_values = np.sin(x_values)\n    cos_values = np.cos(x_values)\n    abs_diff_values = np.abs(sin_values - cos_values)\n\n    # Generator\n    def generator():\n        for x, sin_x, cos_x, abs_diff in zip(x_values, sin_values, cos_values, abs_diff_values):\n            yield (x, sin_x, cos_x, abs_diff)\n\n    # Plot\n    fig, ax = plt.subplots()\n    ax.plot(x_values, sin_values, label='sin(x)')\n    ax.plot(x_values, cos_values, label='cos(x)')\n    ax.plot(x_values, abs_diff_values, label='abs(sin(x) - cos(x))')\n    ax.legend()\n\n    # FFT\n    fft_values = fft(abs_diff_values)\n    fft_magnitude = np.abs(fft_values)\n    mean_fft_magnitude = np.mean(fft_magnitude)\n    median_fft_magnitude = np.median(fft_magnitude)\n\n    return generator(), ax, mean_fft_magnitude, median_fft_magnitude"}
{"task_id": "BigCodeBench/225", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, dct, columns=None, plot_histograms=False):\n    \"\"\"\n    Replaces values in a DataFrame with a dictionary mapping.\n    Optionally, records histograms for specified columns.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    df = df.copy()\n\n    for col, value in dct.items():\n        if columns is None or col in columns:\n            df[col] = df[col].replace(value)\n\n    if plot_histograms:\n        for col in columns:\n            if col in df.columns:\n                df[col].hist(bins=10)\n                plt.title(f\"Histogram of {col}\")\n                plt.show()\n\n    return df", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, dct, columns=None, plot_histograms=False):\n    \"\"\"\n    Replaces values in a DataFrame with a dictionary mapping.\n    Optionally, records histograms for specified columns.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    df = df.copy()\n\n    for col, value in dct.items():\n        if columns is None or col in columns:\n            df[col] = df[col].replace(value)\n\n    if plot_histograms:\n        for col in columns:\n            if col in df.columns:\n                df[col].hist(bins=10)\n                plt.title(f\"Histogram of {col}\")\n                plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/226", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(range_start=0, range_end=10, step=0.1):\n    def generator():\n        for x in np.arange(range_start, range_end, step):\n            yield (x, np.exp(x))\n    return generator()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(range_start=0, range_end=10, step=0.1):\n    def generator():\n        for x in np.arange(range_start, range_end, step):\n            yield (x, np.exp(x))\n    return generator()\n\ndef plot_func(generator):\n    x_values = []\n    y_values = []\n    for x, y in generator:\n        x_values.append(x)\n        y_values.append(y)\n    plt.plot(x_values, y_values)\n    plt.xlabel('x')\n    plt.ylabel('e^x')\n    plt.title('Exponential Function')\n    return plt.gca()\n\n# Testing\ngen = task_func()\nplot = plot_func(gen)\nplot.show()"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from a list L, normalizes it based on the sound pressure level (SPL) of a specified audio file, and generates a spectrogram from the matrix.\n    \"\"\"\n    # Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"The specified audio file {audio_file} does not exist.\")\n\n    # Read the audio file\n    data, sample_rate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10 ** (spl / 20))\n\n    # Generate the spectrogram\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(normalized_matrix.flatten())), ref=np.max)\n\n    # Create the figure for the spectrogram\n    plt.figure(figsize=(14, 5))\n    librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Spectrogram')\n\n    return normalized_matrix, plt.gcf()", "raw_solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from a list L, normalizes it based on the sound pressure level (SPL) of a specified audio file, and generates a spectrogram from the matrix.\n    \"\"\"\n    # Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"The specified audio file {audio_file} does not exist.\")\n\n    # Read the audio file\n    data, sample_rate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10 ** (spl / 20))\n\n    # Generate the spectrogram\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(normalized_matrix.flatten())), ref=np.max)\n\n    # Create the figure for the spectrogram\n    plt.figure(figsize=(14, 5))\n    librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Spectrogram')\n\n    return normalized_matrix, plt.gcf()"}
{"task_id": "BigCodeBench/228", "solution": "import pandas as pd\nimport numpy as np\nfrom typing import Dict\nfrom scipy.stats import pearsonr\ndef task_func(df: pd.DataFrame, dct: Dict[str, float]) -> pd.DataFrame:\n    \"\"\"\n    Replaces certain values in a DataFrame with a dictionary mapping and calculates the Pearson correlation coefficient between each pair of columns.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    # Replace values in DataFrame with dictionary mapping\n    df = df.replace(dct)\n\n    # Calculate Pearson correlation coefficient between each pair of columns\n    corr_matrix = df.corr(method='pearson')\n\n    return corr_matrix\ndata = {\n    'column1': [1, 2, 3, 4, 5],\n    'column2': [2, 4, 6, 8, 10],\n    'column3': [3, 6, 9, 12, 15],\n    'column4': [4, 8, 12, 16, 20],\n    'column5': [5, 10, 15, 20, 25]\n}\ndf = pd.DataFrame(data)\ndct = {'column1': 10, 'column2': 20, 'column3': 30, 'column4': 40, 'column5': 50}", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom typing import Dict\nfrom scipy.stats import pearsonr\n\nCOLUMNS = ['column1', 'column2', 'column3', 'column4', 'column5']\n\ndef task_func(df: pd.DataFrame, dct: Dict[str, float]) -> pd.DataFrame:\n    \"\"\"\n    Replaces certain values in a DataFrame with a dictionary mapping and calculates the Pearson correlation coefficient between each pair of columns.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    # Replace values in DataFrame with dictionary mapping\n    df = df.replace(dct)\n\n    # Calculate Pearson correlation coefficient between each pair of columns\n    corr_matrix = df.corr(method='pearson')\n\n    return corr_matrix\n\n# Testing the function\ndata = {\n    'column1': [1, 2, 3, 4, 5],\n    'column2': [2, 4, 6, 8, 10],\n    'column3': [3, 6, 9, 12, 15],\n    'column4': [4, 8, 12, 16, 20],\n    'column5': [5, 10, 15, 20, 25]\n}\ndf = pd.DataFrame(data)\ndct = {'column1': 10, 'column2': 20, 'column3': 30, 'column4': 40, 'column5': 50}"}
{"task_id": "BigCodeBench/229", "solution": "import json\nimport random\nfrom datetime import datetime, timedelta\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\nACTIONS = ['login', 'logout', 'upload', 'download', 'delete']\ndef task_func(file_path, num_entries, seed=None):\n    \"\"\"\n    Generates a JSON file with user activity data.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    data = []\n    for _ in range(num_entries):\n        user = random.choice(USERS)\n        action = random.choice(ACTIONS)\n        timestamp = (datetime.now() - timedelta(minutes=random.randint(1, 1440))).isoformat()\n        data.append({'user': user, 'action': action, 'timestamp': timestamp})\n\n    with open(file_path, 'w') as f:\n        json.dump(data, f)\n\n    return file_path", "raw_solution": "import json\nimport random\nfrom datetime import datetime, timedelta\n\n# Constants\nUSERS = ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve']\nACTIONS = ['login', 'logout', 'upload', 'download', 'delete']\n\ndef task_func(file_path, num_entries, seed=None):\n    \"\"\"\n    Generates a JSON file with user activity data.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    data = []\n    for _ in range(num_entries):\n        user = random.choice(USERS)\n        action = random.choice(ACTIONS)\n        timestamp = (datetime.now() - timedelta(minutes=random.randint(1, 1440))).isoformat()\n        data.append({'user': user, 'action': action, 'timestamp': timestamp})\n\n    with open(file_path, 'w') as f:\n        json.dump(data, f)\n\n    return file_path"}
{"task_id": "BigCodeBench/230", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nCOLUMNS = ['Name', 'Age', 'Country', 'Score']\ndef task_func(df):\n    \"\"\"\n    Generates a histogram of scores and a boxplot of scores by country from a pandas DataFrame.\n    It considers only unique names for both plots.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        return \"Invalid input\"\n    if not all(col in df.columns for col in COLUMNS):\n        return \"Invalid input\"\n\n    # Ensure 'Name' is unique\n    if df['Name'].nunique() != len(df):\n        df = df.drop_duplicates(subset='Name')\n\n    # Create a new figure\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n    # Histogram of Scores\n    sns.histplot(data=df, x='Score', bins=20, ax=axes[0])\n    axes[0].set_title('Histogram of Scores')\n\n    # Boxplot of Scores by Country\n    sns.boxplot(data=df, x='Country', y='Score', ax=axes[1])\n    axes[1].set_title('Boxplot of Scores by Country')\n\n    return fig", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Country', 'Score']\n\ndef task_func(df):\n    \"\"\"\n    Generates a histogram of scores and a boxplot of scores by country from a pandas DataFrame.\n    It considers only unique names for both plots.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        return \"Invalid input\"\n    if not all(col in df.columns for col in COLUMNS):\n        return \"Invalid input\"\n\n    # Ensure 'Name' is unique\n    if df['Name'].nunique() != len(df):\n        df = df.drop_duplicates(subset='Name')\n\n    # Create a new figure\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n    # Histogram of Scores\n    sns.histplot(data=df, x='Score', bins=20, ax=axes[0])\n    axes[0].set_title('Histogram of Scores')\n\n    # Boxplot of Scores by Country\n    sns.boxplot(data=df, x='Country', y='Score', ax=axes[1])\n    axes[1].set_title('Boxplot of Scores by Country')\n\n    return fig"}
{"task_id": "BigCodeBench/231", "solution": "import numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.axes import Axes\ndef task_func(obj_list) -> Axes:\n    values = [obj.value for obj in obj_list]\n    mean = np.mean(values) if values else 0\n    std_dev = np.std(values) if values else 0\n\n    x = np.linspace(mean - 3.5*std_dev, mean + 3.5*std_dev, 100)\n    plt.plot(x, stats.norm.pdf(x, mean, std_dev), color='red')\n\n    plt.hist(values, bins='auto', alpha=0.75, rwidth=0.85, color='#003f5c', density=True)\n\n    ax = plt.gca()\n    plt.show()\n    return ax", "raw_solution": "import numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport random\nfrom matplotlib.axes import Axes\n\nclass ValueObject:\n    value = 0\n    def __init__(self, mu=0, std=1, seed=77):\n        random.seed(seed)\n        self.value = random.gauss(mu, std)\n\ndef task_func(obj_list) -> Axes:\n    values = [obj.value for obj in obj_list]\n    mean = np.mean(values) if values else 0\n    std_dev = np.std(values) if values else 0\n\n    x = np.linspace(mean - 3.5*std_dev, mean + 3.5*std_dev, 100)\n    plt.plot(x, stats.norm.pdf(x, mean, std_dev), color='red')\n\n    plt.hist(values, bins='auto', alpha=0.75, rwidth=0.85, color='#003f5c', density=True)\n\n    ax = plt.gca()\n    plt.show()\n    return ax"}
{"task_id": "BigCodeBench/232", "solution": "import pandas as pd\nfrom collections import Counter\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Ensure the DataFrame has the correct columns\n    if not all(col in df.columns for col in ['Customer Name', 'Sales', 'Category']):\n        raise ValueError(\"DataFrame does not have the correct columns\")\n\n    # Exclude duplicate customer names\n    df = df.drop_duplicates(subset=['Customer Name'])\n\n    # Calculate total sales\n    total_sales = df['Sales'].sum()\n\n    # Find the most popular category\n    category_counts = Counter(df['Category'])\n    most_popular_category = category_counts.most_common(1)[0][0]\n\n    return {'Total Sales': total_sales, 'Most Popular Category': most_popular_category}", "raw_solution": "import pandas as pd\nfrom collections import Counter\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    # Ensure the DataFrame has the correct columns\n    if not all(col in df.columns for col in ['Customer Name', 'Sales', 'Category']):\n        raise ValueError(\"DataFrame does not have the correct columns\")\n\n    # Exclude duplicate customer names\n    df = df.drop_duplicates(subset=['Customer Name'])\n\n    # Calculate total sales\n    total_sales = df['Sales'].sum()\n\n    # Find the most popular category\n    category_counts = Counter(df['Category'])\n    most_popular_category = category_counts.most_common(1)[0][0]\n\n    return {'Total Sales': total_sales, 'Most Popular Category': most_popular_category}"}
{"task_id": "BigCodeBench/233", "solution": "import random\nimport matplotlib.pyplot as plt\ndef task_func(obj_list, attr, num_bins=30):\n    \"\"\"\n    Create a histogram of the specified attribute from a list of objects and return the histogram plot.\n    \"\"\"\n    # Extract the attribute values from the objects\n    values = [getattr(obj, attr) for obj in obj_list]\n\n    # Create the histogram\n    plt.hist(values, bins=num_bins)\n\n    # Set the title and labels\n    plt.title('Histogram of attribute values')\n    plt.xlabel('Attribute Value')\n    plt.ylabel('Count')\n\n    return plt.gca()", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\nclass Object:\n    def __init__(self, value=None):\n        if value is None:\n            self.value = random.gauss(0, 1)\n        else:\n            self.value = value\n\ndef task_func(obj_list, attr, num_bins=30):\n    \"\"\"\n    Create a histogram of the specified attribute from a list of objects and return the histogram plot.\n    \"\"\"\n    # Extract the attribute values from the objects\n    values = [getattr(obj, attr) for obj in obj_list]\n\n    # Create the histogram\n    plt.hist(values, bins=num_bins)\n\n    # Set the title and labels\n    plt.title('Histogram of attribute values')\n    plt.xlabel('Attribute Value')\n    plt.ylabel('Count')\n\n    return plt.gca()\n\n# Test the function"}
{"task_id": "BigCodeBench/234", "solution": "import pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    df = df.drop_duplicates(subset='name')\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df['age'], df['score'])\n\n    fig, ax = plt.subplots()\n    ax.scatter(df['age'], df['score'])\n    ax.plot(df['age'], slope*df['age'] + intercept, color='red')\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Score')\n    ax.set_title('Linear Regression')\n\n    return (plt, ax)", "raw_solution": "import pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    df = df.drop_duplicates(subset='name')\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df['age'], df['score'])\n\n    fig, ax = plt.subplots()\n    ax.scatter(df['age'], df['score'])\n    ax.plot(df['age'], slope*df['age'] + intercept, color='red')\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Score')\n    ax.set_title('Linear Regression')\n\n    return (plt, ax)"}
{"task_id": "BigCodeBench/235", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom statsmodels.formula.api import ols\ndef task_func(mu, sigma, seed=0, num_samples=1000, num_bins=30):\n    np.random.seed(seed)\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Create histogram\n    counts, bins, _ = plt.hist(samples, bins=num_bins, density=True, color='skyblue', edgecolor='black', label='Histogram')\n\n    # Calculate PDF values\n    pdf_x = np.linspace(bins[0], bins[-1], 1000)\n    pdf_y = norm.pdf(pdf_x, mu, sigma)\n\n    # Plot PDF\n    plt.plot(pdf_x, pdf_y, color='red', label='PDF')\n\n    # Fit a second order polynomial to the histogram\n    bin_centers = (bins[:-1] + bins[1:]) / 2\n    bin_weights = counts / sum(counts)\n    model = ols('I ~ 1 + bin_centers + I(bin_centers**2)', data={'I': bin_weights, 'bin_centers': bin_centers}).fit()\n    ols_y = model.predict(exog={'bin_centers': pdf_x})\n\n    # Plot OLS line\n    plt.plot(pdf_x, ols_y, color='green', label='OLS')\n\n    plt.legend()\n    plt.title(f'Histogram with PDF and OLS ($\\mu$={mu}, $\\sigma$={sigma})')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return plt.gca()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom statsmodels.formula.api import ols\n\ndef task_func(mu, sigma, seed=0, num_samples=1000, num_bins=30):\n    np.random.seed(seed)\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Create histogram\n    counts, bins, _ = plt.hist(samples, bins=num_bins, density=True, color='skyblue', edgecolor='black', label='Histogram')\n\n    # Calculate PDF values\n    pdf_x = np.linspace(bins[0], bins[-1], 1000)\n    pdf_y = norm.pdf(pdf_x, mu, sigma)\n\n    # Plot PDF\n    plt.plot(pdf_x, pdf_y, color='red', label='PDF')\n\n    # Fit a second order polynomial to the histogram\n    bin_centers = (bins[:-1] + bins[1:]) / 2\n    bin_weights = counts / sum(counts)\n    model = ols('I ~ 1 + bin_centers + I(bin_centers**2)', data={'I': bin_weights, 'bin_centers': bin_centers}).fit()\n    ols_y = model.predict(exog={'bin_centers': pdf_x})\n\n    # Plot OLS line\n    plt.plot(pdf_x, ols_y, color='green', label='OLS')\n\n    plt.legend()\n    plt.title(f'Histogram with PDF and OLS ($\\mu$={mu}, $\\sigma$={sigma})')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/236", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\ndef task_func(df, test_size=0.2, random_state=42):\n    \"\"\"\n    Predicts categories based on 'Age' and 'Score' in a given DataFrame using a Random Forest Classifier.\n    Rows with duplicate 'Name' entries are dropped before the prediction.\n    The function uses a Random Forest Classifier from sklearn to make predictions and evaluates the model using accuracy.\n    The function raises a ValueError if input df is not a DataFrame.\n    The function returns the accuracy of the prediction as a float value.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df should be a pandas DataFrame\")\n\n    # Drop rows with duplicate 'Name' entries\n    df = df.drop_duplicates(subset='Name')\n\n    # Define features and target\n    X = df[['Age', 'Score']]\n    y = df['Category']\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Initialize the Random Forest Classifier\n    clf = RandomForestClassifier(random_state=random_state)\n\n    # Fit the model\n    clf.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = clf.predict(X_test)\n\n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef task_func(df, test_size=0.2, random_state=42):\n    \"\"\"\n    Predicts categories based on 'Age' and 'Score' in a given DataFrame using a Random Forest Classifier.\n    Rows with duplicate 'Name' entries are dropped before the prediction.\n    The function uses a Random Forest Classifier from sklearn to make predictions and evaluates the model using accuracy.\n    The function raises a ValueError if input df is not a DataFrame.\n    The function returns the accuracy of the prediction as a float value.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df should be a pandas DataFrame\")\n\n    # Drop rows with duplicate 'Name' entries\n    df = df.drop_duplicates(subset='Name')\n\n    # Define features and target\n    X = df[['Age', 'Score']]\n    y = df['Category']\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Initialize the Random Forest Classifier\n    clf = RandomForestClassifier(random_state=random_state)\n\n    # Fit the model\n    clf.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = clf.predict(X_test)\n\n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy"}
{"task_id": "BigCodeBench/237", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(data, save_plot=False, plot_path=None):\n    \"\"\"\n    Unzips a list of objects and their 3D coordinates, runs PCA to reduce the dimensionality to 2D, and depending on the value of save_plot parameter, either saves the plot to the provided path and returns the 2D coordinates or returns the 2D coordinates and the plot's Axes.\n    \"\"\"\n    if save_plot and plot_path is None:\n        raise ValueError(\"If save_plot is True, plot_path must be provided.\")\n\n    # Unzip the data\n    objects, coordinates_3d = zip(*data)\n    coordinates_3d = np.array(coordinates_3d)\n\n    # Run PCA\n    pca = PCA(n_components=2)\n    coordinates_2d = pca.fit_transform(coordinates_3d)\n\n    if save_plot:\n        # Create a plot\n        fig, ax = plt.subplots()\n        ax.scatter(coordinates_2d[:, 0], coordinates_2d[:, 1])\n\n        # Save the plot\n        plt.savefig(plot_path)\n\n        return coordinates_2d, ax\n    else:\n        return coordinates_2d", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(data, save_plot=False, plot_path=None):\n    \"\"\"\n    Unzips a list of objects and their 3D coordinates, runs PCA to reduce the dimensionality to 2D, and depending on the value of save_plot parameter, either saves the plot to the provided path and returns the 2D coordinates or returns the 2D coordinates and the plot's Axes.\n    \"\"\"\n    if save_plot and plot_path is None:\n        raise ValueError(\"If save_plot is True, plot_path must be provided.\")\n\n    # Unzip the data\n    objects, coordinates_3d = zip(*data)\n    coordinates_3d = np.array(coordinates_3d)\n\n    # Run PCA\n    pca = PCA(n_components=2)\n    coordinates_2d = pca.fit_transform(coordinates_3d)\n\n    if save_plot:\n        # Create a plot\n        fig, ax = plt.subplots()\n        ax.scatter(coordinates_2d[:, 0], coordinates_2d[:, 1])\n\n        # Save the plot\n        plt.savefig(plot_path)\n\n        return coordinates_2d, ax\n    else:\n        return coordinates_2d"}
{"task_id": "BigCodeBench/238", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    # Standardize 'Age' and 'Score' columns\n    scaler = StandardScaler()\n    df[['Age', 'Score']] = scaler.fit_transform(df[['Age', 'Score']])\n\n    # Remove duplicate entries based on 'Name'\n    df = df.drop_duplicates(subset='Name')\n\n    # Plot a scatter plot of standardized 'Age' and 'Score'\n    plt.figure(figsize=(8, 6))\n    plt.scatter(df['Age'], df['Score'])\n    plt.title('Scatter Plot of Standardized Age and Score')\n    plt.xlabel('Age (standardized)')\n    plt.ylabel('Score (standardized)')\n    plt.show()\n\n    return df, plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    # Standardize 'Age' and 'Score' columns\n    scaler = StandardScaler()\n    df[['Age', 'Score']] = scaler.fit_transform(df[['Age', 'Score']])\n\n    # Remove duplicate entries based on 'Name'\n    df = df.drop_duplicates(subset='Name')\n\n    # Plot a scatter plot of standardized 'Age' and 'Score'\n    plt.figure(figsize=(8, 6))\n    plt.scatter(df['Age'], df['Score'])\n    plt.title('Scatter Plot of Standardized Age and Score')\n    plt.xlabel('Age (standardized)')\n    plt.ylabel('Score (standardized)')\n    plt.show()\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values\n    numeric_values = [val for sublist in original for val in sublist if isinstance(val, (int, float))]\n    numeric_values = np.array(numeric_values)\n\n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n\n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n\n    # Generate PDF\n    x_values = np.linspace(stats_dict['min'], stats_dict['max'], 1000)\n    pdf_values = stats.norm.pdf(x_values, stats_dict['mean'], stats_dict['std_dev'])\n\n    ax.plot(x_values, pdf_values, label='PDF')\n    ax.legend()\n\n    return numeric_values, stats_dict, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extract numeric values\n    numeric_values = [val for sublist in original for val in sublist if isinstance(val, (int, float))]\n    numeric_values = np.array(numeric_values)\n\n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n\n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n\n    # Generate PDF\n    x_values = np.linspace(stats_dict['min'], stats_dict['max'], 1000)\n    pdf_values = stats.norm.pdf(x_values, stats_dict['mean'], stats_dict['std_dev'])\n\n    ax.plot(x_values, pdf_values, label='PDF')\n    ax.legend()\n\n    return numeric_values, stats_dict, ax"}
{"task_id": "BigCodeBench/240", "solution": "import pandas as pd\nfrom random import uniform\ndef task_func(n_data_points=1000, min_value=0.0, max_value=10.0, column_name='Value'):\n    \"\"\"\n    Generates a random dataset of floating-point numbers, truncates each value to 3 decimal places,\n    and returns the generated DataFrame with the specified column name.\n    \"\"\"\n    data = [round(uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=[column_name])\n    return df", "raw_solution": "import pandas as pd\nfrom random import uniform\n\ndef task_func(n_data_points=1000, min_value=0.0, max_value=10.0, column_name='Value'):\n    \"\"\"\n    Generates a random dataset of floating-point numbers, truncates each value to 3 decimal places,\n    and returns the generated DataFrame with the specified column name.\n    \"\"\"\n    data = [round(uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n    df = pd.DataFrame(data, columns=[column_name])\n    return df\n\n# Test the function\ndf = task_func(n_data_points=5, min_value=-1.0, max_value=2.0, column_name='Random_Values')"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array], norm='l2')[0]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the original and normalized arrays\n    ax.plot(original_array, label='Original Data')\n    ax.plot(normalized_array, label='Normalized Data')\n\n    # Set the title and labels\n    ax.set_title('Original vs. Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    # Add a legend\n    ax.legend()\n\n    # Return the numpy arrays and the axes object\n    return original_array, normalized_array, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array], norm='l2')[0]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the original and normalized arrays\n    ax.plot(original_array, label='Original Data')\n    ax.plot(normalized_array, label='Normalized Data')\n\n    # Set the title and labels\n    ax.set_title('Original vs. Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    # Add a legend\n    ax.legend()\n\n    # Return the numpy arrays and the axes object\n    return original_array, normalized_array, ax"}
{"task_id": "BigCodeBench/242", "solution": "import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(image_path, kernel_size):\n    \"\"\"\n    Applies a blur effect to an image using a specified kernel size, then visualizes both the original and blurred images side by side.\n    \"\"\"\n    if not isinstance(kernel_size, int) or kernel_size <= 0:\n        raise ValueError(\"Kernel size should be a positive integer.\")\n\n    try:\n        image = cv2.imread(image_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The specified image file {image_path} does not exist.\")\n\n    # Convert the image to RGB (from BGR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Apply blur effect\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n\n    # Create a figure with two subplots\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Display the original and blurred images\n    axs[0].imshow(image)\n    axs[0].set_title('Original Image')\n    axs[1].imshow(blurred_image)\n    axs[1].set_title('Blurred Image')\n\n    # Remove x and y ticks\n    for ax in axs:\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    plt.tight_layout()\n    plt.show()\n\n    return blurred_image, axs", "raw_solution": "import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(image_path, kernel_size):\n    \"\"\"\n    Applies a blur effect to an image using a specified kernel size, then visualizes both the original and blurred images side by side.\n    \"\"\"\n    if not isinstance(kernel_size, int) or kernel_size <= 0:\n        raise ValueError(\"Kernel size should be a positive integer.\")\n\n    try:\n        image = cv2.imread(image_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The specified image file {image_path} does not exist.\")\n\n    # Convert the image to RGB (from BGR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Apply blur effect\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n\n    # Create a figure with two subplots\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Display the original and blurred images\n    axs[0].imshow(image)\n    axs[0].set_title('Original Image')\n    axs[1].imshow(blurred_image)\n    axs[1].set_title('Blurred Image')\n\n    # Remove x and y ticks\n    for ax in axs:\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    plt.tight_layout()\n    plt.show()\n\n    return blurred_image, axs"}
{"task_id": "BigCodeBench/243", "solution": "import pandas as pd\nimport random\ndef task_func(n_data_points=10000):\n    if n_data_points == 0:\n        return pd.DataFrame()\n    else:\n        data = [round(random.uniform(0.0, 10.0), 3) for _ in range(n_data_points)]\n        df = pd.DataFrame(data, columns=['Value'])\n        return df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(n_data_points=10000):\n    if n_data_points == 0:\n        return pd.DataFrame()\n    else:\n        data = [round(random.uniform(0.0, 10.0), 3) for _ in range(n_data_points)]\n        df = pd.DataFrame(data, columns=['Value'])\n        return df\n\n# Test the function"}
{"task_id": "BigCodeBench/244", "solution": "import numpy as np\nfrom scipy.fft import fft\nfrom matplotlib import pyplot as plt\ndef task_func(original):\n    \"\"\"\n    Takes a list of numbers, converts it to a numpy array, calculates the Fast Fourier Transform (FFT) of the array,\n    and plots a histogram of the magnitudes of the FFT data.\n    \"\"\"\n    # Convert the list to a numpy array\n    original_array = np.array(original)\n\n    # Calculate the FFT of the array\n    fft_data = fft(original_array)\n\n    # If the original array is not empty, plot the histogram of the magnitudes of the FFT data\n    if original_array.size != 0:\n        # Create a new figure\n        plt.figure()\n\n        # Plot the histogram\n        plt.hist(np.abs(fft_data), bins='auto')\n\n        # Get the axes object of the plot\n        axes = plt.gca()\n    else:\n        # If the original array is empty, return an empty array for the FFT data and None for the axes object\n        fft_data = np.array([])\n        axes = None\n\n    return original_array, fft_data, axes", "raw_solution": "import numpy as np\nfrom scipy.fft import fft\nfrom matplotlib import pyplot as plt\n\ndef task_func(original):\n    \"\"\"\n    Takes a list of numbers, converts it to a numpy array, calculates the Fast Fourier Transform (FFT) of the array,\n    and plots a histogram of the magnitudes of the FFT data.\n    \"\"\"\n    # Convert the list to a numpy array\n    original_array = np.array(original)\n\n    # Calculate the FFT of the array\n    fft_data = fft(original_array)\n\n    # If the original array is not empty, plot the histogram of the magnitudes of the FFT data\n    if original_array.size != 0:\n        # Create a new figure\n        plt.figure()\n\n        # Plot the histogram\n        plt.hist(np.abs(fft_data), bins='auto')\n\n        # Get the axes object of the plot\n        axes = plt.gca()\n    else:\n        # If the original array is empty, return an empty array for the FFT data and None for the axes object\n        fft_data = np.array([])\n        axes = None\n\n    return original_array, fft_data, axes"}
{"task_id": "BigCodeBench/245", "solution": "import pandas as pd\nimport random\nfrom scipy import stats\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    # Generate a random dataset of floating-point numbers within a specified range\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Calculate statistical measures (mean, median, mode) of the data\n    mean_val = round(sum(data) / len(data), 3)\n    median_val = round(stats.tmean(data), 3)\n    mode_val = round(stats.mode(data)[0][0], 3)\n\n    # Return the results in a dictionary\n    return {'mean': mean_val, 'median': median_val, 'mode': mode_val}", "raw_solution": "import pandas as pd\nimport random\nfrom scipy import stats\n\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    # Generate a random dataset of floating-point numbers within a specified range\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Calculate statistical measures (mean, median, mode) of the data\n    mean_val = round(sum(data) / len(data), 3)\n    median_val = round(stats.tmean(data), 3)\n    mode_val = round(stats.mode(data)[0][0], 3)\n\n    # Return the results in a dictionary\n    return {'mean': mean_val, 'median': median_val, 'mode': mode_val}\n\n# Test the function"}
{"task_id": "BigCodeBench/246", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\nANGLES = np.arange(0, 2*np.pi, 0.01)\ndef task_func(n_waves, seed=0):\n    np.random.seed(seed)\n    if n_waves < 1:\n        return [], [], None\n    waves = [np.sin(i*ANGLES) for i in range(1, n_waves+1)]\n    mixed_signal = np.sum(waves, axis=0)\n    fft_data = np.abs(fft(mixed_signal))\n    fig, ax = plt.subplots()\n    ax.hist(fft_data, bins=50)\n    return waves, fft_data, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\nANGLES = np.arange(0, 2*np.pi, 0.01)\n\ndef task_func(n_waves, seed=0):\n    np.random.seed(seed)\n    if n_waves < 1:\n        return [], [], None\n    waves = [np.sin(i*ANGLES) for i in range(1, n_waves+1)]\n    mixed_signal = np.sum(waves, axis=0)\n    fft_data = np.abs(fft(mixed_signal))\n    fig, ax = plt.subplots()\n    ax.hist(fft_data, bins=50)\n    return waves, fft_data, ax"}
{"task_id": "BigCodeBench/247", "solution": "import pandas as pd\nimport random\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nN_DATA_POINTS = 5000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    \"\"\"\n    Generate a random dataset of floating point numbers, truncate each value to 3 decimal places,\n    and normalize the data using standard scaling (mean = 0, std = 1).\n    \"\"\"\n    if max_value < min_value:\n        raise ValueError(\"max_value should be greater than min_value\")\n\n    # Generate random floats and truncate to 3 decimal places\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Normalize data using StandardScaler\n    scaler = StandardScaler()\n    data = np.array(data).reshape(-1, 1)\n    scaler.fit(data)\n    normalized_data = scaler.transform(data)\n\n    # Create DataFrame\n    df = pd.DataFrame(normalized_data, columns=[\"Normalized Value\"])\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Constants\nN_DATA_POINTS = 5000\nMIN_VALUE = 0.0\nMAX_VALUE = 10.0\n\ndef task_func(n_data_points=5000, min_value=0.0, max_value=10.0):\n    \"\"\"\n    Generate a random dataset of floating point numbers, truncate each value to 3 decimal places,\n    and normalize the data using standard scaling (mean = 0, std = 1).\n    \"\"\"\n    if max_value < min_value:\n        raise ValueError(\"max_value should be greater than min_value\")\n\n    # Generate random floats and truncate to 3 decimal places\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Normalize data using StandardScaler\n    scaler = StandardScaler()\n    data = np.array(data).reshape(-1, 1)\n    scaler.fit(data)\n    normalized_data = scaler.transform(data)\n\n    # Create DataFrame\n    df = pd.DataFrame(normalized_data, columns=[\"Normalized Value\"])\n\n    return df\n\n# Test the function\ndf = task_func(n_data_points=N_DATA_POINTS, min_value=MIN_VALUE, max_value=MAX_VALUE)"}
{"task_id": "BigCodeBench/248", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"The data_list is empty.\")\n    else:\n        unzipped_data = list(itertools.chain(*data_list))\n        plt.plot(unzipped_data)\n        plt.show()\n        return plt.gca()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef task_func(data_list):\n    if not data_list:\n        raise ValueError(\"The data_list is empty.\")\n    else:\n        unzipped_data = list(itertools.chain(*data_list))\n        plt.plot(unzipped_data)\n        plt.show()\n        return plt.gca()"}
{"task_id": "BigCodeBench/249", "solution": "import pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\ndef task_func(n_data_points=10000, min_value=0.0, max_value=10.0, test_size=0.2):\n    # Generate random floats within the specified range\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(data, columns=[\"Value\"])\n\n    # Split the data into train and test sets\n    train, test = train_test_split(df, test_size=test_size, random_state=42)\n\n    return (train, test)", "raw_solution": "import pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(n_data_points=10000, min_value=0.0, max_value=10.0, test_size=0.2):\n    # Generate random floats within the specified range\n    data = [round(random.uniform(min_value, max_value), 3) for _ in range(n_data_points)]\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(data, columns=[\"Value\"])\n\n    # Split the data into train and test sets\n    train, test = train_test_split(df, test_size=test_size, random_state=42)\n\n    return (train, test)\n\n# Test the function\ntrain_set, test_set = task_func()"}
{"task_id": "BigCodeBench/250", "solution": "import numpy as np\nimport itertools\nimport json\ndef task_func(data_list, json_file_name=\"mean_values.json\"):\n    \"\"\"\n    Calculate the mean of the numeric values for each position in the provided data list and return the results.\n    Optionally, the results can be exported to a specified JSON file.\n    \"\"\"\n    # Initialize an empty dictionary to store the mean values\n    mean_values = {}\n\n    # Iterate over the data list\n    for i, group in enumerate(data_list):\n        # Calculate the mean of the numeric values at position i\n        mean_value = np.mean(list(itertools.chain.from_iterable(group)))\n\n        # Add the mean value to the dictionary\n        mean_values[f'Position {i}'] = mean_value\n\n    # If a JSON file name is provided, export the mean values to a JSON file\n    if json_file_name:\n        with open(json_file_name, 'w') as f:\n            json.dump(mean_values, f)\n\n    return mean_values", "raw_solution": "import numpy as np\nimport itertools\nimport json\n\ndef task_func(data_list, json_file_name=\"mean_values.json\"):\n    \"\"\"\n    Calculate the mean of the numeric values for each position in the provided data list and return the results.\n    Optionally, the results can be exported to a specified JSON file.\n    \"\"\"\n    # Initialize an empty dictionary to store the mean values\n    mean_values = {}\n\n    # Iterate over the data list\n    for i, group in enumerate(data_list):\n        # Calculate the mean of the numeric values at position i\n        mean_value = np.mean(list(itertools.chain.from_iterable(group)))\n\n        # Add the mean value to the dictionary\n        mean_values[f'Position {i}'] = mean_value\n\n    # If a JSON file name is provided, export the mean values to a JSON file\n    if json_file_name:\n        with open(json_file_name, 'w') as f:\n            json.dump(mean_values, f)\n\n    return mean_values"}
{"task_id": "BigCodeBench/251", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n\n    # Assuming the data has a column named 'Job'\n    if 'Job' not in data.columns:\n        raise ValueError(\"Input data should have a column named 'Job'\")\n\n    # Count the job distribution\n    job_distribution = data['Job'].value_counts()\n\n    # Create a pie chart\n    fig, ax = plt.subplots()\n    ax.pie(job_distribution, labels=job_distribution.index, autopct='%1.1f%%')\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    return fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n\n    # Assuming the data has a column named 'Job'\n    if 'Job' not in data.columns:\n        raise ValueError(\"Input data should have a column named 'Job'\")\n\n    # Count the job distribution\n    job_distribution = data['Job'].value_counts()\n\n    # Create a pie chart\n    fig, ax = plt.subplots()\n    ax.pie(job_distribution, labels=job_distribution.index, autopct='%1.1f%%')\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    return fig"}
{"task_id": "BigCodeBench/252", "solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\ndef task_func(data, labels):\n    \"\"\"\n    Plots a list of data with different colors. If there are more data series than the predefined colors,\n    the function cycles through the colors. In case of even more series than colors + labels, 'black' is used.\n    \"\"\"\n    # Create a color cycle\n    color_cycle = cycle(COLORS)\n\n    # Create a label cycle\n    label_cycle = cycle(labels) if labels else cycle([''])\n\n    # Create a figure and an axes\n    fig, ax = plt.subplots()\n\n    # Plot each data series with a different color and label\n    for i, (series, label) in enumerate(zip(data, label_cycle)):\n        color = next(color_cycle) if i < len(COLORS) else 'black'\n        ax.plot(series, color=color, label=label)\n\n    # Add legend\n    ax.legend()\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\n\n# Constants\nCOLORS = ['red', 'green', 'blue', 'yellow', 'purple']\n\ndef task_func(data, labels):\n    \"\"\"\n    Plots a list of data with different colors. If there are more data series than the predefined colors,\n    the function cycles through the colors. In case of even more series than colors + labels, 'black' is used.\n    \"\"\"\n    # Create a color cycle\n    color_cycle = cycle(COLORS)\n\n    # Create a label cycle\n    label_cycle = cycle(labels) if labels else cycle([''])\n\n    # Create a figure and an axes\n    fig, ax = plt.subplots()\n\n    # Plot each data series with a different color and label\n    for i, (series, label) in enumerate(zip(data, label_cycle)):\n        color = next(color_cycle) if i < len(COLORS) else 'black'\n        ax.plot(series, color=color, label=label)\n\n    # Add legend\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/253", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\ndef task_func(ax):\n    # Generate random parameters\n    freq = random.uniform(0.1, 10)\n    amp = random.uniform(0.1, 10)\n    phase = random.uniform(0, np.pi)\n    color = random.choice(COLORS)\n\n    # Generate x and y data\n    theta = np.linspace(0, 2*np.pi, 1000)\n    r = amp * np.sin(freq*theta + phase)\n\n    # Plot the data\n    ax.plot(theta, r, color=color)\n\n    # Randomly place radial labels\n    for i in range(10):\n        angle = random.uniform(0, 2*np.pi)\n        dist = random.uniform(0, 1.5*amp)\n        ax.text(angle, dist, str(i), color='black', ha='center', va='center')\n\n    return color", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\ndef task_func(ax):\n    # Generate random parameters\n    freq = random.uniform(0.1, 10)\n    amp = random.uniform(0.1, 10)\n    phase = random.uniform(0, np.pi)\n    color = random.choice(COLORS)\n\n    # Generate x and y data\n    theta = np.linspace(0, 2*np.pi, 1000)\n    r = amp * np.sin(freq*theta + phase)\n\n    # Plot the data\n    ax.plot(theta, r, color=color)\n\n    # Randomly place radial labels\n    for i in range(10):\n        angle = random.uniform(0, 2*np.pi)\n        dist = random.uniform(0, 1.5*amp)\n        ax.text(angle, dist, str(i), color='black', ha='center', va='center')\n\n    return color\n\n# Test the function\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\ncolor = task_func(ax)\nplt.show()"}
{"task_id": "BigCodeBench/254", "solution": "import json\nimport math\ndef task_func(decimal_value, precision=2):\n    \"\"\"\n    Calculates the square root of the given decimal value to a certain precision and then encodes the result as a JSON string.\n    \"\"\"\n    if decimal_value < 0:\n        raise ValueError(\"Decimal value must be non-negative\")\n\n    sqrt_value = round(math.sqrt(decimal_value), precision)\n    json_str = json.dumps({\"square_root\": sqrt_value})\n\n    return json_str", "raw_solution": "import json\nimport math\n\ndef task_func(decimal_value, precision=2):\n    \"\"\"\n    Calculates the square root of the given decimal value to a certain precision and then encodes the result as a JSON string.\n    \"\"\"\n    if decimal_value < 0:\n        raise ValueError(\"Decimal value must be non-negative\")\n\n    sqrt_value = round(math.sqrt(decimal_value), precision)\n    json_str = json.dumps({\"square_root\": sqrt_value})\n\n    return json_str\n\n# Testing the function"}
{"task_id": "BigCodeBench/255", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nFUNCTIONS = [np.sin, np.cos, np.tan]\ndef task_func(ax, func_index):\n    \"\"\"\n    Draws a mathematical function (sine, cosine, or tangent) on a polar diagram 'ax'.\n    The radial ticks are placed at a position corresponding to the index of the function multiplied by 45 degrees.\n    The function raises a ValueError if the input ax is not a matplotlib.axes._axes.Axes.\n    \"\"\"\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax must be a matplotlib.axes._axes.Axes\")\n    if func_index < 0 or func_index >= len(FUNCTIONS):\n        raise ValueError(\"Invalid function index\")\n\n    theta = np.linspace(0, np.pi * 2, 100)\n    r = FUNCTIONS[func_index](theta)\n\n    ax.plot(theta, r)\n    ax.set_theta_zero_location(\"N\")\n    ax.set_theta_direction(-1)\n\n    # Set radial ticks at multiples of 45 degrees\n    for deg in range(0, 360, 45):\n        rad = np.deg2rad(deg)\n        ax.plot([rad, rad], [0, 1], color='black', linestyle='--', linewidth=0.5)\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nFUNCTIONS = [np.sin, np.cos, np.tan]\n\ndef task_func(ax, func_index):\n    \"\"\"\n    Draws a mathematical function (sine, cosine, or tangent) on a polar diagram 'ax'.\n    The radial ticks are placed at a position corresponding to the index of the function multiplied by 45 degrees.\n    The function raises a ValueError if the input ax is not a matplotlib.axes._axes.Axes.\n    \"\"\"\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax must be a matplotlib.axes._axes.Axes\")\n    if func_index < 0 or func_index >= len(FUNCTIONS):\n        raise ValueError(\"Invalid function index\")\n\n    theta = np.linspace(0, np.pi * 2, 100)\n    r = FUNCTIONS[func_index](theta)\n\n    ax.plot(theta, r)\n    ax.set_theta_zero_location(\"N\")\n    ax.set_theta_direction(-1)\n\n    # Set radial ticks at multiples of 45 degrees\n    for deg in range(0, 360, 45):\n        rad = np.deg2rad(deg)\n        ax.plot([rad, rad], [0, 1], color='black', linestyle='--', linewidth=0.5)\n\n    return ax\n\n# Test the function\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\ntask_func(ax, 0)\nplt.show()"}
{"task_id": "BigCodeBench/256", "solution": "import json\nimport random\nimport hashlib\nimport string\nfrom datetime import datetime\ndef task_func(utc_datetime, salt='salt', password_length=10, seed=0):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not isinstance(salt, str):\n        raise ValueError(\"salt must be a string\")\n\n    random.seed(seed)\n    chars = string.ascii_lowercase + string.digits\n    password = ''.join(random.choice(chars) for _ in range(password_length))\n\n    hashed_password = hashlib.sha256()\n    hashed_password.update(password.encode())\n    hashed_password.update(salt.encode())\n    hashed_password.update(str(utc_datetime).encode())\n\n    result = {\n        'hashed_password': hashed_password.hexdigest()\n    }\n\n    return json.dumps(result)", "raw_solution": "import json\nimport random\nimport hashlib\nimport string\nfrom datetime import datetime\n\ndef task_func(utc_datetime, salt='salt', password_length=10, seed=0):\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not isinstance(salt, str):\n        raise ValueError(\"salt must be a string\")\n\n    random.seed(seed)\n    chars = string.ascii_lowercase + string.digits\n    password = ''.join(random.choice(chars) for _ in range(password_length))\n\n    hashed_password = hashlib.sha256()\n    hashed_password.update(password.encode())\n    hashed_password.update(salt.encode())\n    hashed_password.update(str(utc_datetime).encode())\n\n    result = {\n        'hashed_password': hashed_password.hexdigest()\n    }\n\n    return json.dumps(result)"}
{"task_id": "BigCodeBench/257", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(ax, num_turns):\n    \"\"\"\n    Draws a spiral on the polar diagram 'ax' with the specified number of turns 'num_turns'.\n    The spiral starts at the center and expands outward with each turn. The radial ticks on the plot are positioned at intervals corresponding to the number of turns multiplied by 45 degrees.\n    \"\"\"\n    # Generate the angles for the spiral\n    theta = np.linspace(0, num_turns * 2 * np.pi, 1000)\n\n    # Calculate the radius for each angle\n    r = theta * 45 / (2 * np.pi)\n\n    # Plot the spiral\n    ax.plot(theta, r)\n\n    # Set the radial ticks\n    ax.set_rticks([0.5, 1, 1.5, 2, 2.5])\n    ax.set_rlabel_position(225)\n\n    # Set the polar grid\n    ax.grid(True)\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(ax, num_turns):\n    \"\"\"\n    Draws a spiral on the polar diagram 'ax' with the specified number of turns 'num_turns'.\n    The spiral starts at the center and expands outward with each turn. The radial ticks on the plot are positioned at intervals corresponding to the number of turns multiplied by 45 degrees.\n    \"\"\"\n    # Generate the angles for the spiral\n    theta = np.linspace(0, num_turns * 2 * np.pi, 1000)\n\n    # Calculate the radius for each angle\n    r = theta * 45 / (2 * np.pi)\n\n    # Plot the spiral\n    ax.plot(theta, r)\n\n    # Set the radial ticks\n    ax.set_rticks([0.5, 1, 1.5, 2, 2.5])\n    ax.set_rlabel_position(225)\n\n    # Set the polar grid\n    ax.grid(True)\n\n    return ax\n\n# Test the function\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\ntask_func(ax, 2)\nplt.show()"}
{"task_id": "BigCodeBench/258", "solution": "import json\nimport random\nfrom datetime import datetime\nDATA = [\n    {'name': 'John', 'age': 30, 'city': 'New York'},\n    {'name': 'Peter', 'age': 35, 'city': 'London'},\n    {'name': 'Susan', 'age': 25, 'city': 'Sydney'},\n    {'name': 'Alice', 'age': 28, 'city': 'Paris'},\n    {'name': 'Bob', 'age': 40, 'city': 'Tokyo'},\n    {'name': 'Charlie', 'age': 22, 'city': 'Beijing'},\n    {'name': 'David', 'age': 33, 'city': 'Mumbai'},\n    {'name': 'Eve', 'age': 27, 'city': 'Berlin'},\n    {'name': 'Frank', 'age': 32, 'city': 'Moscow'},\n    {'name': 'Grace', 'age': 29, 'city': 'Rome'}\n]\ndef task_func(utc_datetime, seed=0):\n    \"\"\"\n    Select a random person from the DATA and add a UTC timestamp to their data.\n    Then, encode the person's data as a JSON string.\n    \"\"\"\n    random.seed(seed)\n    person = random.choice(DATA)\n    person['timestamp'] = utc_datetime.strftime('%Y-%m-%dT%H:%M:%SZ')\n    return json.dumps(person)\nutc_datetime = datetime.utcnow()", "raw_solution": "import json\nimport random\nfrom datetime import datetime\n\n# Constants\nDATA = [\n    {'name': 'John', 'age': 30, 'city': 'New York'},\n    {'name': 'Peter', 'age': 35, 'city': 'London'},\n    {'name': 'Susan', 'age': 25, 'city': 'Sydney'},\n    {'name': 'Alice', 'age': 28, 'city': 'Paris'},\n    {'name': 'Bob', 'age': 40, 'city': 'Tokyo'},\n    {'name': 'Charlie', 'age': 22, 'city': 'Beijing'},\n    {'name': 'David', 'age': 33, 'city': 'Mumbai'},\n    {'name': 'Eve', 'age': 27, 'city': 'Berlin'},\n    {'name': 'Frank', 'age': 32, 'city': 'Moscow'},\n    {'name': 'Grace', 'age': 29, 'city': 'Rome'}\n]\n\ndef task_func(utc_datetime, seed=0):\n    \"\"\"\n    Select a random person from the DATA and add a UTC timestamp to their data.\n    Then, encode the person's data as a JSON string.\n    \"\"\"\n    random.seed(seed)\n    person = random.choice(DATA)\n    person['timestamp'] = utc_datetime.strftime('%Y-%m-%dT%H:%M:%SZ')\n    return json.dumps(person)\n\n# Test the function\nutc_datetime = datetime.utcnow()"}
{"task_id": "BigCodeBench/259", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(ax, num_points):\n    \"\"\"\n    Plots \"num_points\" random points on the polar diagram represented by \"ax.\" \n    The radial ticks on the plot are positioned based on the number of points divided by 10 degrees.\n    \"\"\"\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax is not a matplotlib Axes object.\")\n    if num_points < 0:\n        raise ValueError(\"Number of points cannot be negative.\")\n\n    # Generate random points\n    theta = np.random.uniform(0, 2*np.pi, num_points)\n    r = np.random.uniform(0, 1, num_points)\n\n    # Plot points\n    ax.plot(theta, r, 'o')\n\n    # Set radial ticks\n    radial_ticks = np.arange(0, 1, 0.1)\n    ax.set_rticks(radial_ticks)\n    ax.set_rlabel_position(radial_ticks * 180 / np.pi)\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(ax, num_points):\n    \"\"\"\n    Plots \"num_points\" random points on the polar diagram represented by \"ax.\" \n    The radial ticks on the plot are positioned based on the number of points divided by 10 degrees.\n    \"\"\"\n    if not isinstance(ax, plt.Axes):\n        raise ValueError(\"Input ax is not a matplotlib Axes object.\")\n    if num_points < 0:\n        raise ValueError(\"Number of points cannot be negative.\")\n\n    # Generate random points\n    theta = np.random.uniform(0, 2*np.pi, num_points)\n    r = np.random.uniform(0, 1, num_points)\n\n    # Plot points\n    ax.plot(theta, r, 'o')\n\n    # Set radial ticks\n    radial_ticks = np.arange(0, 1, 0.1)\n    ax.set_rticks(radial_ticks)\n    ax.set_rlabel_position(radial_ticks * 180 / np.pi)\n\n    return ax\n\n# Test the function\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\ntask_func(ax, 100)\nplt.show()"}
{"task_id": "BigCodeBench/260", "solution": "import json\nimport os\nimport glob\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\ndef task_func(directory):\n    \"\"\"\n    Adds a new key-value pair to all JSON files in a specific directory and saves the updated JSON files.\n    \"\"\"\n    # Get all JSON files in the directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n\n    # Initialize counter for updated JSON files\n    updated_count = 0\n\n    # Iterate over each JSON file\n    for json_file in json_files:\n        # Open the JSON file\n        with open(json_file, 'r+') as file:\n            # Load the JSON data\n            data = json.load(file)\n\n            # Check if the key already exists\n            if KEY not in data:\n                # Add the new key-value pair\n                data[KEY] = VALUE\n\n                # Move read cursor to the beginning of the file\n                file.seek(0)\n\n                # Dump the updated JSON data to the file\n                json.dump(data, file, indent=4)\n\n                # Truncate the rest of the file\n                file.truncate()\n\n                # Increment the updated count\n                updated_count += 1\n\n    # Return the number of updated JSON files\n    return updated_count", "raw_solution": "import json\nimport os\nimport glob\n\n# Constants\nKEY = 'mynewkey'\nVALUE = 'mynewvalue'\n\ndef task_func(directory):\n    \"\"\"\n    Adds a new key-value pair to all JSON files in a specific directory and saves the updated JSON files.\n    \"\"\"\n    # Get all JSON files in the directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n\n    # Initialize counter for updated JSON files\n    updated_count = 0\n\n    # Iterate over each JSON file\n    for json_file in json_files:\n        # Open the JSON file\n        with open(json_file, 'r+') as file:\n            # Load the JSON data\n            data = json.load(file)\n\n            # Check if the key already exists\n            if KEY not in data:\n                # Add the new key-value pair\n                data[KEY] = VALUE\n\n                # Move read cursor to the beginning of the file\n                file.seek(0)\n\n                # Dump the updated JSON data to the file\n                json.dump(data, file, indent=4)\n\n                # Truncate the rest of the file\n                file.truncate()\n\n                # Increment the updated count\n                updated_count += 1\n\n    # Return the number of updated JSON files\n    return updated_count"}
{"task_id": "BigCodeBench/261", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(ax, radius):\n    if not isinstance(ax, plt.Axes):\n        raise TypeError(\"'ax' must be a matplotlib.axes._axes.Axes object\")\n    if radius < 0:\n        raise ValueError(\"Radius cannot be negative\")\n\n    # Generate data for the circle\n    theta = np.linspace(0, 2*np.pi, 100)\n    x = radius * np.cos(theta)\n    y = radius * np.sin(theta)\n\n    # Plot the circle\n    ax.plot(x, y)\n\n    # Set radial ticks\n    ax.set_rticks([0.5, 1, 1.5, 2])\n    ax.set_rlabel_position(225)  # Move radial labels away from plotted line\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(ax, radius):\n    if not isinstance(ax, plt.Axes):\n        raise TypeError(\"'ax' must be a matplotlib.axes._axes.Axes object\")\n    if radius < 0:\n        raise ValueError(\"Radius cannot be negative\")\n\n    # Generate data for the circle\n    theta = np.linspace(0, 2*np.pi, 100)\n    x = radius * np.cos(theta)\n    y = radius * np.sin(theta)\n\n    # Plot the circle\n    ax.plot(x, y)\n\n    # Set radial ticks\n    ax.set_rticks([0.5, 1, 1.5, 2])\n    ax.set_rlabel_position(225)  # Move radial labels away from plotted line\n\n    return ax\n\n# Test the function\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\ntask_func(ax, 1)\nplt.show()"}
{"task_id": "BigCodeBench/262", "solution": "import collections\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(dictionary, new_key, new_value):\n    \"\"\"\n    Adds a new key-value pair to the dictionary and plots the distribution of its values.\n    \"\"\"\n    # Add new key-value pair to the dictionary\n    dictionary[new_key] = new_value\n\n    # Create a counter from the dictionary values\n    counter = collections.Counter(dictionary.values())\n\n    # Plot the distribution of values\n    fig, ax = plt.subplots()\n    sns.barplot(list(counter.keys()), list(counter.values()), ax=ax)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Values')\n\n    return dictionary, ax", "raw_solution": "import collections\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(dictionary, new_key, new_value):\n    \"\"\"\n    Adds a new key-value pair to the dictionary and plots the distribution of its values.\n    \"\"\"\n    # Add new key-value pair to the dictionary\n    dictionary[new_key] = new_value\n\n    # Create a counter from the dictionary values\n    counter = collections.Counter(dictionary.values())\n\n    # Plot the distribution of values\n    fig, ax = plt.subplots()\n    sns.barplot(list(counter.keys()), list(counter.values()), ax=ax)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Values')\n\n    return dictionary, ax"}
{"task_id": "BigCodeBench/263", "solution": "import os\nimport glob\nimport shutil\nimport time\nfrom datetime import datetime, timedelta\nFILE_EXTENSIONS = ['.txt', '.csv', '.xlsx', '.docx', '.pdf']\ndef task_func(my_path: str, days_old: int) -> str:\n    \"\"\"\n    Archive files that were changed older than a specified number of days in a given directory.\n    \"\"\"\n    # Create the archive directory if it doesn't exist\n    archive_path = os.path.join(my_path, 'archive')\n    if not os.path.exists(archive_path):\n        os.makedirs(archive_path)\n\n    # Get the current time\n    now = time.time()\n\n    # Get the files with the specified extensions\n    for file_extension in FILE_EXTENSIONS:\n        files = glob.glob(os.path.join(my_path, '*' + file_extension))\n\n        # Move files older than 'days_old' to the archive directory\n        for file in files:\n            if os.stat(file).st_mtime < now - days_old * 86400:\n                shutil.move(file, archive_path)\n\n    return archive_path", "raw_solution": "import os\nimport glob\nimport shutil\nimport time\nfrom datetime import datetime, timedelta\n\n# Constants\nFILE_EXTENSIONS = ['.txt', '.csv', '.xlsx', '.docx', '.pdf']\n\ndef task_func(my_path: str, days_old: int) -> str:\n    \"\"\"\n    Archive files that were changed older than a specified number of days in a given directory.\n    \"\"\"\n    # Create the archive directory if it doesn't exist\n    archive_path = os.path.join(my_path, 'archive')\n    if not os.path.exists(archive_path):\n        os.makedirs(archive_path)\n\n    # Get the current time\n    now = time.time()\n\n    # Get the files with the specified extensions\n    for file_extension in FILE_EXTENSIONS:\n        files = glob.glob(os.path.join(my_path, '*' + file_extension))\n\n        # Move files older than 'days_old' to the archive directory\n        for file in files:\n            if os.stat(file).st_mtime < now - days_old * 86400:\n                shutil.move(file, archive_path)\n\n    return archive_path"}
{"task_id": "BigCodeBench/264", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(dictionary, key, value, n=100, bins=30, seed=0):\n    \"\"\"\n    Updates the provided dictionary with a specified key-value pair and generates a random dataset of size 'n' following a normal distribution. The mean and standard deviation of the distribution are set to the value associated with the given key. Additionally, it returns a histogram of the generated dataset.\n    \"\"\"\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"The provided value is not a number.\")\n\n    dictionary[key] = value\n\n    np.random.seed(seed)\n    data = np.random.normal(value, 1, n)\n    series = pd.Series(data)\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(series, bins=bins, color='skyblue',\n             edgecolor='black', alpha=0.7)\n    plt.title(f'Histogram of Normal Distribution with Mean={value} and Std=1')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return dictionary, series, plt.gcf()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(dictionary, key, value, n=100, bins=30, seed=0):\n    \"\"\"\n    Updates the provided dictionary with a specified key-value pair and generates a random dataset of size 'n' following a normal distribution. The mean and standard deviation of the distribution are set to the value associated with the given key. Additionally, it returns a histogram of the generated dataset.\n    \"\"\"\n    if not isinstance(value, (int, float)):\n        raise ValueError(\"The provided value is not a number.\")\n\n    dictionary[key] = value\n\n    np.random.seed(seed)\n    data = np.random.normal(value, 1, n)\n    series = pd.Series(data)\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(series, bins=bins, color='skyblue',\n             edgecolor='black', alpha=0.7)\n    plt.title(f'Histogram of Normal Distribution with Mean={value} and Std=1')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return dictionary, series, plt.gcf()"}
{"task_id": "BigCodeBench/265", "solution": "import collections\nimport json\nimport os\ndef task_func(data, json_file_name='data.json'):\n    # Add a new key \"a\" with the value 1 to the input dictionary\n    data['a'] = 1\n\n    # Calculate the frequency of its values\n    freq = dict(collections.Counter(data.values()))\n\n    # Save the updated dictionary along with its frequency distribution to a JSON file\n    data_with_freq = {'data': data, 'freq': freq}\n    with open(json_file_name, 'w') as f:\n        json.dump(data_with_freq, f)\n\n    # Return the path of the JSON file\n    return os.path.abspath(json_file_name)", "raw_solution": "import collections\nimport json\nimport os\n\ndef task_func(data, json_file_name='data.json'):\n    # Add a new key \"a\" with the value 1 to the input dictionary\n    data['a'] = 1\n\n    # Calculate the frequency of its values\n    freq = dict(collections.Counter(data.values()))\n\n    # Save the updated dictionary along with its frequency distribution to a JSON file\n    data_with_freq = {'data': data, 'freq': freq}\n    with open(json_file_name, 'w') as f:\n        json.dump(data_with_freq, f)\n\n    # Return the path of the JSON file\n    return os.path.abspath(json_file_name)"}
{"task_id": "BigCodeBench/266", "solution": "import os\nimport csv\nFILE_NAME = 'file_sizes.csv'\ndef task_func(my_path):\n    \"\"\"\n    This function creates a report on the file size in a directory and writes it to a CSV file.\n    \"\"\"\n    # Create a list to store the file sizes\n    file_sizes = []\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(my_path):\n        for file in files:\n            # Get the file size\n            file_size = os.path.getsize(os.path.join(root, file))\n            # Append the file size to the list\n            file_sizes.append({'file_name': file, 'file_size': file_size})\n\n    # Write the file sizes to a CSV file\n    with open(FILE_NAME, 'w', newline='') as csvfile:\n        fieldnames = ['file_name', 'file_size']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for file_size in file_sizes:\n            writer.writerow(file_size)\n\n    return FILE_NAME", "raw_solution": "import os\nimport csv\n\n# Constants\nFILE_NAME = 'file_sizes.csv'\n\ndef task_func(my_path):\n    \"\"\"\n    This function creates a report on the file size in a directory and writes it to a CSV file.\n    \"\"\"\n    # Create a list to store the file sizes\n    file_sizes = []\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(my_path):\n        for file in files:\n            # Get the file size\n            file_size = os.path.getsize(os.path.join(root, file))\n            # Append the file size to the list\n            file_sizes.append({'file_name': file, 'file_size': file_size})\n\n    # Write the file sizes to a CSV file\n    with open(FILE_NAME, 'w', newline='') as csvfile:\n        fieldnames = ['file_name', 'file_size']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for file_size in file_sizes:\n            writer.writerow(file_size)\n\n    return FILE_NAME"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # 1. Adds a new key \"a\" with the value 1 to the dictionary.\n    data['a'] = 1\n\n    # 2. Generates a signal based on the values in \"data\".\n    signal = list(data.values())\n\n    # 3. Runs a Fast Fourier Transform (FFT) on the signal.\n    fft_signal = fftpack.fft(signal)\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n\n    # 4. Plots and returns the FFT of the signal with a title of 'FFT of the signal'.\n    plt.figure(figsize=(10, 5))\n    plt.plot(freqs, np.abs(fft_signal))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n    plt.show()\n\n    return (freqs, np.abs(fft_signal))", "raw_solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # 1. Adds a new key \"a\" with the value 1 to the dictionary.\n    data['a'] = 1\n\n    # 2. Generates a signal based on the values in \"data\".\n    signal = list(data.values())\n\n    # 3. Runs a Fast Fourier Transform (FFT) on the signal.\n    fft_signal = fftpack.fft(signal)\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n\n    # 4. Plots and returns the FFT of the signal with a title of 'FFT of the signal'.\n    plt.figure(figsize=(10, 5))\n    plt.plot(freqs, np.abs(fft_signal))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n    plt.show()\n\n    return (freqs, np.abs(fft_signal))"}
{"task_id": "BigCodeBench/268", "solution": "import collections\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\ndef task_func(n_keys, n_values):\n    \"\"\"\n    Creates a Python dictionary with a specified number of keys and values.\n    Keys are randomly selected from a predefined list of letters, and values are consecutive integers starting from 1.\n    \"\"\"\n    # Randomly select keys from the list of letters\n    keys = random.sample(LETTERS, n_keys)\n\n    # Create a dictionary with keys and values\n    dict_out = {key: list(range(1, n_values + 1)) for key in keys}\n\n    return dict_out", "raw_solution": "import collections\nimport random\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndef task_func(n_keys, n_values):\n    \"\"\"\n    Creates a Python dictionary with a specified number of keys and values.\n    Keys are randomly selected from a predefined list of letters, and values are consecutive integers starting from 1.\n    \"\"\"\n    # Randomly select keys from the list of letters\n    keys = random.sample(LETTERS, n_keys)\n\n    # Create a dictionary with keys and values\n    dict_out = {key: list(range(1, n_values + 1)) for key in keys}\n\n    return dict_out\n\n# Test the function"}
{"task_id": "BigCodeBench/269", "solution": "import numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    # Add key \"a\" with value 1\n    data_dict[\"a\"] = 1\n\n    # Conduct statistical analysis on values\n    values = list(data_dict.values())\n    mean = round(np.mean(values), 2)\n    median = round(np.median(values), 2)\n    mode = round(stats.mode(values)[0][0], 2)\n    stats_dict = {\"mean\": mean, \"median\": median, \"mode\": mode}\n\n    # Normalize values using MinMaxScaler\n    scaler = MinMaxScaler()\n    normalized_values = scaler.fit_transform(np.array(values).reshape(-1, 1))\n\n    # Plot histogram of normalized values\n    plt.hist(normalized_values, bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Normalized Values')\n    hist_plot = plt.gca()\n\n    return data_dict, stats_dict, hist_plot", "raw_solution": "import numpy as np\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict):\n    # Add key \"a\" with value 1\n    data_dict[\"a\"] = 1\n\n    # Conduct statistical analysis on values\n    values = list(data_dict.values())\n    mean = round(np.mean(values), 2)\n    median = round(np.median(values), 2)\n    mode = round(stats.mode(values)[0][0], 2)\n    stats_dict = {\"mean\": mean, \"median\": median, \"mode\": mode}\n\n    # Normalize values using MinMaxScaler\n    scaler = MinMaxScaler()\n    normalized_values = scaler.fit_transform(np.array(values).reshape(-1, 1))\n\n    # Plot histogram of normalized values\n    plt.hist(normalized_values, bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Normalized Values')\n    hist_plot = plt.gca()\n\n    return data_dict, stats_dict, hist_plot"}
{"task_id": "BigCodeBench/270", "solution": "import re\nfrom collections import Counter\ndef task_func(sentence):\n    words = re.findall(r'\\b\\w+\\b', sentence)\n    return dict(Counter(words))", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(sentence):\n    words = re.findall(r'\\b\\w+\\b', sentence)\n    return dict(Counter(words))\n\n# Test the function"}
{"task_id": "BigCodeBench/271", "solution": "import random\nimport string\nimport hashlib\nimport time\ndef task_func(data_dict: dict, seed=0) -> dict:\n    random.seed(seed)\n    salt = ''.join(random.choices(string.ascii_lowercase, k=5))\n    data_dict['a'] = 1\n    for key in data_dict.keys():\n        data_dict[key] = hashlib.sha256((data_dict[key] + salt).encode()).hexdigest()\n    data_dict['timestamp'] = int(time.time())\n    return data_dict", "raw_solution": "import random\nimport string\nimport hashlib\nimport time\n\ndef task_func(data_dict: dict, seed=0) -> dict:\n    random.seed(seed)\n    salt = ''.join(random.choices(string.ascii_lowercase, k=5))\n    data_dict['a'] = 1\n    for key in data_dict.keys():\n        data_dict[key] = hashlib.sha256((data_dict[key] + salt).encode()).hexdigest()\n    data_dict['timestamp'] = int(time.time())\n    return data_dict"}
{"task_id": "BigCodeBench/272", "solution": "import cgi\nimport http.server\nimport json", "raw_solution": "import cgi\nimport http.server\nimport json\n\nclass TaskHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n\n        if self.headers['Content-Type'] != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            response = {\"status\": \"error\", \"message\": \"Content-Type header is not application/json\"}\n            self.wfile.write(json.dumps(response).encode())\n            return\n\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            response = {\"status\": \"error\", \"message\": \"Invalid JSON\"}\n            self.wfile.write(json.dumps(response).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.end_headers()\n            response = {\"status\": \"error\", \"message\": \"No data received\"}\n            self.wfile.write(json.dumps(response).encode())\n            return\n\n        self.send_response(200)\n        self.end_headers()\n        response = {\"status\": \"success\", \"message\": \"Data received successfully.\"}\n        self.wfile.write(json.dumps(response).encode())\n"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nclass PostHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        # Parse the form data\n        form = cgi.FieldStorage(\n            fp=self.rfile,\n            headers=self.headers,\n            environ={'REQUEST_METHOD': 'POST'}\n        )\n\n        # Check if the 'content-type' header is 'application/json'\n        if self.headers['content-type'] != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Content-Type header is not application/json'\n            }).encode())\n            return\n\n        # Check if the JSON object contains the 'data' key\n        try:\n            data = json.loads(form.value)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Invalid JSON'\n            }).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'No data key in request'\n            }).encode())\n            return\n\n        # If all checks pass, send a success response\n        self.send_response(200)\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\ndef task_func():\n    \"\"\"\n    This function creates a simple HTTP server that listens on port 8000 and handles POST requests.\n    \"\"\"\n    server_address = ('', 8000)\n    httpd = http.server.HTTPServer(server_address, PostHandler)\n    print('Starting server on port 8000...')\n    httpd.serve_forever()", "raw_solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\nclass PostHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        # Parse the form data\n        form = cgi.FieldStorage(\n            fp=self.rfile,\n            headers=self.headers,\n            environ={'REQUEST_METHOD': 'POST'}\n        )\n\n        # Check if the 'content-type' header is 'application/json'\n        if self.headers['content-type'] != 'application/json':\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Content-Type header is not application/json'\n            }).encode())\n            return\n\n        # Check if the JSON object contains the 'data' key\n        try:\n            data = json.loads(form.value)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'Invalid JSON'\n            }).encode())\n            return\n\n        if 'data' not in data:\n            self.send_response(400)\n            self.end_headers()\n            self.wfile.write(json.dumps({\n                'status': 'error',\n                'message': 'No data key in request'\n            }).encode())\n            return\n\n        # If all checks pass, send a success response\n        self.send_response(200)\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\n\ndef task_func():\n    \"\"\"\n    This function creates a simple HTTP server that listens on port 8000 and handles POST requests.\n    \"\"\"\n    server_address = ('', 8000)\n    httpd = http.server.HTTPServer(server_address, PostHandler)\n    print('Starting server on port 8000...')\n    httpd.serve_forever()"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\nfrom json import JSONDecodeError\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            try:\n                # Parse the request body as JSON\n                content_length = int(self.headers['Content-Length'])\n                post_data = self.rfile.read(content_length)\n                email_data = json.loads(post_data)\n\n                # Extract the email data\n                subject = email_data['subject']\n                message = email_data['message']\n                to = email_data['to']\n\n                # Create the email\n                msg = MIMEText(message)\n                msg['Subject'] = subject\n                msg['From'] = smtp_username\n                msg['To'] = to\n\n                # Send the email\n                smtp = smtplib.SMTP(smtp_server, smtp_port)\n                smtp.login(smtp_username, smtp_password)\n                smtp.send_message(msg)\n                smtp.quit()\n\n                # Send a 200 OK response\n                self.send_response(200)\n                self.end_headers()\n                response = 'Email sent successfully'.encode()\n                self.wfile.write(response)\n\n            except JSONDecodeError:\n                # Send a 400 Bad Request response if the email data is not valid JSON\n                self.send_response(400)\n                self.end_headers()\n                response = 'Invalid JSON'.encode()\n                self.wfile.write(response)\n\n            except KeyError:\n                # Send a 400 Bad Request response if the 'subject', 'message', or 'to' keys are missing from the email data\n                self.send_response(400)\n                self.end_headers()\n                response = 'Missing email data'.encode()\n                self.wfile.write(response)\n\n            except smtplib.SMTPAuthenticationError:\n                # Send a 535 Authentication Failed response if there is an authentication issue with the SMTP server\n                self.send_response(535)\n                self.end_headers()\n                response = 'SMTP Authentication Error'.encode()\n                self.wfile.write(response)\n\n    return EmailHandler", "raw_solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\nfrom json import JSONDecodeError\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            try:\n                # Parse the request body as JSON\n                content_length = int(self.headers['Content-Length'])\n                post_data = self.rfile.read(content_length)\n                email_data = json.loads(post_data)\n\n                # Extract the email data\n                subject = email_data['subject']\n                message = email_data['message']\n                to = email_data['to']\n\n                # Create the email\n                msg = MIMEText(message)\n                msg['Subject'] = subject\n                msg['From'] = smtp_username\n                msg['To'] = to\n\n                # Send the email\n                smtp = smtplib.SMTP(smtp_server, smtp_port)\n                smtp.login(smtp_username, smtp_password)\n                smtp.send_message(msg)\n                smtp.quit()\n\n                # Send a 200 OK response\n                self.send_response(200)\n                self.end_headers()\n                response = 'Email sent successfully'.encode()\n                self.wfile.write(response)\n\n            except JSONDecodeError:\n                # Send a 400 Bad Request response if the email data is not valid JSON\n                self.send_response(400)\n                self.end_headers()\n                response = 'Invalid JSON'.encode()\n                self.wfile.write(response)\n\n            except KeyError:\n                # Send a 400 Bad Request response if the 'subject', 'message', or 'to' keys are missing from the email data\n                self.send_response(400)\n                self.end_headers()\n                response = 'Missing email data'.encode()\n                self.wfile.write(response)\n\n            except smtplib.SMTPAuthenticationError:\n                # Send a 535 Authentication Failed response if there is an authentication issue with the SMTP server\n                self.send_response(535)\n                self.end_headers()\n                response = 'SMTP Authentication Error'.encode()\n                self.wfile.write(response)\n\n    return EmailHandler"}
{"task_id": "BigCodeBench/275", "solution": "import numpy as np\nfrom itertools import combinations\ndef task_func(n):\n    if n < 1:\n        raise ValueError(\"Input n should be greater than 0\")\n    return list(combinations(range(1, n+1), 2))", "raw_solution": "import numpy as np\nfrom itertools import combinations\n\ndef task_func(n):\n    if n < 1:\n        raise ValueError(\"Input n should be greater than 0\")\n    return list(combinations(range(1, n+1), 2))\n\n# Testing the function"}
{"task_id": "BigCodeBench/276", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(matrix):\n    \"\"\"\n    Calculates the distribution of the maximum values of each row in the matrix,\n    records the histogram and the estimate of the core density of the distribution,\n    and returns the skew, kurtosis, and the histogram plot of the distribution.\n    \"\"\"\n    # Calculate the maximum values of each row\n    max_values = np.max(matrix, axis=1)\n\n    # Calculate the histogram and bin edges\n    hist, bin_edges = np.histogram(max_values, bins='auto', density=True)\n\n    # Calculate the bin centers\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    # Estimate the core density of the distribution\n    core_density = stats.gaussian_kde(max_values)(bin_centers)\n\n    # Calculate the skewness and kurtosis\n    skewness = stats.skew(max_values)\n    kurtosis = stats.kurtosis(max_values)\n\n    # Create the histogram plot\n    fig, ax = plt.subplots()\n    ax.bar(bin_centers, hist, width=np.diff(bin_edges)[0], align='center')\n    ax.plot(bin_centers, core_density, 'r-', linewidth=2)\n    ax.set_xlabel('Max Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Max Values')\n\n    return skewness, kurtosis, ax", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(matrix):\n    \"\"\"\n    Calculates the distribution of the maximum values of each row in the matrix,\n    records the histogram and the estimate of the core density of the distribution,\n    and returns the skew, kurtosis, and the histogram plot of the distribution.\n    \"\"\"\n    # Calculate the maximum values of each row\n    max_values = np.max(matrix, axis=1)\n\n    # Calculate the histogram and bin edges\n    hist, bin_edges = np.histogram(max_values, bins='auto', density=True)\n\n    # Calculate the bin centers\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    # Estimate the core density of the distribution\n    core_density = stats.gaussian_kde(max_values)(bin_centers)\n\n    # Calculate the skewness and kurtosis\n    skewness = stats.skew(max_values)\n    kurtosis = stats.kurtosis(max_values)\n\n    # Create the histogram plot\n    fig, ax = plt.subplots()\n    ax.bar(bin_centers, hist, width=np.diff(bin_edges)[0], align='center')\n    ax.plot(bin_centers, core_density, 'r-', linewidth=2)\n    ax.set_xlabel('Max Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Max Values')\n\n    return skewness, kurtosis, ax"}
{"task_id": "BigCodeBench/277", "solution": "import random\nfrom itertools import combinations\nimport math\ndef task_func(n):\n    if n < 2:\n        return None\n\n    # Generate n random dots within a unit square\n    dots = [(random.random(), random.random()) for _ in range(n)]\n\n    # Initialize variables to keep track of the closest pair and its distance\n    closest_pair = None\n    min_distance = float('inf')\n\n    # Iterate over all pairs of dots\n    for pair in combinations(dots, 2):\n        # Calculate the Euclidean distance between the pair\n        distance = math.sqrt((pair[0][0] - pair[1][0])**2 + (pair[0][1] - pair[1][1])**2)\n\n        # If this pair is closer than the current closest pair, update the closest pair and its distance\n        if distance < min_distance:\n            closest_pair = pair\n            min_distance = distance\n\n    return closest_pair", "raw_solution": "import random\nfrom itertools import combinations\nimport math\n\ndef task_func(n):\n    if n < 2:\n        return None\n\n    # Generate n random dots within a unit square\n    dots = [(random.random(), random.random()) for _ in range(n)]\n\n    # Initialize variables to keep track of the closest pair and its distance\n    closest_pair = None\n    min_distance = float('inf')\n\n    # Iterate over all pairs of dots\n    for pair in combinations(dots, 2):\n        # Calculate the Euclidean distance between the pair\n        distance = math.sqrt((pair[0][0] - pair[1][0])**2 + (pair[0][1] - pair[1][1])**2)\n\n        # If this pair is closer than the current closest pair, update the closest pair and its distance\n        if distance < min_distance:\n            closest_pair = pair\n            min_distance = distance\n\n    return closest_pair"}
{"task_id": "BigCodeBench/278", "solution": "import numpy as np\nfrom sympy import symbols, solve\ndef task_func(precision=2, seed=0):\n    np.random.seed(seed)\n    a, b, c = np.random.uniform(-10, 10, 3)\n    x = symbols('x')\n    solutions = solve(a*x**2 + b*x + c, x)\n    solutions = [complex(round(s.evalf(precision), precision)) for s in solutions]\n    return tuple(solutions)", "raw_solution": "import numpy as np\nfrom sympy import symbols, solve\n\ndef task_func(precision=2, seed=0):\n    np.random.seed(seed)\n    a, b, c = np.random.uniform(-10, 10, 3)\n    x = symbols('x')\n    solutions = solve(a*x**2 + b*x + c, x)\n    solutions = [complex(round(s.evalf(precision), precision)) for s in solutions]\n    return tuple(solutions)\n\n# Testing the function"}
{"task_id": "BigCodeBench/279", "solution": "import random\nfrom collections import Counter\nCARDS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\ndef task_func(x=1):\n    \"\"\"\n    Draws x random 5-card poker hands from a 52-card pack (without suits) and returns the hands along with a counter of the drawn cards.\n    \"\"\"\n    # Initialize an empty list to store the hands\n    hands = []\n\n    # Initialize an empty Counter to count the drawn cards\n    drawn_cards = Counter()\n\n    # Draw x hands\n    for _ in range(x):\n        # Draw a hand\n        hand = random.sample(CARDS, 5)\n        hands.append(hand)\n\n        # Update the counter\n        drawn_cards.update(hand)\n\n    # Return the hands and the counter\n    return (hands, drawn_cards)", "raw_solution": "import random\nfrom collections import Counter\n\n# Constants\nCARDS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n\ndef task_func(x=1):\n    \"\"\"\n    Draws x random 5-card poker hands from a 52-card pack (without suits) and returns the hands along with a counter of the drawn cards.\n    \"\"\"\n    # Initialize an empty list to store the hands\n    hands = []\n\n    # Initialize an empty Counter to count the drawn cards\n    drawn_cards = Counter()\n\n    # Draw x hands\n    for _ in range(x):\n        # Draw a hand\n        hand = random.sample(CARDS, 5)\n        hands.append(hand)\n\n        # Update the counter\n        drawn_cards.update(hand)\n\n    # Return the hands and the counter\n    return (hands, drawn_cards)"}
{"task_id": "BigCodeBench/280", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\ndef task_func(signal, precision=2, seed=777):\n    \"\"\"\n    Calculates the one-dimensional discrete N-point Fourier Transform (DFT) for a real or complex sequence (signal) using the Fast Fourier Transform (FFT) algorithm.\n    Plots the original signal and the transformed signal, rounding the transformed signal values to the specified accuracy.\n    The title of the plots is 'Original Signal' and 'Transformed Signal'.\n    The function returns:\n        ndarray: A numpy array of transformed signal values (rounded to the specified precision).\n        tuple: A tuple containing the Axes objects for the original signal and transformed signal plots.\n    \"\"\"\n    np.random.seed(seed)\n    # Calculate the FFT\n    transformed_signal = fft(signal)\n    # Round the transformed signal values to the specified precision\n    transformed_signal = np.round(np.abs(transformed_signal), precision)\n\n    # Create the figure and the subplots\n    fig, axs = plt.subplots(2, 1)\n\n    # Plot the original signal\n    axs[0].plot(signal)\n    axs[0].set_title('Original Signal')\n\n    # Plot the transformed signal\n    axs[1].plot(transformed_signal)\n    axs[1].set_title('Transformed Signal')\n\n    # Return the transformed signal and the plot axes\n    return transformed_signal, axs", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\n\ndef task_func(signal, precision=2, seed=777):\n    \"\"\"\n    Calculates the one-dimensional discrete N-point Fourier Transform (DFT) for a real or complex sequence (signal) using the Fast Fourier Transform (FFT) algorithm.\n    Plots the original signal and the transformed signal, rounding the transformed signal values to the specified accuracy.\n    The title of the plots is 'Original Signal' and 'Transformed Signal'.\n    The function returns:\n        ndarray: A numpy array of transformed signal values (rounded to the specified precision).\n        tuple: A tuple containing the Axes objects for the original signal and transformed signal plots.\n    \"\"\"\n    np.random.seed(seed)\n    # Calculate the FFT\n    transformed_signal = fft(signal)\n    # Round the transformed signal values to the specified precision\n    transformed_signal = np.round(np.abs(transformed_signal), precision)\n\n    # Create the figure and the subplots\n    fig, axs = plt.subplots(2, 1)\n\n    # Plot the original signal\n    axs[0].plot(signal)\n    axs[0].set_title('Original Signal')\n\n    # Plot the transformed signal\n    axs[1].plot(transformed_signal)\n    axs[1].set_title('Transformed Signal')\n\n    # Return the transformed signal and the plot axes\n    return transformed_signal, axs"}
{"task_id": "BigCodeBench/281", "solution": "import re\nimport os\nfrom collections import Counter\ndef task_func(folder_path: str) -> dict:\n    \"\"\"\n    Scans a directory for log files and counts the occurrences of each IP address in all files.\n    \"\"\"\n    ip_address_pattern = r'\\b(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\b'\n    ip_addresses = []\n\n    for filename in os.listdir(folder_path):\n        if filename.endswith(\".log\"):\n            with open(os.path.join(folder_path, filename), 'r') as file:\n                for line in file:\n                    ip_addresses.extend(re.findall(ip_address_pattern, line))\n\n    return dict(Counter(ip_addresses))", "raw_solution": "import re\nimport os\nfrom collections import Counter\n\ndef task_func(folder_path: str) -> dict:\n    \"\"\"\n    Scans a directory for log files and counts the occurrences of each IP address in all files.\n    \"\"\"\n    ip_address_pattern = r'\\b(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\b'\n    ip_addresses = []\n\n    for filename in os.listdir(folder_path):\n        if filename.endswith(\".log\"):\n            with open(os.path.join(folder_path, filename), 'r') as file:\n                for line in file:\n                    ip_addresses.extend(re.findall(ip_address_pattern, line))\n\n    return dict(Counter(ip_addresses))"}
{"task_id": "BigCodeBench/282", "solution": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport cv2\nimport os\ndef task_func(file_path, onpick):\n    \"\"\"\n    Draws the color histogram of an image in 3D and calls a function when a data point is selected.\n    The function raises FileNotFoundError if the image file does not exist.\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Load the image\n    img = cv2.imread(file_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Calculate the color histogram\n    hist = cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n\n    # Create a 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Create a list of x, y, z coordinates from the histogram\n    x, y, z = [], [], []\n    for h, s, v in zip(*np.where(hist > 0)):\n        x.append(h)\n        y.append(s)\n        z.append(v)\n\n    # Plot the data points\n    sc = ax.scatter(x, y, z, c='r', marker='o', picker=True)\n\n    # Connect the onpick event to the provided function\n    fig.canvas.mpl_connect('pick_event', onpick)\n\n    return ax\ndef onpick(event):\n    \"\"\"\n    This function is called when a data point is selected in the 3D plot.\n    \"\"\"\n    print(f\"You picked point {event.mouseevent.xdata}, {event.mouseevent.ydata}, {event.mouseevent.zdata}\")", "raw_solution": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport cv2\nimport os\n\ndef task_func(file_path, onpick):\n    \"\"\"\n    Draws the color histogram of an image in 3D and calls a function when a data point is selected.\n    The function raises FileNotFoundError if the image file does not exist.\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Load the image\n    img = cv2.imread(file_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Calculate the color histogram\n    hist = cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n\n    # Create a 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Create a list of x, y, z coordinates from the histogram\n    x, y, z = [], [], []\n    for h, s, v in zip(*np.where(hist > 0)):\n        x.append(h)\n        y.append(s)\n        z.append(v)\n\n    # Plot the data points\n    sc = ax.scatter(x, y, z, c='r', marker='o', picker=True)\n\n    # Connect the onpick event to the provided function\n    fig.canvas.mpl_connect('pick_event', onpick)\n\n    return ax\n\ndef onpick(event):\n    \"\"\"\n    This function is called when a data point is selected in the 3D plot.\n    \"\"\"\n    print(f\"You picked point {event.mouseevent.xdata}, {event.mouseevent.ydata}, {event.mouseevent.zdata}\")\n\n# Test the function\ntry:\n    ax = task_func(\"test.jpg\", onpick)\n    plt.show()\nexcept FileNotFoundError as e:\n    print(e)"}
{"task_id": "BigCodeBench/283", "solution": "import os\nimport json\nfrom collections import Counter\ndef task_func(json_files_path='./json_files/', key='name'):\n    key_counts = Counter()\n    for filename in os.listdir(json_files_path):\n        if filename.endswith('.json'):\n            with open(os.path.join(json_files_path, filename), 'r') as f:\n                data = json.load(f)\n                if key in data:\n                    key_counts[data[key]] += 1\n    return dict(key_counts)", "raw_solution": "import os\nimport json\nfrom collections import Counter\n\ndef task_func(json_files_path='./json_files/', key='name'):\n    key_counts = Counter()\n    for filename in os.listdir(json_files_path):\n        if filename.endswith('.json'):\n            with open(os.path.join(json_files_path, filename), 'r') as f:\n                data = json.load(f)\n                if key in data:\n                    key_counts[data[key]] += 1\n    return dict(key_counts)\n\n# Test the function"}
{"task_id": "BigCodeBench/284", "solution": "import mechanize\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\ndef task_func(url):\n    \"\"\"\n    Extracts all hyperlinks (href attributes) from the specified URL using the mechanize browser object and BeautifulSoup.\n    Absolute URLs are combined with the base URL.\n    \"\"\"\n    browser = mechanize.Browser()\n    browser.set_handle_robots(False)\n    browser.open(url)\n    html = browser.response().read()\n    soup = BeautifulSoup(html, 'html.parser')\n\n    links = []\n    for link in soup.find_all('a'):\n        href = link.get('href')\n        if href and not href.startswith('javascript:') and not href.startswith('mailto:'):\n            href = urljoin(url, href)\n            links.append(href)\n\n    return links\nurl = \"https://www.example.com\"", "raw_solution": "import mechanize\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef task_func(url):\n    \"\"\"\n    Extracts all hyperlinks (href attributes) from the specified URL using the mechanize browser object and BeautifulSoup.\n    Absolute URLs are combined with the base URL.\n    \"\"\"\n    browser = mechanize.Browser()\n    browser.set_handle_robots(False)\n    browser.open(url)\n    html = browser.response().read()\n    soup = BeautifulSoup(html, 'html.parser')\n\n    links = []\n    for link in soup.find_all('a'):\n        href = link.get('href')\n        if href and not href.startswith('javascript:') and not href.startswith('mailto:'):\n            href = urljoin(url, href)\n            links.append(href)\n\n    return links\n\n# Test the function\nurl = \"https://www.example.com\""}
{"task_id": "BigCodeBench/285", "solution": "import mechanize\nfrom bs4 import BeautifulSoup\ndef task_func(url, form_id, data):\n    \"\"\"\n    Submits a form on a given webpage using mechanize and extracts the title of the response page.\n    If the page has no title, it returns 'No Title'.\n    \"\"\"\n    browser = mechanize.Browser()\n    browser.set_handle_robots(False)\n    browser.open(url)\n\n    form = browser.forms()[form_id]\n    for key, value in data.items():\n        form[key] = value\n    response = browser.submit(form)\n\n    soup = BeautifulSoup(response.read(), 'html.parser')\n    title = soup.title.string if soup.title else 'No Title'\n\n    return title", "raw_solution": "import mechanize\nfrom bs4 import BeautifulSoup\n\ndef task_func(url, form_id, data):\n    \"\"\"\n    Submits a form on a given webpage using mechanize and extracts the title of the response page.\n    If the page has no title, it returns 'No Title'.\n    \"\"\"\n    browser = mechanize.Browser()\n    browser.set_handle_robots(False)\n    browser.open(url)\n\n    form = browser.forms()[form_id]\n    for key, value in data.items():\n        form[key] = value\n    response = browser.submit(form)\n\n    soup = BeautifulSoup(response.read(), 'html.parser')\n    title = soup.title.string if soup.title else 'No Title'\n\n    return title"}
{"task_id": "BigCodeBench/286", "solution": "from collections import Counter\nimport os\nimport csv\ndef task_func(output_file, test_directory):\n    \"\"\"\n    Count the number of words in multiple dictionary files (.txt) in a specific directory,\n    export the counts to a CSV file, and then return the total number of words.\n    \"\"\"\n    try:\n        # Initialize a counter\n        word_counter = Counter()\n\n        # Get a list of all .txt files in the directory\n        txt_files = [f for f in os.listdir(test_directory) if f.endswith('.txt')]\n\n        # Iterate over each .txt file\n        for txt_file in txt_files:\n            # Open the file and read its content\n            with open(os.path.join(test_directory, txt_file), 'r') as f:\n                content = f.read().lower()\n\n            # Split the content into words and update the counter\n            words = content.split()\n            word_counter.update(words)\n\n        # Export the counts to a CSV file\n        with open(output_file, 'w', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerow(['Word', 'Count'])\n            for word, count in word_counter.items():\n                writer.writerow([word, count])\n\n        # Return the total number of words\n        return sum(word_counter.values())\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0", "raw_solution": "from collections import Counter\nimport os\nimport csv\n\n# Constants\nFILE_DIR = './yourdictfiles/'\n\ndef task_func(output_file, test_directory):\n    \"\"\"\n    Count the number of words in multiple dictionary files (.txt) in a specific directory,\n    export the counts to a CSV file, and then return the total number of words.\n    \"\"\"\n    try:\n        # Initialize a counter\n        word_counter = Counter()\n\n        # Get a list of all .txt files in the directory\n        txt_files = [f for f in os.listdir(test_directory) if f.endswith('.txt')]\n\n        # Iterate over each .txt file\n        for txt_file in txt_files:\n            # Open the file and read its content\n            with open(os.path.join(test_directory, txt_file), 'r') as f:\n                content = f.read().lower()\n\n            # Split the content into words and update the counter\n            words = content.split()\n            word_counter.update(words)\n\n        # Export the counts to a CSV file\n        with open(output_file, 'w', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerow(['Word', 'Count'])\n            for word, count in word_counter.items():\n                writer.writerow([word, count])\n\n        # Return the total number of words\n        return sum(word_counter.values())\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(directory):\n    \"\"\"\n    Count the number of words in .txt files within a specified directory, export the counts to a JSON file, and then return the total number of words.\n    \"\"\"\n    word_counts = Counter()\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                words = file.read().split()\n                word_counts.update(words)\n\n    # Export the counts to a JSON file\n    with open('word_counts.json', 'w') as file:\n        json.dump(word_counts, file)\n\n    # Return the total number of words\n    return sum(word_counts.values())", "raw_solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(directory):\n    \"\"\"\n    Count the number of words in .txt files within a specified directory, export the counts to a JSON file, and then return the total number of words.\n    \"\"\"\n    word_counts = Counter()\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                words = file.read().split()\n                word_counts.update(words)\n\n    # Export the counts to a JSON file\n    with open('word_counts.json', 'w') as file:\n        json.dump(word_counts, file)\n\n    # Return the total number of words\n    return sum(word_counts.values())"}
{"task_id": "BigCodeBench/288", "solution": "import collections\nimport json\nimport os\nfrom typing import Dict\ndef task_func(directory_path: str) -> Dict[str, int]:\n    \"\"\"\n    Count the total appearances of all keys in all JSON files in the specified directory and return a dictionary with the keys from the JSON files as keys and their respective counts as values.\n    \"\"\"\n    # Initialize an empty dictionary to store the counts\n    counts = collections.defaultdict(int)\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory_path):\n        # Check if the file is a JSON file\n        if filename.endswith('.json'):\n            # Open the file and load the JSON data\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                data = json.load(file)\n\n            # Count the keys in the JSON data\n            counts.update(data)\n\n    # Return the counts dictionary\n    return counts", "raw_solution": "import collections\nimport json\nimport os\nfrom typing import Dict\n\ndef task_func(directory_path: str) -> Dict[str, int]:\n    \"\"\"\n    Count the total appearances of all keys in all JSON files in the specified directory and return a dictionary with the keys from the JSON files as keys and their respective counts as values.\n    \"\"\"\n    # Initialize an empty dictionary to store the counts\n    counts = collections.defaultdict(int)\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory_path):\n        # Check if the file is a JSON file\n        if filename.endswith('.json'):\n            # Open the file and load the JSON data\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                data = json.load(file)\n\n            # Count the keys in the JSON data\n            counts.update(data)\n\n    # Return the counts dictionary\n    return counts"}
{"task_id": "BigCodeBench/289", "solution": "import tensorflow as tf\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\ndef task_func(X, y, n_splits, batch_size, epochs):\n    # Preprocessing\n    scaler = MinMaxScaler()\n    X = scaler.fit_transform(X)\n    y = np.array(y)\n\n    # K-Fold Cross Validation\n    kf = KFold(n_splits=n_splits)\n    histories = []\n\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        # Model\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Dense(20, activation='relu', input_shape=(X_train.shape[1],)),\n            tf.keras.layers.Dense(1, activation='sigmoid')\n        ])\n\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n        # Training\n        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n        histories.append(history)\n\n    return histories", "raw_solution": "import tensorflow as tf\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\ndef task_func(X, y, n_splits, batch_size, epochs):\n    # Preprocessing\n    scaler = MinMaxScaler()\n    X = scaler.fit_transform(X)\n    y = np.array(y)\n\n    # K-Fold Cross Validation\n    kf = KFold(n_splits=n_splits)\n    histories = []\n\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        # Model\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Dense(20, activation='relu', input_shape=(X_train.shape[1],)),\n            tf.keras.layers.Dense(1, activation='sigmoid')\n        ])\n\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n        # Training\n        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n        histories.append(history)\n\n    return histories"}
{"task_id": "BigCodeBench/290", "solution": "import nltk\nfrom collections import Counter\nimport os\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(directory_path):\n    \"\"\"\n    Count the number of unique non-stop words across all '.txt' files in a specified directory.\n    \"\"\"\n    # Initialize a counter to store word counts\n    word_counter = Counter()\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory_path):\n        # Check if the file is a text file\n        if filename.endswith('.txt'):\n            # Read the file\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                # Tokenize the text into words\n                words = nltk.word_tokenize(file.read())\n                # Count the words\n                word_counter.update(words)\n\n    # Remove stopwords from the counter\n    word_counter = {word: count for word, count in word_counter.items() if word not in STOPWORDS}\n\n    # Return the total count of unique non-stop words\n    return sum(word_counter.values())", "raw_solution": "import nltk\nnltk.download('stopwords')\nfrom collections import Counter\nimport os\nfrom nltk.corpus import stopwords\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(directory_path):\n    \"\"\"\n    Count the number of unique non-stop words across all '.txt' files in a specified directory.\n    \"\"\"\n    # Initialize a counter to store word counts\n    word_counter = Counter()\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory_path):\n        # Check if the file is a text file\n        if filename.endswith('.txt'):\n            # Read the file\n            with open(os.path.join(directory_path, filename), 'r') as file:\n                # Tokenize the text into words\n                words = nltk.word_tokenize(file.read())\n                # Count the words\n                word_counter.update(words)\n\n    # Remove stopwords from the counter\n    word_counter = {word: count for word, count in word_counter.items() if word not in STOPWORDS}\n\n    # Return the total count of unique non-stop words\n    return sum(word_counter.values())"}
{"task_id": "BigCodeBench/291", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\ndef task_func(mu, sigma, seed=0):\n    \"\"\"\n    Draws a normal distribution using a 1000 samples, indicating the mean and standard deviation with a color bar.\n    The function returns the Axes object of the plotted distribution.\n    \"\"\"\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, 1000)\n\n    fig, ax = plt.subplots()\n    sns.histplot(data, bins=30, kde=False, ax=ax)\n    ax.axvline(x=mu, color='r', label=f'Mean = {mu}')\n    ax.axvline(x=mu + sigma, color='g', linestyle='--', label=f'Std Dev = {sigma}')\n    ax.axvline(x=mu - sigma, color='g', linestyle='--')\n    ax.legend()\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ndef task_func(mu, sigma, seed=0):\n    \"\"\"\n    Draws a normal distribution using a 1000 samples, indicating the mean and standard deviation with a color bar.\n    The function returns the Axes object of the plotted distribution.\n    \"\"\"\n    np.random.seed(seed)\n    data = np.random.normal(mu, sigma, 1000)\n\n    fig, ax = plt.subplots()\n    sns.histplot(data, bins=30, kde=False, ax=ax)\n    ax.axvline(x=mu, color='r', label=f'Mean = {mu}')\n    ax.axvline(x=mu + sigma, color='g', linestyle='--', label=f'Std Dev = {sigma}')\n    ax.axvline(x=mu - sigma, color='g', linestyle='--')\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/292", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Scale 'Age' and 'Income' columns between 0 and 1 for each group by 'id'\n    scaler = MinMaxScaler()\n    df[['Age', 'Income']] = scaler.fit_transform(df[['Age', 'Income']])\n\n    # Create a histogram of the 'Income' column after scaling\n    hist_data = df['Income'].hist(bins=10)\n\n    return df, hist_data", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    # Scale 'Age' and 'Income' columns between 0 and 1 for each group by 'id'\n    scaler = MinMaxScaler()\n    df[['Age', 'Income']] = scaler.fit_transform(df[['Age', 'Income']])\n\n    # Create a histogram of the 'Income' column after scaling\n    hist_data = df['Income'].hist(bins=10)\n\n    return df, hist_data"}
{"task_id": "BigCodeBench/293", "solution": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(elements, subset_size):\n    # Generate all subsets of a given size from a tuple\n    subsets = list(itertools.combinations(elements, subset_size))\n    \n    # Calculate the sums of the subsets\n    sums = [sum(subset) for subset in subsets]\n    \n    # Draw a histogram of the sums of the subsets\n    fig, ax = plt.subplots()\n    ax.hist(sums, bins='auto')\n    ax.set_title('Histogram of the sums of subsets')\n    ax.set_xlabel('Sum')\n    ax.set_ylabel('Frequency')\n    \n    return ax, subsets, sums\nelements = (1, 2, 3, 4, 5)\nsubset_size = 3", "raw_solution": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(elements, subset_size):\n    # Generate all subsets of a given size from a tuple\n    subsets = list(itertools.combinations(elements, subset_size))\n    \n    # Calculate the sums of the subsets\n    sums = [sum(subset) for subset in subsets]\n    \n    # Draw a histogram of the sums of the subsets\n    fig, ax = plt.subplots()\n    ax.hist(sums, bins='auto')\n    ax.set_title('Histogram of the sums of subsets')\n    ax.set_xlabel('Sum')\n    ax.set_ylabel('Frequency')\n    \n    return ax, subsets, sums\n\n# Test the function\nelements = (1, 2, 3, 4, 5)\nsubset_size = 3\nax, subsets, sums = task_func(elements, subset_size)"}
{"task_id": "BigCodeBench/294", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    \"\"\"\n    Standardize the 'age' and 'income' columns for each group by 'id' in a Pandas DataFrame, and return the standardized DataFrame.\n    \"\"\"\n    # Check if the DataFrame has the required columns\n    if not all(col in df.columns for col in ['id', 'age', 'income']):\n        raise ValueError(\"The DataFrame must have 'id', 'age', and 'income' columns.\")\n\n    # Group by 'id' and standardize 'age' and 'income'\n    df_grouped = df.groupby('id')[['age', 'income']].apply(lambda x: (x - x.mean()) / x.std())\n\n    return df_grouped", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    \"\"\"\n    Standardize the 'age' and 'income' columns for each group by 'id' in a Pandas DataFrame, and return the standardized DataFrame.\n    \"\"\"\n    # Check if the DataFrame has the required columns\n    if not all(col in df.columns for col in ['id', 'age', 'income']):\n        raise ValueError(\"The DataFrame must have 'id', 'age', and 'income' columns.\")\n\n    # Group by 'id' and standardize 'age' and 'income'\n    df_grouped = df.groupby('id')[['age', 'income']].apply(lambda x: (x - x.mean()) / x.std())\n\n    return df_grouped"}
{"task_id": "BigCodeBench/295", "solution": "import itertools\nimport statistics\nfrom collections import Counter\nfrom typing import Tuple, Dict\ndef task_func(elements: Tuple[int, ...], subset_size: int) -> Dict[str, float]:\n    \"\"\"\n    Generate all subsets of a given size from a tuple and calculate the mean, median, and mode of the sums of the subsets.\n    \"\"\"\n    subsets = list(itertools.combinations(elements, subset_size))\n    sums = [sum(subset) for subset in subsets]\n\n    mean = statistics.mean(sums)\n    median = statistics.median(sums)\n    mode = statistics.mode(sums)\n\n    return {\"mean\": mean, \"median\": median, \"mode\": mode}", "raw_solution": "import itertools\nimport statistics\nfrom collections import Counter\nfrom typing import Tuple, Dict\n\ndef task_func(elements: Tuple[int, ...], subset_size: int) -> Dict[str, float]:\n    \"\"\"\n    Generate all subsets of a given size from a tuple and calculate the mean, median, and mode of the sums of the subsets.\n    \"\"\"\n    subsets = list(itertools.combinations(elements, subset_size))\n    sums = [sum(subset) for subset in subsets]\n\n    mean = statistics.mean(sums)\n    median = statistics.median(sums)\n    mode = statistics.mode(sums)\n\n    return {\"mean\": mean, \"median\": median, \"mode\": mode}\n\n# Testing the function"}
{"task_id": "BigCodeBench/296", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    if 'value' not in df.columns:\n        return plt.gca()  # return empty Axes object if 'value' column does not exist\n\n    counts = df['value'].value_counts()\n\n    ax = counts.plot(kind='bar', title='Value Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Count')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n\n    if 'value' not in df.columns:\n        return plt.gca()  # return empty Axes object if 'value' column does not exist\n\n    counts = df['value'].value_counts()\n\n    ax = counts.plot(kind='bar', title='Value Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Count')\n\n    return ax"}
{"task_id": "BigCodeBench/297", "solution": "import itertools\nimport collections\ndef task_func(elements, subset_size):\n    subsets = list(itertools.combinations(elements, subset_size))\n    sums_dict = collections.Counter(sum(subset) for subset in subsets)\n    return sums_dict\nelements = (1, 2, 3, 4)\nsubset_size = 2", "raw_solution": "import itertools\nimport collections\n\ndef task_func(elements, subset_size):\n    subsets = list(itertools.combinations(elements, subset_size))\n    sums_dict = collections.Counter(sum(subset) for subset in subsets)\n    return sums_dict\n\n# Test the function\nelements = (1, 2, 3, 4)\nsubset_size = 2"}
{"task_id": "BigCodeBench/298", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    \"\"\"\n    Splits a list in the 'Value' column of a DataFrame into several columns, scales these columns using StandardScaler,\n    and optionally returns the scaled data using a bar chart. The 'Date' column is converted to datetime and used as the index\n    in the plot.\n    \"\"\"\n    if not all(col in df.columns for col in COLUMNS):\n        raise KeyError(f\"DataFrame must contain columns: {COLUMNS}\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    scaler = StandardScaler()\n    scaled_values = scaler.fit_transform(df['Value'].str.split(',').values.tolist())\n\n    scaled_df = pd.DataFrame(scaled_values, columns=[f'Scaled Value {i+1}' for i in range(scaled_values.shape[1])], index=df.index)\n\n    if plot:\n        ax = scaled_df.plot(kind='bar', figsize=(12, 6))\n        ax.set_title('Scaled Values Over Time')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Scaled Value')\n        plt.show()\n\n    return scaled_df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Splits a list in the 'Value' column of a DataFrame into several columns, scales these columns using StandardScaler,\n    and optionally returns the scaled data using a bar chart. The 'Date' column is converted to datetime and used as the index\n    in the plot.\n    \"\"\"\n    if not all(col in df.columns for col in COLUMNS):\n        raise KeyError(f\"DataFrame must contain columns: {COLUMNS}\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    scaler = StandardScaler()\n    scaled_values = scaler.fit_transform(df['Value'].str.split(',').values.tolist())\n\n    scaled_df = pd.DataFrame(scaled_values, columns=[f'Scaled Value {i+1}' for i in range(scaled_values.shape[1])], index=df.index)\n\n    if plot:\n        ax = scaled_df.plot(kind='bar', figsize=(12, 6))\n        ax.set_title('Scaled Values Over Time')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Scaled Value')\n        plt.show()\n\n    return scaled_df"}
{"task_id": "BigCodeBench/299", "solution": "import itertools\nimport math\nfrom pandas import Series\ndef task_func(elements, subset_size, top_n=2):\n    if subset_size > len(elements) or subset_size == 0:\n        return 1, [1]*top_n\n\n    subsets = list(itertools.combinations(elements, subset_size))\n    sums = [sum(subset) for subset in subsets]\n    sums.sort(reverse=True)\n    top_n_sums = sums[:top_n]\n    product_of_sums = math.prod(sums)\n\n    return product_of_sums, top_n_sums", "raw_solution": "import itertools\nimport math\nfrom pandas import Series\n\ndef task_func(elements, subset_size, top_n=2):\n    if subset_size > len(elements) or subset_size == 0:\n        return 1, [1]*top_n\n\n    subsets = list(itertools.combinations(elements, subset_size))\n    sums = [sum(subset) for subset in subsets]\n    sums.sort(reverse=True)\n    top_n_sums = sums[:top_n]\n    product_of_sums = math.prod(sums)\n\n    return product_of_sums, top_n_sums"}
{"task_id": "BigCodeBench/300", "solution": "import pandas as pd\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    \"\"\"\n    Processes a pandas DataFrame with 'Date' and 'Value' columns. The 'Value' column contains lists of numbers.\n    Converts 'Date' to datetime, splits 'Value' lists into separate columns, calculates Z-scores, and creates a box plot for Z-scores over time.\n    \"\"\"\n    if 'Date' not in df.columns or 'Value' not in df.columns:\n        raise KeyError(\"DataFrame does not have the 'Date' and 'Value' columns.\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = df.set_index('Date')\n\n    # Split 'Value' lists into separate columns\n    df = df['Value'].apply(pd.Series)\n\n    # Calculate Z-scores\n    df = df.apply(zscore)\n\n    # Create a box plot for Z-scores over time\n    fig, ax = plt.subplots(figsize=(10, 6))\n    df.boxplot(ax=ax)\n    ax.set_title('Z-Scores Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Z-Score')\n\n    return df, fig", "raw_solution": "import pandas as pd\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    Processes a pandas DataFrame with 'Date' and 'Value' columns. The 'Value' column contains lists of numbers.\n    Converts 'Date' to datetime, splits 'Value' lists into separate columns, calculates Z-scores, and creates a box plot for Z-scores over time.\n    \"\"\"\n    if 'Date' not in df.columns or 'Value' not in df.columns:\n        raise KeyError(\"DataFrame does not have the 'Date' and 'Value' columns.\")\n\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = df.set_index('Date')\n\n    # Split 'Value' lists into separate columns\n    df = df['Value'].apply(pd.Series)\n\n    # Calculate Z-scores\n    df = df.apply(zscore)\n\n    # Create a box plot for Z-scores over time\n    fig, ax = plt.subplots(figsize=(10, 6))\n    df.boxplot(ax=ax)\n    ax.set_title('Z-Scores Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Z-Score')\n\n    return df, fig"}
{"task_id": "BigCodeBench/301", "solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Calculates solar activity based on the date and time, taking into account the solar cycle of 11 years.\n    The function outputs a float between 0 and 1, representing the solar activity calculated using a cosine\n    function based on the years since the closest solar cycle year.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Convert the date to the desired timezone\n    date = date.astimezone(pytz.timezone(to_tz))\n\n    # Calculate the years since the closest solar cycle year\n    years_since_solar_cycle = date.year - SOLAR_CYCLE_YEARS.min()\n\n    # Calculate the solar activity using a cosine function\n    solar_activity = (math.cos(2 * math.pi * years_since_solar_cycle / 11) + 1) / 2\n\n    return solar_activity", "raw_solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\n\nSOLAR_CYCLE_YEARS = np.array([1986, 1996, 2008, 2019])\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Calculates solar activity based on the date and time, taking into account the solar cycle of 11 years.\n    The function outputs a float between 0 and 1, representing the solar activity calculated using a cosine\n    function based on the years since the closest solar cycle year.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Convert the date to the desired timezone\n    date = date.astimezone(pytz.timezone(to_tz))\n\n    # Calculate the years since the closest solar cycle year\n    years_since_solar_cycle = date.year - SOLAR_CYCLE_YEARS.min()\n\n    # Calculate the solar activity using a cosine function\n    solar_activity = (math.cos(2 * math.pi * years_since_solar_cycle / 11) + 1) / 2\n\n    return solar_activity\n\n# Test the function"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns, calculates the Pearson correlation\n    coefficient between these columns, and optionally visualizes the correlation matrix using a heatmap.\n    \"\"\"\n    if df.empty or 'Value' not in df.columns:\n        raise ValueError(\"Invalid DataFrame input. It should not be empty and contain a 'Value' column.\")\n\n    # Split lists in the 'Value' column into separate columns\n    df_split = pd.DataFrame(df['Value'].to_list())\n    df_split.columns = [f'Value_{i}' for i in range(1, len(df_split.columns) + 1)]\n    df = pd.concat([df.drop('Value', axis=1), df_split], axis=1)\n\n    # Calculate Pearson correlation coefficients\n    corr_matrix = df.corr(method='pearson')\n\n    if plot:\n        # Visualize the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n\n    return corr_matrix", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns, calculates the Pearson correlation\n    coefficient between these columns, and optionally visualizes the correlation matrix using a heatmap.\n    \"\"\"\n    if df.empty or 'Value' not in df.columns:\n        raise ValueError(\"Invalid DataFrame input. It should not be empty and contain a 'Value' column.\")\n\n    # Split lists in the 'Value' column into separate columns\n    df_split = pd.DataFrame(df['Value'].to_list())\n    df_split.columns = [f'Value_{i}' for i in range(1, len(df_split.columns) + 1)]\n    df = pd.concat([df.drop('Value', axis=1), df_split], axis=1)\n\n    # Calculate Pearson correlation coefficients\n    corr_matrix = df.corr(method='pearson')\n\n    if plot:\n        # Visualize the correlation matrix using a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n\n    return corr_matrix"}
{"task_id": "BigCodeBench/303", "solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\nMOON_PHASES_YEARS = np.array([1987, 1994, 2001, 2008, 2015, 2022])\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Calculates the moon phase based on the date and time, taking into account the lunar phase cycle of 7 years.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Convert the date to the target timezone\n    date = date.astimezone(pytz.timezone(to_tz))\n\n    # Calculate the number of days since the first year\n    days_since_first_year = (date - parse(f'{MOON_PHASES_YEARS[0]}-01-01 00:00:00')).days\n\n    # Calculate the lunar phase\n    lunar_phase = (days_since_first_year % 29.53058867) / 29.53058867\n\n    return lunar_phase", "raw_solution": "import pytz\nimport numpy as np\nfrom dateutil.parser import parse\nimport math\n\nMOON_PHASES_YEARS = np.array([1987, 1994, 2001, 2008, 2015, 2022])\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Calculates the moon phase based on the date and time, taking into account the lunar phase cycle of 7 years.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Convert the date to the target timezone\n    date = date.astimezone(pytz.timezone(to_tz))\n\n    # Calculate the number of days since the first year\n    days_since_first_year = (date - parse(f'{MOON_PHASES_YEARS[0]}-01-01 00:00:00')).days\n\n    # Calculate the lunar phase\n    lunar_phase = (days_since_first_year % 29.53058867) / 29.53058867\n\n    return lunar_phase\n\n# Test the function"}
{"task_id": "BigCodeBench/304", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    if df.empty:\n        return (0.0, 0.0)\n\n    # Convert lists to separate columns\n    df = pd.concat([df.drop(['numbers'], axis=1), pd.DataFrame(df['numbers'].tolist())], axis=1)\n\n    # Perform PCA\n    pca = PCA()\n    df_pca = pca.fit_transform(df)\n\n    # Get explained variance ratio\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n    ax.set_title('Explained Variance Ratio of Principal Components')\n    ax.set_xlabel('Principal Component')\n    ax.set_ylabel('Explained Variance Ratio')\n\n    return (explained_variance_ratio, ax)", "raw_solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if df.empty:\n        return (0.0, 0.0)\n\n    # Convert lists to separate columns\n    df = pd.concat([df.drop(['numbers'], axis=1), pd.DataFrame(df['numbers'].tolist())], axis=1)\n\n    # Perform PCA\n    pca = PCA()\n    df_pca = pca.fit_transform(df)\n\n    # Get explained variance ratio\n    explained_variance_ratio = pca.explained_variance_ratio_\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n    ax.set_title('Explained Variance Ratio of Principal Components')\n    ax.set_xlabel('Principal Component')\n    ax.set_ylabel('Explained Variance Ratio')\n\n    return (explained_variance_ratio, ax)"}
{"task_id": "BigCodeBench/305", "solution": "from collections import Counter\nimport itertools\nimport random\nALPHABET = 'abcdefghijklmnopqrstuvwxyz'\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    for i, lst in enumerate(list_of_lists):\n        if not lst:\n            list_of_lists[i] = random.choices(ALPHABET, k=5)\n    flat_list = list(itertools.chain(*list_of_lists))\n    counter = Counter(flat_list)\n    return counter\nlist_of_lists = [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i'], ['j', 'k', 'l'], ['m', 'n', 'o']]", "raw_solution": "from collections import Counter\nimport itertools\nimport random\n\n# Constants\nALPHABET = 'abcdefghijklmnopqrstuvwxyz'\n\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    for i, lst in enumerate(list_of_lists):\n        if not lst:\n            list_of_lists[i] = random.choices(ALPHABET, k=5)\n    flat_list = list(itertools.chain(*list_of_lists))\n    counter = Counter(flat_list)\n    return counter\n\n# Testing\nlist_of_lists = [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i'], ['j', 'k', 'l'], ['m', 'n', 'o']]"}
{"task_id": "BigCodeBench/306", "solution": "import os\nimport logging\ndef task_func(directory):\n    \"\"\"\n    Removes all jQuery files from a specified directory.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    removed_files = []\n    removed_count = 0\n    for filename in os.listdir(directory):\n        if 'jquery' in filename.lower():\n            file_path = os.path.join(directory, filename)\n            os.remove(file_path)\n            removed_files.append(filename)\n            removed_count += 1\n\n    logging.basicConfig(filename='jquery_removal.log', level=logging.INFO)\n    logging.info(f\"Removed {removed_count} files: {removed_files}\")\n\n    return (removed_count, removed_files)", "raw_solution": "import os\nimport logging\n\ndef task_func(directory):\n    \"\"\"\n    Removes all jQuery files from a specified directory.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n\n    removed_files = []\n    removed_count = 0\n    for filename in os.listdir(directory):\n        if 'jquery' in filename.lower():\n            file_path = os.path.join(directory, filename)\n            os.remove(file_path)\n            removed_files.append(filename)\n            removed_count += 1\n\n    logging.basicConfig(filename='jquery_removal.log', level=logging.INFO)\n    logging.info(f\"Removed {removed_count} files: {removed_files}\")\n\n    return (removed_count, removed_files)"}
{"task_id": "BigCodeBench/307", "solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    combined_data = []\n    for sublist in list_of_lists:\n        if not sublist:\n            sublist = random.sample(range(0, 101), 5)\n        combined_data.extend(sublist)\n\n    plt.figure(figsize=(10, 6))\n    sns.histplot(combined_data, bins=20, kde=False)\n    plt.title('Histogram of Combined Data')\n    plt.xlabel('Data')\n    plt.ylabel('Frequency')\n    return plt.gca()", "raw_solution": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n\ndef task_func(list_of_lists, seed=0):\n    random.seed(seed)\n    combined_data = []\n    for sublist in list_of_lists:\n        if not sublist:\n            sublist = random.sample(range(0, 101), 5)\n        combined_data.extend(sublist)\n\n    plt.figure(figsize=(10, 6))\n    sns.histplot(combined_data, bins=20, kde=False)\n    plt.title('Histogram of Combined Data')\n    plt.xlabel('Data')\n    plt.ylabel('Frequency')\n    return plt.gca()"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func():\n    # Generate random grades for each student in each subject\n    random.seed(0)\n    data = {field: [random.randint(0, 100) for _ in range(100)] for field in FIELDS}\n    data['Student'] = STUDENTS\n\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data)\n    df.set_index('Student', inplace=True)\n\n    # Calculate the average grade for each student\n    df['Average'] = df.mean(axis=1)\n\n    # Calculate the average grade for each subject\n    subject_averages = df.mean()\n\n    # Append the subject averages to the DataFrame\n    df = df.append(pd.Series(subject_averages, name='Average Grade'))\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func():\n    # Generate random grades for each student in each subject\n    random.seed(0)\n    data = {field: [random.randint(0, 100) for _ in range(100)] for field in FIELDS}\n    data['Student'] = STUDENTS\n\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data)\n    df.set_index('Student', inplace=True)\n\n    # Calculate the average grade for each student\n    df['Average'] = df.mean(axis=1)\n\n    # Calculate the average grade for each subject\n    subject_averages = df.mean()\n\n    # Append the subject averages to the DataFrame\n    df = df.append(pd.Series(subject_averages, name='Average Grade'))\n\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/309", "solution": "import numpy as np\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(list_of_lists, seed=42):\n    \"\"\"\n    Scales the values in a list of lists to a (0,1) range using MinMaxScaler.\n    If any inner list is empty, the function fills it with five random integers between 0 and 100, and then scales the values.\n    \"\"\"\n    random.seed(seed)\n    for i in range(len(list_of_lists)):\n        if not list_of_lists[i]:\n            list_of_lists[i] = [random.randint(0, 100) for _ in range(5)]\n    scaler = MinMaxScaler()\n    list_of_lists = scaler.fit_transform(list_of_lists)\n    return list_of_lists.tolist()", "raw_solution": "import numpy as np\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(list_of_lists, seed=42):\n    \"\"\"\n    Scales the values in a list of lists to a (0,1) range using MinMaxScaler.\n    If any inner list is empty, the function fills it with five random integers between 0 and 100, and then scales the values.\n    \"\"\"\n    random.seed(seed)\n    for i in range(len(list_of_lists)):\n        if not list_of_lists[i]:\n            list_of_lists[i] = [random.randint(0, 100) for _ in range(5)]\n    scaler = MinMaxScaler()\n    list_of_lists = scaler.fit_transform(list_of_lists)\n    return list_of_lists.tolist()\n\n# Testing the function"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Generate data\n    data = []\n    for i in range(PEOPLE_COUNT):\n        name = f'Person{i+1}'\n        age = random.randint(18, 80)\n        height = round(random.gauss(170, 10), 2)\n        weight = round(random.gauss(70, 10), 2)\n        data.append([name, age, height, weight])\n\n    # Calculate averages\n    avg_age = round(mean(row[1] for row in data), 2)\n    avg_height = round(mean(row[2] for row in data), 2)\n    avg_weight = round(mean(row[3] for row in data), 2)\n    data.append([f'Average', avg_age, avg_height, avg_weight])\n\n    # Write to CSV\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    return filename", "raw_solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    # Generate data\n    data = []\n    for i in range(PEOPLE_COUNT):\n        name = f'Person{i+1}'\n        age = random.randint(18, 80)\n        height = round(random.gauss(170, 10), 2)\n        weight = round(random.gauss(70, 10), 2)\n        data.append([name, age, height, weight])\n\n    # Calculate averages\n    avg_age = round(mean(row[1] for row in data), 2)\n    avg_height = round(mean(row[2] for row in data), 2)\n    avg_weight = round(mean(row[3] for row in data), 2)\n    data.append([f'Average', avg_age, avg_height, avg_weight])\n\n    # Write to CSV\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    return filename"}
{"task_id": "BigCodeBench/311", "solution": "import numpy as np\nimport random\nfrom scipy import stats\nfrom typing import List, Dict, Union\ndef task_func(list_of_lists: List[List[Union[int, float]]], size: int = 5, seed: int = 0) -> Dict[str, float]:\n    \"\"\"\n    Calculate the mean, median, and mode of values in a list of lists.\n    If a list is empty, fill it with SIZE (default: 5) random integers between 0 and 100,\n    and then calculate the statistics.\n    \"\"\"\n    random.seed(seed)\n    result = {}\n\n    for i, lst in enumerate(list_of_lists):\n        if not lst:\n            list_of_lists[i] = random.sample(range(0, 100), size)\n\n    flattened_list = [item for sublist in list_of_lists for item in sublist]\n\n    result['mean'] = np.mean(flattened_list)\n    result['median'] = np.median(flattened_list)\n    result['mode'] = stats.mode(flattened_list)[0][0]\n\n    return result", "raw_solution": "import numpy as np\nimport random\nfrom scipy import stats\nfrom typing import List, Dict, Union\n\ndef task_func(list_of_lists: List[List[Union[int, float]]], size: int = 5, seed: int = 0) -> Dict[str, float]:\n    \"\"\"\n    Calculate the mean, median, and mode of values in a list of lists.\n    If a list is empty, fill it with SIZE (default: 5) random integers between 0 and 100,\n    and then calculate the statistics.\n    \"\"\"\n    random.seed(seed)\n    result = {}\n\n    for i, lst in enumerate(list_of_lists):\n        if not lst:\n            list_of_lists[i] = random.sample(range(0, 100), size)\n\n    flattened_list = [item for sublist in list_of_lists for item in sublist]\n\n    result['mean'] = np.mean(flattened_list)\n    result['median'] = np.median(flattened_list)\n    result['mode'] = stats.mode(flattened_list)[0][0]\n\n    return result"}
{"task_id": "BigCodeBench/312", "solution": "import random\nimport matplotlib.pyplot as plt\nimport numpy as np\nDISTRIBUTION_SIZE = 1000\ndef task_func(bins=30):\n    \"\"\"\n    Generates a Gaussian distribution and plots its histogram.\n    Returns a tuple containing the distribution list and the Axes patch object of the histogram plot.\n    \"\"\"\n    # Generate a Gaussian distribution\n    distribution = np.random.normal(0, 1, DISTRIBUTION_SIZE)\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the histogram\n    ax.hist(distribution, bins=bins)\n\n    # Set the title and labels\n    ax.set_title('Gaussian Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    # Return the distribution and the axes object\n    return distribution, ax", "raw_solution": "import random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nDISTRIBUTION_SIZE = 1000\n\ndef task_func(bins=30):\n    \"\"\"\n    Generates a Gaussian distribution and plots its histogram.\n    Returns a tuple containing the distribution list and the Axes patch object of the histogram plot.\n    \"\"\"\n    # Generate a Gaussian distribution\n    distribution = np.random.normal(0, 1, DISTRIBUTION_SIZE)\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the histogram\n    ax.hist(distribution, bins=bins)\n\n    # Set the title and labels\n    ax.set_title('Gaussian Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    # Return the distribution and the axes object\n    return distribution, ax\n\n# Test the function\ndistribution, ax = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom typing import Tuple, Dict\ndef task_func(directory: str) -> Tuple[str, Dict[str, list]]:\n    \"\"\"\n    Organizes files in a directory based on the first text that is not enclosed in square brackets.\n    Moves the files to subdirectories named after this text. If no matching text is found, the file is not moved.\n    \"\"\"\n    # Create a dictionary to store the files for each subdirectory\n    subdir_files = {}\n\n    # Iterate over each file in the directory\n    for filename in os.listdir(directory):\n        # Use a regular expression to find the first text that is not enclosed in square brackets\n        match = re.search(r'\\[([^\\]]*)\\]', filename)\n        if match:\n            # If a match is found, get the text and create a subdirectory if it doesn't exist\n            subdir = os.path.join(directory, match.group(1))\n            if not os.path.exists(subdir):\n                os.mkdir(subdir)\n            # Move the file to the subdirectory\n            shutil.move(os.path.join(directory, filename), subdir)\n            # Add the file to the list of files for the subdirectory\n            if subdir in subdir_files:\n                subdir_files[subdir].append(filename)\n            else:\n                subdir_files[subdir] = [filename]\n\n    # Return the directory path and the dictionary of subdirectories and files\n    return directory, subdir_files", "raw_solution": "import re\nimport os\nimport shutil\nfrom typing import Tuple, Dict\n\ndef task_func(directory: str) -> Tuple[str, Dict[str, list]]:\n    \"\"\"\n    Organizes files in a directory based on the first text that is not enclosed in square brackets.\n    Moves the files to subdirectories named after this text. If no matching text is found, the file is not moved.\n    \"\"\"\n    # Create a dictionary to store the files for each subdirectory\n    subdir_files = {}\n\n    # Iterate over each file in the directory\n    for filename in os.listdir(directory):\n        # Use a regular expression to find the first text that is not enclosed in square brackets\n        match = re.search(r'\\[([^\\]]*)\\]', filename)\n        if match:\n            # If a match is found, get the text and create a subdirectory if it doesn't exist\n            subdir = os.path.join(directory, match.group(1))\n            if not os.path.exists(subdir):\n                os.mkdir(subdir)\n            # Move the file to the subdirectory\n            shutil.move(os.path.join(directory, filename), subdir)\n            # Add the file to the list of files for the subdirectory\n            if subdir in subdir_files:\n                subdir_files[subdir].append(filename)\n            else:\n                subdir_files[subdir] = [filename]\n\n    # Return the directory path and the dictionary of subdirectories and files\n    return directory, subdir_files"}
{"task_id": "BigCodeBench/314", "solution": "import socket\nimport ssl\nimport http.client\ndef task_func(SERVER_NAME, SERVER_PORT, path):\n    try:\n        context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2)\n        conn = http.client.HTTPSConnection(SERVER_NAME, SERVER_PORT, context=context)\n        conn.request(\"GET\", path)\n        response = conn.getresponse()\n        response_body = response.read().decode()\n        conn.close()\n        return response_body\n    except ssl.SSLError as e:\n        raise Exception(f\"SSL Error: {str(e)}\")", "raw_solution": "import socket\nimport ssl\nimport http.client\n\ndef task_func(SERVER_NAME, SERVER_PORT, path):\n    try:\n        context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2)\n        conn = http.client.HTTPSConnection(SERVER_NAME, SERVER_PORT, context=context)\n        conn.request(\"GET\", path)\n        response = conn.getresponse()\n        response_body = response.read().decode()\n        conn.close()\n        return response_body\n    except ssl.SSLError as e:\n        raise Exception(f\"SSL Error: {str(e)}\")"}
{"task_id": "BigCodeBench/315", "solution": "import os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.exceptions import HTTPError\ndef task_func(dir, api_key, recipient_email):\n    try:\n        # Get a list of files in the directory\n        files = os.listdir(dir)\n\n        # Create the email content\n        message = Mail(\n            from_email='from_email@example.com',\n            to_emails=recipient_email,\n            subject='List of files',\n            plain_text_content='Files: ' + ', '.join(files))\n\n        # Send the email\n        sg = SendGridAPIClient(api_key)\n        response = sg.send(message)\n\n        # Return True if the email was sent successfully\n        return 200 <= response.status_code < 300\n\n    except FileNotFoundError:\n        # Return False if the directory does not exist\n        return False\n\n    except HTTPError as e:\n        # Raise the HTTPError if it occurs\n        raise e\n\n    except Exception as e:\n        # Raise the exception if it occurs\n        raise e", "raw_solution": "import os\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\nfrom python_http_client.exceptions import HTTPError\n\ndef task_func(dir, api_key, recipient_email):\n    try:\n        # Get a list of files in the directory\n        files = os.listdir(dir)\n\n        # Create the email content\n        message = Mail(\n            from_email='from_email@example.com',\n            to_emails=recipient_email,\n            subject='List of files',\n            plain_text_content='Files: ' + ', '.join(files))\n\n        # Send the email\n        sg = SendGridAPIClient(api_key)\n        response = sg.send(message)\n\n        # Return True if the email was sent successfully\n        return 200 <= response.status_code < 300\n\n    except FileNotFoundError:\n        # Return False if the directory does not exist\n        return False\n\n    except HTTPError as e:\n        # Raise the HTTPError if it occurs\n        raise e\n\n    except Exception as e:\n        # Raise the exception if it occurs\n        raise e"}
{"task_id": "BigCodeBench/316", "solution": "import pandas as pd\nimport random\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\ndef task_func(value_range=(0, 100)):\n    \"\"\"\n    Generate a category distribution within a specified range and return as a DataFrame.\n    \"\"\"\n    # Generate random counts for each category within the specified range\n    counts = [random.randint(value_range[0], value_range[1]) for _ in CATEGORIES]\n\n    # Create a DataFrame from the categories and counts\n    df = pd.DataFrame({\n        'Category': CATEGORIES,\n        'Count': counts\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\n# Constants\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(value_range=(0, 100)):\n    \"\"\"\n    Generate a category distribution within a specified range and return as a DataFrame.\n    \"\"\"\n    # Generate random counts for each category within the specified range\n    counts = [random.randint(value_range[0], value_range[1]) for _ in CATEGORIES]\n\n    # Create a DataFrame from the categories and counts\n    df = pd.DataFrame({\n        'Category': CATEGORIES,\n        'Count': counts\n    })\n\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/317", "solution": "import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\ndef task_func(example_str):\n    # Extract all texts not enclosed in square brackets\n    text = re.sub(r'\\[.*?\\]', '', example_str)\n\n    # Calculate TF-IDF values\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform([text])\n    feature_names = vectorizer.get_feature_names_out()\n    tfidf_scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n\n    # Create a dictionary with words as keys and TF-IDF scores as values\n    tfidf_dict = dict(zip(feature_names, tfidf_scores))\n\n    return tfidf_dict", "raw_solution": "import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\ndef task_func(example_str):\n    # Extract all texts not enclosed in square brackets\n    text = re.sub(r'\\[.*?\\]', '', example_str)\n\n    # Calculate TF-IDF values\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform([text])\n    feature_names = vectorizer.get_feature_names_out()\n    tfidf_scores = np.asarray(tfidf_matrix.sum(axis=0)).ravel()\n\n    # Create a dictionary with words as keys and TF-IDF scores as values\n    tfidf_dict = dict(zip(feature_names, tfidf_scores))\n\n    return tfidf_dict"}
{"task_id": "BigCodeBench/318", "solution": "import random\nimport math\nimport matplotlib.pyplot as plt\ndef task_func(points_count=1000, radius=1):\n    \"\"\"\n    Generates a specified number of random points within a circle of a given radius and plots them using a scatter plot.\n    \"\"\"\n    # Generate points_count random points within a circle of radius radius\n    points = [(radius * math.cos(theta), radius * math.sin(theta)) for theta in [random.uniform(0, 2 * math.pi) for _ in range(points_count)]]\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(*zip(*points))\n    ax.set_aspect('equal', 'box')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter plot of random points within a circle')\n\n    return ax", "raw_solution": "import random\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(points_count=1000, radius=1):\n    \"\"\"\n    Generates a specified number of random points within a circle of a given radius and plots them using a scatter plot.\n    \"\"\"\n    # Generate points_count random points within a circle of radius radius\n    points = [(radius * math.cos(theta), radius * math.sin(theta)) for theta in [random.uniform(0, 2 * math.pi) for _ in range(points_count)]]\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(*zip(*points))\n    ax.set_aspect('equal', 'box')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter plot of random points within a circle')\n\n    return ax\n\n# Test the function\nax = task_func(points_count=1000, radius=1)\nplt.show()"}
{"task_id": "BigCodeBench/319", "solution": "import re\nimport matplotlib.pyplot as plt\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\ndef task_func(example_str, top_n=30):\n    # Extract all texts that are not enclosed in square brackets\n    text = re.sub(r'\\[.*?\\]', '', example_str)\n\n    # Tokenize the text into words\n    words = word_tokenize(text)\n\n    # Create a frequency distribution\n    fdist = FreqDist(words)\n\n    # Get the top_n most common words\n    top_words = fdist.most_common(top_n)\n\n    # Convert the top_words list of tuples to a dictionary\n    top_words_dict = {word: freq for word, freq in top_words}\n\n    # Plot the frequency distribution\n    fdist.plot(top_n)\n\n    return plt.gca(), top_words_dict", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\n\ndef task_func(example_str, top_n=30):\n    # Extract all texts that are not enclosed in square brackets\n    text = re.sub(r'\\[.*?\\]', '', example_str)\n\n    # Tokenize the text into words\n    words = word_tokenize(text)\n\n    # Create a frequency distribution\n    fdist = FreqDist(words)\n\n    # Get the top_n most common words\n    top_words = fdist.most_common(top_n)\n\n    # Convert the top_words list of tuples to a dictionary\n    top_words_dict = {word: freq for word, freq in top_words}\n\n    # Plot the frequency distribution\n    fdist.plot(top_n)\n\n    return plt.gca(), top_words_dict"}
{"task_id": "BigCodeBench/320", "solution": "import subprocess\nimport os\nimport random\ndef task_func(directory, file_list):\n    \"\"\"\n    Selects a random file from the given list of files in the specified directory and runs it as a subprocess.\n    Returns the exit code of the subprocess, or None if the process is still running or if the file list is empty.\n    \"\"\"\n    if not file_list:\n        return None\n\n    # Select a random file from the list\n    file_path = os.path.join(directory, random.choice(file_list))\n\n    # Run the file as a subprocess\n    process = subprocess.Popen(['python', file_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Wait for the process to finish\n    stdout, stderr = process.communicate()\n\n    # Return the exit code of the subprocess\n    return process.returncode", "raw_solution": "import subprocess\nimport os\nimport random\n\ndef task_func(directory, file_list):\n    \"\"\"\n    Selects a random file from the given list of files in the specified directory and runs it as a subprocess.\n    Returns the exit code of the subprocess, or None if the process is still running or if the file list is empty.\n    \"\"\"\n    if not file_list:\n        return None\n\n    # Select a random file from the list\n    file_path = os.path.join(directory, random.choice(file_list))\n\n    # Run the file as a subprocess\n    process = subprocess.Popen(['python', file_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Wait for the process to finish\n    stdout, stderr = process.communicate()\n\n    # Return the exit code of the subprocess\n    return process.returncode"}
{"task_id": "BigCodeBench/321", "solution": "import pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(text):\n    # Extract names not surrounded by square brackets\n    names = re.findall(r'(?<=\\s|^)([A-Za-z]+)(?=\\s|$)', text)\n\n    # Count the frequency of each name\n    freqs = pd.Series(names).value_counts()\n\n    # Create a bar chart of the name frequencies\n    fig, ax = plt.subplots()\n    freqs.plot(kind='bar', ax=ax)\n    ax.set_title('Name Frequencies')\n    ax.set_xlabel('Name')\n    ax.set_ylabel('Frequency')\n\n    # Calculate skewness and kurtosis\n    skewness = freqs.skew()\n    kurtosis = freqs.kurt()\n\n    return freqs, ax, skewness, kurtosis", "raw_solution": "import pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(text):\n    # Extract names not surrounded by square brackets\n    names = re.findall(r'(?<=\\s|^)([A-Za-z]+)(?=\\s|$)', text)\n\n    # Count the frequency of each name\n    freqs = pd.Series(names).value_counts()\n\n    # Create a bar chart of the name frequencies\n    fig, ax = plt.subplots()\n    freqs.plot(kind='bar', ax=ax)\n    ax.set_title('Name Frequencies')\n    ax.set_xlabel('Name')\n    ax.set_ylabel('Frequency')\n\n    # Calculate skewness and kurtosis\n    skewness = freqs.skew()\n    kurtosis = freqs.kurt()\n\n    return freqs, ax, skewness, kurtosis"}
{"task_id": "BigCodeBench/322", "solution": "import subprocess\nimport os\nimport shutil\nimport sys\nDIRECTORY = 'c:\\Program Files\\VMware\\VMware Server'\nBACKUP_DIRECTORY = 'c:\\Program Files\\VMware\\VMware Server\\Backup'\ndef task_func(filename):\n    # Check if the file exists in the directory\n    if not os.path.exists(os.path.join(DIRECTORY, filename)):\n        print(f\"File {filename} does not exist in the directory {DIRECTORY}\")\n        return -1\n\n    # Check if the backup directory exists, if not, create it\n    if not os.path.exists(BACKUP_DIRECTORY):\n        os.makedirs(BACKUP_DIRECTORY)\n\n    # Backup the file\n    try:\n        shutil.copy2(os.path.join(DIRECTORY, filename), BACKUP_DIRECTORY)\n    except Exception as e:\n        print(f\"Backup failed: {str(e)}\")\n        return -1\n\n    # Execute the file as a subprocess\n    try:\n        result = subprocess.run([os.path.join(BACKUP_DIRECTORY, filename)], capture_output=True, text=True, shell=True)\n        return result.returncode\n    except Exception as e:\n        print(f\"Execution failed: {str(e)}\")\n        return -1", "raw_solution": "import subprocess\nimport os\nimport shutil\nimport sys\n\n# Constants\nDIRECTORY = 'c:\\Program Files\\VMware\\VMware Server'\nBACKUP_DIRECTORY = 'c:\\Program Files\\VMware\\VMware Server\\Backup'\n\ndef task_func(filename):\n    # Check if the file exists in the directory\n    if not os.path.exists(os.path.join(DIRECTORY, filename)):\n        print(f\"File {filename} does not exist in the directory {DIRECTORY}\")\n        return -1\n\n    # Check if the backup directory exists, if not, create it\n    if not os.path.exists(BACKUP_DIRECTORY):\n        os.makedirs(BACKUP_DIRECTORY)\n\n    # Backup the file\n    try:\n        shutil.copy2(os.path.join(DIRECTORY, filename), BACKUP_DIRECTORY)\n    except Exception as e:\n        print(f\"Backup failed: {str(e)}\")\n        return -1\n\n    # Execute the file as a subprocess\n    try:\n        result = subprocess.run([os.path.join(BACKUP_DIRECTORY, filename)], capture_output=True, text=True, shell=True)\n        return result.returncode\n    except Exception as e:\n        print(f\"Execution failed: {str(e)}\")\n        return -1\n\n# Test the function"}
{"task_id": "BigCodeBench/323", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.mixture import GaussianMixture\ndef task_func(text, num_gaussians=1, seed=42):\n    if num_gaussians <= 0:\n        raise ValueError(\"num_gaussians should be greater than 0\")\n\n    # Extract names from the text\n    names = re.findall(r'\\[(.*?)\\]', text)\n\n    # Tokenize the names into words\n    words = [word for name in names for word in name.split()]\n\n    # Count the frequency of each word\n    word_freq = dict(Counter(words))\n\n    # Fit a mixture of num_gaussians 1-D Gaussian distributions to the word frequencies\n    gmm = GaussianMixture(n_components=num_gaussians, random_state=seed)\n    X = np.array(list(word_freq.values())).reshape(-1, 1)\n    gmm.fit(X)\n\n    # Return the means and variances of the fitted Gaussians\n    means = gmm.means_.flatten()\n    variances = gmm.covariances_.flatten()\n\n    return {\n        \"word_freq\": word_freq,\n        \"means\": means,\n        \"variances\": variances\n    }", "raw_solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.mixture import GaussianMixture\n\ndef task_func(text, num_gaussians=1, seed=42):\n    if num_gaussians <= 0:\n        raise ValueError(\"num_gaussians should be greater than 0\")\n\n    # Extract names from the text\n    names = re.findall(r'\\[(.*?)\\]', text)\n\n    # Tokenize the names into words\n    words = [word for name in names for word in name.split()]\n\n    # Count the frequency of each word\n    word_freq = dict(Counter(words))\n\n    # Fit a mixture of num_gaussians 1-D Gaussian distributions to the word frequencies\n    gmm = GaussianMixture(n_components=num_gaussians, random_state=seed)\n    X = np.array(list(word_freq.values())).reshape(-1, 1)\n    gmm.fit(X)\n\n    # Return the means and variances of the fitted Gaussians\n    means = gmm.means_.flatten()\n    variances = gmm.covariances_.flatten()\n\n    return {\n        \"word_freq\": word_freq,\n        \"means\": means,\n        \"variances\": variances\n    }"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\nfrom typing import List\ndef task_func(file_list: List[str]) -> List[int]:\n    \"\"\"\n    Run files from list of files as subprocesses at the same time.\n    \"\"\"\n    exit_codes = []\n    threads = []\n\n    def run_file(file: str):\n        \"\"\"\n        Helper function to run a file as a subprocess.\n        \"\"\"\n        process = subprocess.Popen(['python', file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        exit_codes.append(process.wait())\n\n    # Start each file as a separate thread\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes", "raw_solution": "import subprocess\nimport time\nimport threading\nfrom typing import List\n\ndef task_func(file_list: List[str]) -> List[int]:\n    \"\"\"\n    Run files from list of files as subprocesses at the same time.\n    \"\"\"\n    exit_codes = []\n    threads = []\n\n    def run_file(file: str):\n        \"\"\"\n        Helper function to run a file as a subprocess.\n        \"\"\"\n        process = subprocess.Popen(['python', file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        exit_codes.append(process.wait())\n\n    # Start each file as a separate thread\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes\n\n# Test the function"}
{"task_id": "BigCodeBench/325", "solution": "import re\nimport os\nfrom pathlib import Path\nimport glob\ndef task_func(directory_path: str, regex_pattern: str = r'(?<=\\().+?(?=\\))|\\w+') -> dict:\n    \"\"\"\n    Extracts matches from all text files in a specified directory based on a regular expression pattern.\n    It captures whatever is between parentheses as a single match, and any character outside the parentheses as individual matches in the string.\n    The function returns a dictionary where keys are file names (without path) and values are lists of matches extracted from the files.\n    \"\"\"\n    # Initialize an empty dictionary to store the results\n    results = {}\n\n    # Use glob to get all text files in the directory\n    for file_path in glob.glob(os.path.join(directory_path, '*.txt')):\n        # Get the file name without path\n        file_name = os.path.basename(file_path)\n\n        # Open the file and read its content\n        with open(file_path, 'r') as file:\n            content = file.read()\n\n        # Use re.findall to get all matches based on the regex pattern\n        matches = re.findall(regex_pattern, content)\n\n        # Add the matches to the results dictionary\n        results[file_name] = matches\n\n    # Return the results dictionary\n    return results", "raw_solution": "import re\nimport os\nfrom pathlib import Path\nimport glob\n\ndef task_func(directory_path: str, regex_pattern: str = r'(?<=\\().+?(?=\\))|\\w+') -> dict:\n    \"\"\"\n    Extracts matches from all text files in a specified directory based on a regular expression pattern.\n    It captures whatever is between parentheses as a single match, and any character outside the parentheses as individual matches in the string.\n    The function returns a dictionary where keys are file names (without path) and values are lists of matches extracted from the files.\n    \"\"\"\n    # Initialize an empty dictionary to store the results\n    results = {}\n\n    # Use glob to get all text files in the directory\n    for file_path in glob.glob(os.path.join(directory_path, '*.txt')):\n        # Get the file name without path\n        file_name = os.path.basename(file_path)\n\n        # Open the file and read its content\n        with open(file_path, 'r') as file:\n            content = file.read()\n\n        # Use re.findall to get all matches based on the regex pattern\n        matches = re.findall(regex_pattern, content)\n\n        # Add the matches to the results dictionary\n        results[file_name] = matches\n\n    # Return the results dictionary\n    return results"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    \"\"\"\n    Finds and runs all .bat files in a given directory, returning their file names and exit codes.\n    \"\"\"\n    # Get all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n\n    # Initialize an empty list to store the file names and exit codes\n    results = []\n\n    # Iterate over each .bat file\n    for file in bat_files:\n        # Try to run the file\n        try:\n            # Run the file and capture its output\n            output = subprocess.run(file, capture_output=True, text=True, shell=True, check=True)\n\n            # Get the exit code\n            exit_code = output.returncode\n        except subprocess.CalledProcessError as e:\n            # If the file could not be executed, set the exit code to None\n            exit_code = None\n\n        # Append the file name and exit code to the results list\n        results.append((file, exit_code))\n\n    # Return the results list\n    return results", "raw_solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    Finds and runs all .bat files in a given directory, returning their file names and exit codes.\n    \"\"\"\n    # Get all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n\n    # Initialize an empty list to store the file names and exit codes\n    results = []\n\n    # Iterate over each .bat file\n    for file in bat_files:\n        # Try to run the file\n        try:\n            # Run the file and capture its output\n            output = subprocess.run(file, capture_output=True, text=True, shell=True, check=True)\n\n            # Get the exit code\n            exit_code = output.returncode\n        except subprocess.CalledProcessError as e:\n            # If the file could not be executed, set the exit code to None\n            exit_code = None\n\n        # Append the file name and exit code to the results list\n        results.append((file, exit_code))\n\n    # Return the results list\n    return results"}
{"task_id": "BigCodeBench/327", "solution": "import csv\nimport re\nfrom collections import Counter\ndef task_func(file_path, regex_pattern=r'\\(.+?\\)|\\w+|[\\W_]+'):\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    matches = []\n    for row in data:\n        for cell in row:\n            matches.extend(re.findall(regex_pattern, cell))\n\n    return dict(Counter(matches))", "raw_solution": "import csv\nimport re\nfrom collections import Counter\n\ndef task_func(file_path, regex_pattern=r'\\(.+?\\)|\\w+|[\\W_]+'):\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    matches = []\n    for row in data:\n        for cell in row:\n            matches.extend(re.findall(regex_pattern, cell))\n\n    return dict(Counter(matches))\n\n# Test the function"}
{"task_id": "BigCodeBench/328", "solution": "import collections\nimport random\nfrom typing import OrderedDict\ndef task_func(number_teams=5) -> OrderedDict:\n    \"\"\"\n    Generates a random sports ranking for a given number of teams and sorts it by points in descending order.\n    \"\"\"\n    # Initialize an empty OrderedDict to store the team names and points\n    ranking = collections.OrderedDict()\n\n    # Generate a random number of points for each team and add it to the ranking\n    for i in range(1, number_teams + 1):\n        team_name = f\"Team {i}\"\n        points = random.randint(1, 100)\n        ranking[team_name] = points\n\n    # Sort the ranking by points in descending order and return it\n    return collections.OrderedDict(sorted(ranking.items(), key=lambda x: x[1], reverse=True))", "raw_solution": "import collections\nimport random\nfrom typing import OrderedDict\n\ndef task_func(number_teams=5) -> OrderedDict:\n    \"\"\"\n    Generates a random sports ranking for a given number of teams and sorts it by points in descending order.\n    \"\"\"\n    # Initialize an empty OrderedDict to store the team names and points\n    ranking = collections.OrderedDict()\n\n    # Generate a random number of points for each team and add it to the ranking\n    for i in range(1, number_teams + 1):\n        team_name = f\"Team {i}\"\n        points = random.randint(1, 100)\n        ranking[team_name] = points\n\n    # Sort the ranking by points in descending order and return it\n    return collections.OrderedDict(sorted(ranking.items(), key=lambda x: x[1], reverse=True))\n\n# Test the function"}
{"task_id": "BigCodeBench/329", "solution": "import re\nimport json\nimport os\ndef task_func(file_path: str, regex_pattern=r'\\(.+?\\)|\\w') -> dict:\n    \"\"\"\n    Extracts matches from a JSON file based on a predefined regular pattern.\n    The default regular expression pattern is designed to extract any content between parentheses as a single match and any individual character outside the parentheses as a separate match.\n    \"\"\"\n    result = {}\n    for root, dirs, files in os.walk(file_path):\n        for file in files:\n            if file.endswith('.json'):\n                with open(os.path.join(root, file), 'r') as f:\n                    data = json.load(f)\n                    matches = re.findall(regex_pattern, str(data))\n                    result[file] = matches\n    return result", "raw_solution": "import re\nimport json\nimport os\n\ndef task_func(file_path: str, regex_pattern=r'\\(.+?\\)|\\w') -> dict:\n    \"\"\"\n    Extracts matches from a JSON file based on a predefined regular pattern.\n    The default regular expression pattern is designed to extract any content between parentheses as a single match and any individual character outside the parentheses as a separate match.\n    \"\"\"\n    result = {}\n    for root, dirs, files in os.walk(file_path):\n        for file in files:\n            if file.endswith('.json'):\n                with open(os.path.join(root, file), 'r') as f:\n                    data = json.load(f)\n                    matches = re.findall(regex_pattern, str(data))\n                    result[file] = matches\n    return result"}
{"task_id": "BigCodeBench/330", "solution": "import heapq\nimport random\nfrom typing import Tuple, List\ndef task_func(list_length: int = 5, k: int = 3) -> Tuple[List[int], List[int]]:\n    \"\"\"\n    Generates a list of random integers of a given length, finds the k largest numbers using heapq,\n    and returns the original list and the k largest numbers.\n    \"\"\"\n    # Generate a list of random integers\n    random_list = [random.randint(0, 100) for _ in range(list_length)]\n\n    # Use heapq to find the k largest numbers\n    k_largest = heapq.nlargest(k, random_list)\n\n    return random_list, k_largest", "raw_solution": "import heapq\nimport random\nfrom typing import Tuple, List\n\ndef task_func(list_length: int = 5, k: int = 3) -> Tuple[List[int], List[int]]:\n    \"\"\"\n    Generates a list of random integers of a given length, finds the k largest numbers using heapq,\n    and returns the original list and the k largest numbers.\n    \"\"\"\n    # Generate a list of random integers\n    random_list = [random.randint(0, 100) for _ in range(list_length)]\n\n    # Use heapq to find the k largest numbers\n    k_largest = heapq.nlargest(k, random_list)\n\n    return random_list, k_largest\n\n# Test the function\nrandom_list, k_largest = task_func(list_length=10, k=5)"}
{"task_id": "BigCodeBench/331", "solution": "import bisect\nimport random\nfrom typing import List, Tuple\ndef task_func(num: int, list_length: int = 5, min_value: int = 0, max_value: int = 100) -> Tuple[List[int], List[int]]:\n    \"\"\"\n    Inserts a number into a randomly generated sorted list and returns the new sorted list.\n    \"\"\"\n    # Generate a sorted list of random integers\n    random_list = sorted([random.randint(min_value, max_value) for _ in range(list_length)])\n    \n    # Insert the number into the list using binary search\n    bisect.insort(random_list, num)\n    \n    return random_list, random_list", "raw_solution": "import bisect\nimport random\nfrom typing import List, Tuple\n\ndef task_func(num: int, list_length: int = 5, min_value: int = 0, max_value: int = 100) -> Tuple[List[int], List[int]]:\n    \"\"\"\n    Inserts a number into a randomly generated sorted list and returns the new sorted list.\n    \"\"\"\n    # Generate a sorted list of random integers\n    random_list = sorted([random.randint(min_value, max_value) for _ in range(list_length)])\n    \n    # Insert the number into the list using binary search\n    bisect.insort(random_list, num)\n    \n    return random_list, random_list\n\n# Test the function"}
{"task_id": "BigCodeBench/332", "solution": "import re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\ndef task_func(text: str) -> dict:\n    \"\"\"\n    Count the number of non-stop words in a given text.\n    \"\"\"\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Convert to lowercase and split into words\n    words = text.lower().split()\n\n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in words if word not in stop_words]\n\n    # Count the words\n    word_counts = dict(Counter(words))\n\n    return word_counts", "raw_solution": "import re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\n\ndef task_func(text: str) -> dict:\n    \"\"\"\n    Count the number of non-stop words in a given text.\n    \"\"\"\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Convert to lowercase and split into words\n    words = text.lower().split()\n\n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in words if word not in stop_words]\n\n    # Count the words\n    word_counts = dict(Counter(words))\n\n    return word_counts"}
{"task_id": "BigCodeBench/333", "solution": "import heapq\nimport random\ndef task_func(k, list_length = 5, min_value = 0, max_value = 100):\n    # Generate a list of random integers\n    random_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    \n    # Use heapq to find the k smallest numbers\n    k_smallest = heapq.nsmallest(k, random_list)\n    \n    return (random_list, k_smallest)", "raw_solution": "import heapq\nimport random\n\ndef task_func(k, list_length = 5, min_value = 0, max_value = 100):\n    # Generate a list of random integers\n    random_list = [random.randint(min_value, max_value) for _ in range(list_length)]\n    \n    # Use heapq to find the k smallest numbers\n    k_smallest = heapq.nsmallest(k, random_list)\n    \n    return (random_list, k_smallest)\n\n# Test the function"}
{"task_id": "BigCodeBench/334", "solution": "from nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\ndef task_func(documents):\n    \"\"\"\n    Calculates the TF-IDF score of the words in a list of documents.\n    \"\"\"\n    # Tokenize the documents\n    documents = [word_tokenize(doc) for doc in documents]\n\n    # Create a TfidfVectorizer\n    vectorizer = TfidfVectorizer(input='content', encoding='utf-8', decode_error='ignore', tokenizer=word_tokenize)\n\n    # Fit and transform the documents\n    tfidf_matrix = vectorizer.fit_transform(documents)\n\n    # Get the feature names (words)\n    words = vectorizer.get_feature_names_out()\n\n    # Create a DataFrame with words as columns and documents as rows\n    df = pd.DataFrame(tfidf_matrix.toarray(), columns=words)\n\n    return df", "raw_solution": "from nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\n\ndef task_func(documents):\n    \"\"\"\n    Calculates the TF-IDF score of the words in a list of documents.\n    \"\"\"\n    # Tokenize the documents\n    documents = [word_tokenize(doc) for doc in documents]\n\n    # Create a TfidfVectorizer\n    vectorizer = TfidfVectorizer(input='content', encoding='utf-8', decode_error='ignore', tokenizer=word_tokenize)\n\n    # Fit and transform the documents\n    tfidf_matrix = vectorizer.fit_transform(documents)\n\n    # Get the feature names (words)\n    words = vectorizer.get_feature_names_out()\n\n    # Create a DataFrame with words as columns and documents as rows\n    df = pd.DataFrame(tfidf_matrix.toarray(), columns=words)\n\n    return df"}
{"task_id": "BigCodeBench/335", "solution": "import collections\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e']\ndef task_func(string_length=100):\n    # Generate a random string of given length from the predefined list of letters\n    random_string = ''.join(random.choice(LETTERS) for _ in range(string_length))\n\n    # Count the frequency of each letter in the string\n    letter_freq = collections.Counter(random_string)\n\n    # Sort the frequencies in descending order\n    sorted_freq = collections.OrderedDict(sorted(letter_freq.items(), key=lambda x: x[1], reverse=True))\n\n    return sorted_freq", "raw_solution": "import collections\nimport random\n\n# Constants\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef task_func(string_length=100):\n    # Generate a random string of given length from the predefined list of letters\n    random_string = ''.join(random.choice(LETTERS) for _ in range(string_length))\n\n    # Count the frequency of each letter in the string\n    letter_freq = collections.Counter(random_string)\n\n    # Sort the frequencies in descending order\n    sorted_freq = collections.OrderedDict(sorted(letter_freq.items(), key=lambda x: x[1], reverse=True))\n\n    return sorted_freq\n\n# Test the function"}
{"task_id": "BigCodeBench/336", "solution": "import re\nimport os\nimport glob\nfrom pathlib import Path\ndef task_func(pattern, directory, extensions):\n    \"\"\"\n    Find all files in a specific directory that contain a regex pattern in their contents in a case insensitive manner.\n    \"\"\"\n    pattern = re.compile(pattern, re.IGNORECASE)\n    files = []\n    for ext in extensions:\n        files.extend(glob.glob(f'{directory}/**/*.{ext}', recursive=True))\n\n    matching_files = []\n    for file in files:\n        with open(file, 'r', encoding='utf-8') as f:\n            if pattern.search(f.read()):\n                matching_files.append(os.path.abspath(file))\n\n    return matching_files", "raw_solution": "import re\nimport os\nimport glob\nfrom pathlib import Path\n\ndef task_func(pattern, directory, extensions):\n    \"\"\"\n    Find all files in a specific directory that contain a regex pattern in their contents in a case insensitive manner.\n    \"\"\"\n    pattern = re.compile(pattern, re.IGNORECASE)\n    files = []\n    for ext in extensions:\n        files.extend(glob.glob(f'{directory}/**/*.{ext}', recursive=True))\n\n    matching_files = []\n    for file in files:\n        with open(file, 'r', encoding='utf-8') as f:\n            if pattern.search(f.read()):\n                matching_files.append(os.path.abspath(file))\n\n    return matching_files\n\n# Test the function"}
{"task_id": "BigCodeBench/337", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nCOLORS = ['r', 'g', 'b']\ndef task_func(df, group_col, value_col):\n    \"\"\"\n    Creates a bar chart of data in multiple groups with error bars.\n    \"\"\"\n    # Check if the columns exist in the dataframe\n    if group_col not in df.columns or value_col not in df.columns:\n        raise ValueError(f\"Either {group_col} or {value_col} is not a column in the dataframe\")\n\n    # Check if the 'Value' column has numeric values\n    if not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise TypeError(f\"The '{value_col}' has non-numeric values\")\n\n    # Group the dataframe by the group column and calculate the mean and standard deviation\n    grouped_df = df.groupby(group_col).agg({value_col: ['mean', 'std']})\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Create a bar chart with error bars\n    ax.bar(grouped_df.index, grouped_df[value_col]['mean'], yerr=grouped_df[value_col]['std'], color=[COLORS[i % len(COLORS)] for i in range(len(grouped_df))])\n\n    # Set the title\n    ax.set_title(f\"Bar chart of {value_col} by {group_col}\")\n\n    # Set the xlabel and ylabel\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef task_func(df, group_col, value_col):\n    \"\"\"\n    Creates a bar chart of data in multiple groups with error bars.\n    \"\"\"\n    # Check if the columns exist in the dataframe\n    if group_col not in df.columns or value_col not in df.columns:\n        raise ValueError(f\"Either {group_col} or {value_col} is not a column in the dataframe\")\n\n    # Check if the 'Value' column has numeric values\n    if not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise TypeError(f\"The '{value_col}' has non-numeric values\")\n\n    # Group the dataframe by the group column and calculate the mean and standard deviation\n    grouped_df = df.groupby(group_col).agg({value_col: ['mean', 'std']})\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Create a bar chart with error bars\n    ax.bar(grouped_df.index, grouped_df[value_col]['mean'], yerr=grouped_df[value_col]['std'], color=[COLORS[i % len(COLORS)] for i in range(len(grouped_df))])\n\n    # Set the title\n    ax.set_title(f\"Bar chart of {value_col} by {group_col}\")\n\n    # Set the xlabel and ylabel\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n\n    return ax"}
{"task_id": "BigCodeBench/338", "solution": "import random\nimport string\nfrom matplotlib import pyplot as plt\nfrom collections import Counter\nfrom typing import List, Tuple\ndef task_func(elements: List[str], seed: int = 100) -> Tuple[List[str], plt.Axes, dict]:\n    \"\"\"\n    Formats each string in the given list \"elements\" into a pattern \"% {0}%\", where {0} is a randomly generated alphanumeric string of length 5.\n    Returns the plot axes of an histogram of the occurrence of each character across all the strings and a dictionary containing the count of each character in all the formatted strings.\n    \"\"\"\n    random.seed(seed)\n    formatted_elements = []\n    all_chars = ''\n    for element in elements:\n        random_str = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n        formatted_element = f\"% {random_str}%\"\n        formatted_elements.append(formatted_element)\n        all_chars += formatted_element\n\n    # Create histogram\n    char_counts = Counter(all_chars)\n    fig, ax = plt.subplots()\n    ax.bar(char_counts.keys(), char_counts.values())\n    ax.set_title('Histogram of character occurrence')\n    ax.set_xlabel('Character')\n    ax.set_ylabel('Count')\n\n    return formatted_elements, ax, char_counts", "raw_solution": "import random\nimport string\nfrom matplotlib import pyplot as plt\nfrom collections import Counter\nfrom typing import List, Tuple\n\ndef task_func(elements: List[str], seed: int = 100) -> Tuple[List[str], plt.Axes, dict]:\n    \"\"\"\n    Formats each string in the given list \"elements\" into a pattern \"% {0}%\", where {0} is a randomly generated alphanumeric string of length 5.\n    Returns the plot axes of an histogram of the occurrence of each character across all the strings and a dictionary containing the count of each character in all the formatted strings.\n    \"\"\"\n    random.seed(seed)\n    formatted_elements = []\n    all_chars = ''\n    for element in elements:\n        random_str = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n        formatted_element = f\"% {random_str}%\"\n        formatted_elements.append(formatted_element)\n        all_chars += formatted_element\n\n    # Create histogram\n    char_counts = Counter(all_chars)\n    fig, ax = plt.subplots()\n    ax.bar(char_counts.keys(), char_counts.values())\n    ax.set_title('Histogram of character occurrence')\n    ax.set_xlabel('Character')\n    ax.set_ylabel('Count')\n\n    return formatted_elements, ax, char_counts"}
{"task_id": "BigCodeBench/339", "solution": "import json\nimport urllib.parse\nimport hmac\nimport hashlib\ndef task_func(req_data, secret_key):\n    if not isinstance(req_data, dict):\n        raise TypeError(\"`req_data` must be a dictionary\")\n    json_data = json.dumps(req_data, sort_keys=True)\n    signature = hmac.new(\n        key=secret_key.encode(),\n        msg=json_data.encode(),\n        digestmod=hashlib.sha256\n    ).hexdigest()\n    url_encoded_signature = urllib.parse.quote(signature.encode())\n    url_encoded_signature = url_encoded_signature.replace('+', '%20')\n    return url_encoded_signature", "raw_solution": "import json\nimport urllib.parse\nimport hmac\nimport hashlib\n\ndef task_func(req_data, secret_key):\n    if not isinstance(req_data, dict):\n        raise TypeError(\"`req_data` must be a dictionary\")\n    json_data = json.dumps(req_data, sort_keys=True)\n    signature = hmac.new(\n        key=secret_key.encode(),\n        msg=json_data.encode(),\n        digestmod=hashlib.sha256\n    ).hexdigest()\n    url_encoded_signature = urllib.parse.quote(signature.encode())\n    url_encoded_signature = url_encoded_signature.replace('+', '%20')\n    return url_encoded_signature"}
{"task_id": "BigCodeBench/340", "solution": "import json\nimport hashlib\nimport blake3\ndef task_func(req_data):\n    \"\"\"\n    Hashes the specified request data with BLAKE3 and then converts it into a hexadecimal representation.\n    Additionally, generates an MD5 hash of the BLAKE3 hash for demonstration purposes.\n    \"\"\"\n    # Convert the request data to a JSON string\n    json_str = json.dumps(req_data)\n\n    # Create a BLAKE3 hash object\n    blake3_hash = blake3.hash(json_str.encode())\n\n    # Convert the BLAKE3 hash to a hexadecimal string\n    blake3_hex = blake3_hash.hexdigest()\n\n    # Create an MD5 hash object\n    md5_hash = hashlib.md5(blake3_hex.encode())\n\n    # Convert the MD5 hash to a hexadecimal string\n    md5_hex = md5_hash.hexdigest()\n\n    return blake3_hex, md5_hex", "raw_solution": "import json\nimport hashlib\nimport blake3\n\ndef task_func(req_data):\n    \"\"\"\n    Hashes the specified request data with BLAKE3 and then converts it into a hexadecimal representation.\n    Additionally, generates an MD5 hash of the BLAKE3 hash for demonstration purposes.\n    \"\"\"\n    # Convert the request data to a JSON string\n    json_str = json.dumps(req_data)\n\n    # Create a BLAKE3 hash object\n    blake3_hash = blake3.hash(json_str.encode())\n\n    # Convert the BLAKE3 hash to a hexadecimal string\n    blake3_hex = blake3_hash.hexdigest()\n\n    # Create an MD5 hash object\n    md5_hash = hashlib.md5(blake3_hex.encode())\n\n    # Convert the MD5 hash to a hexadecimal string\n    md5_hex = md5_hash.hexdigest()\n\n    return blake3_hex, md5_hex"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"The input df must be DataFrame, not be empty\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the column {col}\")\n\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n    if df[col].dtype in ['int64', 'float64']:\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.countplot(y=df[col], ax=axes[0])\n    axes[0].set_title(f'Histogram of {col}')\n\n    sns.boxplot(y=df[col], ax=axes[1])\n    axes[1].set_title(f'Box plot of {col}')\n\n    plt.tight_layout()\n\n    return fig", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"The input df must be DataFrame, not be empty\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the column {col}\")\n\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n    if df[col].dtype in ['int64', 'float64']:\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.countplot(y=df[col], ax=axes[0])\n    axes[0].set_title(f'Histogram of {col}')\n\n    sns.boxplot(y=df[col], ax=axes[1])\n    axes[1].set_title(f'Box plot of {col}')\n\n    plt.tight_layout()\n\n    return fig"}
{"task_id": "BigCodeBench/342", "solution": "import string\nimport random\nimport re\ndef task_func(elements, pattern, seed=100):\n    random.seed(seed)\n    formatted_elements = []\n    for element in elements:\n        replaced_element = \"\".join(random.choice(string.ascii_letters) if char == '{' else char for char in element)\n        formatted_element = f\"%{replaced_element}%\"\n        formatted_elements.append(formatted_element)\n    formatted_string = \"\".join(formatted_elements)\n    search_result = bool(re.search(pattern, formatted_string))\n    return formatted_elements, search_result", "raw_solution": "import string\nimport random\nimport re\n\ndef task_func(elements, pattern, seed=100):\n    random.seed(seed)\n    formatted_elements = []\n    for element in elements:\n        replaced_element = \"\".join(random.choice(string.ascii_letters) if char == '{' else char for char in element)\n        formatted_element = f\"%{replaced_element}%\"\n        formatted_elements.append(formatted_element)\n    formatted_string = \"\".join(formatted_elements)\n    search_result = bool(re.search(pattern, formatted_string))\n    return formatted_elements, search_result"}
{"task_id": "BigCodeBench/343", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCOLORS = ['r', 'g', 'b', 'y', 'm']\ndef task_func(df, col, title=None):\n    \"\"\"\n    Draws a pie chart of the number of unique values in a given DataFrame column.\n    The pie chart can have a title if specified.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a pandas DataFrame.\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty.\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the column '{col}'.\")\n\n    # Get the unique values and their counts\n    values, counts = df[col].value_counts().values, df[col].value_counts().index\n\n    # Create the pie chart\n    fig, ax = plt.subplots()\n    ax.pie(counts, labels=values, colors=COLORS[:len(counts)], autopct='%1.1f%%')\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    if title:\n        ax.set_title(title)\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants for pie chart colors\nCOLORS = ['r', 'g', 'b', 'y', 'm']\n\ndef task_func(df, col, title=None):\n    \"\"\"\n    Draws a pie chart of the number of unique values in a given DataFrame column.\n    The pie chart can have a title if specified.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a pandas DataFrame.\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty.\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the column '{col}'.\")\n\n    # Get the unique values and their counts\n    values, counts = df[col].value_counts().values, df[col].value_counts().index\n\n    # Create the pie chart\n    fig, ax = plt.subplots()\n    ax.pie(counts, labels=values, colors=COLORS[:len(counts)], autopct='%1.1f%%')\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    if title:\n        ax.set_title(title)\n\n    return ax"}
{"task_id": "BigCodeBench/344", "solution": "import os\nimport shutil\ndef task_func(src_folder, backup_dir):\n    \"\"\"\n    Backs up a given source folder to the specified backup directory, then deletes the source folder.\n    \"\"\"\n    # Check if source folder exists\n    if not os.path.exists(src_folder):\n        raise ValueError(\"Source folder does not exist.\")\n\n    # Check if backup directory exists, if not create it\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    # Create backup of source folder\n    backup_path = os.path.join(backup_dir, os.path.basename(src_folder))\n    shutil.make_archive(backup_path, 'zip', src_folder)\n\n    # Delete source folder\n    try:\n        shutil.rmtree(src_folder)\n    except Exception as e:\n        raise Exception(\"An error occurred while deleting the source folder.\")\n\n    return True", "raw_solution": "import os\nimport shutil\n\ndef task_func(src_folder, backup_dir):\n    \"\"\"\n    Backs up a given source folder to the specified backup directory, then deletes the source folder.\n    \"\"\"\n    # Check if source folder exists\n    if not os.path.exists(src_folder):\n        raise ValueError(\"Source folder does not exist.\")\n\n    # Check if backup directory exists, if not create it\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    # Create backup of source folder\n    backup_path = os.path.join(backup_dir, os.path.basename(src_folder))\n    shutil.make_archive(backup_path, 'zip', src_folder)\n\n    # Delete source folder\n    try:\n        shutil.rmtree(src_folder)\n    except Exception as e:\n        raise Exception(\"An error occurred while deleting the source folder.\")\n\n    return True"}
{"task_id": "BigCodeBench/345", "solution": "import pandas as pd\nimport seaborn as sns\ndef task_func(df, col1, col2):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"Input df is empty\")\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(f\"Input df does not contain the specified columns: {col1} and {col2}\")\n    if not pd.api.types.is_numeric_dtype(df[col1]) or not pd.api.types.is_numeric_dtype(df[col2]):\n        raise TypeError(\"df uses non-numeric data\")\n\n    sns.regplot(x=df[col1], y=df[col2])\n    return sns.plt.gca()", "raw_solution": "import pandas as pd\nimport seaborn as sns\n\ndef task_func(df, col1, col2):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"Input df is empty\")\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(f\"Input df does not contain the specified columns: {col1} and {col2}\")\n    if not pd.api.types.is_numeric_dtype(df[col1]) or not pd.api.types.is_numeric_dtype(df[col2]):\n        raise TypeError(\"df uses non-numeric data\")\n\n    sns.regplot(x=df[col1], y=df[col2])\n    return sns.plt.gca()"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Runs a Python script as a process with predefined arguments.\n    By default, waits for the process to complete. If wait is False, the function returns None.\n    Raises:\n        - ValueError: If the script does not exist.\n        - subprocess.CalledProcessError: If the script raises an exception.\n    Returns:\n        int: The return code of the subprocess. If 'wait' is False, returns None.\n    \"\"\"\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n\n    try:\n        if wait:\n            result = subprocess.run([sys.executable, script_path] + list(args), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            return result.returncode\n        else:\n            subprocess.Popen([sys.executable, script_path] + list(args))\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output, stderr=e.stderr)", "raw_solution": "import subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Runs a Python script as a process with predefined arguments.\n    By default, waits for the process to complete. If wait is False, the function returns None.\n    Raises:\n        - ValueError: If the script does not exist.\n        - subprocess.CalledProcessError: If the script raises an exception.\n    Returns:\n        int: The return code of the subprocess. If 'wait' is False, returns None.\n    \"\"\"\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n\n    try:\n        if wait:\n            result = subprocess.run([sys.executable, script_path] + list(args), check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            return result.returncode\n        else:\n            subprocess.Popen([sys.executable, script_path] + list(args))\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output, stderr=e.stderr)\n\n# Test the function"}
{"task_id": "BigCodeBench/347", "solution": "import pandas as pd\nimport re\nimport numpy as np\nPATTERN = r\"([a-fA-F\\d]{32})\"\ndef task_func(df, column):\n    \"\"\"\n    Find all matches of the regex pattern '([a-fA-F\\d]{32})' in a Pandas DataFrame column and count the occurrence of any unique match in the data.\n    \"\"\"\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' does not exist in the DataFrame.\")\n\n    # Find all matches of the pattern\n    matches = df[column].str.extractall(PATTERN)\n\n    # Count the occurrence of each unique match\n    counts = matches[0].value_counts()\n\n    return counts", "raw_solution": "import pandas as pd\nimport re\nimport numpy as np\n\n# Constants\nPATTERN = r\"([a-fA-F\\d]{32})\"\n\ndef task_func(df, column):\n    \"\"\"\n    Find all matches of the regex pattern '([a-fA-F\\d]{32})' in a Pandas DataFrame column and count the occurrence of any unique match in the data.\n    \"\"\"\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' does not exist in the DataFrame.\")\n\n    # Find all matches of the pattern\n    matches = df[column].str.extractall(PATTERN)\n\n    # Count the occurrence of each unique match\n    counts = matches[0].value_counts()\n\n    return counts"}
{"task_id": "BigCodeBench/348", "solution": "import subprocess\nimport os\nimport signal\nimport time\ndef task_func(process_name: str) -> int:\n    \"\"\"\n    Stops all running processes with a specific name.\n    \"\"\"\n    # Get a list of all running processes\n    output = subprocess.check_output(\"ps -A\", shell=True).decode(\"utf-8\").split(\"\\n\")\n\n    # Initialize counter for processes stopped\n    count = 0\n\n    # Iterate over each line in the output\n    for line in output:\n        # If the process name is in the line\n        if process_name in line:\n            # Get the PID from the line\n            pid = int(line.split()[0])\n\n            # Try to send the termination signal to the process\n            try:\n                os.kill(pid, signal.SIGTERM)\n                count += 1\n            except ProcessLookupError:\n                pass\n\n    # Wait for 1 second to allow processes to terminate\n    time.sleep(1)\n\n    # Return the number of processes stopped\n    return count", "raw_solution": "import subprocess\nimport os\nimport signal\nimport time\n\ndef task_func(process_name: str) -> int:\n    \"\"\"\n    Stops all running processes with a specific name.\n    \"\"\"\n    # Get a list of all running processes\n    output = subprocess.check_output(\"ps -A\", shell=True).decode(\"utf-8\").split(\"\\n\")\n\n    # Initialize counter for processes stopped\n    count = 0\n\n    # Iterate over each line in the output\n    for line in output:\n        # If the process name is in the line\n        if process_name in line:\n            # Get the PID from the line\n            pid = int(line.split()[0])\n\n            # Try to send the termination signal to the process\n            try:\n                os.kill(pid, signal.SIGTERM)\n                count += 1\n            except ProcessLookupError:\n                pass\n\n    # Wait for 1 second to allow processes to terminate\n    time.sleep(1)\n\n    # Return the number of processes stopped\n    return count"}
{"task_id": "BigCodeBench/349", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories):\n    data = []\n    for i, category in enumerate(categories):\n        for j, product in enumerate(product_list):\n            quantity_sold = random.randint(1, 100)\n            revenue = quantity_sold * random.randint(10, 100)\n            data.append([product, category, quantity_sold, revenue])\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue'])\n    return df\nproduct_list = ['Product 1', 'Product 2', 'Product 3']\ncategories = ['Category 1', 'Category 2', 'Category 3']", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories):\n    data = []\n    for i, category in enumerate(categories):\n        for j, product in enumerate(product_list):\n            quantity_sold = random.randint(1, 100)\n            revenue = quantity_sold * random.randint(10, 100)\n            data.append([product, category, quantity_sold, revenue])\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue'])\n    return df\n\n# Test the function\nproduct_list = ['Product 1', 'Product 2', 'Product 3']\ncategories = ['Category 1', 'Category 2', 'Category 3']\ndf = task_func(product_list, categories)"}
{"task_id": "BigCodeBench/350", "solution": "import subprocess\nimport os\nimport shutil\nfrom glob import glob\ndef task_func(src_folder, dst_folder):\n    \"\"\"\n    Compress all files in the specified source folder and move the compressed files to a destination folder.\n    \"\"\"\n    if not os.path.exists(src_folder):\n        return {\n            'success': False,\n            'message': f'Source folder {src_folder} does not exist.',\n            'failed_files': []\n        }\n\n    if not os.path.exists(dst_folder):\n        return {\n            'success': False,\n            'message': f'Destination folder {dst_folder} does not exist.',\n            'failed_files': []\n        }\n\n    failed_files = []\n    for file in glob(os.path.join(src_folder, '*')):\n        if os.path.isfile(file):\n            try:\n                # Compress the file using gzip\n                subprocess.run(['gzip', file], check=True)\n\n                # Move the compressed file to the destination folder\n                shutil.move(file + '.gz', dst_folder)\n            except Exception as e:\n                failed_files.append(file)\n\n    success = len(failed_files) == 0\n    message = 'All files were compressed and moved successfully.' if success else 'Some files failed to compress or move.'\n\n    return {\n        'success': success,\n        'message': message,\n        'failed_files': failed_files\n    }", "raw_solution": "import subprocess\nimport os\nimport shutil\nfrom glob import glob\n\ndef task_func(src_folder, dst_folder):\n    \"\"\"\n    Compress all files in the specified source folder and move the compressed files to a destination folder.\n    \"\"\"\n    if not os.path.exists(src_folder):\n        return {\n            'success': False,\n            'message': f'Source folder {src_folder} does not exist.',\n            'failed_files': []\n        }\n\n    if not os.path.exists(dst_folder):\n        return {\n            'success': False,\n            'message': f'Destination folder {dst_folder} does not exist.',\n            'failed_files': []\n        }\n\n    failed_files = []\n    for file in glob(os.path.join(src_folder, '*')):\n        if os.path.isfile(file):\n            try:\n                # Compress the file using gzip\n                subprocess.run(['gzip', file], check=True)\n\n                # Move the compressed file to the destination folder\n                shutil.move(file + '.gz', dst_folder)\n            except Exception as e:\n                failed_files.append(file)\n\n    success = len(failed_files) == 0\n    message = 'All files were compressed and moved successfully.' if success else 'Some files failed to compress or move.'\n\n    return {\n        'success': success,\n        'message': message,\n        'failed_files': failed_files\n    }"}
{"task_id": "BigCodeBench/351", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    \"\"\"\n    Generates a sales report for a list of products in different categories.\n    The report includes the quantity sold and revenue generated for each product.\n    \"\"\"\n    data = []\n    for category in categories:\n        for product in product_list:\n            quantity_sold = random.randint(min_value, max_value)\n            revenue = quantity_sold * random.uniform(min_value, max_value)\n            data.append([product, category, quantity_sold, revenue])\n\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue'])\n    return df\nproduct_list = ['Product 1', 'Product 2', 'Product 3']\ncategories = ['Category 1', 'Category 2', 'Category 3']", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    \"\"\"\n    Generates a sales report for a list of products in different categories.\n    The report includes the quantity sold and revenue generated for each product.\n    \"\"\"\n    data = []\n    for category in categories:\n        for product in product_list:\n            quantity_sold = random.randint(min_value, max_value)\n            revenue = quantity_sold * random.uniform(min_value, max_value)\n            data.append([product, category, quantity_sold, revenue])\n\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue'])\n    return df\n\n# Test the function\nproduct_list = ['Product 1', 'Product 2', 'Product 3']\ncategories = ['Category 1', 'Category 2', 'Category 3']\ndf = task_func(product_list, categories)"}
{"task_id": "BigCodeBench/352", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom collections import Counter\ndef task_func(text_dict, word_keys, top_k=2):\n    \"\"\"\n    Calculates the frequency of certain words in a text dictionary and returns a bar chart's Axes object and a dictionary\n    containing the frequencies of the top_k most common words in text_dict.\n    \"\"\"\n    if top_k < 0:\n        raise ValueError(\"top_k cannot be a negative integer\")\n\n    # Create a Counter object from the text_dict\n    word_counter = Counter(text_dict)\n\n    # Calculate the frequency of the provided words in the dictionary\n    word_frequencies = {word: word_counter[word] for word in word_keys}\n\n    # Create a DataFrame from the word_frequencies dictionary\n    df = pd.DataFrame(list(word_frequencies.items()), columns=[\"Word\", \"Frequency\"])\n\n    # Sort the DataFrame by frequency in descending order\n    df = df.sort_values(\"Frequency\", ascending=False)\n\n    # Get the top_k most common words\n    top_words = df.head(top_k)\n\n    # Create a bar chart of the top_k most common words\n    ax = top_words.plot(kind=\"bar\", x=\"Word\", y=\"Frequency\", legend=False)\n\n    return ax, dict(top_words.values)", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom collections import Counter\n\ndef task_func(text_dict, word_keys, top_k=2):\n    \"\"\"\n    Calculates the frequency of certain words in a text dictionary and returns a bar chart's Axes object and a dictionary\n    containing the frequencies of the top_k most common words in text_dict.\n    \"\"\"\n    if top_k < 0:\n        raise ValueError(\"top_k cannot be a negative integer\")\n\n    # Create a Counter object from the text_dict\n    word_counter = Counter(text_dict)\n\n    # Calculate the frequency of the provided words in the dictionary\n    word_frequencies = {word: word_counter[word] for word in word_keys}\n\n    # Create a DataFrame from the word_frequencies dictionary\n    df = pd.DataFrame(list(word_frequencies.items()), columns=[\"Word\", \"Frequency\"])\n\n    # Sort the DataFrame by frequency in descending order\n    df = df.sort_values(\"Frequency\", ascending=False)\n\n    # Get the top_k most common words\n    top_words = df.head(top_k)\n\n    # Create a bar chart of the top_k most common words\n    ax = top_words.plot(kind=\"bar\", x=\"Word\", y=\"Frequency\", legend=False)\n\n    return ax, dict(top_words.values)"}
{"task_id": "BigCodeBench/353", "solution": "import pandas as pd\nimport random\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    \"\"\"\n    Generates a sales report for a list of products in different categories.\n    The report includes the quantity sold, revenue for 1 product, and total revenue generated for each product.\n    \"\"\"\n    data = []\n    for category in categories:\n        for product in product_list:\n            quantity_sold = random.randint(min_value, max_value)\n            revenue = random.uniform(min_value, max_value)\n            total_revenue = quantity_sold * revenue\n            data.append([product, category, quantity_sold, revenue, total_revenue])\n\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue', 'Total Revenue'])\n    return df\nproduct_list = ['Product A', 'Product B', 'Product C']\ncategories = ['Category 1', 'Category 2', 'Category 3']", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories, min_value = 10, max_value = 100):\n    \"\"\"\n    Generates a sales report for a list of products in different categories.\n    The report includes the quantity sold, revenue for 1 product, and total revenue generated for each product.\n    \"\"\"\n    data = []\n    for category in categories:\n        for product in product_list:\n            quantity_sold = random.randint(min_value, max_value)\n            revenue = random.uniform(min_value, max_value)\n            total_revenue = quantity_sold * revenue\n            data.append([product, category, quantity_sold, revenue, total_revenue])\n\n    df = pd.DataFrame(data, columns=['Product', 'Category', 'Quantity Sold', 'Revenue', 'Total Revenue'])\n    return df\n\n# Test the function\nproduct_list = ['Product A', 'Product B', 'Product C']\ncategories = ['Category 1', 'Category 2', 'Category 3']\ndf = task_func(product_list, categories)"}
{"task_id": "BigCodeBench/354", "solution": "import collections\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(sentences_dict, word_keys):\n    \"\"\"\n    This function takes a dictionary of sentences and a list of words as input.\n    It calculates the occurrence of each word in the sentences and returns a bar chart.\n    \"\"\"\n    # Create a counter for each word\n    word_counts = collections.Counter()\n    for word in word_keys:\n        word_counts[word] = 0\n\n    # Count the occurrences of each word in the sentences\n    for sentence in sentences_dict.values():\n        for word in word_counts.keys():\n            word_counts[word] += sentence.count(word)\n\n    # Create a bar chart\n    plt.figure(figsize=(10, 5))\n    plt.bar(word_counts.keys(), word_counts.values())\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Words in Sentences')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return plt.gca()", "raw_solution": "import collections\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nWORDS = ['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I']\n\ndef task_func(sentences_dict, word_keys):\n    \"\"\"\n    This function takes a dictionary of sentences and a list of words as input.\n    It calculates the occurrence of each word in the sentences and returns a bar chart.\n    \"\"\"\n    # Create a counter for each word\n    word_counts = collections.Counter()\n    for word in word_keys:\n        word_counts[word] = 0\n\n    # Count the occurrences of each word in the sentences\n    for sentence in sentences_dict.values():\n        for word in word_counts.keys():\n            word_counts[word] += sentence.count(word)\n\n    # Create a bar chart\n    plt.figure(figsize=(10, 5))\n    plt.bar(word_counts.keys(), word_counts.values())\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Words in Sentences')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/355", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import get_window\ndef task_func(amplitude, frequency, time):\n    # Generate the wave\n    wave = amplitude * np.exp(2j * np.pi * frequency * time)\n\n    # Apply a Hann window\n    window = get_window('hann', len(time))\n    wave_windowed = wave * window\n\n    # Plot the wave\n    fig, ax = plt.subplots()\n    ax.plot(time, wave_windowed.real, label='Real')\n    ax.plot(time, wave_windowed.imag, label='Imaginary')\n    ax.set_title('Complex Wave with Hann Window')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n\n    return wave_windowed, fig, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import get_window\n\ndef task_func(amplitude, frequency, time):\n    # Generate the wave\n    wave = amplitude * np.exp(2j * np.pi * frequency * time)\n\n    # Apply a Hann window\n    window = get_window('hann', len(time))\n    wave_windowed = wave * window\n\n    # Plot the wave\n    fig, ax = plt.subplots()\n    ax.plot(time, wave_windowed.real, label='Real')\n    ax.plot(time, wave_windowed.imag, label='Imaginary')\n    ax.set_title('Complex Wave with Hann Window')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n\n    return wave_windowed, fig, ax"}
{"task_id": "BigCodeBench/356", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport cmath\ndef task_func(x, y):\n    \"\"\"\n    Draws the phase of a complex function over a range of x and y and returns the matplotlib axes object along with the 2D array of calculated phase values.\n    \"\"\"\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"Both x and y must be numpy.ndarray.\")\n    if len(x) != len(y):\n        raise ValueError(\"x and y must have the same length.\")\n\n    # Create a complex number array from x and y\n    z = x + 1j * y\n\n    # Calculate the phase\n    phase = np.angle(z)\n\n    # Create a new figure and axes\n    fig, ax = plt.subplots()\n\n    # Plot the phase\n    cax = ax.imshow(phase, extent=[np.min(x), np.max(x), np.min(y), np.max(y)], origin='lower')\n    fig.colorbar(cax)\n\n    return ax, phase", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport cmath\n\ndef task_func(x, y):\n    \"\"\"\n    Draws the phase of a complex function over a range of x and y and returns the matplotlib axes object along with the 2D array of calculated phase values.\n    \"\"\"\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"Both x and y must be numpy.ndarray.\")\n    if len(x) != len(y):\n        raise ValueError(\"x and y must have the same length.\")\n\n    # Create a complex number array from x and y\n    z = x + 1j * y\n\n    # Calculate the phase\n    phase = np.angle(z)\n\n    # Create a new figure and axes\n    fig, ax = plt.subplots()\n\n    # Plot the phase\n    cax = ax.imshow(phase, extent=[np.min(x), np.max(x), np.min(y), np.max(y)], origin='lower')\n    fig.colorbar(cax)\n\n    return ax, phase"}
{"task_id": "BigCodeBench/357", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(x):\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input x should be a numpy.ndarray\")\n\n    real_part = norm.rvs(loc=0, scale=1, size=x.shape[0])\n    imag_part = norm.rvs(loc=2, scale=2, size=x.shape[0])\n    complex_distribution = real_part + 1j * imag_part\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(complex_distribution, bins=30, density=True)\n    plt.title(\"Complex Distribution\")\n    plt.xlabel(\"Real Part\")\n    plt.ylabel(\"Probability Density\")\n    plt.grid(True)\n    plt.show()\n\n    return complex_distribution", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(x):\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input x should be a numpy.ndarray\")\n\n    real_part = norm.rvs(loc=0, scale=1, size=x.shape[0])\n    imag_part = norm.rvs(loc=2, scale=2, size=x.shape[0])\n    complex_distribution = real_part + 1j * imag_part\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(complex_distribution, bins=30, density=True)\n    plt.title(\"Complex Distribution\")\n    plt.xlabel(\"Real Part\")\n    plt.ylabel(\"Probability Density\")\n    plt.grid(True)\n    plt.show()\n\n    return complex_distribution"}
{"task_id": "BigCodeBench/358", "solution": "import itertools\nimport json\ndef task_func(json_list, r):\n    \"\"\"\n    Generate all possible combinations of r elements from a given number list taken from JSON string input.\n    \"\"\"\n    try:\n        # Parse JSON string\n        data = json.loads(json_list)\n    except json.JSONDecodeError:\n        raise Exception(\"Invalid JSON\")\n\n    # Check if JSON data is not empty and has 'number_list' key\n    if not data or 'number_list' not in data:\n        raise Exception(\"Invalid JSON data. It should not be empty and should have 'number_list' key.\")\n\n    # Extract 'number_list' from JSON data\n    number_list = data['number_list']\n\n    # Check if 'number_list' is a list\n    if not isinstance(number_list, list):\n        raise Exception(\"'number_list' should be a list.\")\n\n    # Generate combinations\n    combinations = list(itertools.combinations(number_list, r))\n\n    return combinations", "raw_solution": "import itertools\nimport json\n\ndef task_func(json_list, r):\n    \"\"\"\n    Generate all possible combinations of r elements from a given number list taken from JSON string input.\n    \"\"\"\n    try:\n        # Parse JSON string\n        data = json.loads(json_list)\n    except json.JSONDecodeError:\n        raise Exception(\"Invalid JSON\")\n\n    # Check if JSON data is not empty and has 'number_list' key\n    if not data or 'number_list' not in data:\n        raise Exception(\"Invalid JSON data. It should not be empty and should have 'number_list' key.\")\n\n    # Extract 'number_list' from JSON data\n    number_list = data['number_list']\n\n    # Check if 'number_list' is a list\n    if not isinstance(number_list, list):\n        raise Exception(\"'number_list' should be a list.\")\n\n    # Generate combinations\n    combinations = list(itertools.combinations(number_list, r))\n\n    return combinations"}
{"task_id": "BigCodeBench/359", "solution": "from scipy import stats\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data_dict, data_keys):\n    \"\"\"\n    Calculates the correlation between two data series and returns a scatter plot along with the correlation coefficient.\n    \"\"\"\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Calculate the correlation\n    correlation, _ = stats.pearsonr(df[data_keys[0]], df[data_keys[1]])\n\n    # Create the scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(df[data_keys[0]], df[data_keys[1]])\n    ax.set_xlabel(data_keys[0])\n    ax.set_ylabel(data_keys[1])\n    ax.set_title(f'Scatter plot of {data_keys[0]} and {data_keys[1]}, correlation = {correlation:.2f}')\n\n    return correlation, ax.figure", "raw_solution": "from scipy import stats\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data_dict, data_keys):\n    \"\"\"\n    Calculates the correlation between two data series and returns a scatter plot along with the correlation coefficient.\n    \"\"\"\n    # Convert the dictionary to a pandas DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Calculate the correlation\n    correlation, _ = stats.pearsonr(df[data_keys[0]], df[data_keys[1]])\n\n    # Create the scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(df[data_keys[0]], df[data_keys[1]])\n    ax.set_xlabel(data_keys[0])\n    ax.set_ylabel(data_keys[1])\n    ax.set_title(f'Scatter plot of {data_keys[0]} and {data_keys[1]}, correlation = {correlation:.2f}')\n\n    return correlation, ax.figure"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    Loads data from an Excel spreadsheet (.xlsx), calculates the mean and standard deviation of each column,\n    and draws a bar chart.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(file_location):\n        raise FileNotFoundError(f\"The file {file_location} does not exist.\")\n\n    # Load data from Excel\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except KeyError:\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the workbook.\")\n\n    # Calculate mean and standard deviation\n    stats = df.describe().T.astype(float)\n    stats['mean'] = stats['mean'].round(2)\n    stats['std'] = stats['std'].round(2)\n    stats_dict = stats.to_dict()\n\n    # Draw bar chart\n    fig, ax = plt.subplots()\n    ax.bar(stats.index, stats['mean'], yerr=stats['std'], align='center', alpha=0.5, ecolor='black', capsize=10)\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n\n    return stats_dict, fig", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    Loads data from an Excel spreadsheet (.xlsx), calculates the mean and standard deviation of each column,\n    and draws a bar chart.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(file_location):\n        raise FileNotFoundError(f\"The file {file_location} does not exist.\")\n\n    # Load data from Excel\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except KeyError:\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the workbook.\")\n\n    # Calculate mean and standard deviation\n    stats = df.describe().T.astype(float)\n    stats['mean'] = stats['mean'].round(2)\n    stats['std'] = stats['std'].round(2)\n    stats_dict = stats.to_dict()\n\n    # Draw bar chart\n    fig, ax = plt.subplots()\n    ax.bar(stats.index, stats['mean'], yerr=stats['std'], align='center', alpha=0.5, ecolor='black', capsize=10)\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n\n    return stats_dict, fig"}
{"task_id": "BigCodeBench/361", "solution": "import pandas as pd\nimport logging\nimport os\ndef task_func(sheet_name, excel_file_location=\"test.xlsx\", csv_file_location=\"test.csv\"):\n    \"\"\"\n    Reads data from an Excel spreadsheet, converts it to a CSV file, then calculates the sum of each column in the CSV file.\n    \"\"\"\n    # Check if the Excel file exists\n    if not os.path.isfile(excel_file_location):\n        logging.error(f\"FileNotFoundError: The file {excel_file_location} does not exist.\")\n        raise FileNotFoundError(f\"The file {excel_file_location} does not exist.\")\n\n    # Read the Excel file\n    try:\n        df = pd.read_excel(excel_file_location, sheet_name=sheet_name)\n    except KeyError:\n        logging.error(f\"ValueError: The sheet {sheet_name} does not exist in the file {excel_file_location}.\")\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the file {excel_file_location}.\")\n\n    # Write the data to a CSV file\n    df.to_csv(csv_file_location, index=False)\n\n    # Read the CSV file\n    df = pd.read_csv(csv_file_location)\n\n    # Calculate the sum of each column\n    column_sums = df.sum().to_dict()\n\n    logging.info(f\"The sum of each column is: {column_sums}\")\n\n    return column_sums", "raw_solution": "import pandas as pd\nimport logging\nimport os\n\n# Set up basic configuration for logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef task_func(sheet_name, excel_file_location=\"test.xlsx\", csv_file_location=\"test.csv\"):\n    \"\"\"\n    Reads data from an Excel spreadsheet, converts it to a CSV file, then calculates the sum of each column in the CSV file.\n    \"\"\"\n    # Check if the Excel file exists\n    if not os.path.isfile(excel_file_location):\n        logging.error(f\"FileNotFoundError: The file {excel_file_location} does not exist.\")\n        raise FileNotFoundError(f\"The file {excel_file_location} does not exist.\")\n\n    # Read the Excel file\n    try:\n        df = pd.read_excel(excel_file_location, sheet_name=sheet_name)\n    except KeyError:\n        logging.error(f\"ValueError: The sheet {sheet_name} does not exist in the file {excel_file_location}.\")\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the file {excel_file_location}.\")\n\n    # Write the data to a CSV file\n    df.to_csv(csv_file_location, index=False)\n\n    # Read the CSV file\n    df = pd.read_csv(csv_file_location)\n\n    # Calculate the sum of each column\n    column_sums = df.sum().to_dict()\n\n    logging.info(f\"The sum of each column is: {column_sums}\")\n\n    return column_sums"}
{"task_id": "BigCodeBench/362", "solution": "import pandas as pd\nimport os\ndef task_func(original_file_location=\"test.xlsx\", new_file_location=\"new_test.xlsx\", sheet_name=\"Sheet1\"):\n    \"\"\"\n    Copies data from an Excel spreadsheet into a new Excel file, then reads the new Excel file and returns its contents.\n    \"\"\"\n    # Check if original file exists\n    if not os.path.isfile(original_file_location):\n        raise FileNotFoundError(f\"The file {original_file_location} does not exist.\")\n\n    # Read the original file\n    df = pd.read_excel(original_file_location, sheet_name=sheet_name)\n\n    # Write the data to a new Excel file\n    df.to_excel(new_file_location, index=False)\n\n    # Check if new file exists\n    if not os.path.isfile(new_file_location):\n        raise FileNotFoundError(f\"The file {new_file_location} does not exist after writing.\")\n\n    # Read the new file\n    df_new = pd.read_excel(new_file_location)\n\n    return df_new", "raw_solution": "import pandas as pd\nimport os\n\ndef task_func(original_file_location=\"test.xlsx\", new_file_location=\"new_test.xlsx\", sheet_name=\"Sheet1\"):\n    \"\"\"\n    Copies data from an Excel spreadsheet into a new Excel file, then reads the new Excel file and returns its contents.\n    \"\"\"\n    # Check if original file exists\n    if not os.path.isfile(original_file_location):\n        raise FileNotFoundError(f\"The file {original_file_location} does not exist.\")\n\n    # Read the original file\n    df = pd.read_excel(original_file_location, sheet_name=sheet_name)\n\n    # Write the data to a new Excel file\n    df.to_excel(new_file_location, index=False)\n\n    # Check if new file exists\n    if not os.path.isfile(new_file_location):\n        raise FileNotFoundError(f\"The file {new_file_location} does not exist after writing.\")\n\n    # Read the new file\n    df_new = pd.read_excel(new_file_location)\n\n    return df_new"}
{"task_id": "BigCodeBench/363", "solution": "from multiprocessing import Pool\nimport math\ndef calculate_factorial(number: int) -> tuple:\n    if not isinstance(number, int) or number < 0:\n        raise ValueError(\"Input must be a non-negative integer\")\n    return number, math.factorial(number)\ndef task_func(numbers: list) -> dict:\n    with Pool(processes=len(numbers)) as pool:\n        result = pool.map(calculate_factorial, numbers)\n    return dict(result)", "raw_solution": "from multiprocessing import Pool\nimport math\n\ndef calculate_factorial(number: int) -> tuple:\n    if not isinstance(number, int) or number < 0:\n        raise ValueError(\"Input must be a non-negative integer\")\n    return number, math.factorial(number)\n\ndef task_func(numbers: list) -> dict:\n    with Pool(processes=len(numbers)) as pool:\n        result = pool.map(calculate_factorial, numbers)\n    return dict(result)\n\n# Testing"}
{"task_id": "BigCodeBench/364", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nFEATURES = ['feature '+str(i) for i in range(1, 11)]\nTARGET = 'target'\ndef task_func(df):\n    \"\"\"\n    Trains a linear regression model on the given DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    # Split the data into features and target\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    return model", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nFEATURES = ['feature '+str(i) for i in range(1, 11)]\nTARGET = 'target'\n\ndef task_func(df):\n    \"\"\"\n    Trains a linear regression model on the given DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df is not a DataFrame.\")\n\n    # Split the data into features and target\n    X = df[FEATURES]\n    y = df[TARGET]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    return model"}
{"task_id": "BigCodeBench/365", "solution": "from collections import Counter\nimport json\nimport random\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\ndef task_func(n, file_name, seed=77):\n    \"\"\"\n    This function creates a json file with a number of n randomly selected words from a constant list named WORDS.\n    The function returns the name of the json file generated.\n    \"\"\"\n    random.seed(seed)\n    selected_words = random.sample(WORDS, n)\n\n    with open(file_name, 'w') as f:\n        json.dump(selected_words, f)\n\n    return file_name", "raw_solution": "from collections import Counter\nimport json\nimport random\n\n# Constants\nWORDS = ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew']\n\ndef task_func(n, file_name, seed=77):\n    \"\"\"\n    This function creates a json file with a number of n randomly selected words from a constant list named WORDS.\n    The function returns the name of the json file generated.\n    \"\"\"\n    random.seed(seed)\n    selected_words = random.sample(WORDS, n)\n\n    with open(file_name, 'w') as f:\n        json.dump(selected_words, f)\n\n    return file_name"}
{"task_id": "BigCodeBench/366", "solution": "import matplotlib.pyplot as plt\nimport random\nCOLORS = ['#00bfbf', '#000000', '#0000ff']\ndef task_func(number_list, bins):\n    \"\"\"\n    Creates a histogram subplot of a list of numbers.\n    \"\"\"\n    # Generate a random color\n    color = random.choice(COLORS)\n\n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(number_list, bins=bins, color=color)\n\n    # Set the plot title and labels\n    ax.set_title('Histogram')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport random\n\n# Constants\nCOLORS = ['#00bfbf', '#000000', '#0000ff']\n\ndef task_func(number_list, bins):\n    \"\"\"\n    Creates a histogram subplot of a list of numbers.\n    \"\"\"\n    # Generate a random color\n    color = random.choice(COLORS)\n\n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(number_list, bins=bins, color=color)\n\n    # Set the plot title and labels\n    ax.set_title('Histogram')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    return ax"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects\")\n\n    # Create a dictionary to store the count of activities for each day of the week\n    activity_counts = defaultdict(int)\n    for activity in activities:\n        activity_counts[activity.strftime(\"%A\")] += 1\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(activity_counts.keys(), activity_counts.values())\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    return ax", "raw_solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects\")\n\n    # Create a dictionary to store the count of activities for each day of the week\n    activity_counts = defaultdict(int)\n    for activity in activities:\n        activity_counts[activity.strftime(\"%A\")] += 1\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(activity_counts.keys(), activity_counts.values())\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    return ax"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n\n    # Filter out directories\n    files = [f for f in files if os.path.isfile(os.path.join(src_dir, f))]\n\n    # If there are no files in the source directory, return an empty string\n    if not files:\n        return \"\"\n\n    # Select a random file\n    file_name = random.choice(files)\n\n    # Construct the full path to the file\n    src_path = os.path.join(src_dir, file_name)\n    dest_path = os.path.join(dest_dir, file_name)\n\n    # Move the file\n    shutil.move(src_path, dest_path)\n\n    # Return the name of the file moved\n    return file_name", "raw_solution": "import os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n\n    # Filter out directories\n    files = [f for f in files if os.path.isfile(os.path.join(src_dir, f))]\n\n    # If there are no files in the source directory, return an empty string\n    if not files:\n        return \"\"\n\n    # Select a random file\n    file_name = random.choice(files)\n\n    # Construct the full path to the file\n    src_path = os.path.join(src_dir, file_name)\n    dest_path = os.path.join(dest_dir, file_name)\n\n    # Move the file\n    shutil.move(src_path, dest_path)\n\n    # Return the name of the file moved\n    return file_name"}
{"task_id": "BigCodeBench/369", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(l):\n    # Convert list to numpy array\n    arr = np.array(l)\n\n    # Fit a Gaussian distribution to the data\n    mu, std = stats.norm.fit(arr)\n\n    # Create a histogram of the data\n    plt.hist(arr, bins='auto', alpha=0.7, rwidth=0.85, color='#003f5c', density=True)\n\n    # Generate a range of x values for the fitted Gaussian distribution\n    x_range = np.linspace(min(arr), max(arr), 1000)\n\n    # Plot the fitted Gaussian distribution\n    plt.plot(x_range, stats.norm.pdf(x_range, mu, std), color='#ff7c43')\n\n    # Set the title of the plot\n    plt.title(f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\")\n\n    # Return the axes object\n    ax = plt.gca()\n    return ax", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(l):\n    # Convert list to numpy array\n    arr = np.array(l)\n\n    # Fit a Gaussian distribution to the data\n    mu, std = stats.norm.fit(arr)\n\n    # Create a histogram of the data\n    plt.hist(arr, bins='auto', alpha=0.7, rwidth=0.85, color='#003f5c', density=True)\n\n    # Generate a range of x values for the fitted Gaussian distribution\n    x_range = np.linspace(min(arr), max(arr), 1000)\n\n    # Plot the fitted Gaussian distribution\n    plt.plot(x_range, stats.norm.pdf(x_range, mu, std), color='#ff7c43')\n\n    # Set the title of the plot\n    plt.title(f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\")\n\n    # Return the axes object\n    ax = plt.gca()\n    return ax"}
{"task_id": "BigCodeBench/370", "solution": "import os\nimport re\nimport json\nimport glob\ndef task_func(directory_path: str) -> list:\n    \"\"\"\n    Protects all double quotes in all JSON files in the specified directory by prepending them with a double backslash.\n    \"\"\"\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The directory {directory_path} does not exist.\")\n\n    json_files = glob.glob(os.path.join(directory_path, \"**/*.json\"), recursive=True)\n    processed_files = []\n\n    for json_file in json_files:\n        with open(json_file, 'r') as file:\n            data = json.load(file)\n\n        # Convert the data to a string and escape double quotes\n        data_str = json.dumps(data)\n        data_str = data_str.replace('\"', '\\\\\"')\n\n        with open(json_file, 'w') as file:\n            file.write(data_str)\n\n        processed_files.append(json_file)\n\n    return processed_files", "raw_solution": "import os\nimport re\nimport json\nimport glob\n\ndef task_func(directory_path: str) -> list:\n    \"\"\"\n    Protects all double quotes in all JSON files in the specified directory by prepending them with a double backslash.\n    \"\"\"\n    if not os.path.exists(directory_path):\n        raise FileNotFoundError(f\"The directory {directory_path} does not exist.\")\n\n    json_files = glob.glob(os.path.join(directory_path, \"**/*.json\"), recursive=True)\n    processed_files = []\n\n    for json_file in json_files:\n        with open(json_file, 'r') as file:\n            data = json.load(file)\n\n        # Convert the data to a string and escape double quotes\n        data_str = json.dumps(data)\n        data_str = data_str.replace('\"', '\\\\\"')\n\n        with open(json_file, 'w') as file:\n            file.write(data_str)\n\n        processed_files.append(json_file)\n\n    return processed_files"}
{"task_id": "BigCodeBench/371", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(l):\n    \"\"\"\n    Scale the input field to the range [0, 1] and display it as a DataFrame.\n    \"\"\"\n    # Convert the list to a numpy array\n    import numpy as np\n    np_array = np.array(l).reshape(-1, 1)\n\n    # Initialize a MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    scaled_array = scaler.fit_transform(np_array)\n\n    # Create a DataFrame\n    df = pd.DataFrame(scaled_array, columns=['Scaled Values'])\n\n    return df", "raw_solution": "from sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(l):\n    \"\"\"\n    Scale the input field to the range [0, 1] and display it as a DataFrame.\n    \"\"\"\n    # Convert the list to a numpy array\n    import numpy as np\n    np_array = np.array(l).reshape(-1, 1)\n\n    # Initialize a MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    scaled_array = scaler.fit_transform(np_array)\n\n    # Create a DataFrame\n    df = pd.DataFrame(scaled_array, columns=['Scaled Values'])\n\n    return df\n\n# Test the function\ndf = task_func([1, 2, 3, 4, 5])"}
{"task_id": "BigCodeBench/372", "solution": "import re\nimport glob\nfrom docx import Document\ndef task_func(directory_path: str) -> int:\n    \"\"\"\n    Processes all Word (.docx) files in the provided directory, searching for double quotes in the text and adding a backslash before each double quote to \"protect\" it.\n    \"\"\"\n    # Get all .docx files in the directory\n    files = glob.glob(f\"{directory_path}/*.docx\")\n\n    # Initialize counter\n    count = 0\n\n    # Process each file\n    for file in files:\n        # Open the document\n        doc = Document(file)\n\n        # Iterate over each paragraph in the document\n        for paragraph in doc.paragraphs:\n            # Check if the paragraph contains a double quote\n            if '\"' in paragraph.text:\n                # Add a backslash before each double quote\n                paragraph.text = re.sub(r'\"', r'\\\\\"', paragraph.text)\n\n        # Save the document\n        doc.save(file)\n\n        # Increment the counter\n        count += 1\n\n    # Return the number of files processed\n    return count", "raw_solution": "import re\nimport glob\nfrom docx import Document\n\ndef task_func(directory_path: str) -> int:\n    \"\"\"\n    Processes all Word (.docx) files in the provided directory, searching for double quotes in the text and adding a backslash before each double quote to \"protect\" it.\n    \"\"\"\n    # Get all .docx files in the directory\n    files = glob.glob(f\"{directory_path}/*.docx\")\n\n    # Initialize counter\n    count = 0\n\n    # Process each file\n    for file in files:\n        # Open the document\n        doc = Document(file)\n\n        # Iterate over each paragraph in the document\n        for paragraph in doc.paragraphs:\n            # Check if the paragraph contains a double quote\n            if '\"' in paragraph.text:\n                # Add a backslash before each double quote\n                paragraph.text = re.sub(r'\"', r'\\\\\"', paragraph.text)\n\n        # Save the document\n        doc.save(file)\n\n        # Increment the counter\n        count += 1\n\n    # Return the number of files processed\n    return count"}
{"task_id": "BigCodeBench/373", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(l, x_data, plot=False):\n    def func(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    popt, _ = curve_fit(func, x_data, l)\n    fitted_values = func(x_data, *popt)\n\n    if plot:\n        fig, ax = plt.subplots()\n        ax.plot(x_data, l, 'ko', label=\"Original Data\")\n        ax.plot(x_data, fitted_values, 'r-', label=\"Fitted Curve\")\n        ax.legend()\n        plt.show()\n        return popt, fitted_values, ax\n    else:\n        return popt, fitted_values\nx = np.linspace(0, 10, 100)", "raw_solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(l, x_data, plot=False):\n    def func(x, a, b, c):\n        return a * x**2 + b * x + c\n\n    popt, _ = curve_fit(func, x_data, l)\n    fitted_values = func(x_data, *popt)\n\n    if plot:\n        fig, ax = plt.subplots()\n        ax.plot(x_data, l, 'ko', label=\"Original Data\")\n        ax.plot(x_data, fitted_values, 'r-', label=\"Fitted Curve\")\n        ax.legend()\n        plt.show()\n        return popt, fitted_values, ax\n    else:\n        return popt, fitted_values\n\n# Test the function\nx = np.linspace(0, 10, 100)\ny = 3*x**2 + 2*x + 1 + np.random.normal(0, 10, 100)\nparams, fitted_values, ax = task_func(y, x, plot=True)"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Protects all double quotes in all Excel (.xlsx) files in the specified directory by prefixing them with a double backslash.\n    \"\"\"\n    # Get all .xlsx files in the directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Initialize counter\n    num_files_processed = 0\n\n    # Process each file\n    for file in xlsx_files:\n        # Load workbook\n        wb = load_workbook(filename=file)\n\n        # Iterate over each worksheet\n        for sheet in wb:\n            # Iterate over each cell\n            for row in sheet:\n                for cell in row:\n                    # If cell contains double quotes, replace them with double backslashes\n                    if cell and '\"' in cell.value:\n                        cell.value = re.sub(r'\"', '\\\\\"', cell.value)\n\n        # Save workbook\n        wb.save(file)\n\n        # Increment counter\n        num_files_processed += 1\n\n    return num_files_processed", "raw_solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Protects all double quotes in all Excel (.xlsx) files in the specified directory by prefixing them with a double backslash.\n    \"\"\"\n    # Get all .xlsx files in the directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Initialize counter\n    num_files_processed = 0\n\n    # Process each file\n    for file in xlsx_files:\n        # Load workbook\n        wb = load_workbook(filename=file)\n\n        # Iterate over each worksheet\n        for sheet in wb:\n            # Iterate over each cell\n            for row in sheet:\n                for cell in row:\n                    # If cell contains double quotes, replace them with double backslashes\n                    if cell and '\"' in cell.value:\n                        cell.value = re.sub(r'\"', '\\\\\"', cell.value)\n\n        # Save workbook\n        wb.save(file)\n\n        # Increment counter\n        num_files_processed += 1\n\n    return num_files_processed"}
{"task_id": "BigCodeBench/375", "solution": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(l):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given array and record the first two main components.\n    \"\"\"\n    # Convert list to numpy array\n    arr = np.array(l)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    pca.fit(arr)\n\n    # Get the first two components\n    first_component = pca.components_[0]\n    second_component = pca.components_[1]\n\n    # Create a scatter plot\n    plt.scatter(arr.dot(first_component), arr.dot(second_component))\n    plt.title('PCA Result')\n    plt.xlabel('First Principal Component')\n    plt.ylabel('Second Principal Component')\n\n    # Return the axes object\n    ax = plt.gca()\n    return ax", "raw_solution": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(l):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given array and record the first two main components.\n    \"\"\"\n    # Convert list to numpy array\n    arr = np.array(l)\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    pca.fit(arr)\n\n    # Get the first two components\n    first_component = pca.components_[0]\n    second_component = pca.components_[1]\n\n    # Create a scatter plot\n    plt.scatter(arr.dot(first_component), arr.dot(second_component))\n    plt.title('PCA Result')\n    plt.xlabel('First Principal Component')\n    plt.ylabel('Second Principal Component')\n\n    # Return the axes object\n    ax = plt.gca()\n    return ax"}
{"task_id": "BigCodeBench/376", "solution": "import nltk\nimport re\nfrom collections import Counter\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(text):\n    # Split the text into words, convert to lowercase, remove punctuation and common stopwords\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Calculate the frequency of each word\n    word_freq = dict(Counter(words))\n\n    return word_freq\ntext = \"This is a sample text. It contains several words, some of which are repeated. Let's see how the function works.\"", "raw_solution": "import nltk\nimport re\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef task_func(text):\n    # Split the text into words, convert to lowercase, remove punctuation and common stopwords\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    words = [word for word in words if word not in STOPWORDS]\n\n    # Calculate the frequency of each word\n    word_freq = dict(Counter(words))\n\n    return word_freq\n\n# Test the function\ntext = \"This is a sample text. It contains several words, some of which are repeated. Let's see how the function works.\""}
{"task_id": "BigCodeBench/377", "solution": "from texttable import Texttable\nimport psutil\ndef task_func():\n    def get_size(bytes, suffix=\"B\"):\n        \"\"\"\n        Scale bytes to its proper format\n        e.g:\n            1253656 => '1.20MB'\n            1253656672 => '1.17GB'\n        \"\"\"\n        factor = 1024\n        for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n            if bytes < factor:\n                return f\"{bytes:.2f}{unit}{suffix}\"\n            bytes /= factor\n\n    def get_cpu_usage():\n        return psutil.cpu_percent(interval=1)\n\n    def get_memory_usage():\n        svmem = psutil.virtual_memory()\n        return svmem.percent\n\n    def get_disk_usage():\n        disk = psutil.disk_usage('/')\n        return disk.percent\n\n    def create_table():\n        table = Texttable()\n        table.add_rows([\n            [\"Item\", \"Value\"],\n            [\"CPU Usage (%)\", get_cpu_usage()],\n            [\"Memory Usage (%)\", get_memory_usage()],\n            [\"Disk Usage (%)\", get_disk_usage()]\n        ])\n        return table.draw()\n\n    return create_table()", "raw_solution": "from texttable import Texttable\nimport psutil\n\ndef task_func():\n    def get_size(bytes, suffix=\"B\"):\n        \"\"\"\n        Scale bytes to its proper format\n        e.g:\n            1253656 => '1.20MB'\n            1253656672 => '1.17GB'\n        \"\"\"\n        factor = 1024\n        for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n            if bytes < factor:\n                return f\"{bytes:.2f}{unit}{suffix}\"\n            bytes /= factor\n\n    def get_cpu_usage():\n        return psutil.cpu_percent(interval=1)\n\n    def get_memory_usage():\n        svmem = psutil.virtual_memory()\n        return svmem.percent\n\n    def get_disk_usage():\n        disk = psutil.disk_usage('/')\n        return disk.percent\n\n    def create_table():\n        table = Texttable()\n        table.add_rows([\n            [\"Item\", \"Value\"],\n            [\"CPU Usage (%)\", get_cpu_usage()],\n            [\"Memory Usage (%)\", get_memory_usage()],\n            [\"Disk Usage (%)\", get_disk_usage()]\n        ])\n        return table.draw()\n\n    return create_table()\n"}
{"task_id": "BigCodeBench/378", "solution": "import pandas as pd\nfrom texttable import Texttable\nimport os\nimport glob\ndef task_func(data_dir='./data/'):\n    \"\"\"\n    Generates a summary table of all ascendingly sorted CSV files in a specified directory using Texttable.\n    \"\"\"\n    # Check if directory exists\n    if not os.path.isdir(data_dir):\n        raise FileNotFoundError(f\"Directory {data_dir} does not exist.\")\n\n    # Get all CSV files in the directory\n    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n\n    # Check if there are CSV files in the directory\n    if not csv_files:\n        raise ValueError(f\"No CSV files found in directory {data_dir}.\")\n\n    # Initialize Texttable\n    t = Texttable()\n    t.add_rows([[\"File Name\", \"Number of Rows\", \"Number of Columns\"]])\n\n    # Process each CSV file\n    for csv_file in csv_files:\n        try:\n            # Read CSV file\n            df = pd.read_csv(csv_file)\n        except pd.errors.EmptyDataError:\n            raise pd.errors.EmptyDataError(f\"Empty CSV file encountered: {csv_file}\")\n\n        # Add row to table\n        t.add_row([os.path.basename(csv_file), df.shape[0], df.shape[1]])\n\n    # Return table as string\n    return t.draw()", "raw_solution": "import pandas as pd\nfrom texttable import Texttable\nimport os\nimport glob\n\ndef task_func(data_dir='./data/'):\n    \"\"\"\n    Generates a summary table of all ascendingly sorted CSV files in a specified directory using Texttable.\n    \"\"\"\n    # Check if directory exists\n    if not os.path.isdir(data_dir):\n        raise FileNotFoundError(f\"Directory {data_dir} does not exist.\")\n\n    # Get all CSV files in the directory\n    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n\n    # Check if there are CSV files in the directory\n    if not csv_files:\n        raise ValueError(f\"No CSV files found in directory {data_dir}.\")\n\n    # Initialize Texttable\n    t = Texttable()\n    t.add_rows([[\"File Name\", \"Number of Rows\", \"Number of Columns\"]])\n\n    # Process each CSV file\n    for csv_file in csv_files:\n        try:\n            # Read CSV file\n            df = pd.read_csv(csv_file)\n        except pd.errors.EmptyDataError:\n            raise pd.errors.EmptyDataError(f\"Empty CSV file encountered: {csv_file}\")\n\n        # Add row to table\n        t.add_row([os.path.basename(csv_file), df.shape[0], df.shape[1]])\n\n    # Return table as string\n    return t.draw()"}
{"task_id": "BigCodeBench/379", "solution": "import pandas as pd\nimport numpy as np\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length):\n    \"\"\"\n    Generate a pandas DataFrame with specified length and random data.\n    \"\"\"\n    # Generate random data\n    data = np.random.randint(0, 100, size=(length, len(COLUMNS)))\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(length):\n    \"\"\"\n    Generate a pandas DataFrame with specified length and random data.\n    \"\"\"\n    # Generate random data\n    data = np.random.randint(0, 100, size=(length, len(COLUMNS)))\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    return df\n\n# Test the function\ndf = task_func(10)"}
{"task_id": "BigCodeBench/380", "solution": "import os\nimport shutil\ndef task_func(directory):\n    \"\"\"\n    Arrange files in a directory by their extensions.\n    Create a new directory for each extension and move the files to the corresponding directories.\n    \"\"\"\n    # Create a dictionary to store files by extension\n    files_by_ext = {}\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Get the file extension\n        ext = os.path.splitext(filename)[1]\n\n        # If the extension is not in the dictionary, create a new list for it\n        if ext not in files_by_ext:\n            files_by_ext[ext] = []\n\n        # Add the file to the list for its extension\n        files_by_ext[ext].append(filename)\n\n    # Create a new directory for each extension\n    for ext, files in files_by_ext.items():\n        # Create the new directory\n        new_dir = os.path.join(directory, ext)\n        os.makedirs(new_dir, exist_ok=True)\n\n        # Move the files to the new directory\n        for filename in files:\n            old_path = os.path.join(directory, filename)\n            new_path = os.path.join(new_dir, filename)\n            shutil.move(old_path, new_path)\n\n    return None", "raw_solution": "import os\nimport shutil\n\ndef task_func(directory):\n    \"\"\"\n    Arrange files in a directory by their extensions.\n    Create a new directory for each extension and move the files to the corresponding directories.\n    \"\"\"\n    # Create a dictionary to store files by extension\n    files_by_ext = {}\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Get the file extension\n        ext = os.path.splitext(filename)[1]\n\n        # If the extension is not in the dictionary, create a new list for it\n        if ext not in files_by_ext:\n            files_by_ext[ext] = []\n\n        # Add the file to the list for its extension\n        files_by_ext[ext].append(filename)\n\n    # Create a new directory for each extension\n    for ext, files in files_by_ext.items():\n        # Create the new directory\n        new_dir = os.path.join(directory, ext)\n        os.makedirs(new_dir, exist_ok=True)\n\n        # Move the files to the new directory\n        for filename in files:\n            old_path = os.path.join(directory, filename)\n            new_path = os.path.join(new_dir, filename)\n            shutil.move(old_path, new_path)\n\n    return None"}
{"task_id": "BigCodeBench/381", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_path='arena.csv', target_column='Index', seed=42):\n    \"\"\"\n    Trains a random forest model on data from a CSV file, using one column as the target variable (y) and the rest as features (X), and visualizes the feature importances in a bar plot.\n    This function also handles missing values by dropping rows with any NaN values.\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    df = pd.read_csv(file_path)\n\n    if target_column not in df.columns:\n        raise ValueError(f\"The target column {target_column} is not found in the dataframe.\")\n\n    df = df.dropna()\n    df = df[df.abs() < 1e32].dropna()  # Remove rows with infinity or values too large for dtype('float32')\n\n    y = df[target_column]\n    X = df.drop(columns=[target_column])\n\n    rf = RandomForestRegressor(random_state=seed)\n    rf.fit(X, y)\n\n    importances = rf.feature_importances_\n\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x=X.columns, y=importances)\n    plt.xlabel('Features')\n    plt.ylabel('Importance')\n    plt.title('Feature Importances')\n    ax = plt.gca()\n\n    return ax, importances", "raw_solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_path='arena.csv', target_column='Index', seed=42):\n    \"\"\"\n    Trains a random forest model on data from a CSV file, using one column as the target variable (y) and the rest as features (X), and visualizes the feature importances in a bar plot.\n    This function also handles missing values by dropping rows with any NaN values.\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    df = pd.read_csv(file_path)\n\n    if target_column not in df.columns:\n        raise ValueError(f\"The target column {target_column} is not found in the dataframe.\")\n\n    df = df.dropna()\n    df = df[df.abs() < 1e32].dropna()  # Remove rows with infinity or values too large for dtype('float32')\n\n    y = df[target_column]\n    X = df.drop(columns=[target_column])\n\n    rf = RandomForestRegressor(random_state=seed)\n    rf.fit(X, y)\n\n    importances = rf.feature_importances_\n\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x=X.columns, y=importances)\n    plt.xlabel('Features')\n    plt.ylabel('Importance')\n    plt.title('Feature Importances')\n    ax = plt.gca()\n\n    return ax, importances"}
{"task_id": "BigCodeBench/382", "solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(length):\n    \"\"\"\n    Generates a normal distribution with a given length, plots its histogram alongside the probability density function,\n    and returns the distribution and the plot.\n    \"\"\"\n    # Define the mean and standard deviation\n    mu, sigma = 0, 1\n\n    # Generate the normal distribution\n    distribution = np.random.normal(mu, sigma, length)\n\n    # Create the plot\n    fig, ax = plt.subplots(1, 1)\n\n    # Plot the histogram\n    ax.hist(distribution, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n\n    # Plot the probability density function\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2, label='PDF')\n\n    # Set plot labels\n    ax.set_title('Normal Distribution with PDF')\n    ax.legend()\n\n    return distribution, ax.figure", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(length):\n    \"\"\"\n    Generates a normal distribution with a given length, plots its histogram alongside the probability density function,\n    and returns the distribution and the plot.\n    \"\"\"\n    # Define the mean and standard deviation\n    mu, sigma = 0, 1\n\n    # Generate the normal distribution\n    distribution = np.random.normal(mu, sigma, length)\n\n    # Create the plot\n    fig, ax = plt.subplots(1, 1)\n\n    # Plot the histogram\n    ax.hist(distribution, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n\n    # Plot the probability density function\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2, label='PDF')\n\n    # Set plot labels\n    ax.set_title('Normal Distribution with PDF')\n    ax.legend()\n\n    return distribution, ax.figure"}
{"task_id": "BigCodeBench/383", "solution": "import pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nfrom textblob import TextBlob\nfrom matplotlib import pyplot as plt\ndef task_func(text, n, top_k):\n    \"\"\"\n    This function takes a text string, an integer n, and an integer top_k as input.\n    It generates n-grams from the text string, counts the frequency of each n-gram,\n    and plots the top k n-grams with their frequencies.\n    \"\"\"\n    # Generate n-grams\n    ngrams = zip(*[text[i:] for i in range(n)])\n    ngrams = [' '.join(ngram) for ngram in ngrams]\n\n    # Count the frequency of each n-gram\n    counter = Counter(ngrams)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame.from_dict(counter, orient='index', columns=['Frequency'])\n\n    # Sort the DataFrame by frequency\n    df = df.sort_values(by='Frequency', ascending=False)\n\n    # Get the top k n-grams\n    top_k_ngrams = df.head(top_k)\n\n    # Plot the top k n-grams\n    sns.barplot(x=top_k_ngrams.index, y=top_k_ngrams['Frequency'])\n    plt.xlabel('n-grams')\n    plt.ylabel('Frequency')\n    plt.title(f'Top {top_k} n-grams')\n    plt.xticks(rotation=90)\n    plt.show()\n\n    return None", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nfrom textblob import TextBlob\nfrom matplotlib import pyplot as plt\n\ndef task_func(text, n, top_k):\n    \"\"\"\n    This function takes a text string, an integer n, and an integer top_k as input.\n    It generates n-grams from the text string, counts the frequency of each n-gram,\n    and plots the top k n-grams with their frequencies.\n    \"\"\"\n    # Generate n-grams\n    ngrams = zip(*[text[i:] for i in range(n)])\n    ngrams = [' '.join(ngram) for ngram in ngrams]\n\n    # Count the frequency of each n-gram\n    counter = Counter(ngrams)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame.from_dict(counter, orient='index', columns=['Frequency'])\n\n    # Sort the DataFrame by frequency\n    df = df.sort_values(by='Frequency', ascending=False)\n\n    # Get the top k n-grams\n    top_k_ngrams = df.head(top_k)\n\n    # Plot the top k n-grams\n    sns.barplot(x=top_k_ngrams.index, y=top_k_ngrams['Frequency'])\n    plt.xlabel('n-grams')\n    plt.ylabel('Frequency')\n    plt.title(f'Top {top_k} n-grams')\n    plt.xticks(rotation=90)\n    plt.show()\n\n    return None"}
{"task_id": "BigCodeBench/384", "solution": "import collections\nimport random\nimport itertools\ndef task_func(animal_dict, max_count=10, seed=0):\n    random.seed(seed)\n    reversed_dict = collections.defaultdict(list)\n    counter = collections.Counter()\n\n    for name, animal in animal_dict.items():\n        reversed_dict[animal].append(name)\n        counter[animal] = random.randint(1, max_count)\n\n    return (dict(reversed_dict), counter)\nanimal_dict = {'Alice': 'Cat', 'Bob': 'Dog', 'Charlie': 'Elephant', 'David': 'Lion', 'Eva': 'Tiger', 'Frank': 'Bear', 'Grace': 'Giraffe', 'Harry': 'Horse', 'Isla': 'Rabbit', 'John': 'Snake', 'Kevin': 'Zebra'}", "raw_solution": "import collections\nimport random\nimport itertools\n\nANIMALS = ['Cat', 'Dog', 'Elephant', 'Lion', 'Tiger', 'Bear', 'Giraffe', 'Horse', 'Rabbit', 'Snake', 'Zebra']\n\ndef task_func(animal_dict, max_count=10, seed=0):\n    random.seed(seed)\n    reversed_dict = collections.defaultdict(list)\n    counter = collections.Counter()\n\n    for name, animal in animal_dict.items():\n        reversed_dict[animal].append(name)\n        counter[animal] = random.randint(1, max_count)\n\n    return (dict(reversed_dict), counter)\n\n# Testing the function\nanimal_dict = {'Alice': 'Cat', 'Bob': 'Dog', 'Charlie': 'Elephant', 'David': 'Lion', 'Eva': 'Tiger', 'Frank': 'Bear', 'Grace': 'Giraffe', 'Harry': 'Horse', 'Isla': 'Rabbit', 'John': 'Snake', 'Kevin': 'Zebra'}"}
{"task_id": "BigCodeBench/385", "solution": "import matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(fruit_dict):\n    # Count the frequency of each fruit\n    fruit_counts = Counter(fruit_dict.values())\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(fruit_counts.keys(), fruit_counts.values())\n    ax.set_xlabel('Fruit')\n    ax.set_ylabel('Count')\n    ax.set_title('Fruit Counts')\n\n    return fruit_counts, ax\nfruit_dict = {\n    'Alice': 'Apple',\n    'Bob': 'Banana',\n    'Charlie': 'Cherry',\n    'David': 'Date',\n    'Eve': 'Elderberry',\n    'Frank': 'Fig',\n    'Grace': 'Grape',\n    'Harry': 'Honeydew',\n    'Isla': 'Indian Prune',\n    'Jack': 'Jackfruit',\n    'Kelly': 'Kiwi',\n    'Liam': 'Lemon',\n    'Mia': 'Mango',\n    'Nick': 'Nectarine',\n    'Oliver': 'Orange',\n    'Penny': 'Papaya',\n    'Quinn': 'Quince',\n    'Ryan': 'Raspberry',\n    'Sophia': 'Strawberry',\n    'Tobias': 'Tomato',\n    'Uma': 'Ugli Fruit',\n    'Victor': 'Victoria Plum',\n    'Wyatt': 'Watermelon',\n    'Xander': 'Xigua',\n    'Yara': 'Yangmei',\n    'Zachary': 'Zapote'\n}", "raw_solution": "import matplotlib.pyplot as plt\nfrom collections import Counter\n\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry', 'Fig', 'Grape', 'Honeydew', 'Indian Prune', 'Jackfruit']\n\ndef task_func(fruit_dict):\n    # Count the frequency of each fruit\n    fruit_counts = Counter(fruit_dict.values())\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(fruit_counts.keys(), fruit_counts.values())\n    ax.set_xlabel('Fruit')\n    ax.set_ylabel('Count')\n    ax.set_title('Fruit Counts')\n\n    return fruit_counts, ax\n\n# Test the function\nfruit_dict = {\n    'Alice': 'Apple',\n    'Bob': 'Banana',\n    'Charlie': 'Cherry',\n    'David': 'Date',\n    'Eve': 'Elderberry',\n    'Frank': 'Fig',\n    'Grace': 'Grape',\n    'Harry': 'Honeydew',\n    'Isla': 'Indian Prune',\n    'Jack': 'Jackfruit',\n    'Kelly': 'Kiwi',\n    'Liam': 'Lemon',\n    'Mia': 'Mango',\n    'Nick': 'Nectarine',\n    'Oliver': 'Orange',\n    'Penny': 'Papaya',\n    'Quinn': 'Quince',\n    'Ryan': 'Raspberry',\n    'Sophia': 'Strawberry',\n    'Tobias': 'Tomato',\n    'Uma': 'Ugli Fruit',\n    'Victor': 'Victoria Plum',\n    'Wyatt': 'Watermelon',\n    'Xander': 'Xigua',\n    'Yara': 'Yangmei',\n    'Zachary': 'Zapote'\n}\n\nfruit_counts, ax = task_func(fruit_dict)\nplt.show()"}
{"task_id": "BigCodeBench/386", "solution": "import numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length, min_value = 0, max_value = 100):\n    \"\"\"\n    Generates a pandas DataFrame with specified ranges and length, and calculates the cumulative distribution function (CDF).\n    \"\"\"\n    # Generate a DataFrame with random values\n    df = pd.DataFrame(np.random.randint(min_value, max_value, size=(length, len(COLUMNS))), columns=COLUMNS)\n\n    # Calculate the cumulative distribution function (CDF)\n    cdf = df.apply(lambda x: x.sort_values().values)\n\n    return cdf", "raw_solution": "import numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(length, min_value = 0, max_value = 100):\n    \"\"\"\n    Generates a pandas DataFrame with specified ranges and length, and calculates the cumulative distribution function (CDF).\n    \"\"\"\n    # Generate a DataFrame with random values\n    df = pd.DataFrame(np.random.randint(min_value, max_value, size=(length, len(COLUMNS))), columns=COLUMNS)\n\n    # Calculate the cumulative distribution function (CDF)\n    cdf = df.apply(lambda x: x.sort_values().values)\n\n    return cdf\n\n# Test the function"}
{"task_id": "BigCodeBench/387", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney', 'Paris', 'Berlin', 'Moscow', 'Madrid', 'Rome']\ndef task_func(city_dict, max_range=1000000, seed=0):\n    np.random.seed(seed)\n    city_population = {}\n    for city in CITIES:\n        if city in city_dict.values():\n            city_population[city] = np.random.randint(1, max_range)\n        else:\n            city_population[city] = -1\n\n    fig, ax = plt.subplots()\n    ax.bar(city_population.keys(), city_population.values())\n    ax.set_xlabel('Cities')\n    ax.set_ylabel('Population')\n    ax.set_title('City Population')\n    plt.show()\n\n    return city_population, ax\ncity_dict = {'John': 'New York', 'Jane': 'London', 'Bob': 'Berlin', 'Alice': 'Tokyo'}", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCITIES = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney', 'Paris', 'Berlin', 'Moscow', 'Madrid', 'Rome']\n\ndef task_func(city_dict, max_range=1000000, seed=0):\n    np.random.seed(seed)\n    city_population = {}\n    for city in CITIES:\n        if city in city_dict.values():\n            city_population[city] = np.random.randint(1, max_range)\n        else:\n            city_population[city] = -1\n\n    fig, ax = plt.subplots()\n    ax.bar(city_population.keys(), city_population.values())\n    ax.set_xlabel('Cities')\n    ax.set_ylabel('Population')\n    ax.set_title('City Population')\n    plt.show()\n\n    return city_population, ax\n\n# Testing the function\ncity_dict = {'John': 'New York', 'Jane': 'London', 'Bob': 'Berlin', 'Alice': 'Tokyo'}\npopulation_dict, ax = task_func(city_dict, max_range=1000000, seed=0)"}
{"task_id": "BigCodeBench/388", "solution": "import collections\nimport pandas as pd\ndef task_func(my_tuple, path_csv_files):\n    \"\"\"\n    Count the occurrences of each value in the specified columns in multiple CSV files.\n    \"\"\"\n    result = {}\n    for csv_file in path_csv_files:\n        df = pd.read_csv(csv_file)\n        for column in my_tuple:\n            if column in df.columns:\n                counts = df[column].value_counts()\n                result[column] = counts.to_dict()\n    return result", "raw_solution": "import collections\nimport pandas as pd\n\ndef task_func(my_tuple, path_csv_files):\n    \"\"\"\n    Count the occurrences of each value in the specified columns in multiple CSV files.\n    \"\"\"\n    result = {}\n    for csv_file in path_csv_files:\n        df = pd.read_csv(csv_file)\n        for column in my_tuple:\n            if column in df.columns:\n                counts = df[column].value_counts()\n                result[column] = counts.to_dict()\n    return result"}
{"task_id": "BigCodeBench/389", "solution": "import re\nimport os\nimport shutil\ndef task_func(directory):\n    \"\"\"\n    Finds files in the given directory that have filenames containing \"like\" or \"what\",\n    creates a new subdirectory called \"Interesting Files\", and moves those files to the new subdirectory.\n    \"\"\"\n    # Create the \"Interesting Files\" subdirectory if it doesn't exist\n    if not os.path.exists(os.path.join(directory, \"Interesting Files\")):\n        os.makedirs(os.path.join(directory, \"Interesting Files\"))\n\n    # List to store the names of the files moved\n    files_moved = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the filename contains \"like\" or \"what\"\n        if re.search(r'like|what', filename, re.IGNORECASE):\n            # Move the file to the \"Interesting Files\" subdirectory\n            shutil.move(os.path.join(directory, filename), os.path.join(directory, \"Interesting Files\"))\n            # Add the filename to the list of files moved\n            files_moved.append(filename)\n\n    # Return the list of files moved\n    return files_moved", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(directory):\n    \"\"\"\n    Finds files in the given directory that have filenames containing \"like\" or \"what\",\n    creates a new subdirectory called \"Interesting Files\", and moves those files to the new subdirectory.\n    \"\"\"\n    # Create the \"Interesting Files\" subdirectory if it doesn't exist\n    if not os.path.exists(os.path.join(directory, \"Interesting Files\")):\n        os.makedirs(os.path.join(directory, \"Interesting Files\"))\n\n    # List to store the names of the files moved\n    files_moved = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the filename contains \"like\" or \"what\"\n        if re.search(r'like|what', filename, re.IGNORECASE):\n            # Move the file to the \"Interesting Files\" subdirectory\n            shutil.move(os.path.join(directory, filename), os.path.join(directory, \"Interesting Files\"))\n            # Add the filename to the list of files moved\n            files_moved.append(filename)\n\n    # Return the list of files moved\n    return files_moved"}
{"task_id": "BigCodeBench/390", "solution": "import pandas as pd\nimport requests\nfrom io import StringIO\ndef task_func(csv_url_dict, sort_by_column=\"title\"):\n    \"\"\"\n    Fetches data from a given dictionary that includes a CSV URL and returns a pandas DataFrame sorted based on two specified columns.\n    \"\"\"\n    if not csv_url_dict:\n        raise ValueError(\"The dictionary is empty.\")\n    if \"URL\" not in csv_url_dict:\n        raise ValueError('The key \"URL\" does not exist in the dictionary.')\n\n    url = csv_url_dict[\"URL\"]\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        print (\"HTTP Error:\",errh)\n    except requests.exceptions.ConnectionError as errc:\n        print (\"Error Connecting:\",errc)\n    except requests.exceptions.Timeout as errt:\n        print (\"Timeout Error:\",errt)\n    except requests.exceptions.RequestException as err:\n        print (\"Something went wrong\",err)\n\n    data = response.content.decode('utf-8')\n    df = pd.read_csv(StringIO(data))\n    sorted_df = df.sort_values(by=[sort_by_column])\n    return sorted_df", "raw_solution": "import pandas as pd\nimport requests\nfrom io import StringIO\n\ndef task_func(csv_url_dict, sort_by_column=\"title\"):\n    \"\"\"\n    Fetches data from a given dictionary that includes a CSV URL and returns a pandas DataFrame sorted based on two specified columns.\n    \"\"\"\n    if not csv_url_dict:\n        raise ValueError(\"The dictionary is empty.\")\n    if \"URL\" not in csv_url_dict:\n        raise ValueError('The key \"URL\" does not exist in the dictionary.')\n\n    url = csv_url_dict[\"URL\"]\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        print (\"HTTP Error:\",errh)\n    except requests.exceptions.ConnectionError as errc:\n        print (\"Error Connecting:\",errc)\n    except requests.exceptions.Timeout as errt:\n        print (\"Timeout Error:\",errt)\n    except requests.exceptions.RequestException as err:\n        print (\"Something went wrong\",err)\n\n    data = response.content.decode('utf-8')\n    df = pd.read_csv(StringIO(data))\n    sorted_df = df.sort_values(by=[sort_by_column])\n    return sorted_df"}
{"task_id": "BigCodeBench/391", "solution": "import os\nimport glob\nimport shutil\ndef task_func(directory, archive_dir='archive'):\n    \"\"\"\n    Archives all JSON files in a given directory by moving them to a specified archive directory.\n    \"\"\"\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n\n    # Get all JSON files in the directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n\n    # Initialize error messages list\n    error_messages = []\n\n    # Move each JSON file to the archive directory\n    for json_file in json_files:\n        try:\n            shutil.move(json_file, archive_dir)\n        except Exception as e:\n            error_messages.append(f\"Failed to move {json_file} to {archive_dir}: {str(e)}\")\n\n    # Return a tuple with a boolean indicating success and a list of error messages\n    return (len(error_messages) == 0, error_messages)", "raw_solution": "import os\nimport glob\nimport shutil\n\ndef task_func(directory, archive_dir='archive'):\n    \"\"\"\n    Archives all JSON files in a given directory by moving them to a specified archive directory.\n    \"\"\"\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(archive_dir):\n        os.makedirs(archive_dir)\n\n    # Get all JSON files in the directory\n    json_files = glob.glob(os.path.join(directory, '*.json'))\n\n    # Initialize error messages list\n    error_messages = []\n\n    # Move each JSON file to the archive directory\n    for json_file in json_files:\n        try:\n            shutil.move(json_file, archive_dir)\n        except Exception as e:\n            error_messages.append(f\"Failed to move {json_file} to {archive_dir}: {str(e)}\")\n\n    # Return a tuple with a boolean indicating success and a list of error messages\n    return (len(error_messages) == 0, error_messages)"}
{"task_id": "BigCodeBench/392", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nCOLORS = ['r', 'g', 'b']\ndef task_func(df, group_col, value_col, group_name):\n    \"\"\"\n    Creates a bar subplot of a specific group from the input dataframe.\n    \"\"\"\n    # Check if group_name exists in df\n    if group_name not in df[group_col].values:\n        raise ValueError(f\"The group_name {group_name} does not exist in df.\")\n\n    # Filter df for the specific group\n    group_df = df[df[group_col] == group_name]\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(group_df[group_col], group_df[value_col], color=COLORS[0])\n\n    # Set chart title and labels\n    ax.set_title(f'Bar chart of {value_col} for {group_name}')\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef task_func(df, group_col, value_col, group_name):\n    \"\"\"\n    Creates a bar subplot of a specific group from the input dataframe.\n    \"\"\"\n    # Check if group_name exists in df\n    if group_name not in df[group_col].values:\n        raise ValueError(f\"The group_name {group_name} does not exist in df.\")\n\n    # Filter df for the specific group\n    group_df = df[df[group_col] == group_name]\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(group_df[group_col], group_df[value_col], color=COLORS[0])\n\n    # Set chart title and labels\n    ax.set_title(f'Bar chart of {value_col} for {group_name}')\n    ax.set_xlabel(group_col)\n    ax.set_ylabel(value_col)\n\n    return ax"}
{"task_id": "BigCodeBench/393", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mu, sigma, num_samples=1000, seed=77):\n    np.random.seed(seed)\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Create a figure with two subplots\n    fig, axs = plt.subplots(2, figsize=(10, 10))\n\n    # Histogram\n    axs[0].hist(samples, bins=30, density=True)\n    axs[0].set_title('Histogram')\n    axs[0].set_xlabel('Value')\n    axs[0].set_ylabel('Frequency')\n\n    # Q-Q plot\n    stats.probplot(samples, plot=axs[1])\n    axs[1].set_title('Q-Q Plot')\n\n    return fig", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(mu, sigma, num_samples=1000, seed=77):\n    np.random.seed(seed)\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mu, sigma, num_samples)\n\n    # Create a figure with two subplots\n    fig, axs = plt.subplots(2, figsize=(10, 10))\n\n    # Histogram\n    axs[0].hist(samples, bins=30, density=True)\n    axs[0].set_title('Histogram')\n    axs[0].set_xlabel('Value')\n    axs[0].set_ylabel('Frequency')\n\n    # Q-Q plot\n    stats.probplot(samples, plot=axs[1])\n    axs[1].set_title('Q-Q Plot')\n\n    return fig"}
{"task_id": "BigCodeBench/394", "solution": "import collections\nimport string\nimport random\ndef task_func(length, seed=0):\n    random.seed(seed)\n    random_string = ''.join(random.choices(string.ascii_letters, k=length))\n    frequency = collections.Counter(random_string)\n    return frequency", "raw_solution": "import collections\nimport string\nimport random\n\ndef task_func(length, seed=0):\n    random.seed(seed)\n    random_string = ''.join(random.choices(string.ascii_letters, k=length))\n    frequency = collections.Counter(random_string)\n    return frequency\n\n# Test the function"}
{"task_id": "BigCodeBench/395", "solution": "import re\nimport os\nimport glob\nimport natsort\nimport pandas as pd\ndef task_func(directory='./', file_pattern='*.txt', regex=r'([0-9]+)'):\n    \"\"\"\n    Extract numeric data from all text files matching a given pattern in a directory and compile it into a Pandas DataFrame.\n    \"\"\"\n    # Check if directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"Directory {directory} does not exist.\")\n\n    # Get all files matching the pattern\n    files = natsort.natsorted(glob.glob(os.path.join(directory, file_pattern)))\n\n    # Check if any files were found\n    if not files:\n        raise ValueError(f\"No files matching the pattern {file_pattern} were found in the directory {directory}.\")\n\n    # Initialize an empty list to store the data\n    data = []\n\n    # Loop over all files\n    for file in files:\n        # Read the file\n        with open(file, 'r') as f:\n            content = f.read()\n\n        # Extract numeric data using regex\n        numeric_data = re.findall(regex, content)\n\n        # If numeric data was found, add it to the data list\n        if numeric_data:\n            data.append({'Filename': file, 'Numeric Data': numeric_data})\n\n    # Convert the data list to a DataFrame\n    df = pd.DataFrame(data)\n\n    return df", "raw_solution": "import re\nimport os\nimport glob\nimport natsort\nimport pandas as pd\n\ndef task_func(directory='./', file_pattern='*.txt', regex=r'([0-9]+)'):\n    \"\"\"\n    Extract numeric data from all text files matching a given pattern in a directory and compile it into a Pandas DataFrame.\n    \"\"\"\n    # Check if directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"Directory {directory} does not exist.\")\n\n    # Get all files matching the pattern\n    files = natsort.natsorted(glob.glob(os.path.join(directory, file_pattern)))\n\n    # Check if any files were found\n    if not files:\n        raise ValueError(f\"No files matching the pattern {file_pattern} were found in the directory {directory}.\")\n\n    # Initialize an empty list to store the data\n    data = []\n\n    # Loop over all files\n    for file in files:\n        # Read the file\n        with open(file, 'r') as f:\n            content = f.read()\n\n        # Extract numeric data using regex\n        numeric_data = re.findall(regex, content)\n\n        # If numeric data was found, add it to the data list\n        if numeric_data:\n            data.append({'Filename': file, 'Numeric Data': numeric_data})\n\n    # Convert the data list to a DataFrame\n    df = pd.DataFrame(data)\n\n    return df"}
{"task_id": "BigCodeBench/396", "solution": "import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(mu, sigma, sample_size, seed=0):\n    \"\"\"\n    Create a Gaussian kernel density estimate diagram of a normal distribution with a given mean and a standard deviation using a random sample of a size determined by the sample_size parameter. The density diagram is plotted using default settings in a deterministic matplotlib plot. Return the axes object.\n    \"\"\"\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n\n    # Create a histogram using the sample data\n    ax = plt.hist(sample, bins=30, density=True)\n\n    # Plot the Gaussian kernel density estimate\n    x = np.linspace(min(sample), max(sample), 1000)\n    kde = stats.gaussian_kde(sample)\n    plt.plot(x, kde(x))\n\n    plt.title(f'Gaussian Kernel Density Estimate ($\\mu$={mu}, $\\sigma$={sigma})')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n\n    return ax", "raw_solution": "import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(mu, sigma, sample_size, seed=0):\n    \"\"\"\n    Create a Gaussian kernel density estimate diagram of a normal distribution with a given mean and a standard deviation using a random sample of a size determined by the sample_size parameter. The density diagram is plotted using default settings in a deterministic matplotlib plot. Return the axes object.\n    \"\"\"\n    np.random.seed(seed)\n    sample = np.random.normal(mu, sigma, sample_size)\n\n    # Create a histogram using the sample data\n    ax = plt.hist(sample, bins=30, density=True)\n\n    # Plot the Gaussian kernel density estimate\n    x = np.linspace(min(sample), max(sample), 1000)\n    kde = stats.gaussian_kde(sample)\n    plt.plot(x, kde(x))\n\n    plt.title(f'Gaussian Kernel Density Estimate ($\\mu$={mu}, $\\sigma$={sigma})')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n\n    return ax"}
{"task_id": "BigCodeBench/397", "solution": "import re\nimport urllib.request\nimport json\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\ndef task_func(API_URL):\n    try:\n        response = urllib.request.urlopen(API_URL)\n        data = response.read()\n        json_data = json.loads(data)\n        ip_address = json_data['ip']\n        if re.match(IP_REGEX, ip_address):\n            return ip_address\n        else:\n            return \"Invalid IP address\"\n    except Exception as e:\n        return str(e)\nAPI_URL = \"https://api.ipify.org?format=json\"", "raw_solution": "import re\nimport urllib.request\nimport json\n\n# Constants\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef task_func(API_URL):\n    try:\n        response = urllib.request.urlopen(API_URL)\n        data = response.read()\n        json_data = json.loads(data)\n        ip_address = json_data['ip']\n        if re.match(IP_REGEX, ip_address):\n            return ip_address\n        else:\n            return \"Invalid IP address\"\n    except Exception as e:\n        return str(e)\n\n# Test the function\nAPI_URL = \"https://api.ipify.org?format=json\""}
{"task_id": "BigCodeBench/398", "solution": "import json\nimport os\ndef task_func(file_path):\n    \"\"\"\n    Checks if the data in a JSON file is a list of dictionaries.\n    \"\"\"\n    if not os.path.exists(file_path):\n        return False\n\n    with open(file_path, 'r') as file:\n        try:\n            data = json.load(file)\n        except json.JSONDecodeError:\n            return False\n\n    if not isinstance(data, list):\n        return False\n\n    for item in data:\n        if not isinstance(item, dict):\n            return False\n\n    return True", "raw_solution": "import json\nimport os\n\ndef task_func(file_path):\n    \"\"\"\n    Checks if the data in a JSON file is a list of dictionaries.\n    \"\"\"\n    if not os.path.exists(file_path):\n        return False\n\n    with open(file_path, 'r') as file:\n        try:\n            data = json.load(file)\n        except json.JSONDecodeError:\n            return False\n\n    if not isinstance(data, list):\n        return False\n\n    for item in data:\n        if not isinstance(item, dict):\n            return False\n\n    return True\n\n# Test the function"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n\n    x = np.linspace(0, 2*np.pi, sample_size)\n    y_sin = np.sin(frequency * x)\n    y_cos = np.cos(frequency * x)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y_sin, label='Sine')\n    ax.plot(x, y_cos, label='Cosine')\n    ax.legend()\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Sine and Cosine Waves')\n    return fig, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n\n    x = np.linspace(0, 2*np.pi, sample_size)\n    y_sin = np.sin(frequency * x)\n    y_cos = np.cos(frequency * x)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y_sin, label='Sine')\n    ax.plot(x, y_cos, label='Cosine')\n    ax.legend()\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Sine and Cosine Waves')\n    return fig, ax"}
{"task_id": "BigCodeBench/400", "solution": "import json\nimport os\ndef task_func(directory, string):\n    \"\"\"\n    Searches for a specific string within the JSON data of files in a given directory and its subdirectories.\n    \"\"\"\n    # Initialize an empty list to store the file paths\n    file_paths = []\n\n    # Use os.walk to recursively traverse the directory and its subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file is a JSON file\n            if file.endswith('.json'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    try:\n                        data = json.load(f)\n                        # Use json.dumps to convert the data to a string and check if the string contains the given string\n                        if string in json.dumps(data):\n                            file_paths.append(file_path)\n                    except json.JSONDecodeError:\n                        # If the file is not a valid JSON file, skip it\n                        continue\n\n    return file_paths", "raw_solution": "import json\nimport os\n\ndef task_func(directory, string):\n    \"\"\"\n    Searches for a specific string within the JSON data of files in a given directory and its subdirectories.\n    \"\"\"\n    # Initialize an empty list to store the file paths\n    file_paths = []\n\n    # Use os.walk to recursively traverse the directory and its subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file is a JSON file\n            if file.endswith('.json'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    try:\n                        data = json.load(f)\n                        # Use json.dumps to convert the data to a string and check if the string contains the given string\n                        if string in json.dumps(data):\n                            file_paths.append(file_path)\n                    except json.JSONDecodeError:\n                        # If the file is not a valid JSON file, skip it\n                        continue\n\n    return file_paths"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    \"\"\"\n    Initializes a Flask-Mail instance for sending emails using the generated Flask application with the specified app_name.\n    \"\"\"\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables, use defaults if not available\n    app.config['MAIL_SERVER'] = os.environ.get('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.environ.get('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = os.environ.get('MAIL_USE_TLS', False) in ['True', 'true', '1']\n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD', None)\n\n    mail = Mail(app)\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']", "raw_solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    \"\"\"\n    Initializes a Flask-Mail instance for sending emails using the generated Flask application with the specified app_name.\n    \"\"\"\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables, use defaults if not available\n    app.config['MAIL_SERVER'] = os.environ.get('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.environ.get('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = os.environ.get('MAIL_USE_TLS', False) in ['True', 'true', '1']\n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD', None)\n\n    mail = Mail(app)\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']"}
{"task_id": "BigCodeBench/402", "solution": "import re\nimport requests\nimport json\nimport csv\nimport os\nAPI_URL = 'https://api.example.com/data'\ndef task_func(pattern):\n    \"\"\"\n    This function makes a GET request to an API, extracts data that matches a RegEx pattern, and writes it to a CSV file.\n    \"\"\"\n    # Make GET request\n    response = requests.get(API_URL)\n    data = json.loads(response.text)['data']\n\n    # Extract data that matches the pattern\n    matched_data = [d for d in data if re.search(pattern, d)]\n\n    # Write data to CSV file\n    csv_file_path = os.path.join(os.getcwd(), 'matched_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        for row in matched_data:\n            writer.writerow([row])\n\n    return csv_file_path", "raw_solution": "import re\nimport requests\nimport json\nimport csv\nimport os\n\n# Constants\nAPI_URL = 'https://api.example.com/data'\n\ndef task_func(pattern):\n    \"\"\"\n    This function makes a GET request to an API, extracts data that matches a RegEx pattern, and writes it to a CSV file.\n    \"\"\"\n    # Make GET request\n    response = requests.get(API_URL)\n    data = json.loads(response.text)['data']\n\n    # Extract data that matches the pattern\n    matched_data = [d for d in data if re.search(pattern, d)]\n\n    # Write data to CSV file\n    csv_file_path = os.path.join(os.getcwd(), 'matched_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        for row in matched_data:\n            writer.writerow([row])\n\n    return csv_file_path"}
{"task_id": "BigCodeBench/403", "solution": "from PIL import Image, ImageFilter\nimport cv2\nimport numpy as np\nimport os\ndef task_func(img_path, blur_radius=5):\n    \"\"\"\n    Opens an RGB image from a specific path, applies a blur filter, converts it to grayscale,\n    and then displays both the original and the edited images side by side.\n    Returns numpy arrays representing both the original and the processed images.\n    \"\"\"\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The image file does not exist at the specified path: {img_path}\")\n\n    # Open the image\n    img = Image.open(img_path)\n\n    # Apply a blur filter\n    img_blur = img.filter(ImageFilter.GaussianBlur(blur_radius))\n\n    # Convert the image to grayscale\n    img_gray = img_blur.convert('L')\n\n    # Convert the image to numpy arrays\n    img_np = np.array(img)\n    img_blur_np = np.array(img_blur)\n    img_gray_np = np.array(img_gray)\n\n    # Display the original and the edited images side by side\n    cv2.imshow('Original Image', img_np)\n    cv2.imshow('Blurred and Grayscaled Image', img_gray_np)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n    return (img_np, img_gray_np)", "raw_solution": "from PIL import Image, ImageFilter\nimport cv2\nimport numpy as np\nimport os\n\ndef task_func(img_path, blur_radius=5):\n    \"\"\"\n    Opens an RGB image from a specific path, applies a blur filter, converts it to grayscale,\n    and then displays both the original and the edited images side by side.\n    Returns numpy arrays representing both the original and the processed images.\n    \"\"\"\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The image file does not exist at the specified path: {img_path}\")\n\n    # Open the image\n    img = Image.open(img_path)\n\n    # Apply a blur filter\n    img_blur = img.filter(ImageFilter.GaussianBlur(blur_radius))\n\n    # Convert the image to grayscale\n    img_gray = img_blur.convert('L')\n\n    # Convert the image to numpy arrays\n    img_np = np.array(img)\n    img_blur_np = np.array(img_blur)\n    img_gray_np = np.array(img_gray)\n\n    # Display the original and the edited images side by side\n    cv2.imshow('Original Image', img_np)\n    cv2.imshow('Blurred and Grayscaled Image', img_gray_np)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n    return (img_np, img_gray_np)"}
{"task_id": "BigCodeBench/404", "solution": "import cv2\nimport numpy as np\nimport os\ndef task_func(img_path):\n    \"\"\"\n    This function opens an RGB image, converts it to grayscale, finds contours using the cv2 library, and returns the original image and contours.\n    If the image file does not exist at the specified path, it raises a FileNotFoundError.\n    \"\"\"\n    if not os.path.isfile(img_path):\n        raise FileNotFoundError(f\"The image file {img_path} does not exist.\")\n\n    # Open the image\n    img = cv2.imread(img_path)\n\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Find contours in the image\n    contours, _ = cv2.findContours(gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    return img, contours", "raw_solution": "import cv2\nimport numpy as np\nimport os\n\ndef task_func(img_path):\n    \"\"\"\n    This function opens an RGB image, converts it to grayscale, finds contours using the cv2 library, and returns the original image and contours.\n    If the image file does not exist at the specified path, it raises a FileNotFoundError.\n    \"\"\"\n    if not os.path.isfile(img_path):\n        raise FileNotFoundError(f\"The image file {img_path} does not exist.\")\n\n    # Open the image\n    img = cv2.imread(img_path)\n\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Find contours in the image\n    contours, _ = cv2.findContours(gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    return img, contours"}
{"task_id": "BigCodeBench/405", "solution": "import random\nimport matplotlib.pyplot as plt\ndef task_func(points: int):\n    \"\"\"\n    Generates a list of random numbers and a plot of these numbers.\n    The indices of the numbers in the list are on the x-axis and the numbers themselves are on the y-axis.\n    Returns a tuple containing:\n    A list of generated random numbers.\n    A matplotlib Axes object representing the plot.\n    \"\"\"\n    # Generate a list of random numbers\n    random_numbers = [random.random() for _ in range(points)]\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the random numbers\n    ax.plot(range(points), random_numbers)\n\n    # Set the x and y labels\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Random Number')\n\n    # Return the list of random numbers and the plot\n    return random_numbers, ax.figure", "raw_solution": "import random\nimport matplotlib.pyplot as plt\n\ndef task_func(points: int):\n    \"\"\"\n    Generates a list of random numbers and a plot of these numbers.\n    The indices of the numbers in the list are on the x-axis and the numbers themselves are on the y-axis.\n    Returns a tuple containing:\n    A list of generated random numbers.\n    A matplotlib Axes object representing the plot.\n    \"\"\"\n    # Generate a list of random numbers\n    random_numbers = [random.random() for _ in range(points)]\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the random numbers\n    ax.plot(range(points), random_numbers)\n\n    # Set the x and y labels\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Random Number')\n\n    # Return the list of random numbers and the plot\n    return random_numbers, ax.figure"}
{"task_id": "BigCodeBench/406", "solution": "from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\ndef task_func(img_path, angle):\n    \"\"\"\n    Rotates an image by a certain angle and displays both the original and the rotated images side by side.\n    Returns both images as numpy arrays.\n    \"\"\"\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The image file {img_path} does not exist.\")\n\n    # Open the image\n    img = Image.open(img_path)\n\n    # Rotate the image\n    rotated_img = img.rotate(angle, expand=True)\n\n    # Convert the images to numpy arrays\n    img_array = np.array(img)\n    rotated_img_array = np.array(rotated_img)\n\n    # Display the original and rotated images side by side\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(img)\n    axs[0].set_title('Original Image')\n    axs[1].imshow(rotated_img)\n    axs[1].set_title(f'Rotated Image by {angle} degrees')\n    plt.show()\n\n    return img_array, rotated_img_array", "raw_solution": "from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\ndef task_func(img_path, angle):\n    \"\"\"\n    Rotates an image by a certain angle and displays both the original and the rotated images side by side.\n    Returns both images as numpy arrays.\n    \"\"\"\n    if not os.path.exists(img_path):\n        raise FileNotFoundError(f\"The image file {img_path} does not exist.\")\n\n    # Open the image\n    img = Image.open(img_path)\n\n    # Rotate the image\n    rotated_img = img.rotate(angle, expand=True)\n\n    # Convert the images to numpy arrays\n    img_array = np.array(img)\n    rotated_img_array = np.array(rotated_img)\n\n    # Display the original and rotated images side by side\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(img)\n    axs[0].set_title('Original Image')\n    axs[1].imshow(rotated_img)\n    axs[1].set_title(f'Rotated Image by {angle} degrees')\n    plt.show()\n\n    return img_array, rotated_img_array"}
{"task_id": "BigCodeBench/407", "solution": "import os\nimport csv\nfrom openpyxl import load_workbook\ndef task_func(file_name, excel_file_path, csv_file_path) -> str:\n    \"\"\"\n    Converts an Excel file (.xls or .xlsx) to a CSV file by reading the contents of the Excel file and writing them to a new CSV file with the same name but a different extension.\n    Allows specifying separate paths for the Excel file source and the CSV file destination.\n    This function assumes the active sheet is the one to be converted.\n    \"\"\"\n    # Load the workbook\n    workbook = load_workbook(filename=os.path.join(excel_file_path, file_name), read_only=True)\n\n    # Get the active worksheet\n    worksheet = workbook.active\n\n    # Create the CSV file path\n    csv_file_name = f\"{os.path.splitext(file_name)[0]}.csv\"\n    csv_file_path = os.path.join(csv_file_path, csv_file_name)\n\n    # Open the CSV file for writing\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n\n        # Write the data from the worksheet to the CSV file\n        for row in worksheet.iter_rows(values_only=True):\n            writer.writerow(row)\n\n    return csv_file_name", "raw_solution": "import os\nimport csv\nfrom openpyxl import load_workbook\n\ndef task_func(file_name, excel_file_path, csv_file_path) -> str:\n    \"\"\"\n    Converts an Excel file (.xls or .xlsx) to a CSV file by reading the contents of the Excel file and writing them to a new CSV file with the same name but a different extension.\n    Allows specifying separate paths for the Excel file source and the CSV file destination.\n    This function assumes the active sheet is the one to be converted.\n    \"\"\"\n    # Load the workbook\n    workbook = load_workbook(filename=os.path.join(excel_file_path, file_name), read_only=True)\n\n    # Get the active worksheet\n    worksheet = workbook.active\n\n    # Create the CSV file path\n    csv_file_name = f\"{os.path.splitext(file_name)[0]}.csv\"\n    csv_file_path = os.path.join(csv_file_path, csv_file_name)\n\n    # Open the CSV file for writing\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n\n        # Write the data from the worksheet to the CSV file\n        for row in worksheet.iter_rows(values_only=True):\n            writer.writerow(row)\n\n    return csv_file_name"}
{"task_id": "BigCodeBench/408", "solution": "import sqlite3\nimport pandas as pd\nfrom typing import List, Dict\ndef task_func(db_file: str, query: str) -> pd.DataFrame:\n    \"\"\"\n    Connects to a given SQLite database, executes a given SQL query, and returns the results as a pandas DataFrame.\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # Execute the SQL query\n    df = pd.read_sql_query(query, conn)\n\n    # Close the connection\n    conn.close()\n\n    # Return the results as a pandas DataFrame\n    return df", "raw_solution": "import sqlite3\nimport pandas as pd\nfrom typing import List, Dict\n\ndef task_func(db_file: str, query: str) -> pd.DataFrame:\n    \"\"\"\n    Connects to a given SQLite database, executes a given SQL query, and returns the results as a pandas DataFrame.\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_file)\n\n    # Execute the SQL query\n    df = pd.read_sql_query(query, conn)\n\n    # Close the connection\n    conn.close()\n\n    # Return the results as a pandas DataFrame\n    return df"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculates the mean, median, and standard deviation of the data from a specific column in an Excel file.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(os.path.join(excel_file_path, file_name)):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path.\")\n\n    # Load the Excel file\n    df = pd.read_excel(os.path.join(excel_file_path, file_name))\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file.\")\n\n    # Calculate the mean, median, and standard deviation\n    mean_val = df[column_name].mean()\n    median_val = df[column_name].median()\n    std_dev = df[column_name].std()\n\n    # Return the results in a dictionary\n    return {\n        'mean': mean_val,\n        'median': median_val,\n        'standard_deviation': std_dev\n    }", "raw_solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculates the mean, median, and standard deviation of the data from a specific column in an Excel file.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(os.path.join(excel_file_path, file_name)):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path.\")\n\n    # Load the Excel file\n    df = pd.read_excel(os.path.join(excel_file_path, file_name))\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file.\")\n\n    # Calculate the mean, median, and standard deviation\n    mean_val = df[column_name].mean()\n    median_val = df[column_name].median()\n    std_dev = df[column_name].std()\n\n    # Return the results in a dictionary\n    return {\n        'mean': mean_val,\n        'median': median_val,\n        'standard_deviation': std_dev\n    }"}
{"task_id": "BigCodeBench/410", "solution": "import os\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Union\ndef task_func(excel_directory: str, file_name: str, column_name: str, start_date: str, end_date: str) -> Union[pd.DataFrame, None]:\n    \"\"\"\n    Filters data in a specific date range from a column in an Excel file and returns a Pandas DataFrame of the filtered data.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(os.path.join(excel_directory, file_name)):\n        raise FileNotFoundError(f\"The specified Excel file {file_name} does not exist.\")\n\n    # Check if start_date and end_date are in the correct format\n    try:\n        start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n    except ValueError:\n        raise ValueError(\"start_date and end_date should be in the format 'YYYY-MM-DD'.\")\n\n    # Load the Excel file into a DataFrame\n    df = pd.read_excel(os.path.join(excel_directory, file_name))\n\n    # Check if column_name exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column {column_name} does not exist in the DataFrame.\")\n\n    # Filter the DataFrame based on the date range\n    df = df[df[column_name].between(start_date, end_date)]\n\n    return df", "raw_solution": "import os\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Union\n\ndef task_func(excel_directory: str, file_name: str, column_name: str, start_date: str, end_date: str) -> Union[pd.DataFrame, None]:\n    \"\"\"\n    Filters data in a specific date range from a column in an Excel file and returns a Pandas DataFrame of the filtered data.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(os.path.join(excel_directory, file_name)):\n        raise FileNotFoundError(f\"The specified Excel file {file_name} does not exist.\")\n\n    # Check if start_date and end_date are in the correct format\n    try:\n        start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n    except ValueError:\n        raise ValueError(\"start_date and end_date should be in the format 'YYYY-MM-DD'.\")\n\n    # Load the Excel file into a DataFrame\n    df = pd.read_excel(os.path.join(excel_directory, file_name))\n\n    # Check if column_name exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column {column_name} does not exist in the DataFrame.\")\n\n    # Filter the DataFrame based on the date range\n    df = df[df[column_name].between(start_date, end_date)]\n\n    return df"}
{"task_id": "BigCodeBench/411", "solution": "import pandas as pd\nimport json\nfrom pathlib import Path\ndef task_func(data: dict, output_path: str = \"./default_data_output.json\") -> str:\n    \"\"\"\n    Converts the given DataFrame to a dictionary, dropping the column named 'c' if it exists, and then saves it as a JSON file.\n    \"\"\"\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Drop the column named 'c' if it exists\n    df = df.drop('c', axis=1, errors='ignore')\n\n    # Convert the DataFrame to a dictionary\n    df_dict = df.to_dict(orient='records')\n\n    # Save the dictionary as a JSON file\n    with open(output_path, 'w') as f:\n        json.dump(df_dict, f)\n\n    # Return the path where the JSON file was saved\n    return Path(output_path).resolve()", "raw_solution": "import pandas as pd\nimport json\nfrom pathlib import Path\n\ndef task_func(data: dict, output_path: str = \"./default_data_output.json\") -> str:\n    \"\"\"\n    Converts the given DataFrame to a dictionary, dropping the column named 'c' if it exists, and then saves it as a JSON file.\n    \"\"\"\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Drop the column named 'c' if it exists\n    df = df.drop('c', axis=1, errors='ignore')\n\n    # Convert the DataFrame to a dictionary\n    df_dict = df.to_dict(orient='records')\n\n    # Save the dictionary as a JSON file\n    with open(output_path, 'w') as f:\n        json.dump(df_dict, f)\n\n    # Return the path where the JSON file was saved\n    return Path(output_path).resolve()"}
{"task_id": "BigCodeBench/412", "solution": "import json\nimport base64\nimport unicodedata\ndef task_func(json_file: str) -> dict:\n    \"\"\"\n    This function reads a JSON file where each key is a unique identifier, and the corresponding value is a base64 encoded string.\n    After decoding, it applies Unicode normalization form C (NFC) to each decoded string to ensure the canonical composition of characters.\n    The function returns a dictionary where the keys are preserved, and the values are the normalized, decoded strings. Decoding is performed using the UTF-8 encoding scheme.\n    \"\"\"\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    decoded_data = {key: unicodedata.normalize('NFC', base64.b64decode(value).decode('utf-8')) for key, value in data.items()}\n\n    return decoded_data", "raw_solution": "import json\nimport base64\nimport unicodedata\n\ndef task_func(json_file: str) -> dict:\n    \"\"\"\n    This function reads a JSON file where each key is a unique identifier, and the corresponding value is a base64 encoded string.\n    After decoding, it applies Unicode normalization form C (NFC) to each decoded string to ensure the canonical composition of characters.\n    The function returns a dictionary where the keys are preserved, and the values are the normalized, decoded strings. Decoding is performed using the UTF-8 encoding scheme.\n    \"\"\"\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    decoded_data = {key: unicodedata.normalize('NFC', base64.b64decode(value).decode('utf-8')) for key, value in data.items()}\n\n    return decoded_data"}
{"task_id": "BigCodeBench/413", "solution": "import os\nfrom flask import Flask\nfrom flask_mail import Mail\ndef task_func(app):\n    \"\"\"\n    Initialize a Flask application with Flask-Mail.\n    The function retrieves email server details from environment variables.\n    If the variables do not exist, default values are used.\n    The function returns a tuple containing the Flask-Mail instance and the app's mail configurations.\n    \"\"\"\n    # Initialize Flask-Mail\n    mail = Mail(app)\n\n    # Retrieve email server details from environment variables\n    # If the variables do not exist, use defaults\n    MAIL_SERVER = os.environ.get('MAIL_SERVER', 'smtp.example.com')\n    MAIL_PORT = int(os.environ.get('MAIL_PORT', '587'))\n    MAIL_USE_TLS = os.environ.get('MAIL_USE_TLS', 'True').lower() in ['true', 'on', '1']\n    MAIL_USERNAME = os.environ.get('MAIL_USERNAME', 'username')\n    MAIL_PASSWORD = os.environ.get('MAIL_PASSWORD', 'password')\n    MAIL_DEFAULT_SENDER = os.environ.get('MAIL_DEFAULT_SENDER', 'sender@example.com')\n\n    # Configure mail\n    app.config['MAIL_SERVER'] = MAIL_SERVER\n    app.config['MAIL_PORT'] = MAIL_PORT\n    app.config['MAIL_USE_TLS'] = MAIL_USE_TLS\n    app.config['MAIL_USERNAME'] = MAIL_USERNAME\n    app.config['MAIL_PASSWORD'] = MAIL_PASSWORD\n    app.config['MAIL_DEFAULT_SENDER'] = MAIL_DEFAULT_SENDER\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD'], app.config['MAIL_DEFAULT_SENDER']\napp = Flask(__name__)", "raw_solution": "import os\nfrom flask import Flask\nfrom flask_mail import Mail\n\ndef task_func(app):\n    \"\"\"\n    Initialize a Flask application with Flask-Mail.\n    The function retrieves email server details from environment variables.\n    If the variables do not exist, default values are used.\n    The function returns a tuple containing the Flask-Mail instance and the app's mail configurations.\n    \"\"\"\n    # Initialize Flask-Mail\n    mail = Mail(app)\n\n    # Retrieve email server details from environment variables\n    # If the variables do not exist, use defaults\n    MAIL_SERVER = os.environ.get('MAIL_SERVER', 'smtp.example.com')\n    MAIL_PORT = int(os.environ.get('MAIL_PORT', '587'))\n    MAIL_USE_TLS = os.environ.get('MAIL_USE_TLS', 'True').lower() in ['true', 'on', '1']\n    MAIL_USERNAME = os.environ.get('MAIL_USERNAME', 'username')\n    MAIL_PASSWORD = os.environ.get('MAIL_PASSWORD', 'password')\n    MAIL_DEFAULT_SENDER = os.environ.get('MAIL_DEFAULT_SENDER', 'sender@example.com')\n\n    # Configure mail\n    app.config['MAIL_SERVER'] = MAIL_SERVER\n    app.config['MAIL_PORT'] = MAIL_PORT\n    app.config['MAIL_USE_TLS'] = MAIL_USE_TLS\n    app.config['MAIL_USERNAME'] = MAIL_USERNAME\n    app.config['MAIL_PASSWORD'] = MAIL_PASSWORD\n    app.config['MAIL_DEFAULT_SENDER'] = MAIL_DEFAULT_SENDER\n\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD'], app.config['MAIL_DEFAULT_SENDER']\n\n# Test the function\napp = Flask(__name__)\nmail, mail_server, mail_port, mail_use_tls, mail_username, mail_password, mail_default_sender = task_func(app)"}
{"task_id": "BigCodeBench/414", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(data, column=\"c\"):\n    \"\"\"\n    This function removes a column from a data dictionary if it exists, and then plots the remaining data if it contains numeric data.\n    \"\"\"\n    if column in data.columns:\n        df = data.drop(columns=column)\n    else:\n        df = data\n        print(f\"Column {column} does not exist in the data.\")\n\n    # Check if there's numeric data to plot\n    numeric_data = df.select_dtypes(include=[np.number])\n    if not numeric_data.empty:\n        ax = numeric_data.plot(kind='line')\n    else:\n        ax = None\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(data, column=\"c\"):\n    \"\"\"\n    This function removes a column from a data dictionary if it exists, and then plots the remaining data if it contains numeric data.\n    \"\"\"\n    if column in data.columns:\n        df = data.drop(columns=column)\n    else:\n        df = data\n        print(f\"Column {column} does not exist in the data.\")\n\n    # Check if there's numeric data to plot\n    numeric_data = df.select_dtypes(include=[np.number])\n    if not numeric_data.empty:\n        ax = numeric_data.plot(kind='line')\n    else:\n        ax = None\n\n    return df, ax"}
{"task_id": "BigCodeBench/415", "solution": "import pandas as pd\nimport codecs\ndef task_func(dataframe: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Decodes all Unicode escape strings in a particular column (\"UnicodeString\") in a given Pandas DataFrame.\n    \"\"\"\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input should be a Pandas DataFrame\")\n    if \"UnicodeString\" not in dataframe.columns:\n        raise KeyError(\"Column 'UnicodeString' does not exist in the DataFrame\")\n\n    dataframe['UnicodeString'] = dataframe['UnicodeString'].apply(lambda x: codecs.decode(x.encode(), 'unicode_escape'))\n    return dataframe", "raw_solution": "import pandas as pd\nimport codecs\n\ndef task_func(dataframe: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Decodes all Unicode escape strings in a particular column (\"UnicodeString\") in a given Pandas DataFrame.\n    \"\"\"\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input should be a Pandas DataFrame\")\n    if \"UnicodeString\" not in dataframe.columns:\n        raise KeyError(\"Column 'UnicodeString' does not exist in the DataFrame\")\n\n    dataframe['UnicodeString'] = dataframe['UnicodeString'].apply(lambda x: codecs.decode(x.encode(), 'unicode_escape'))\n    return dataframe"}
{"task_id": "BigCodeBench/416", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data, column=\"c\"):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data should be a pandas DataFrame\")\n    if column not in data.columns:\n        raise ValueError(f\"Column {column} not found in data\")\n    if data.empty or not any(data.dtypes[data.dtypes != \"object\"].__iter__()):\n        return None\n\n    # Remove non-numeric columns\n    data = data.select_dtypes(include=[pd.np.number])\n\n    # Remove the specified column\n    data = data.drop(columns=[column])\n\n    # Create a correlation matrix\n    corr = data.corr()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.title(\"Heatmap of Correlation Matrix\")\n    plt.show()\n\n    return plt.gca()", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column=\"c\"):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data should be a pandas DataFrame\")\n    if column not in data.columns:\n        raise ValueError(f\"Column {column} not found in data\")\n    if data.empty or not any(data.dtypes[data.dtypes != \"object\"].__iter__()):\n        return None\n\n    # Remove non-numeric columns\n    data = data.select_dtypes(include=[pd.np.number])\n\n    # Remove the specified column\n    data = data.drop(columns=[column])\n\n    # Create a correlation matrix\n    corr = data.corr()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.title(\"Heatmap of Correlation Matrix\")\n    plt.show()\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y, learning_rate=0.01):\n    \"\"\"\n    Trains a simple neural network on given input data and target labels.\n    \"\"\"\n    # Split the data into a training set (75%) and a test set (25%)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct a Sequential model with one dense hidden layer and a sigmoid activation function\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model using binary cross-entropy loss and SGD optimizer with a specified learning rate\n    sgd = SGD(lr=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # Fit the model to the training data (without verbose output), also evaluating it on the test set as validation data\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plots the model's training and validation loss over epochs\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    return model, plt.gca()", "raw_solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y, learning_rate=0.01):\n    \"\"\"\n    Trains a simple neural network on given input data and target labels.\n    \"\"\"\n    # Split the data into a training set (75%) and a test set (25%)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct a Sequential model with one dense hidden layer and a sigmoid activation function\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model using binary cross-entropy loss and SGD optimizer with a specified learning rate\n    sgd = SGD(lr=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # Fit the model to the training data (without verbose output), also evaluating it on the test set as validation data\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plots the model's training and validation loss over epochs\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_dim=X_train.shape[1], activation='sigmoid'),\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.001), metrics=['AUC'])\n\n    # Fit the model\n    history = model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and ROC area for each class\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model, plt.gca()", "raw_solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_dim=X_train.shape[1], activation='sigmoid'),\n    ])\n\n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.001), metrics=['AUC'])\n\n    # Fit the model\n    history = model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and ROC area for each class\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/419", "solution": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(X, Y):\n    # Splitting the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Constructing the Keras Sequential model\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_dim=X.shape[1], activation='sigmoid'),\n    ])\n\n    # Compiling the model\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\n    # Fitting the model\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=0)\n\n    # Predicting probabilities\n    Y_pred_prob = model.predict(X_test)\n\n    # Calculating precision and recall\n    precision, recall, _ = precision_recall_curve(Y_test, Y_pred_prob)\n\n    # Plotting the Precision-Recall curve\n    plt.plot(recall, precision, marker='.')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    ax = plt.gca()\n\n    return model, ax", "raw_solution": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(X, Y):\n    # Splitting the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Constructing the Keras Sequential model\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_dim=X.shape[1], activation='sigmoid'),\n    ])\n\n    # Compiling the model\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\n    # Fitting the model\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=0)\n\n    # Predicting probabilities\n    Y_pred_prob = model.predict(X_test)\n\n    # Calculating precision and recall\n    precision, recall, _ = precision_recall_curve(Y_test, Y_pred_prob)\n\n    # Plotting the Precision-Recall curve\n    plt.plot(recall, precision, marker='.')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    ax = plt.gca()\n\n    return model, ax"}
{"task_id": "BigCodeBench/420", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data):\n    \"\"\"\n    Scales numeric columns of a data dictionary using the StandardScaler.\n    \"\"\"\n    # Create a copy of the dataframe to avoid modifying the original data\n    data_copy = data.copy()\n\n    # Initialize the StandardScaler\n    scaler = StandardScaler()\n\n    # Iterate over each column in the dataframe\n    for col in data_copy.columns:\n        # Try to convert the column to float\n        try:\n            data_copy[col] = pd.to_numeric(data_copy[col], errors='raise')\n        except ValueError:\n            # If a ValueError is raised, the column cannot be converted to float\n            continue\n\n        # If the column can be converted to float, scale it\n        data_copy[col] = scaler.fit_transform(data_copy[col].values.reshape(-1, 1))\n\n    return data_copy", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data):\n    \"\"\"\n    Scales numeric columns of a data dictionary using the StandardScaler.\n    \"\"\"\n    # Create a copy of the dataframe to avoid modifying the original data\n    data_copy = data.copy()\n\n    # Initialize the StandardScaler\n    scaler = StandardScaler()\n\n    # Iterate over each column in the dataframe\n    for col in data_copy.columns:\n        # Try to convert the column to float\n        try:\n            data_copy[col] = pd.to_numeric(data_copy[col], errors='raise')\n        except ValueError:\n            # If a ValueError is raised, the column cannot be converted to float\n            continue\n\n        # If the column can be converted to float, scale it\n        data_copy[col] = scaler.fit_transform(data_copy[col].values.reshape(-1, 1))\n\n    return data_copy"}
{"task_id": "BigCodeBench/421", "solution": "import requests\nimport os\nimport json\nimport time\nHEADERS = {\n    'accept': 'text/json',\n    'Content-Type': 'application/json'\n}\ndef task_func(url, directory, metadata):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"Directory {directory} does not exist.\")\n    if not isinstance(url, str):\n        raise TypeError(f\"Invalid URL: {url}\")\n\n    status_codes = []\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'r') as file:\n                file_data = file.read()\n            data = {\n                'metadata': metadata,\n                'file_data': file_data\n            }\n            response = requests.post(url, headers=HEADERS, data=json.dumps(data))\n            status_codes.append(response.status_code)\n            time.sleep(1)\n\n    return status_codes", "raw_solution": "import requests\nimport os\nimport json\nimport time\n\nHEADERS = {\n    'accept': 'text/json',\n    'Content-Type': 'application/json'\n}\n\ndef task_func(url, directory, metadata):\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"Directory {directory} does not exist.\")\n    if not isinstance(url, str):\n        raise TypeError(f\"Invalid URL: {url}\")\n\n    status_codes = []\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'r') as file:\n                file_data = file.read()\n            data = {\n                'metadata': metadata,\n                'file_data': file_data\n            }\n            response = requests.post(url, headers=HEADERS, data=json.dumps(data))\n            status_codes.append(response.status_code)\n            time.sleep(1)\n\n    return status_codes"}
{"task_id": "BigCodeBench/422", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef task_func(df, target_column, column_to_remove=\"c\", test_size=0.2):\n    \"\"\"\n    Splits the data into train and test datasets after removing a specified column if it exists.\n    \"\"\"\n    if column_to_remove in df.columns:\n        df = df.drop(columns=column_to_remove)\n    X = df.drop(columns=target_column)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    return X_train, X_test, y_train, y_test\ndf = pd.DataFrame({\n    \"a\": [1, 2, 3, 4, 5],\n    \"b\": [10, 20, 30, 40, 50],\n    \"c\": [100, 200, 300, 400, 500],\n    \"target\": [1000, 2000, 3000, 4000, 5000]\n})", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(df, target_column, column_to_remove=\"c\", test_size=0.2):\n    \"\"\"\n    Splits the data into train and test datasets after removing a specified column if it exists.\n    \"\"\"\n    if column_to_remove in df.columns:\n        df = df.drop(columns=column_to_remove)\n    X = df.drop(columns=target_column)\n    y = df[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    return X_train, X_test, y_train, y_test\n\n# Test the function\ndf = pd.DataFrame({\n    \"a\": [1, 2, 3, 4, 5],\n    \"b\": [10, 20, 30, 40, 50],\n    \"c\": [100, 200, 300, 400, 500],\n    \"target\": [1000, 2000, 3000, 4000, 5000]\n})\nX_train, X_test, y_train, y_test = task_func(df, \"target\", \"c\")"}
{"task_id": "BigCodeBench/423", "solution": "import numpy as np\nimport cv2\nimport os\ndef task_func(image_path='image.jpg', threshold=128):\n    \"\"\"\n    Reads an RGB image, converts it to grayscale, binarizes it using a given threshold, and returns both the original and binarized images as numpy arrays.\n    \"\"\"\n    # Check if the image file exists\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n\n    # Check if the threshold is an integer and in the range 0-255\n    if not isinstance(threshold, int) or threshold < 0 or threshold > 255:\n        raise ValueError(f\"The threshold value {threshold} is not an integer or not in the range 0-255.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Binarize the image using the given threshold\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    return gray_image, binary_image", "raw_solution": "import numpy as np\nimport cv2\nimport os\n\ndef task_func(image_path='image.jpg', threshold=128):\n    \"\"\"\n    Reads an RGB image, converts it to grayscale, binarizes it using a given threshold, and returns both the original and binarized images as numpy arrays.\n    \"\"\"\n    # Check if the image file exists\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n\n    # Check if the threshold is an integer and in the range 0-255\n    if not isinstance(threshold, int) or threshold < 0 or threshold > 255:\n        raise ValueError(f\"The threshold value {threshold} is not an integer or not in the range 0-255.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Binarize the image using the given threshold\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    return gray_image, binary_image"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    Reads an RGB image, applies K-means clustering to segment the image into 'n_clusters' regions,\n    and saves each region as a separate image. The function returns numpy arrays of the original\n    and segmented images.\n    \"\"\"\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"Image file {image_path} does not exist.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' must be a positive integer.\")\n\n    # Read the image\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to be a list of RGB pixels\n    img_list = img.reshape((-1, 3))\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(img_list)\n\n    # Get the cluster labels for each pixel\n    labels = kmeans.predict(img_list)\n\n    # Create the segmented image\n    segmented_img = kmeans.cluster_centers_[labels].reshape(img.shape)\n\n    # Convert the segmented image to uint8\n    segmented_img = np.uint8(segmented_img)\n\n    return img, segmented_img", "raw_solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    Reads an RGB image, applies K-means clustering to segment the image into 'n_clusters' regions,\n    and saves each region as a separate image. The function returns numpy arrays of the original\n    and segmented images.\n    \"\"\"\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"Image file {image_path} does not exist.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' must be a positive integer.\")\n\n    # Read the image\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to be a list of RGB pixels\n    img_list = img.reshape((-1, 3))\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(img_list)\n\n    # Get the cluster labels for each pixel\n    labels = kmeans.predict(img_list)\n\n    # Create the segmented image\n    segmented_img = kmeans.cluster_centers_[labels].reshape(img.shape)\n\n    # Convert the segmented image to uint8\n    segmented_img = np.uint8(segmented_img)\n\n    return img, segmented_img"}
{"task_id": "BigCodeBench/425", "solution": "import cv2\nimport os\nfrom matplotlib import pyplot as plt\ndef task_func(image_path='image.jpg', histogram_path='histogram.png'):\n    \"\"\"\n    Reads an image, creates a histogram of the image pixel intensities, saves the histogram as a PNG file,\n    and returns the histogram plot object. Also displays the original image and its histogram.\n    \"\"\"\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n\n    # Read the image\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Create a histogram\n    histogram, bins = np.histogram(image.flatten(), 256, [0, 256])\n\n    # Plot the histogram\n    plt.figure(figsize=(12, 6))\n    plt.subplot(121), plt.imshow(image, 'gray'), plt.title('Image'), plt.xticks([]), plt.yticks([])\n    plt.subplot(122), plt.plot(histogram), plt.title('Histogram'), plt.xlim([0, 256])\n    plt.show()\n\n    # Save the histogram as a PNG file\n    plt.savefig(histogram_path)\n\n    return plt.gca()", "raw_solution": "import cv2\nimport os\nfrom matplotlib import pyplot as plt\n\ndef task_func(image_path='image.jpg', histogram_path='histogram.png'):\n    \"\"\"\n    Reads an image, creates a histogram of the image pixel intensities, saves the histogram as a PNG file,\n    and returns the histogram plot object. Also displays the original image and its histogram.\n    \"\"\"\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n\n    # Read the image\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Create a histogram\n    histogram, bins = np.histogram(image.flatten(), 256, [0, 256])\n\n    # Plot the histogram\n    plt.figure(figsize=(12, 6))\n    plt.subplot(121), plt.imshow(image, 'gray'), plt.title('Image'), plt.xticks([]), plt.yticks([])\n    plt.subplot(122), plt.plot(histogram), plt.title('Histogram'), plt.xlim([0, 256])\n    plt.show()\n\n    # Save the histogram as a PNG file\n    plt.savefig(histogram_path)\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/426", "solution": "import numpy as np\nimport cv2\nimport os\ndef task_func(image_path='image.jpg', threshold=128):\n    \"\"\"\n    Reads an image, converts it to grayscale, binarizes it using a given threshold, and saves it as 'binary_image.jpg'.\n    The function returns numpy arrays of the original and binarized images, and ensures that the threshold value is valid.\n    The function raises FileNotFoundError if the image file does not exist at the specified path, and ValueError if the threshold is not an integer or not in the range 0-255.\n    \"\"\"\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    \n    if not isinstance(threshold, int) or threshold < 0 or threshold > 255:\n        raise ValueError(f\"The threshold value {threshold} is not valid. It should be an integer in the range 0-255.\")\n    \n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Binarize the image using the given threshold\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    # Save the binary image\n    cv2.imwrite('binary_image.jpg', binary_image)\n\n    return gray_image, binary_image", "raw_solution": "import numpy as np\nimport cv2\nimport os\n\ndef task_func(image_path='image.jpg', threshold=128):\n    \"\"\"\n    Reads an image, converts it to grayscale, binarizes it using a given threshold, and saves it as 'binary_image.jpg'.\n    The function returns numpy arrays of the original and binarized images, and ensures that the threshold value is valid.\n    The function raises FileNotFoundError if the image file does not exist at the specified path, and ValueError if the threshold is not an integer or not in the range 0-255.\n    \"\"\"\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    \n    if not isinstance(threshold, int) or threshold < 0 or threshold > 255:\n        raise ValueError(f\"The threshold value {threshold} is not valid. It should be an integer in the range 0-255.\")\n    \n    # Read the image\n    image = cv2.imread(image_path)\n\n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Binarize the image using the given threshold\n    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n\n    # Save the binary image\n    cv2.imwrite('binary_image.jpg', binary_image)\n\n    return gray_image, binary_image"}
{"task_id": "BigCodeBench/427", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\ndef task_func(df1, df2, features=[\"feature1\", \"feature2\", \"feature3\"], target=\"target\"):\n    # Merge the two dataframes based on 'id' column\n    df = pd.merge(df1, df2, on='id')\n\n    # Create a Linear Regression model\n    lr = LinearRegression()\n\n    # Fit the model with the specified features and target\n    lr.fit(df[features], df[target])\n\n    # Get the coefficients and intercept\n    coefficients = lr.coef_.tolist()\n    intercept = lr.intercept_\n\n    # Predict the target values\n    df['predicted'] = lr.predict(df[features])\n\n    # Calculate residuals\n    df['residuals'] = df[target] - df['predicted']\n\n    # Plot the residuals\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df['predicted'], df['residuals'])\n    plt.title('Residuals Plot')\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Residuals')\n    plt.grid(True)\n    residuals_plot = plt.gca()\n\n    # Return the results\n    return {\n        'coefficients': coefficients,\n        'intercept': intercept,\n        'residuals_plot': residuals_plot\n    }", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2, features=[\"feature1\", \"feature2\", \"feature3\"], target=\"target\"):\n    # Merge the two dataframes based on 'id' column\n    df = pd.merge(df1, df2, on='id')\n\n    # Create a Linear Regression model\n    lr = LinearRegression()\n\n    # Fit the model with the specified features and target\n    lr.fit(df[features], df[target])\n\n    # Get the coefficients and intercept\n    coefficients = lr.coef_.tolist()\n    intercept = lr.intercept_\n\n    # Predict the target values\n    df['predicted'] = lr.predict(df[features])\n\n    # Calculate residuals\n    df['residuals'] = df[target] - df['predicted']\n\n    # Plot the residuals\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df['predicted'], df['residuals'])\n    plt.title('Residuals Plot')\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Residuals')\n    plt.grid(True)\n    residuals_plot = plt.gca()\n\n    # Return the results\n    return {\n        'coefficients': coefficients,\n        'intercept': intercept,\n        'residuals_plot': residuals_plot\n    }"}
{"task_id": "BigCodeBench/428", "solution": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df1, df2):\n    # Merge df1 and df2 on 'id' column using outer join\n    merged_df = pd.merge(df1, df2, on='id', how='outer')\n\n    # Scale numeric features in merged_df\n    scaler = StandardScaler()\n    merged_df[merged_df.select_dtypes(include=['int64', 'float64']).columns] = scaler.fit_transform(merged_df[merged_df.select_dtypes(include=['int64', 'float64']).columns])\n\n    # Create pair plot of the scaled features\n    pair_plot = sns.pairplot(merged_df, diag_kind='kde')\n\n    return merged_df, pair_plot", "raw_solution": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df1, df2):\n    # Merge df1 and df2 on 'id' column using outer join\n    merged_df = pd.merge(df1, df2, on='id', how='outer')\n\n    # Scale numeric features in merged_df\n    scaler = StandardScaler()\n    merged_df[merged_df.select_dtypes(include=['int64', 'float64']).columns] = scaler.fit_transform(merged_df[merged_df.select_dtypes(include=['int64', 'float64']).columns])\n\n    # Create pair plot of the scaled features\n    pair_plot = sns.pairplot(merged_df, diag_kind='kde')\n\n    return merged_df, pair_plot"}
{"task_id": "BigCodeBench/429", "solution": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, List\ndef task_func(df1: pd.DataFrame, df2: pd.DataFrame) -> Tuple[List[str], plt.Axes]:\n    \"\"\"\n    Perform the feature selection with SelectKBest (k=2) and return a heatmap of the feature correlations.\n    \"\"\"\n    # Perform feature selection\n    selector = SelectKBest(score_func=f_classif, k=2)\n    selector.fit(df1, df2)\n    selected_features = selector.get_support(indices=True)\n    selected_features = [df1.columns[i] for i in selected_features]\n\n    # Compute correlation matrix\n    corr = df1[selected_features].corr()\n\n    # Create heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.title('Heatmap of feature correlations')\n    plt.xlabel('Features')\n    plt.ylabel('Features')\n    plt.tight_layout()\n\n    return selected_features, plt.gca()", "raw_solution": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, List\n\ndef task_func(df1: pd.DataFrame, df2: pd.DataFrame) -> Tuple[List[str], plt.Axes]:\n    \"\"\"\n    Perform the feature selection with SelectKBest (k=2) and return a heatmap of the feature correlations.\n    \"\"\"\n    # Perform feature selection\n    selector = SelectKBest(score_func=f_classif, k=2)\n    selector.fit(df1, df2)\n    selected_features = selector.get_support(indices=True)\n    selected_features = [df1.columns[i] for i in selected_features]\n\n    # Compute correlation matrix\n    corr = df1[selected_features].corr()\n\n    # Create heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n    plt.title('Heatmap of feature correlations')\n    plt.xlabel('Features')\n    plt.ylabel('Features')\n    plt.tight_layout()\n\n    return selected_features, plt.gca()"}
{"task_id": "BigCodeBench/430", "solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    \"\"\"\n    Merges two datasets, performs KMeans clustering, and returns cluster labels and a scatterplot.\n    \"\"\"\n    # Merge the two datasets\n    df = pd.concat([df1, df2])\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2, n_init=10)\n    labels = kmeans.fit_predict(df[[column1, column2]])\n\n    # Create a scatterplot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df[column1], df[column2], c=labels)\n\n    return labels, ax", "raw_solution": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    \"\"\"\n    Merges two datasets, performs KMeans clustering, and returns cluster labels and a scatterplot.\n    \"\"\"\n    # Merge the two datasets\n    df = pd.concat([df1, df2])\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=2, n_init=10)\n    labels = kmeans.fit_predict(df[[column1, column2]])\n\n    # Create a scatterplot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df[column1], df[column2], c=labels)\n\n    return labels, ax"}
{"task_id": "BigCodeBench/431", "solution": "import cv2\nimport os\nimport numpy as np\ndef task_func(image_file: str) -> np.ndarray:\n    \"\"\"\n    Creates a histogram of the pixel values of a grayscale image.\n    \"\"\"\n    if not os.path.isfile(image_file):\n        raise FileNotFoundError(f\"The specified image file {image_file} does not exist.\")\n\n    try:\n        img = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n    except cv2.error as e:\n        raise ValueError(f\"The image file {image_file} is not a valid image: {str(e)}\")\n\n    hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n    return hist.flatten()", "raw_solution": "import cv2\nimport os\nimport numpy as np\n\ndef task_func(image_file: str) -> np.ndarray:\n    \"\"\"\n    Creates a histogram of the pixel values of a grayscale image.\n    \"\"\"\n    if not os.path.isfile(image_file):\n        raise FileNotFoundError(f\"The specified image file {image_file} does not exist.\")\n\n    try:\n        img = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n    except cv2.error as e:\n        raise ValueError(f\"The image file {image_file} is not a valid image: {str(e)}\")\n\n    hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n    return hist.flatten()"}
{"task_id": "BigCodeBench/432", "solution": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    \"\"\"\n    Merges two dataframes based on the 'id' column, performs a chi-square independence test on the merged dataframe,\n    and draws a heatmap of the contingency table created from the features in column1, column2.\n    \"\"\"\n    # Merge the two dataframes\n    df = pd.merge(df1, df2, on='id')\n\n    # Create a contingency table\n    contingency_table = pd.crosstab(df[column1], df[column2])\n\n    # Perform the chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(contingency_table, annot=True, cmap='YlGnBu')\n\n    return (p, heatmap)", "raw_solution": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency\n\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    \"\"\"\n    Merges two dataframes based on the 'id' column, performs a chi-square independence test on the merged dataframe,\n    and draws a heatmap of the contingency table created from the features in column1, column2.\n    \"\"\"\n    # Merge the two dataframes\n    df = pd.merge(df1, df2, on='id')\n\n    # Create a contingency table\n    contingency_table = pd.crosstab(df[column1], df[column2])\n\n    # Perform the chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(contingency_table, annot=True, cmap='YlGnBu')\n\n    return (p, heatmap)"}
{"task_id": "BigCodeBench/433", "solution": "import base64\nimport hashlib\nimport hmac\nimport binascii\ndef task_func(s, signature, secret_key):\n    \"\"\"\n    Validates the HMAC SHA-1 signature of a base64-encoded message against a provided signature using a specified secret key.\n    \"\"\"\n    # Decode the base64-encoded message\n    message = base64.b64decode(s)\n\n    # Compute the HMAC SHA-1 hash of the message using the secret key\n    computed_signature = hmac.new(secret_key.encode(), message, hashlib.sha1).digest()\n\n    # Convert the provided signature from hex to bytes\n    provided_signature = binascii.unhexlify(signature)\n\n    # Compare the computed and provided signatures\n    return hmac.compare_digest(computed_signature, provided_signature)", "raw_solution": "import base64\nimport hashlib\nimport hmac\nimport binascii\n\ndef task_func(s, signature, secret_key):\n    \"\"\"\n    Validates the HMAC SHA-1 signature of a base64-encoded message against a provided signature using a specified secret key.\n    \"\"\"\n    # Decode the base64-encoded message\n    message = base64.b64decode(s)\n\n    # Compute the HMAC SHA-1 hash of the message using the secret key\n    computed_signature = hmac.new(secret_key.encode(), message, hashlib.sha1).digest()\n\n    # Convert the provided signature from hex to bytes\n    provided_signature = binascii.unhexlify(signature)\n\n    # Compare the computed and provided signatures\n    return hmac.compare_digest(computed_signature, provided_signature)\n\n# Test the function"}
{"task_id": "BigCodeBench/434", "solution": "import pandas as pd\nimport re\nimport random\nfrom typing import List\ndef task_func(s: str, seed: int = 0) -> pd.DataFrame:\n    \"\"\"\n    Generates a Pandas DataFrame of products with their ID, quantity, code, price, product, and description based on a specified string of product data.\n    \"\"\"\n    random.seed(seed)\n    product_names = ['Apple', 'Banana', 'Orange', 'Pear', 'Grape']\n    product_dict = {}\n    data = []\n\n    for line in s.split('\\n'):\n        id, quantity, code, price, *description = line.split()\n        description = ' '.join(description)\n        if code not in product_dict:\n            product_dict[code] = random.choice(product_names)\n        data.append([id, int(quantity), code, int(price), product_dict[code], description])\n\n    data_df = pd.DataFrame(data, columns=['ID', 'Quantity', 'Code', 'Price', 'Product', 'Description'])\n    return data_df\ns = '1 10 A10B 100 This is a description with spaces\\n2 20 B20C 200 Another description example'", "raw_solution": "import pandas as pd\nimport re\nimport random\nfrom typing import List\n\ndef task_func(s: str, seed: int = 0) -> pd.DataFrame:\n    \"\"\"\n    Generates a Pandas DataFrame of products with their ID, quantity, code, price, product, and description based on a specified string of product data.\n    \"\"\"\n    random.seed(seed)\n    product_names = ['Apple', 'Banana', 'Orange', 'Pear', 'Grape']\n    product_dict = {}\n    data = []\n\n    for line in s.split('\\n'):\n        id, quantity, code, price, *description = line.split()\n        description = ' '.join(description)\n        if code not in product_dict:\n            product_dict[code] = random.choice(product_names)\n        data.append([id, int(quantity), code, int(price), product_dict[code], description])\n\n    data_df = pd.DataFrame(data, columns=['ID', 'Quantity', 'Code', 'Price', 'Product', 'Description'])\n    return data_df\n\ns = '1 10 A10B 100 This is a description with spaces\\n2 20 B20C 200 Another description example'\ndf = task_func(s)"}
{"task_id": "BigCodeBench/435", "solution": "import pandas as pd\nfrom random import randint\ndef task_func(name: str, age: int, code: str, salary: float, bio: str) -> pd.DataFrame:\n    \"\"\"\n    Generate a Pandas DataFrame of employees with their details based on the input provided.\n    \"\"\"\n    job_titles = ['Engineer', 'Manager', 'Analyst', 'Developer', 'Tester']\n    job_title = job_titles[randint(0, len(job_titles)-1)]\n\n    data_dict = {\n        'Name': [name],\n        'Age': [age],\n        'Code': [code],\n        'Salary': [salary],\n        'Bio': [bio],\n        'Job Title': [job_title]\n    }\n\n    data_df = pd.DataFrame(data_dict)\n\n    return data_df", "raw_solution": "import pandas as pd\nfrom random import randint\n\ndef task_func(name: str, age: int, code: str, salary: float, bio: str) -> pd.DataFrame:\n    \"\"\"\n    Generate a Pandas DataFrame of employees with their details based on the input provided.\n    \"\"\"\n    job_titles = ['Engineer', 'Manager', 'Analyst', 'Developer', 'Tester']\n    job_title = job_titles[randint(0, len(job_titles)-1)]\n\n    data_dict = {\n        'Name': [name],\n        'Age': [age],\n        'Code': [code],\n        'Salary': [salary],\n        'Bio': [bio],\n        'Job Title': [job_title]\n    }\n\n    data_df = pd.DataFrame(data_dict)\n\n    return data_df\n\n# Test the function"}
{"task_id": "BigCodeBench/436", "solution": "import string\nimport matplotlib.pyplot as plt\nfrom collections import Counter\ndef task_func(s):\n    if not isinstance(s, str):\n        raise TypeError(\"Input must be a string\")\n\n    s = s.lower()\n    letter_freq = Counter(ch for ch in s if ch in string.ascii_lowercase)\n\n    fig, ax = plt.subplots()\n    ax.bar(range(len(letter_freq)), list(letter_freq.values()), tick_label=list(letter_freq.keys()))\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequencies')\n\n    return (dict(letter_freq), fig, ax)", "raw_solution": "import string\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\ndef task_func(s):\n    if not isinstance(s, str):\n        raise TypeError(\"Input must be a string\")\n\n    s = s.lower()\n    letter_freq = Counter(ch for ch in s if ch in string.ascii_lowercase)\n\n    fig, ax = plt.subplots()\n    ax.bar(range(len(letter_freq)), list(letter_freq.values()), tick_label=list(letter_freq.keys()))\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequencies')\n\n    return (dict(letter_freq), fig, ax)"}
{"task_id": "BigCodeBench/437", "solution": "import pandas as pd\nimport pickle\nimport os\ndef task_func(df, file_name=\"save.pkl\"):\n    \"\"\"\n    Saves the provided Pandas DataFrame \"df\" in a pickle file with the given name,\n    reads it back for validation, and deletes the intermediate file.\n    \"\"\"\n    # Save DataFrame to pickle file\n    df.to_pickle(file_name)\n\n    # Load DataFrame from pickle file\n    loaded_df = pd.read_pickle(file_name)\n\n    # Delete the intermediate file\n    os.remove(file_name)\n\n    return loaded_df", "raw_solution": "import pandas as pd\nimport pickle\nimport os\n\ndef task_func(df, file_name=\"save.pkl\"):\n    \"\"\"\n    Saves the provided Pandas DataFrame \"df\" in a pickle file with the given name,\n    reads it back for validation, and deletes the intermediate file.\n    \"\"\"\n    # Save DataFrame to pickle file\n    df.to_pickle(file_name)\n\n    # Load DataFrame from pickle file\n    loaded_df = pd.read_pickle(file_name)\n\n    # Delete the intermediate file\n    os.remove(file_name)\n\n    return loaded_df"}
{"task_id": "BigCodeBench/438", "solution": "import pickle\nimport os\nimport matplotlib.pyplot as plt\ndef task_func(numbers, file_path=\"save.pkl\"):\n    if not isinstance(numbers, list) or not all(isinstance(i, (int, float)) for i in numbers):\n        raise TypeError(\"Input should be a list of numbers\")\n\n    # Create a figure and plot the numbers\n    fig, ax = plt.subplots()\n    ax.plot(numbers)\n\n    # Save the figure to a pickle file\n    with open(file_path, 'wb') as f:\n        pickle.dump(fig, f)\n\n    # Load the figure from the pickle file\n    with open(file_path, 'rb') as f:\n        loaded_fig = pickle.load(f)\n\n    # Delete the pickle file\n    os.remove(file_path)\n\n    return loaded_fig", "raw_solution": "import pickle\nimport os\nimport matplotlib.pyplot as plt\n\ndef task_func(numbers, file_path=\"save.pkl\"):\n    if not isinstance(numbers, list) or not all(isinstance(i, (int, float)) for i in numbers):\n        raise TypeError(\"Input should be a list of numbers\")\n\n    # Create a figure and plot the numbers\n    fig, ax = plt.subplots()\n    ax.plot(numbers)\n\n    # Save the figure to a pickle file\n    with open(file_path, 'wb') as f:\n        pickle.dump(fig, f)\n\n    # Load the figure from the pickle file\n    with open(file_path, 'rb') as f:\n        loaded_fig = pickle.load(f)\n\n    # Delete the pickle file\n    os.remove(file_path)\n\n    return loaded_fig"}
{"task_id": "BigCodeBench/439", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(P, T):\n    \"\"\"\n    Calculates the product of a matrix 'P' and a 3D tensor 'T' using numpy and visualizes the results as a heatmap.\n    \"\"\"\n    # Perform matrix-tensor multiplication\n    result = np.matmul(P, T)\n\n    # Create a new figure and axes\n    fig, ax = plt.subplots()\n\n    # Visualize the result as a heatmap\n    sns.heatmap(result, ax=ax)\n\n    return result, ax\nP = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nT = np.array([[[10, 11, 12], [13, 14, 15], [16, 17, 18]],\n              [[19, 20, 21], [22, 23, 24], [25, 26, 27]],\n              [[28, 29, 30], [31, 32, 33], [34, 35, 36]]])", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(P, T):\n    \"\"\"\n    Calculates the product of a matrix 'P' and a 3D tensor 'T' using numpy and visualizes the results as a heatmap.\n    \"\"\"\n    # Perform matrix-tensor multiplication\n    result = np.matmul(P, T)\n\n    # Create a new figure and axes\n    fig, ax = plt.subplots()\n\n    # Visualize the result as a heatmap\n    sns.heatmap(result, ax=ax)\n\n    return result, ax\n\n# Test the function\nP = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nT = np.array([[[10, 11, 12], [13, 14, 15], [16, 17, 18]],\n              [[19, 20, 21], [22, 23, 24], [25, 26, 27]],\n              [[28, 29, 30], [31, 32, 33], [34, 35, 36]]])\n\nresult, ax = task_func(P, T)"}
{"task_id": "BigCodeBench/440", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(P, T):\n    \"\"\"\n    Performs matrix-tensor multiplication between a matrix P and a 3D tensor T,\n    normalizes the result using sklearn's StandardScaler, and returns the result as a pandas DataFrame.\n    \"\"\"\n    # Check if the shapes of P and T are compatible for multiplication\n    if P.shape[1] != T.shape[0]:\n        raise ValueError(\"Shapes of P and T are not compatible for multiplication\")\n\n    # Perform matrix-tensor multiplication\n    result = np.matmul(P, T)\n\n    # Flatten the result\n    result = result.flatten()\n\n    # Normalize the result using sklearn's StandardScaler\n    scaler = StandardScaler()\n    result = scaler.fit_transform(result.reshape(-1, 1))\n\n    # Convert the result to a pandas DataFrame\n    df = pd.DataFrame(result, columns=[\"feature_\" + str(i) for i in range(result.shape[1])])\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(P, T):\n    \"\"\"\n    Performs matrix-tensor multiplication between a matrix P and a 3D tensor T,\n    normalizes the result using sklearn's StandardScaler, and returns the result as a pandas DataFrame.\n    \"\"\"\n    # Check if the shapes of P and T are compatible for multiplication\n    if P.shape[1] != T.shape[0]:\n        raise ValueError(\"Shapes of P and T are not compatible for multiplication\")\n\n    # Perform matrix-tensor multiplication\n    result = np.matmul(P, T)\n\n    # Flatten the result\n    result = result.flatten()\n\n    # Normalize the result using sklearn's StandardScaler\n    scaler = StandardScaler()\n    result = scaler.fit_transform(result.reshape(-1, 1))\n\n    # Convert the result to a pandas DataFrame\n    df = pd.DataFrame(result, columns=[\"feature_\" + str(i) for i in range(result.shape[1])])\n\n    return df"}
{"task_id": "BigCodeBench/441", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(P, T):\n    \"\"\"\n    Calculates the product of a matrix P and a 3D tensor T with numpy and visualizes the result in 3D with matplotlib.\n    The product of the matrix and tensor is based on the Einstein summation.\n    \"\"\"\n    # Calculate the product\n    result = np.einsum('ij,ajk->aij', P, T)\n\n    # Create a new figure and 3D axes\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Visualize the result\n    ax.scatter(result[:, 0], result[:, 1], result[:, 2])\n\n    return result, ax\nP = np.array([[1, 2], [3, 4], [5, 6]])\nT = np.array([[[7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24]]])", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef task_func(P, T):\n    \"\"\"\n    Calculates the product of a matrix P and a 3D tensor T with numpy and visualizes the result in 3D with matplotlib.\n    The product of the matrix and tensor is based on the Einstein summation.\n    \"\"\"\n    # Calculate the product\n    result = np.einsum('ij,ajk->aij', P, T)\n\n    # Create a new figure and 3D axes\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Visualize the result\n    ax.scatter(result[:, 0], result[:, 1], result[:, 2])\n\n    return result, ax\n\n# Test the function\nP = np.array([[1, 2], [3, 4], [5, 6]])\nT = np.array([[[7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24]]])\n\nresult, ax = task_func(P, T)\nplt.show()"}
{"task_id": "BigCodeBench/442", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(P, T, tensor_shape=(3, 3, 3)):\n    \"\"\"\n    Calculates the product of a matrix P and a 3D tensor T, then applies PCA to reduce the dimensionality of the result.\n    The resulting 2D data is then visualized.\n    \"\"\"\n    # Calculate the product of P and T\n    T = T.reshape(tensor_shape)\n    result = np.matmul(P, T)\n\n    # Apply PCA to reduce the dimensionality of the result\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(result)\n\n    # Visualize the PCA result\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Result Visualization')\n\n    return pca_result, ax", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(P, T, tensor_shape=(3, 3, 3)):\n    \"\"\"\n    Calculates the product of a matrix P and a 3D tensor T, then applies PCA to reduce the dimensionality of the result.\n    The resulting 2D data is then visualized.\n    \"\"\"\n    # Calculate the product of P and T\n    T = T.reshape(tensor_shape)\n    result = np.matmul(P, T)\n\n    # Apply PCA to reduce the dimensionality of the result\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(result)\n\n    # Visualize the PCA result\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Result Visualization')\n\n    return pca_result, ax"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculates the product of a matrix 'P' and a 3D tensor 'T', flattens the result, applies KMeans clustering to the flattened data, and visualizes it.\n    \"\"\"\n    # Calculate the product of P and T\n    product = np.matmul(P, T)\n\n    # Flatten the result\n    flattened = product.flatten()\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n\n    # Visualize the KMeans clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result)\n    ax.set_title('KMeans Clustering Visualization')\n\n    return cluster_result, ax", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculates the product of a matrix 'P' and a 3D tensor 'T', flattens the result, applies KMeans clustering to the flattened data, and visualizes it.\n    \"\"\"\n    # Calculate the product of P and T\n    product = np.matmul(P, T)\n\n    # Flatten the result\n    flattened = product.flatten()\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n\n    # Visualize the KMeans clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened)), flattened, c=cluster_result)\n    ax.set_title('KMeans Clustering Visualization')\n\n    return cluster_result, ax"}
{"task_id": "BigCodeBench/444", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ndef task_func(n_points=100, random_seed=None):\n    \"\"\"\n    Generates an array of random 3D dots in the range [0, 1) for each dimension and draws them in a 3D scatter plot.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    points = np.random.rand(n_points, 3)\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n\n    return points, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef task_func(n_points=100, random_seed=None):\n    \"\"\"\n    Generates an array of random 3D dots in the range [0, 1) for each dimension and draws them in a 3D scatter plot.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    points = np.random.rand(n_points, 3)\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n\n    return points, ax\n\n# Test the function\npoints, plot = task_func(n_points=100, random_seed=42)\nplt.show()"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculates the Voronoi diagram for a number of points in 2D and plots it.\n    Jittering is applied prior to plotting.\n    \"\"\"\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"'points' must be a numpy array\")\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"'points' must be a 2D array\")\n    if not isinstance(seed, int):\n        raise TypeError(\"'seed' must be an integer\")\n\n    np.random.seed(seed)\n    jitter = np.random.uniform(-1, 1, points.shape)\n    points += jitter\n\n    vor = Voronoi(points)\n\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    ax.scatter(points[:, 0], points[:, 1], color='r')\n\n    return vor, ax", "raw_solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculates the Voronoi diagram for a number of points in 2D and plots it.\n    Jittering is applied prior to plotting.\n    \"\"\"\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"'points' must be a numpy array\")\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"'points' must be a 2D array\")\n    if not isinstance(seed, int):\n        raise TypeError(\"'seed' must be an integer\")\n\n    np.random.seed(seed)\n    jitter = np.random.uniform(-1, 1, points.shape)\n    points += jitter\n\n    vor = Voronoi(points)\n\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    ax.scatter(points[:, 0], points[:, 1], color='r')\n\n    return vor, ax"}
{"task_id": "BigCodeBench/446", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\ndef task_func(n_samples=100, centers=3, n_features=2, random_seed=42):\n    \"\"\"\n    Generates isotropic Gaussian blobs to form clusters and visualizes them.\n    \"\"\"\n    np.random.seed(random_seed)\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=random_seed)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n\n    return (X, y, ax)", "raw_solution": "import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\ndef task_func(n_samples=100, centers=3, n_features=2, random_seed=42):\n    \"\"\"\n    Generates isotropic Gaussian blobs to form clusters and visualizes them.\n    \"\"\"\n    np.random.seed(random_seed)\n    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=random_seed)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n\n    return (X, y, ax)\n\n# Test the function\nX, y, ax = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/447", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(data, n_components=2, random_state=None):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the provided dataset to reduce its dimensionality, and visualizes the results using a scatter plot.\n    \"\"\"\n    pca = PCA(n_components=n_components, random_state=random_state)\n    transformed_data = pca.fit_transform(data)\n\n    fig, ax = plt.subplots()\n    if n_components == 1:\n        ax.scatter(transformed_data, np.zeros_like(transformed_data))\n    elif n_components == 2:\n        ax.scatter(transformed_data[:, 0], transformed_data[:, 1])\n    else:\n        ax.scatter(transformed_data[:, 0], np.zeros_like(transformed_data[:, 0]))\n\n    return {\"transformed_data\": transformed_data, \"ax\": ax}", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(data, n_components=2, random_state=None):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the provided dataset to reduce its dimensionality, and visualizes the results using a scatter plot.\n    \"\"\"\n    pca = PCA(n_components=n_components, random_state=random_state)\n    transformed_data = pca.fit_transform(data)\n\n    fig, ax = plt.subplots()\n    if n_components == 1:\n        ax.scatter(transformed_data, np.zeros_like(transformed_data))\n    elif n_components == 2:\n        ax.scatter(transformed_data[:, 0], transformed_data[:, 1])\n    else:\n        ax.scatter(transformed_data[:, 0], np.zeros_like(transformed_data[:, 0]))\n\n    return {\"transformed_data\": transformed_data, \"ax\": ax}"}
{"task_id": "BigCodeBench/448", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(mu=0, sigma=1):\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title(f'Normal Distribution with Mean {mu} and Standard Deviation {sigma}')\n    ax.set_xlabel('x')\n    ax.set_ylabel('Probability Density')\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(mu=0, sigma=1):\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    y = norm.pdf(x, mu, sigma)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title(f'Normal Distribution with Mean {mu} and Standard Deviation {sigma}')\n    ax.set_xlabel('x')\n    ax.set_ylabel('Probability Density')\n    return ax\n\n# Test the function\nax = task_func(0, 1)\nplt.show()"}
{"task_id": "BigCodeBench/449", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, list):\n    \"\"\"\n    Standardizes the features of a pandas DataFrame using sklearn's StandardScaler,\n    and plots a histogram for each feature with 20 bins.\n    \"\"\"\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Fit and transform the data\n    standardized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Create a list to hold the Axes objects\n    axes_list = []\n\n    # Plot histograms for each feature\n    for i, column in enumerate(standardized_data.columns):\n        # Create a new figure and axis\n        fig, ax = plt.subplots()\n\n        # Add the histogram to the axis\n        ax.hist(standardized_data[column], bins=20)\n\n        # Add the axis to the list\n        axes_list.append(ax)\n\n    # Return the standardized data and the list of axes\n    return standardized_data, axes_list", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, list):\n    \"\"\"\n    Standardizes the features of a pandas DataFrame using sklearn's StandardScaler,\n    and plots a histogram for each feature with 20 bins.\n    \"\"\"\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Fit and transform the data\n    standardized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    # Create a list to hold the Axes objects\n    axes_list = []\n\n    # Plot histograms for each feature\n    for i, column in enumerate(standardized_data.columns):\n        # Create a new figure and axis\n        fig, ax = plt.subplots()\n\n        # Add the histogram to the axis\n        ax.hist(standardized_data[column], bins=20)\n\n        # Add the axis to the list\n        axes_list.append(ax)\n\n    # Return the standardized data and the list of axes\n    return standardized_data, axes_list"}
{"task_id": "BigCodeBench/450", "solution": "from scipy.spatial.distance import cdist\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(n_samples=200, centers=4, plot_path=None, random_seed=None):\n    # Generate synthetic 2D dataset\n    X, y = make_blobs(n_samples=n_samples, centers=centers, random_state=random_seed)\n\n    # Calculate Euclidean distance between individual samples\n    dist_matrix = cdist(X, X, metric='euclidean')\n\n    # Visualize the dataset\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    plt.colorbar(scatter, ax=ax)\n\n    if plot_path is not None:\n        plt.savefig(plot_path)\n        plt.close(fig)\n        return None\n    else:\n        return dist_matrix, plt.gca()", "raw_solution": "from scipy.spatial.distance import cdist\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(n_samples=200, centers=4, plot_path=None, random_seed=None):\n    # Generate synthetic 2D dataset\n    X, y = make_blobs(n_samples=n_samples, centers=centers, random_state=random_seed)\n\n    # Calculate Euclidean distance between individual samples\n    dist_matrix = cdist(X, X, metric='euclidean')\n\n    # Visualize the dataset\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    plt.colorbar(scatter, ax=ax)\n\n    if plot_path is not None:\n        plt.savefig(plot_path)\n        plt.close(fig)\n        return None\n    else:\n        return dist_matrix, plt.gca()"}
{"task_id": "BigCodeBench/451", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(n_components=2, N_SAMPLES=500, N_FEATURES=50, random_seed=None):\n    \"\"\"\n    Generate a high-dimensional dataset, run PCA to reduce its dimensionality, and then draw a heatmap of the covariance matrix of the transformed data.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate a high-dimensional dataset\n    X = np.random.randn(N_SAMPLES, N_FEATURES)\n\n    # Run PCA to reduce its dimensionality\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(X)\n\n    # Draw a heatmap of the covariance matrix of the transformed data\n    cov_matrix = np.cov(transformed_data, rowvar=False)\n    heatmap_axes = None\n    if n_components > 1:\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cov_matrix, annot=True, fmt=\".3f\", cmap='coolwarm', cbar=True)\n        plt.title('Covariance Matrix of Transformed Data')\n        heatmap_axes = plt.gca()\n\n    return transformed_data, heatmap_axes", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(n_components=2, N_SAMPLES=500, N_FEATURES=50, random_seed=None):\n    \"\"\"\n    Generate a high-dimensional dataset, run PCA to reduce its dimensionality, and then draw a heatmap of the covariance matrix of the transformed data.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Generate a high-dimensional dataset\n    X = np.random.randn(N_SAMPLES, N_FEATURES)\n\n    # Run PCA to reduce its dimensionality\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(X)\n\n    # Draw a heatmap of the covariance matrix of the transformed data\n    cov_matrix = np.cov(transformed_data, rowvar=False)\n    heatmap_axes = None\n    if n_components > 1:\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cov_matrix, annot=True, fmt=\".3f\", cmap='coolwarm', cbar=True)\n        plt.title('Covariance Matrix of Transformed Data')\n        heatmap_axes = plt.gca()\n\n    return transformed_data, heatmap_axes"}
{"task_id": "BigCodeBench/452", "solution": "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\ndef task_func(n_samples=100, n_features=10, random_seed=None):\n    # Generate synthetic data\n    X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=n_features, coef=True, random_state=random_seed)\n\n    # Split data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n\n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Get predictions\n    predictions = model.predict(X_test)\n\n    # Get coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n\n    # Calculate MSE\n    mse = mean_squared_error(y_test, predictions)\n\n    return (predictions, coefficients, intercept, mse)", "raw_solution": "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\ndef task_func(n_samples=100, n_features=10, random_seed=None):\n    # Generate synthetic data\n    X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=n_features, coef=True, random_state=random_seed)\n\n    # Split data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n\n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Get predictions\n    predictions = model.predict(X_test)\n\n    # Get coefficients and intercept\n    coefficients = model.coef_\n    intercept = model.intercept_\n\n    # Calculate MSE\n    mse = mean_squared_error(y_test, predictions)\n\n    return (predictions, coefficients, intercept, mse)"}
{"task_id": "BigCodeBench/453", "solution": "import re\nimport string\nfrom random import choice\ndef task_func(n, pattern):\n    while True:\n        random_string = ''.join(choice(string.ascii_letters) for _ in range(n))\n        if re.match(pattern, random_string):\n            return random_string", "raw_solution": "import re\nimport string\nfrom random import choice\n\ndef task_func(n, pattern):\n    while True:\n        random_string = ''.join(choice(string.ascii_letters) for _ in range(n))\n        if re.match(pattern, random_string):\n            return random_string\n\n# Test the function"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n    \"\"\"\n    # Check if source and destination directories exist\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory {src_dir} does not exist.\")\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"Destination directory {dest_dir} does not exist.\")\n\n    # Get a list of files with the specified extension in the source directory\n    files = glob.glob(os.path.join(src_dir, f\"*.{ext}\"))\n\n    # Initialize an empty list to store the paths of successfully moved files\n    moved_files = []\n\n    # Move each file to the destination directory\n    for file in files:\n        # Get the file name without the extension\n        file_name = os.path.splitext(os.path.basename(file))[0]\n        # Construct the destination file path\n        dest_file = os.path.join(dest_dir, f\"{file_name}.{ext}\")\n        # If the destination file does not exist, move the file\n        if not os.path.isfile(dest_file):\n            shutil.move(file, dest_dir)\n            moved_files.append(dest_file)\n\n    # Return the list of successfully moved files\n    return moved_files", "raw_solution": "import os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n    \"\"\"\n    # Check if source and destination directories exist\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory {src_dir} does not exist.\")\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"Destination directory {dest_dir} does not exist.\")\n\n    # Get a list of files with the specified extension in the source directory\n    files = glob.glob(os.path.join(src_dir, f\"*.{ext}\"))\n\n    # Initialize an empty list to store the paths of successfully moved files\n    moved_files = []\n\n    # Move each file to the destination directory\n    for file in files:\n        # Get the file name without the extension\n        file_name = os.path.splitext(os.path.basename(file))[0]\n        # Construct the destination file path\n        dest_file = os.path.join(dest_dir, f\"{file_name}.{ext}\")\n        # If the destination file does not exist, move the file\n        if not os.path.isfile(dest_file):\n            shutil.move(file, dest_dir)\n            moved_files.append(dest_file)\n\n    # Return the list of successfully moved files\n    return moved_files"}
{"task_id": "BigCodeBench/455", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(mean, std_dev, n):\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, n)\n\n    # Create a range for the x-axis of the histogram\n    x = np.linspace(mean - 4*std_dev, mean + 4*std_dev, 100)\n\n    # Plotting\n    plt.figure(figsize=(12, 6))\n\n    # Histogram\n    plt.subplot(2, 1, 1)\n    plt.hist(samples, bins=30, density=True)\n    plt.title('Histogram of generated samples')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n\n    # Probability density function\n    plt.subplot(2, 1, 2)\n    plt.plot(x, stats.norm.pdf(x, mean, std_dev))\n    plt.title('Probability density function')\n    plt.xlabel('Value')\n    plt.ylabel('Probability')\n\n    plt.tight_layout()\n    plt.show()\n\n    return samples", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(mean, std_dev, n):\n    # Generate samples from a normal distribution\n    samples = np.random.normal(mean, std_dev, n)\n\n    # Create a range for the x-axis of the histogram\n    x = np.linspace(mean - 4*std_dev, mean + 4*std_dev, 100)\n\n    # Plotting\n    plt.figure(figsize=(12, 6))\n\n    # Histogram\n    plt.subplot(2, 1, 1)\n    plt.hist(samples, bins=30, density=True)\n    plt.title('Histogram of generated samples')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n\n    # Probability density function\n    plt.subplot(2, 1, 2)\n    plt.plot(x, stats.norm.pdf(x, mean, std_dev))\n    plt.title('Probability density function')\n    plt.xlabel('Value')\n    plt.ylabel('Probability')\n\n    plt.tight_layout()\n    plt.show()\n\n    return samples\n\n# Test the function\nsamples = task_func(5, 2, 500)"}
{"task_id": "BigCodeBench/456", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, plt.Axes):\n    \"\"\"\n    Normalizes the data and visualizes it using a heatmap.\n    \"\"\"\n    # Normalize data\n    scaler = MinMaxScaler()\n    normalized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n\n    # Create heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(normalized_data, cmap='YlGnBu', cbar_kws={\"label\": \"Normalized Value\"})\n\n    return normalized_data, heatmap.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data: pd.DataFrame) -> (pd.DataFrame, plt.Axes):\n    \"\"\"\n    Normalizes the data and visualizes it using a heatmap.\n    \"\"\"\n    # Normalize data\n    scaler = MinMaxScaler()\n    normalized_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n\n    # Create heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(normalized_data, cmap='YlGnBu', cbar_kws={\"label\": \"Normalized Value\"})\n\n    return normalized_data, heatmap.figure"}
{"task_id": "BigCodeBench/457", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    \"\"\"\n    Draws a histogram of all elements in a nested list 'L' and returns the Axes object of the plot.\n    \"\"\"\n    if not isinstance(L, list):\n        raise TypeError(\"Input should be a list of list of integers.\")\n    for sublist in L:\n        if not isinstance(sublist, list):\n            raise TypeError(\"Input should be a list of list of integers.\")\n        for item in sublist:\n            if not isinstance(item, int):\n                raise TypeError(\"Input should be a list of list of integers.\")\n\n    # Flatten the list and ignore empty sublists\n    flat_list = [item for sublist in L for item in sublist if sublist]\n\n    # Convert the list to a pandas Series\n    series = pd.Series(flat_list)\n\n    # Create a histogram\n    ax = series.plot.hist(bins=series.unique(), rwidth=0.8, grid=True)\n\n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    \"\"\"\n    Draws a histogram of all elements in a nested list 'L' and returns the Axes object of the plot.\n    \"\"\"\n    if not isinstance(L, list):\n        raise TypeError(\"Input should be a list of list of integers.\")\n    for sublist in L:\n        if not isinstance(sublist, list):\n            raise TypeError(\"Input should be a list of list of integers.\")\n        for item in sublist:\n            if not isinstance(item, int):\n                raise TypeError(\"Input should be a list of list of integers.\")\n\n    # Flatten the list and ignore empty sublists\n    flat_list = [item for sublist in L for item in sublist if sublist]\n\n    # Convert the list to a pandas Series\n    series = pd.Series(flat_list)\n\n    # Create a histogram\n    ax = series.plot.hist(bins=series.unique(), rwidth=0.8, grid=True)\n\n    return ax"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    \"\"\"\n    Loads a JSON string into a dictionary, normalizes the dictionary by doubling the numerical values,\n    and then creates a Pandas DataFrame from the dictionary.\n    \"\"\"\n    # Check if the JSON string is empty\n    if not json_str:\n        return pd.DataFrame()\n\n    # Load JSON string into a dictionary\n    data_dict = json.loads(json_str)\n\n    # Normalize the dictionary by doubling the numerical values\n    def normalize_value(value):\n        if isinstance(value, (int, float)):\n            return value * 2\n        elif isinstance(value, str):\n            # Extract numerical values from the string\n            numbers = re.findall(r\"\\d+\\.?\\d*\", value)\n            # Convert extracted numbers to float and double them\n            return [float(num) * 2 for num in numbers]\n        elif isinstance(value, list):\n            return [normalize_value(item) for item in value]\n        else:\n            return value\n\n    data_dict = {key: normalize_value(value) for key, value in data_dict.items()}\n\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(data_dict)\n\n    return df", "raw_solution": "import json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    \"\"\"\n    Loads a JSON string into a dictionary, normalizes the dictionary by doubling the numerical values,\n    and then creates a Pandas DataFrame from the dictionary.\n    \"\"\"\n    # Check if the JSON string is empty\n    if not json_str:\n        return pd.DataFrame()\n\n    # Load JSON string into a dictionary\n    data_dict = json.loads(json_str)\n\n    # Normalize the dictionary by doubling the numerical values\n    def normalize_value(value):\n        if isinstance(value, (int, float)):\n            return value * 2\n        elif isinstance(value, str):\n            # Extract numerical values from the string\n            numbers = re.findall(r\"\\d+\\.?\\d*\", value)\n            # Convert extracted numbers to float and double them\n            return [float(num) * 2 for num in numbers]\n        elif isinstance(value, list):\n            return [normalize_value(item) for item in value]\n        else:\n            return value\n\n    data_dict = {key: normalize_value(value) for key, value in data_dict.items()}\n\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(data_dict)\n\n    return df"}
{"task_id": "BigCodeBench/459", "solution": "import subprocess\nimport os\nimport time\nfrom datetime import datetime\ndef task_func(script_dir, scripts, delay):\n    \"\"\"\n    Executes a list of bash scripts with a specified delay between each script.\n    Raises ValueError if delay is negative or no scripts are provided.\n    Returns a list of timestamps indicating the start time of each script execution.\n    \"\"\"\n    if delay < 0:\n        raise ValueError(\"Delay cannot be negative\")\n    if not scripts:\n        raise ValueError(\"No scripts provided\")\n\n    timestamps = []\n\n    for script in scripts:\n        script_path = os.path.join(script_dir, script)\n        if not os.path.exists(script_path):\n            raise ValueError(f\"Script {script} not found\")\n\n        timestamp = datetime.now()\n        timestamps.append(timestamp)\n\n        print(f\"Executing script {script} at {timestamp}\")\n        subprocess.run([\"bash\", script_path])\n\n        if delay > 0:\n            print(f\"Waiting for {delay} seconds before executing next script\")\n            time.sleep(delay)\n\n    return timestamps", "raw_solution": "import subprocess\nimport os\nimport time\nfrom datetime import datetime\n\ndef task_func(script_dir, scripts, delay):\n    \"\"\"\n    Executes a list of bash scripts with a specified delay between each script.\n    Raises ValueError if delay is negative or no scripts are provided.\n    Returns a list of timestamps indicating the start time of each script execution.\n    \"\"\"\n    if delay < 0:\n        raise ValueError(\"Delay cannot be negative\")\n    if not scripts:\n        raise ValueError(\"No scripts provided\")\n\n    timestamps = []\n\n    for script in scripts:\n        script_path = os.path.join(script_dir, script)\n        if not os.path.exists(script_path):\n            raise ValueError(f\"Script {script} not found\")\n\n        timestamp = datetime.now()\n        timestamps.append(timestamp)\n\n        print(f\"Executing script {script} at {timestamp}\")\n        subprocess.run([\"bash\", script_path])\n\n        if delay > 0:\n            print(f\"Waiting for {delay} seconds before executing next script\")\n            time.sleep(delay)\n\n    return timestamps"}
{"task_id": "BigCodeBench/460", "solution": "import subprocess\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(script_path, output_file_path):\n    \"\"\"\n    Executes a script to produce a CSV, reads the CSV, and plots a bar graph from the data.\n    \"\"\"\n    try:\n        # Execute the script\n        subprocess.check_output([\"python\", script_path, output_file_path])\n    except subprocess.CalledProcessError as e:\n        raise ValueError(f\"Script execution failed: {str(e)}\")\n\n    # Check if the CSV file exists\n    if not os.path.exists(output_file_path):\n        raise ValueError(f\"CSV file {output_file_path} does not exist\")\n\n    # Read the CSV into a DataFrame\n    try:\n        df = pd.read_csv(output_file_path)\n    except pd.errors.ParserError as e:\n        raise ValueError(f\"Invalid CSV file: {str(e)}\")\n\n    # Check if the CSV has exactly 2 columns\n    if df.shape[1] != 2:\n        raise ValueError(f\"CSV file {output_file_path} does not have exactly 2 columns\")\n\n    # Plot a bar graph\n    fig, ax = plt.subplots()\n    ax.bar(df.iloc[:, 0], df.iloc[:, 1])\n    ax.set_xlabel(df.columns[0])\n    ax.set_ylabel(df.columns[1])\n\n    return df, ax.figure", "raw_solution": "import subprocess\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(script_path, output_file_path):\n    \"\"\"\n    Executes a script to produce a CSV, reads the CSV, and plots a bar graph from the data.\n    \"\"\"\n    try:\n        # Execute the script\n        subprocess.check_output([\"python\", script_path, output_file_path])\n    except subprocess.CalledProcessError as e:\n        raise ValueError(f\"Script execution failed: {str(e)}\")\n\n    # Check if the CSV file exists\n    if not os.path.exists(output_file_path):\n        raise ValueError(f\"CSV file {output_file_path} does not exist\")\n\n    # Read the CSV into a DataFrame\n    try:\n        df = pd.read_csv(output_file_path)\n    except pd.errors.ParserError as e:\n        raise ValueError(f\"Invalid CSV file: {str(e)}\")\n\n    # Check if the CSV has exactly 2 columns\n    if df.shape[1] != 2:\n        raise ValueError(f\"CSV file {output_file_path} does not have exactly 2 columns\")\n\n    # Plot a bar graph\n    fig, ax = plt.subplots()\n    ax.bar(df.iloc[:, 0], df.iloc[:, 1])\n    ax.set_xlabel(df.columns[0])\n    ax.set_ylabel(df.columns[1])\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n\n    start_time = time.time()\n    process = subprocess.Popen(['python', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    cpu_percent = 0.0\n    memory_usage = 0\n    while True:\n        if time.time() - start_time > timeout:\n            process.terminate()\n            break\n\n        if psutil.pid_exists(process.pid):\n            process_info = psutil.Process(process.pid)\n            cpu_percent += process_info.cpu_percent(interval=1)\n            memory_usage += process_info.memory_info().rss\n        else:\n            break\n\n    return {'CPU Usage': cpu_percent, 'Memory Usage': memory_usage}", "raw_solution": "import subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n\n    start_time = time.time()\n    process = subprocess.Popen(['python', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    cpu_percent = 0.0\n    memory_usage = 0\n    while True:\n        if time.time() - start_time > timeout:\n            process.terminate()\n            break\n\n        if psutil.pid_exists(process.pid):\n            process_info = psutil.Process(process.pid)\n            cpu_percent += process_info.cpu_percent(interval=1)\n            memory_usage += process_info.memory_info().rss\n        else:\n            break\n\n    return {'CPU Usage': cpu_percent, 'Memory Usage': memory_usage}\n\n# Test the function"}
{"task_id": "BigCodeBench/462", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport random\ndef task_func(num_rows=100, categories=[\"a\", \"b\", \"c\", \"d\", \"e\"], random_seed=42):\n    if num_rows < 1:\n        raise ValueError(\"num_rows must be greater than 0\")\n    random.seed(random_seed)\n    data = {\"Category\": [random.choice(categories) for _ in range(num_rows)],\n            \"Value\": [random.randint(1, 100) for _ in range(num_rows)]}\n    df = pd.DataFrame(data)\n    counts = df['Category'].value_counts()\n    fig, ax = plt.subplots()\n    counts.plot(kind='bar', ax=ax)\n    ax.set_title('Category Counts')\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\ndef task_func(num_rows=100, categories=[\"a\", \"b\", \"c\", \"d\", \"e\"], random_seed=42):\n    if num_rows < 1:\n        raise ValueError(\"num_rows must be greater than 0\")\n    random.seed(random_seed)\n    data = {\"Category\": [random.choice(categories) for _ in range(num_rows)],\n            \"Value\": [random.randint(1, 100) for _ in range(num_rows)]}\n    df = pd.DataFrame(data)\n    counts = df['Category'].value_counts()\n    fig, ax = plt.subplots()\n    counts.plot(kind='bar', ax=ax)\n    ax.set_title('Category Counts')\n    return df, ax.figure"}
{"task_id": "BigCodeBench/463", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_str, separator=\",\", bins=20):\n    \"\"\"\n    Converts a string of numerical values separated by a specified separator into a pandas numerical series with int64,\n    and then draw a histogram of the data.\n    \"\"\"\n    if not data_str:\n        raise ValueError(\"Data is empty\")\n\n    data = data_str.split(separator)\n    try:\n        series = pd.Series(data).astype(int)\n    except ValueError:\n        raise ValueError(\"Failed to convert data to integers\")\n\n    fig, ax = plt.subplots()\n    ax.hist(series, bins=bins, grid=True, rwidth=0.9, color='#607c8e')\n    plt.show()\n\n    return series, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_str, separator=\",\", bins=20):\n    \"\"\"\n    Converts a string of numerical values separated by a specified separator into a pandas numerical series with int64,\n    and then draw a histogram of the data.\n    \"\"\"\n    if not data_str:\n        raise ValueError(\"Data is empty\")\n\n    data = data_str.split(separator)\n    try:\n        series = pd.Series(data).astype(int)\n    except ValueError:\n        raise ValueError(\"Failed to convert data to integers\")\n\n    fig, ax = plt.subplots()\n    ax.hist(series, bins=bins, grid=True, rwidth=0.9, color='#607c8e')\n    plt.show()\n\n    return series, ax"}
{"task_id": "BigCodeBench/464", "solution": "import json\nfrom datetime import datetime\nfrom decimal import Decimal\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        return super().default(obj)\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)", "raw_solution": "import json\nfrom datetime import datetime\nfrom decimal import Decimal\n\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        return super().default(obj)\n\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)\n\n# Testing"}
{"task_id": "BigCodeBench/465", "solution": "import json\nfrom datetime import datetime\nimport numpy as np\nfrom decimal import Decimal\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        else:\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)", "raw_solution": "import json\nfrom datetime import datetime\nimport numpy as np\nfrom decimal import Decimal\n\nclass CustomEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, Decimal):\n            return str(obj)\n        else:\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n\ndef task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomEncoder)\n\n# Testing"}
{"task_id": "BigCodeBench/466", "solution": "import json\nfrom enum import Enum\nfrom typing import Any\nclass EnumEncoder(json.JSONEncoder):\n    def default(self, obj: Any) -> Any:\n        if isinstance(obj, Enum):\n            return obj.name\n        return super().default(obj)\ndef task_func(my_obj):\n    \"\"\"\n    Serializes an object into a JSON string with support for complex data types like Enum.\n    \"\"\"\n    return json.dumps(my_obj, cls=EnumEncoder)", "raw_solution": "import json\nfrom enum import Enum\nfrom typing import Any\n\nclass Color(Enum):\n    RED = 1\n    GREEN = 2\n    BLUE = 3\n\nclass EnumEncoder(json.JSONEncoder):\n    def default(self, obj: Any) -> Any:\n        if isinstance(obj, Enum):\n            return obj.name\n        return super().default(obj)\n\ndef task_func(my_obj):\n    \"\"\"\n    Serializes an object into a JSON string with support for complex data types like Enum.\n    \"\"\"\n    return json.dumps(my_obj, cls=EnumEncoder)\n\n# Test the function"}
{"task_id": "BigCodeBench/467", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(n, seed=0):\n    np.random.seed(seed)\n    points = [(x, y) for x, y in np.random.rand(n, 2)]\n\n    fig, ax = plt.subplots()\n    ax.scatter([x for x, y in points], [y for x, y in points])\n    ax.set_title(\"Scatter plot of random points\")\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n\n    return fig, points", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(n, seed=0):\n    np.random.seed(seed)\n    points = [(x, y) for x, y in np.random.rand(n, 2)]\n\n    fig, ax = plt.subplots()\n    ax.scatter([x for x, y in points], [y for x, y in points])\n    ax.set_title(\"Scatter plot of random points\")\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n\n    return fig, points"}
{"task_id": "BigCodeBench/468", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(file_path=\"data.csv\", columns=[\"A\", \"B\", \"C\"]):\n    # Read CSV file into DataFrame\n    df = pd.read_csv(file_path)\n\n    # Convert numeric values into floats\n    df[columns] = df[columns].apply(pd.to_numeric, errors='coerce')\n\n    # Draw a line chart of data in the specified columns\n    ax = df[columns].plot(kind='line')\n\n    # Compute the cube-root of the data\n    cube_root_series = np.cbrt(df[columns])\n\n    return df, ax.figure, cube_root_series", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(file_path=\"data.csv\", columns=[\"A\", \"B\", \"C\"]):\n    # Read CSV file into DataFrame\n    df = pd.read_csv(file_path)\n\n    # Convert numeric values into floats\n    df[columns] = df[columns].apply(pd.to_numeric, errors='coerce')\n\n    # Draw a line chart of data in the specified columns\n    ax = df[columns].plot(kind='line')\n\n    # Compute the cube-root of the data\n    cube_root_series = np.cbrt(df[columns])\n\n    return df, ax.figure, cube_root_series"}
{"task_id": "BigCodeBench/469", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom typing import Tuple\ndef task_func(student_grades, possible_grades=[\"A\", \"B\", \"C\", \"D\", \"F\"]):\n    # Normalize grades to uppercase and remove leading/trailing whitespaces\n    student_grades = [grade.strip().upper() for grade in student_grades]\n\n    # Filter out grades not in possible grades\n    student_grades = [grade for grade in student_grades if grade in possible_grades]\n\n    # Count grades\n    grade_counts = Counter(student_grades)\n\n    # Create DataFrame\n    df = pd.DataFrame.from_dict(grade_counts, orient='index', columns=['Count'])\n    df.index.name = 'Grade'\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Grade')\n    ax.set_ylabel('Number of Students')\n    ax.set_title('Grade Distribution')\n\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom typing import Tuple\n\ndef task_func(student_grades, possible_grades=[\"A\", \"B\", \"C\", \"D\", \"F\"]):\n    # Normalize grades to uppercase and remove leading/trailing whitespaces\n    student_grades = [grade.strip().upper() for grade in student_grades]\n\n    # Filter out grades not in possible grades\n    student_grades = [grade for grade in student_grades if grade in possible_grades]\n\n    # Count grades\n    grade_counts = Counter(student_grades)\n\n    # Create DataFrame\n    df = pd.DataFrame.from_dict(grade_counts, orient='index', columns=['Count'])\n    df.index.name = 'Grade'\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Grade')\n    ax.set_ylabel('Number of Students')\n    ax.set_title('Grade Distribution')\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/470", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(myList):\n    \"\"\"\n    Draws a histogram of the values in a list and returns the plot's Axes.\n    \"\"\"\n    # Create a histogram\n    ax = plt.hist(myList, bins=range(min(myList), max(myList) + 2), edgecolor='black')\n\n    # Set labels and title\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Values')\n\n    # Return the Axes object\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(myList):\n    \"\"\"\n    Draws a histogram of the values in a list and returns the plot's Axes.\n    \"\"\"\n    # Create a histogram\n    ax = plt.hist(myList, bins=range(min(myList), max(myList) + 2), edgecolor='black')\n\n    # Set labels and title\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Values')\n\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/471", "solution": "from collections import Counter\nimport pandas as pd\ndef task_func(myList):\n    \"\"\"\n    Count the frequency of each word in a list and return a DataFrame of words and their number.\n    \"\"\"\n    # Count the frequency of each word in the list\n    word_counts = Counter(myList)\n\n    # Create a DataFrame from the word counts\n    df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n\n    return df\nmyList = ['apple', 'banana', 'apple', 'orange', 'banana', 'banana']", "raw_solution": "from collections import Counter\nimport pandas as pd\n\ndef task_func(myList):\n    \"\"\"\n    Count the frequency of each word in a list and return a DataFrame of words and their number.\n    \"\"\"\n    # Count the frequency of each word in the list\n    word_counts = Counter(myList)\n\n    # Create a DataFrame from the word counts\n    df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n\n    return df\n\n# Test the function\nmyList = ['apple', 'banana', 'apple', 'orange', 'banana', 'banana']"}
{"task_id": "BigCodeBench/472", "solution": "import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\ndef task_func(myList, n_clusters):\n    \"\"\"\n    Clusters a list of 2D points using KMeans and visualizes the clusters.\n    \"\"\"\n    # Validate inputs\n    if not all(isinstance(i, (list, tuple)) and len(i) == 2 for i in myList):\n        raise ValueError(\"All elements in myList must be 2-element lists or tuples.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer.\")\n\n    # Convert list of lists/tuples to numpy array\n    points = np.array(myList)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    kmeans.fit(points)\n\n    # Create scatter plot\n    plt.scatter(points[:, 0], points[:, 1], c=kmeans.labels_)\n\n    # Plot cluster centers\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', color='r')\n\n    return plt.gca()", "raw_solution": "import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef task_func(myList, n_clusters):\n    \"\"\"\n    Clusters a list of 2D points using KMeans and visualizes the clusters.\n    \"\"\"\n    # Validate inputs\n    if not all(isinstance(i, (list, tuple)) and len(i) == 2 for i in myList):\n        raise ValueError(\"All elements in myList must be 2-element lists or tuples.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer.\")\n\n    # Convert list of lists/tuples to numpy array\n    points = np.array(myList)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    kmeans.fit(points)\n\n    # Create scatter plot\n    plt.scatter(points[:, 0], points[:, 1], c=kmeans.labels_)\n\n    # Plot cluster centers\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', color='r')\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/473", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(n_walks, n_steps, seed=None):\n    \"\"\"\n    Generates and plots `n_walks` number of random walks, each with `n_steps` steps.\n    \"\"\"\n    if n_walks <= 0 or n_steps <= 0:\n        raise ValueError(\"Both `n_walks` and `n_steps` must be greater than 0.\")\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Define the possible steps\n    steps = [-1, 1]\n\n    # Generate the random walks\n    walks = np.random.choice(steps, size=(n_walks, n_steps))\n    walks = np.cumsum(walks, axis=1)\n\n    # Define the colors\n    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\n    # Create the plot\n    fig, ax = plt.subplots()\n\n    # Plot each walk\n    for walk, color in zip(walks, itertools.cycle(colors)):\n        ax.plot(walk, color=color)\n\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Position')\n    ax.set_title('Random Walks')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef task_func(n_walks, n_steps, seed=None):\n    \"\"\"\n    Generates and plots `n_walks` number of random walks, each with `n_steps` steps.\n    \"\"\"\n    if n_walks <= 0 or n_steps <= 0:\n        raise ValueError(\"Both `n_walks` and `n_steps` must be greater than 0.\")\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Define the possible steps\n    steps = [-1, 1]\n\n    # Generate the random walks\n    walks = np.random.choice(steps, size=(n_walks, n_steps))\n    walks = np.cumsum(walks, axis=1)\n\n    # Define the colors\n    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\n    # Create the plot\n    fig, ax = plt.subplots()\n\n    # Plot each walk\n    for walk, color in zip(walks, itertools.cycle(colors)):\n        ax.plot(walk, color=color)\n\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Position')\n    ax.set_title('Random Walks')\n\n    return ax"}
{"task_id": "BigCodeBench/474", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(n_samples=1000, mu=0, sigma=1, random_seed=0):\n    np.random.seed(random_seed)\n    samples = np.random.normal(mu, sigma, n_samples)\n\n    fig, ax = plt.subplots(1, 1)\n    values = [mu - 3*sigma, mu - 2*sigma, mu - sigma, mu, mu + sigma, mu + 2*sigma, mu + 3*sigma]\n    colors = ['r', 'g', 'b', 'y', 'c', 'm', 'k']\n    for i in range(7):\n        values[i] = round(values[i], 2)\n        density = norm.pdf(values[i], mu, sigma)\n        ax.plot(values[i], density, 'o', color=colors[i])\n\n    ax.hist(samples, bins=30, density=True, alpha=0.5, color='g')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Histogram and PDF Plot')\n\n    return ax, samples", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(n_samples=1000, mu=0, sigma=1, random_seed=0):\n    np.random.seed(random_seed)\n    samples = np.random.normal(mu, sigma, n_samples)\n\n    fig, ax = plt.subplots(1, 1)\n    values = [mu - 3*sigma, mu - 2*sigma, mu - sigma, mu, mu + sigma, mu + 2*sigma, mu + 3*sigma]\n    colors = ['r', 'g', 'b', 'y', 'c', 'm', 'k']\n    for i in range(7):\n        values[i] = round(values[i], 2)\n        density = norm.pdf(values[i], mu, sigma)\n        ax.plot(values[i], density, 'o', color=colors[i])\n\n    ax.hist(samples, bins=30, density=True, alpha=0.5, color='g')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Histogram and PDF Plot')\n\n    return ax, samples\n\n# Test the function\nax, samples = task_func(n_samples=1000, mu=0, sigma=1, random_seed=0)\nplt.show()"}
{"task_id": "BigCodeBench/475", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\ndef task_func(data, date_format, country, country_codes=None):\n    \"\"\"\n    Draws a histogram of the data from a DataFrame column of the pandas after converting the data into a specific format,\n    and returns the matplotlib Axes object.\n    \"\"\"\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n\n    # Check if date_format is a string\n    if not isinstance(date_format, str):\n        raise ValueError(\"'date_format' should be a string\")\n\n    # Check if country is in data\n    if country not in data.columns:\n        raise ValueError(\"'country' should be a column in the DataFrame\")\n\n    # Check if country_codes is a dictionary\n    if country_codes is not None and not isinstance(country_codes, dict):\n        raise ValueError(\"'country_codes' should be a dictionary\")\n\n    # Convert the date column to datetime\n    data[country] = pd.to_datetime(data[country], format=date_format)\n\n    # Plot the histogram\n    ax = data[country].hist(bins=30)\n\n    # Set the title and y label\n    plt.title('Date Distribution')\n    plt.ylabel('Frequency')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\ndef task_func(data, date_format, country, country_codes=None):\n    \"\"\"\n    Draws a histogram of the data from a DataFrame column of the pandas after converting the data into a specific format,\n    and returns the matplotlib Axes object.\n    \"\"\"\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' should be a pandas DataFrame\")\n\n    # Check if date_format is a string\n    if not isinstance(date_format, str):\n        raise ValueError(\"'date_format' should be a string\")\n\n    # Check if country is in data\n    if country not in data.columns:\n        raise ValueError(\"'country' should be a column in the DataFrame\")\n\n    # Check if country_codes is a dictionary\n    if country_codes is not None and not isinstance(country_codes, dict):\n        raise ValueError(\"'country_codes' should be a dictionary\")\n\n    # Convert the date column to datetime\n    data[country] = pd.to_datetime(data[country], format=date_format)\n\n    # Plot the histogram\n    ax = data[country].hist(bins=30)\n\n    # Set the title and y label\n    plt.title('Date Distribution')\n    plt.ylabel('Frequency')\n\n    return ax"}
{"task_id": "BigCodeBench/476", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import curve_fit\ndef task_func(X, Y):\n    def quad_func(x, a, b, c):\n        return a*x**2 + b*x + c\n\n    popt, _ = curve_fit(quad_func, X, Y)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X, Y, color='red', label='Data')\n    ax.plot(X, quad_func(X, *popt), color='blue', label='Fit')\n    ax.legend()\n    plt.show()\n\n    return (popt, ax)\nX = np.array([1, 2, 3, 4, 5])\nY = np.array([1, 3, 5, 7, 9])", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef task_func(X, Y):\n    def quad_func(x, a, b, c):\n        return a*x**2 + b*x + c\n\n    popt, _ = curve_fit(quad_func, X, Y)\n\n    fig, ax = plt.subplots()\n    ax.scatter(X, Y, color='red', label='Data')\n    ax.plot(X, quad_func(X, *popt), color='blue', label='Fit')\n    ax.legend()\n    plt.show()\n\n    return (popt, ax)\n\n# Test the function\nX = np.array([1, 2, 3, 4, 5])\nY = np.array([1, 3, 5, 7, 9])"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n\n    # Generate DataFrame\n    df = pd.DataFrame({\n        \"x\": np.random.rand(N),\n        \"y\": np.random.rand(N),\n    })\n\n    # Generate categories\n    if N >= len(CATEGORIES):\n        df[\"category\"] = np.random.choice(CATEGORIES, N)\n    else:\n        df[\"category\"] = np.random.choice(CATEGORIES, N, replace=False)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    for category in CATEGORIES:\n        df_category = df[df[\"category\"] == category]\n        ax.scatter(df_category[\"x\"], df_category[\"y\"], label=category)\n    ax.legend()\n\n    return df, ax.figure", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n\n    # Generate DataFrame\n    df = pd.DataFrame({\n        \"x\": np.random.rand(N),\n        \"y\": np.random.rand(N),\n    })\n\n    # Generate categories\n    if N >= len(CATEGORIES):\n        df[\"category\"] = np.random.choice(CATEGORIES, N)\n    else:\n        df[\"category\"] = np.random.choice(CATEGORIES, N, replace=False)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    for category in CATEGORIES:\n        df_category = df[df[\"category\"] == category]\n        ax.scatter(df_category[\"x\"], df_category[\"y\"], label=category)\n    ax.legend()\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/478", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(data_list, seed=None):\n    \"\"\"\n    Removes a random comma-separated value (treated as a \"substring\") from each string in a list and returns a pandas DataFrame containing the original and modified strings.\n    \"\"\"\n    random.seed(seed)\n    original_strings = []\n    modified_strings = []\n    for string in data_list:\n        original_strings.append(string)\n        # Split the string by commas\n        parts = string.split(',')\n        # Remove a random part\n        parts.pop(random.randint(0, len(parts) - 1))\n        # Join the parts back together\n        modified_string = ','.join(parts)\n        modified_strings.append(modified_string)\n    df = pd.DataFrame({'Original String': original_strings, 'Modified String': modified_strings})\n    return df", "raw_solution": "import pandas as pd\nimport re\nimport random\n\ndef task_func(data_list, seed=None):\n    \"\"\"\n    Removes a random comma-separated value (treated as a \"substring\") from each string in a list and returns a pandas DataFrame containing the original and modified strings.\n    \"\"\"\n    random.seed(seed)\n    original_strings = []\n    modified_strings = []\n    for string in data_list:\n        original_strings.append(string)\n        # Split the string by commas\n        parts = string.split(',')\n        # Remove a random part\n        parts.pop(random.randint(0, len(parts) - 1))\n        # Join the parts back together\n        modified_string = ','.join(parts)\n        modified_strings.append(modified_string)\n    df = pd.DataFrame({'Original String': original_strings, 'Modified String': modified_strings})\n    return df"}
{"task_id": "BigCodeBench/479", "solution": "import random\nimport string\nimport pandas as pd\ndef task_func(data_list, seed=0):\n    random.seed(seed)\n    result = []\n    for data in data_list:\n        if data == \"\":\n            continue\n        start = random.randint(0, len(data) - 1)\n        end = random.randint(start, len(data))\n        substring = data[start:end]\n        new_substring = ''.join(random.choices(string.ascii_lowercase, k=len(substring)))\n        result.append((data.replace(substring, new_substring), new_substring))\n    df = pd.DataFrame(result, columns=['Original String', 'Modified String'])\n    return df\ndata_list = [\"hello,world\", \"python,programming\", \"data,science\", \"random,string\", \"\"]", "raw_solution": "import random\nimport string\nimport pandas as pd\n\ndef task_func(data_list, seed=0):\n    random.seed(seed)\n    result = []\n    for data in data_list:\n        if data == \"\":\n            continue\n        start = random.randint(0, len(data) - 1)\n        end = random.randint(start, len(data))\n        substring = data[start:end]\n        new_substring = ''.join(random.choices(string.ascii_lowercase, k=len(substring)))\n        result.append((data.replace(substring, new_substring), new_substring))\n    df = pd.DataFrame(result, columns=['Original String', 'Modified String'])\n    return df\n\n# Test the function\ndata_list = [\"hello,world\", \"python,programming\", \"data,science\", \"random,string\", \"\"]\ndf = task_func(data_list, seed=0)"}
{"task_id": "BigCodeBench/480", "solution": "import re\nimport random\nimport pandas as pd\ndef task_func(data_list, seed=None):\n    \"\"\"\n    Shuffles the substrings within each string in a given list.\n    \"\"\"\n    # Set the seed for the random number generator\n    if seed is not None:\n        random.seed(seed)\n\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over each string in the list\n    for string in data_list:\n        # Split the string into substrings based on commas\n        substrings = [s.strip() for s in re.split(',', string)]\n\n        # Shuffle the substrings\n        random.shuffle(substrings)\n\n        # Join the substrings back into a string\n        shuffled_string = ', '.join(substrings)\n\n        # Append the original string and the shuffled string to the results list\n        results.append({'Original String': string, 'Shuffled String': shuffled_string})\n\n    # Convert the results list to a pandas DataFrame\n    df = pd.DataFrame(results)\n\n    return df", "raw_solution": "import re\nimport random\nimport pandas as pd\n\ndef task_func(data_list, seed=None):\n    \"\"\"\n    Shuffles the substrings within each string in a given list.\n    \"\"\"\n    # Set the seed for the random number generator\n    if seed is not None:\n        random.seed(seed)\n\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over each string in the list\n    for string in data_list:\n        # Split the string into substrings based on commas\n        substrings = [s.strip() for s in re.split(',', string)]\n\n        # Shuffle the substrings\n        random.shuffle(substrings)\n\n        # Join the substrings back into a string\n        shuffled_string = ', '.join(substrings)\n\n        # Append the original string and the shuffled string to the results list\n        results.append({'Original String': string, 'Shuffled String': shuffled_string})\n\n    # Convert the results list to a pandas DataFrame\n    df = pd.DataFrame(results)\n\n    return df"}
{"task_id": "BigCodeBench/481", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(data_list, seed=42):\n    random.seed(seed)\n    result = []\n    for data in data_list:\n        words = data.split(',')\n        random.shuffle(words)\n        randomized_data = ', '.join(words)\n        randomized_data = re.sub(' +', ' ', randomized_data)\n        result.append({'Original String': data, 'Randomized String': randomized_data})\n    return pd.DataFrame(result)\ndata_list = [\"apple, banana, cherry, date\", \"dog, cat, mouse, bird\", \"red, blue, green, yellow\"]", "raw_solution": "import pandas as pd\nimport random\nimport re\n\ndef task_func(data_list, seed=42):\n    random.seed(seed)\n    result = []\n    for data in data_list:\n        words = data.split(',')\n        random.shuffle(words)\n        randomized_data = ', '.join(words)\n        randomized_data = re.sub(' +', ' ', randomized_data)\n        result.append({'Original String': data, 'Randomized String': randomized_data})\n    return pd.DataFrame(result)\n\n# Testing the function\ndata_list = [\"apple, banana, cherry, date\", \"dog, cat, mouse, bird\", \"red, blue, green, yellow\"]\ndf = task_func(data_list)"}
{"task_id": "BigCodeBench/482", "solution": "import pandas as pd\nimport random\nimport re\ndef random_operation(s):\n    \"\"\"\n    Applies a random operation to a string.\n    \"\"\"\n    s = s.strip()\n    if not s:\n        return s\n\n    # Split the string into substrings\n    substrings = s.split(',')\n\n    # Randomly select an operation\n    operation = random.choice(['remove', 'replace', 'shuffle', 'randomize'])\n\n    if operation == 'remove':\n        # Remove a random substring\n        if len(substrings) > 1:\n            substrings.remove(random.choice(substrings))\n    elif operation == 'replace':\n        # Replace a random substring with 'random_string'\n        replace_with = 'random_string'\n        substrings[random.randint(0, len(substrings) - 1)] = replace_with\n    elif operation == 'shuffle':\n        # Shuffle the substrings\n        random.shuffle(substrings)\n    elif operation == 'randomize':\n        # Randomize the order of the substrings\n        random.shuffle(substrings)\n\n    # Join the substrings back into a string\n    return ', '.join(substrings)\ndef task_func(data_list, seed=None):\n    \"\"\"\n    Applies a random operation to substrings in a list of strings.\n    \"\"\"\n    # Set the seed for the random number generator\n    if seed is not None:\n        random.seed(seed)\n\n    # Apply the random operation to each string\n    modified_strings = [random_operation(s) for s in data_list]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Original String': data_list,\n        'Modified String': modified_strings\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport re\n\ndef random_operation(s):\n    \"\"\"\n    Applies a random operation to a string.\n    \"\"\"\n    s = s.strip()\n    if not s:\n        return s\n\n    # Split the string into substrings\n    substrings = s.split(',')\n\n    # Randomly select an operation\n    operation = random.choice(['remove', 'replace', 'shuffle', 'randomize'])\n\n    if operation == 'remove':\n        # Remove a random substring\n        if len(substrings) > 1:\n            substrings.remove(random.choice(substrings))\n    elif operation == 'replace':\n        # Replace a random substring with 'random_string'\n        replace_with = 'random_string'\n        substrings[random.randint(0, len(substrings) - 1)] = replace_with\n    elif operation == 'shuffle':\n        # Shuffle the substrings\n        random.shuffle(substrings)\n    elif operation == 'randomize':\n        # Randomize the order of the substrings\n        random.shuffle(substrings)\n\n    # Join the substrings back into a string\n    return ', '.join(substrings)\n\ndef task_func(data_list, seed=None):\n    \"\"\"\n    Applies a random operation to substrings in a list of strings.\n    \"\"\"\n    # Set the seed for the random number generator\n    if seed is not None:\n        random.seed(seed)\n\n    # Apply the random operation to each string\n    modified_strings = [random_operation(s) for s in data_list]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Original String': data_list,\n        'Modified String': modified_strings\n    })\n\n    return df"}
{"task_id": "BigCodeBench/483", "solution": "import re\nimport pandas as pd\nfrom typing import List\ndef task_func(df: pd.DataFrame, column_name: str, pattern: str) -> pd.DataFrame:\n    \"\"\"\n    Reverse the order of words in a specific column of a pandas DataFrame where the words match a user-specified regular expression pattern.\n    \"\"\"\n    if not pattern:\n        return df.copy()\n\n    def reverse_matching_words(words: List[str], pattern: str) -> List[str]:\n        \"\"\"\n        Helper function to reverse the order of words that match a pattern.\n        \"\"\"\n        return [word if not re.match(pattern, word) else words.pop() for word in words]\n\n    new_column_data = [reverse_matching_words(words.split(), pattern) for words in df[column_name]]\n    new_column_data = [\" \".join(words) for words in new_column_data]\n\n    new_df = df.copy()\n    new_df[column_name] = new_column_data\n\n    return new_df", "raw_solution": "import re\nimport pandas as pd\nfrom typing import List\n\ndef task_func(df: pd.DataFrame, column_name: str, pattern: str) -> pd.DataFrame:\n    \"\"\"\n    Reverse the order of words in a specific column of a pandas DataFrame where the words match a user-specified regular expression pattern.\n    \"\"\"\n    if not pattern:\n        return df.copy()\n\n    def reverse_matching_words(words: List[str], pattern: str) -> List[str]:\n        \"\"\"\n        Helper function to reverse the order of words that match a pattern.\n        \"\"\"\n        return [word if not re.match(pattern, word) else words.pop() for word in words]\n\n    new_column_data = [reverse_matching_words(words.split(), pattern) for words in df[column_name]]\n    new_column_data = [\" \".join(words) for words in new_column_data]\n\n    new_df = df.copy()\n    new_df[column_name] = new_column_data\n\n    return new_df"}
{"task_id": "BigCodeBench/484", "solution": "import math\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport pandas as pd\ndef task_func(\n    start_time,\n    end_time,\n    step,\n    columns=[\"Timestamp\", \"Sensor1\", \"Sensor2\", \"Sensor3\", \"SensorStatus\"],\n    sensor_statuses=[\"OK\", \"MAINTENANCE_REQUIRED\", \"ERROR\"],\n    random_seed=42,\n):\n    np.random.seed(random_seed)\n    start_time = datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n    end_time = datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\")\n    time_range = pd.date_range(start=start_time, end=end_time, freq=step)\n    data = {\n        columns[0]: time_range,\n        columns[1]: np.sin(time_range.to_julian_date()) + np.random.normal(0, 0.1, len(time_range)),\n        columns[2]: np.cos(time_range.to_julian_date()) + np.random.normal(0, 0.1, len(time_range)),\n        columns[3]: np.tan(time_range.to_julian_date()) + np.random.normal(0, 0.1, len(time_range)),\n        columns[4]: np.random.choice(sensor_statuses, len(time_range)),\n    }\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import math\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport pandas as pd\n\ndef task_func(\n    start_time,\n    end_time,\n    step,\n    columns=[\"Timestamp\", \"Sensor1\", \"Sensor2\", \"Sensor3\", \"SensorStatus\"],\n    sensor_statuses=[\"OK\", \"MAINTENANCE_REQUIRED\", \"ERROR\"],\n    random_seed=42,\n):\n    np.random.seed(random_seed)\n    start_time = datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n    end_time = datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\")\n    time_range = pd.date_range(start=start_time, end=end_time, freq=step)\n    data = {\n        columns[0]: time_range,\n        columns[1]: np.sin(time_range.to_julian_date()) + np.random.normal(0, 0.1, len(time_range)),\n        columns[2]: np.cos(time_range.to_julian_date()) + np.random.normal(0, 0.1, len(time_range)),\n        columns[3]: np.tan(time_range.to_julian_date()) + np.random.normal(0, 0.1, len(time_range)),\n        columns[4]: np.random.choice(sensor_statuses, len(time_range)),\n    }\n    df = pd.DataFrame(data)\n    return df\n\n# Test the function\ndf = task_func(\"2022-01-01 00:00:00\", \"2022-01-02 00:00:00\", \"10s\")"}
{"task_id": "BigCodeBench/485", "solution": "from datetime import datetime, timedelta\nimport pytz\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time):\n    \"\"\"\n    Plots the hourly difference between UTC and specified global time zones across a date range.\n    \"\"\"\n    # Define the time zones\n    time_zones = [\"UTC\", \"America/Los_Angeles\", \"Europe/Paris\", \"Asia/Kolkata\", \"Australia/Sydney\"]\n    colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\n\n    # Convert start_time and end_time to datetime objects\n    start_time = datetime.strptime(start_time, \"%Y-%m-%d\")\n    end_time = datetime.strptime(end_time, \"%Y-%m-%d\")\n\n    # Generate a list of dates between start_time and end_time\n    dates = [start_time + timedelta(days=x) for x in range((end_time-start_time).days + 1)]\n\n    # Initialize a dictionary to store the time differences for each date\n    time_diffs = {date: {} for date in dates}\n\n    # Calculate the time difference for each date and time zone\n    for date in dates:\n        for i, time_zone in enumerate(time_zones):\n            # Create a timezone object\n            tz = pytz.timezone(time_zone)\n\n            # Create a datetime object for the date and time zone\n            dt = tz.localize(date)\n\n            # Calculate the time difference in hours\n            time_diff = dt.utcoffset().total_seconds() / 3600\n\n            # Store the time difference in the dictionary\n            time_diffs[date][time_zone] = time_diff\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the time differences\n    for i, time_zone in enumerate(time_zones):\n        # Extract the time differences for the time zone\n        diffs = [time_diffs[date][time_zone] for date in dates]\n\n        # Plot the time differences\n        ax.plot(dates, diffs, color=colors[i % len(colors)], label=time_zone)\n\n    # Set the x-axis label\n    ax.set_xlabel(\"Date\")\n\n    # Set the y-axis label\n    ax.set_ylabel(\"Time difference (hours)\")\n\n    # Set the title\n    ax.set_title(\"Time difference between UTC and global time zones\")\n\n    # Add a legend\n    ax.legend()\n\n    # Return the axes object\n    return ax", "raw_solution": "from datetime import datetime, timedelta\nimport pytz\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time):\n    \"\"\"\n    Plots the hourly difference between UTC and specified global time zones across a date range.\n    \"\"\"\n    # Define the time zones\n    time_zones = [\"UTC\", \"America/Los_Angeles\", \"Europe/Paris\", \"Asia/Kolkata\", \"Australia/Sydney\"]\n    colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\n\n    # Convert start_time and end_time to datetime objects\n    start_time = datetime.strptime(start_time, \"%Y-%m-%d\")\n    end_time = datetime.strptime(end_time, \"%Y-%m-%d\")\n\n    # Generate a list of dates between start_time and end_time\n    dates = [start_time + timedelta(days=x) for x in range((end_time-start_time).days + 1)]\n\n    # Initialize a dictionary to store the time differences for each date\n    time_diffs = {date: {} for date in dates}\n\n    # Calculate the time difference for each date and time zone\n    for date in dates:\n        for i, time_zone in enumerate(time_zones):\n            # Create a timezone object\n            tz = pytz.timezone(time_zone)\n\n            # Create a datetime object for the date and time zone\n            dt = tz.localize(date)\n\n            # Calculate the time difference in hours\n            time_diff = dt.utcoffset().total_seconds() / 3600\n\n            # Store the time difference in the dictionary\n            time_diffs[date][time_zone] = time_diff\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the time differences\n    for i, time_zone in enumerate(time_zones):\n        # Extract the time differences for the time zone\n        diffs = [time_diffs[date][time_zone] for date in dates]\n\n        # Plot the time differences\n        ax.plot(dates, diffs, color=colors[i % len(colors)], label=time_zone)\n\n    # Set the x-axis label\n    ax.set_xlabel(\"Date\")\n\n    # Set the y-axis label\n    ax.set_ylabel(\"Time difference (hours)\")\n\n    # Set the title\n    ax.set_title(\"Time difference between UTC and global time zones\")\n\n    # Add a legend\n    ax.legend()\n\n    # Return the axes object\n    return ax"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    np.random.seed(seed)\n    start_time = pd.to_datetime(start_time)\n    end_time = pd.to_datetime(end_time)\n    time_range = pd.date_range(start=start_time, end=end_time, freq=step)\n    values = np.random.normal(size=len(time_range)) + trend * np.arange(len(time_range))\n    df = pd.DataFrame({'Time': time_range, 'Value': values})\n    ax = df.plot(x='Time', y='Value', kind='line')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    return ax\nstart_time = '2022-01-01'\nend_time = '2022-01-31'\nstep = 'D'\ntrend = 2", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    np.random.seed(seed)\n    start_time = pd.to_datetime(start_time)\n    end_time = pd.to_datetime(end_time)\n    time_range = pd.date_range(start=start_time, end=end_time, freq=step)\n    values = np.random.normal(size=len(time_range)) + trend * np.arange(len(time_range))\n    df = pd.DataFrame({'Time': time_range, 'Value': values})\n    ax = df.plot(x='Time', y='Value', kind='line')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    return ax\n\n# Test the function\nstart_time = '2022-01-01'\nend_time = '2022-01-31'\nstep = 'D'\ntrend = 2\nax = task_func(start_time, end_time, step, trend)\nplt.show()"}
{"task_id": "BigCodeBench/487", "solution": "import os\nimport pandas as pd\nimport re\nfrom typing import List, Tuple\ndef task_func(file_path: str) -> pd.DataFrame:\n    \"\"\"\n    Parses a log file to extract log entries into a DataFrame.\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The specified log file {file_path} does not exist.\")\n\n    log_entries = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            match = re.match(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+) - (\\w+) - (.+)', line)\n            if match:\n                timestamp, level, message = match.groups()\n                log_entries.append((timestamp, level, message))\n\n    if not log_entries:\n        return pd.DataFrame(columns=['Timestamp', 'Level', 'Message'])\n\n    df = pd.DataFrame(log_entries, columns=['Timestamp', 'Level', 'Message'])\n    return df", "raw_solution": "import os\nimport pandas as pd\nimport re\nfrom typing import List, Tuple\n\ndef task_func(file_path: str) -> pd.DataFrame:\n    \"\"\"\n    Parses a log file to extract log entries into a DataFrame.\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The specified log file {file_path} does not exist.\")\n\n    log_entries = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            match = re.match(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+) - (\\w+) - (.+)', line)\n            if match:\n                timestamp, level, message = match.groups()\n                log_entries.append((timestamp, level, message))\n\n    if not log_entries:\n        return pd.DataFrame(columns=['Timestamp', 'Level', 'Message'])\n\n    df = pd.DataFrame(log_entries, columns=['Timestamp', 'Level', 'Message'])\n    return df"}
{"task_id": "BigCodeBench/488", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, amplitude, period, seed=0):\n    \"\"\"\n    Generate a time series with a given seasonality from the start UTC time to the end UTC time with a given step, and plot the time series with the seasonality.\n    \"\"\"\n    np.random.seed(seed)\n\n    # Convert start_time and end_time to datetime objects\n    start_time = datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n    end_time = datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\")\n\n    # Generate a time series with a given seasonality\n    time_range = pd.date_range(start=start_time, end=end_time, freq=step)\n    seasonality = amplitude * np.sin(2 * np.pi * (time_range - start_time).total_seconds() / period)\n    time_series = np.random.normal(0, 1, len(time_range)) + seasonality\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Timestamp': time_range,\n        'Value': time_series\n    })\n\n    # Plot the time series with the seasonality\n    ax = df.plot(x='Timestamp', y='Value', figsize=(12, 6))\n    ax.set_xlabel(\"Timestamp\")\n    ax.set_ylabel(\"Value\")\n    ax.set_title(\"Time Series with Seasonality\")\n\n    return ax", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, amplitude, period, seed=0):\n    \"\"\"\n    Generate a time series with a given seasonality from the start UTC time to the end UTC time with a given step, and plot the time series with the seasonality.\n    \"\"\"\n    np.random.seed(seed)\n\n    # Convert start_time and end_time to datetime objects\n    start_time = datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n    end_time = datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\")\n\n    # Generate a time series with a given seasonality\n    time_range = pd.date_range(start=start_time, end=end_time, freq=step)\n    seasonality = amplitude * np.sin(2 * np.pi * (time_range - start_time).total_seconds() / period)\n    time_series = np.random.normal(0, 1, len(time_range)) + seasonality\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Timestamp': time_range,\n        'Value': time_series\n    })\n\n    # Plot the time series with the seasonality\n    ax = df.plot(x='Timestamp', y='Value', figsize=(12, 6))\n    ax.set_xlabel(\"Timestamp\")\n    ax.set_ylabel(\"Value\")\n    ax.set_title(\"Time Series with Seasonality\")\n\n    return ax"}
{"task_id": "BigCodeBench/489", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\ndef task_func(epoch_milliseconds, seed=0):\n    \"\"\"\n    Generate user activity logs from a given epoch time to the current time.\n    \"\"\"\n    # Check if the start time is after the current system time\n    if epoch_milliseconds > int(time.time() * 1000):\n        raise ValueError(\"Start time cannot be after the current system time.\")\n\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Define the list of users and activities\n    users = ['user1', 'user2', 'user3', 'user4', 'user5']\n    activities = ['login', 'logout', 'browse', 'search', 'purchase']\n\n    # Initialize the DataFrame to store the logs\n    logs = pd.DataFrame(columns=['User', 'Activity', 'Time'])\n\n    # Convert the epoch time to a datetime object\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n\n    # Generate logs until the current system time\n    while True:\n        # Generate a random number of seconds to increment the time\n        sleep_time = random.randint(1, 10)\n\n        # Increment the start time by the random number of seconds\n        new_time = start_time + timedelta(seconds=sleep_time)\n\n        # If the new time is after the current system time, break the loop\n        if new_time > datetime.now():\n            break\n\n        # Generate a random user and activity\n        user = random.choice(users)\n        activity = random.choice(activities)\n\n        # Append the log to the DataFrame\n        logs = logs.append({'User': user, 'Activity': activity, 'Time': new_time}, ignore_index=True)\n\n        # Update the start time\n        start_time = new_time\n\n    return logs", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\n\ndef task_func(epoch_milliseconds, seed=0):\n    \"\"\"\n    Generate user activity logs from a given epoch time to the current time.\n    \"\"\"\n    # Check if the start time is after the current system time\n    if epoch_milliseconds > int(time.time() * 1000):\n        raise ValueError(\"Start time cannot be after the current system time.\")\n\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Define the list of users and activities\n    users = ['user1', 'user2', 'user3', 'user4', 'user5']\n    activities = ['login', 'logout', 'browse', 'search', 'purchase']\n\n    # Initialize the DataFrame to store the logs\n    logs = pd.DataFrame(columns=['User', 'Activity', 'Time'])\n\n    # Convert the epoch time to a datetime object\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n\n    # Generate logs until the current system time\n    while True:\n        # Generate a random number of seconds to increment the time\n        sleep_time = random.randint(1, 10)\n\n        # Increment the start time by the random number of seconds\n        new_time = start_time + timedelta(seconds=sleep_time)\n\n        # If the new time is after the current system time, break the loop\n        if new_time > datetime.now():\n            break\n\n        # Generate a random user and activity\n        user = random.choice(users)\n        activity = random.choice(activities)\n\n        # Append the log to the DataFrame\n        logs = logs.append({'User': user, 'Activity': activity, 'Time': new_time}, ignore_index=True)\n\n        # Update the start time\n        start_time = new_time\n\n    return logs"}
{"task_id": "BigCodeBench/490", "solution": "import xmltodict\nimport json\ndef task_func(xml_str, file_path):\n    \"\"\"\n    Converts an XML string into a dictionary representation and saves it as a JSON file.\n    \"\"\"\n    # Convert XML to dictionary\n    dict_obj = xmltodict.parse(xml_str)\n\n    # Save dictionary as JSON file\n    with open(file_path, 'w') as json_file:\n        json_file.write(json.dumps(dict_obj))\n\n    return dict_obj", "raw_solution": "import xmltodict\nimport json\n\ndef task_func(xml_str, file_path):\n    \"\"\"\n    Converts an XML string into a dictionary representation and saves it as a JSON file.\n    \"\"\"\n    # Convert XML to dictionary\n    dict_obj = xmltodict.parse(xml_str)\n\n    # Save dictionary as JSON file\n    with open(file_path, 'w') as json_file:\n        json_file.write(json.dumps(dict_obj))\n\n    return dict_obj"}
{"task_id": "BigCodeBench/491", "solution": "import random\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\ndef task_func(epoch_milliseconds, seed=None):\n    \"\"\"\n    Generates and plots sales trend for different categories from a particular epoch milliseconds to the current UTC time.\n    \"\"\"\n    if epoch_milliseconds < 0 or epoch_milliseconds > int(datetime.now().timestamp() * 1000):\n        raise ValueError(\"Invalid start time. It should be a positive number and not after the current time.\")\n\n    start_time = datetime.utcfromtimestamp(epoch_milliseconds / 1000)\n    end_time = datetime.utcnow()\n\n    if seed is not None:\n        random.seed(seed)\n\n    categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports']\n    sales_data = {}\n\n    for category in categories:\n        sales_data[category] = []\n        current_time = start_time\n\n        while current_time <= end_time:\n            sales_data[category].append((current_time, random.randint(10, 50)))\n            current_time += timedelta(days=1)\n\n    fig, ax = plt.subplots()\n\n    for category in categories:\n        sales_times, sales_units = zip(*sales_data[category])\n        sales_days = [(time - start_time).days for time in sales_times]\n        ax.plot(sales_days, sales_units, label=category)\n\n    ax.set_xlabel('Days since start date')\n    ax.set_ylabel('Sales (units)')\n    ax.legend()\n\n    return sales_data, ax", "raw_solution": "import random\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\n\ndef task_func(epoch_milliseconds, seed=None):\n    \"\"\"\n    Generates and plots sales trend for different categories from a particular epoch milliseconds to the current UTC time.\n    \"\"\"\n    if epoch_milliseconds < 0 or epoch_milliseconds > int(datetime.now().timestamp() * 1000):\n        raise ValueError(\"Invalid start time. It should be a positive number and not after the current time.\")\n\n    start_time = datetime.utcfromtimestamp(epoch_milliseconds / 1000)\n    end_time = datetime.utcnow()\n\n    if seed is not None:\n        random.seed(seed)\n\n    categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports']\n    sales_data = {}\n\n    for category in categories:\n        sales_data[category] = []\n        current_time = start_time\n\n        while current_time <= end_time:\n            sales_data[category].append((current_time, random.randint(10, 50)))\n            current_time += timedelta(days=1)\n\n    fig, ax = plt.subplots()\n\n    for category in categories:\n        sales_times, sales_units = zip(*sales_data[category])\n        sales_days = [(time - start_time).days for time in sales_times]\n        ax.plot(sales_days, sales_units, label=category)\n\n    ax.set_xlabel('Days since start date')\n    ax.set_ylabel('Sales (units)')\n    ax.legend()\n\n    return sales_data, ax"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generates sales data for five products from a given epoch time up to the current time.\n    \"\"\"\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or not isinstance(random_seed, int) or not isinstance(products, list):\n        raise ValueError(\"Invalid input type\")\n    if len(products) != 5:\n        raise ValueError(\"Exactly 5 products are required\")\n    for product in products:\n        if not isinstance(product, str):\n            raise ValueError(\"Product names must be strings\")\n\n    # Set random seed\n    random.seed(random_seed)\n\n    # Generate sales data\n    data = []\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n    delta = end_date - start_date\n    for day in range(delta.days + 1):\n        for product in products:\n            data.append({\n                \"Product\": product,\n                \"Date\": start_date + timedelta(days=day),\n                \"Sales\": random.randint(10, 50),\n            })\n\n    return pd.DataFrame(data)\nrandom_seed = 0\nepoch_milliseconds = int(time.mktime(datetime(2022, 1, 1).timetuple()) * 1000)", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\nimport time\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generates sales data for five products from a given epoch time up to the current time.\n    \"\"\"\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or not isinstance(random_seed, int) or not isinstance(products, list):\n        raise ValueError(\"Invalid input type\")\n    if len(products) != 5:\n        raise ValueError(\"Exactly 5 products are required\")\n    for product in products:\n        if not isinstance(product, str):\n            raise ValueError(\"Product names must be strings\")\n\n    # Set random seed\n    random.seed(random_seed)\n\n    # Generate sales data\n    data = []\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n    delta = end_date - start_date\n    for day in range(delta.days + 1):\n        for product in products:\n            data.append({\n                \"Product\": product,\n                \"Date\": start_date + timedelta(days=day),\n                \"Sales\": random.randint(10, 50),\n            })\n\n    return pd.DataFrame(data)\n\n# Test the function\nrandom_seed = 0\nepoch_milliseconds = int(time.mktime(datetime(2022, 1, 1).timetuple()) * 1000)\ndf = task_func(epoch_milliseconds, random_seed)"}
{"task_id": "BigCodeBench/493", "solution": "from datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(\n    epoch_milliseconds,\n    teams=[\"Team1\", \"Team2\", \"Team3\", \"Team4\", \"Team5\"],\n    random_seed=0,\n):\n    random.seed(random_seed)\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n    num_days = (end_date - start_date).days\n    team_performance = {}\n\n    for team in teams:\n        team_performance[team] = [random.uniform(0.1, 1) for _ in range(num_days)]\n\n    fig, ax = plt.subplots()\n\n    for team, performance in team_performance.items():\n        days = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(num_days)]\n        ax.plot(days, performance, label=team)\n\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Performance')\n    ax.legend()\n\n    return team_performance, fig", "raw_solution": "from datetime import datetime, timedelta\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    epoch_milliseconds,\n    teams=[\"Team1\", \"Team2\", \"Team3\", \"Team4\", \"Team5\"],\n    random_seed=0,\n):\n    random.seed(random_seed)\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n    num_days = (end_date - start_date).days\n    team_performance = {}\n\n    for team in teams:\n        team_performance[team] = [random.uniform(0.1, 1) for _ in range(num_days)]\n\n    fig, ax = plt.subplots()\n\n    for team, performance in team_performance.items():\n        days = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(num_days)]\n        ax.plot(days, performance, label=team)\n\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Performance')\n    ax.legend()\n\n    return team_performance, fig"}
{"task_id": "BigCodeBench/494", "solution": "from datetime import datetime\nimport pytz\nimport re\nfrom faker import Faker\ndef task_func(epoch_milliseconds, seed=0, timezones=[\"UTC\"]):\n    \"\"\"\n    Converts a given epoch in milliseconds into a datetime object in the current system time's timezone.\n    Generates a fake event name using Faker.\n    Checks if specified timezones are valid (i.e. in pytz.all_timezones or can be parsed using regex from UTC\u00b1HH:MM format),\n    ignoring invalid ones. If none is valid or if timezones were not specified, selects UTC; otherwise, randomly selects a valid one.\n    Returns a dictionary with the fake event name as key and a list as value, where the list itself contains a schedule, i.e. a dictionary\n    with keys 'date', 'time', 'timezone'.\n    \"\"\"\n    faker = Faker()\n    faker.seed(seed)\n    valid_timezones = []\n    for tz in timezones:\n        if tz in pytz.all_timezones or re.match(r\"^UTC[+-]\\d{2}:\\d{2}$\", tz):\n            valid_timezones.append(tz)\n    if not valid_timezones:\n        valid_timezones.append(\"UTC\")\n    event_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    event_name = faker.bs()\n    event_timezone = faker.random_element(elements=valid_timezones)\n    event_schedule = {\n        'date': event_time.strftime('%Y-%m-%d'),\n        'time': event_time.strftime('%H:%M:%S'),\n        'timezone': event_timezone\n    }\n    return {event_name: [event_schedule]}", "raw_solution": "from datetime import datetime\nimport pytz\nimport re\nfrom faker import Faker\n\ndef task_func(epoch_milliseconds, seed=0, timezones=[\"UTC\"]):\n    \"\"\"\n    Converts a given epoch in milliseconds into a datetime object in the current system time's timezone.\n    Generates a fake event name using Faker.\n    Checks if specified timezones are valid (i.e. in pytz.all_timezones or can be parsed using regex from UTC\u00b1HH:MM format),\n    ignoring invalid ones. If none is valid or if timezones were not specified, selects UTC; otherwise, randomly selects a valid one.\n    Returns a dictionary with the fake event name as key and a list as value, where the list itself contains a schedule, i.e. a dictionary\n    with keys 'date', 'time', 'timezone'.\n    \"\"\"\n    faker = Faker()\n    faker.seed(seed)\n    valid_timezones = []\n    for tz in timezones:\n        if tz in pytz.all_timezones or re.match(r\"^UTC[+-]\\d{2}:\\d{2}$\", tz):\n            valid_timezones.append(tz)\n    if not valid_timezones:\n        valid_timezones.append(\"UTC\")\n    event_time = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    event_name = faker.bs()\n    event_timezone = faker.random_element(elements=valid_timezones)\n    event_schedule = {\n        'date': event_time.strftime('%Y-%m-%d'),\n        'time': event_time.strftime('%H:%M:%S'),\n        'timezone': event_timezone\n    }\n    return {event_name: [event_schedule]}"}
{"task_id": "BigCodeBench/495", "solution": "import pandas as pd\nimport numpy as np\nfrom faker import Faker\ndef task_func(days, random_seed=0):\n    np.random.seed(random_seed)\n    fake = Faker()\n    date_rng = pd.date_range(start='2023-01-01', periods=days)\n    data = {\n        'Groceries': np.random.randint(0, 100, days),\n        'Entertainment': np.random.randint(0, 100, days),\n        'Rent': np.random.randint(0, 100, days),\n        'Utilities': np.random.randint(0, 100, days),\n        'Miscellaneous': np.random.randint(0, 100, days)\n    }\n    df = pd.DataFrame(data, index=date_rng)\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom faker import Faker\n\ndef task_func(days, random_seed=0):\n    np.random.seed(random_seed)\n    fake = Faker()\n    date_rng = pd.date_range(start='2023-01-01', periods=days)\n    data = {\n        'Groceries': np.random.randint(0, 100, days),\n        'Entertainment': np.random.randint(0, 100, days),\n        'Rent': np.random.randint(0, 100, days),\n        'Utilities': np.random.randint(0, 100, days),\n        'Miscellaneous': np.random.randint(0, 100, days)\n    }\n    df = pd.DataFrame(data, index=date_rng)\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/496", "solution": "from datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    Draws a graph of temperature trends over the past week using randomly generated data.\n    \"\"\"\n    if days_in_past < 1:\n        raise ValueError(\"days_in_past must be greater than 0\")\n\n    np.random.seed(random_seed)\n    temperatures = np.random.randint(low=15, high=35, size=days_in_past)\n\n    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_in_past)]\n\n    fig, ax = plt.subplots()\n    ax.plot(dates, temperatures)\n\n    ax.set(xlabel='Date', ylabel='Temperature (\u00b0C)',\n           title='Temperature Trend')\n    ax.grid()\n\n    return ax", "raw_solution": "from datetime import datetime, timedelta\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    Draws a graph of temperature trends over the past week using randomly generated data.\n    \"\"\"\n    if days_in_past < 1:\n        raise ValueError(\"days_in_past must be greater than 0\")\n\n    np.random.seed(random_seed)\n    temperatures = np.random.randint(low=15, high=35, size=days_in_past)\n\n    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_in_past)]\n\n    fig, ax = plt.subplots()\n    ax.plot(dates, temperatures)\n\n    ax.set(xlabel='Date', ylabel='Temperature (\u00b0C)',\n           title='Temperature Trend')\n    ax.grid()\n\n    return ax"}
{"task_id": "BigCodeBench/497", "solution": "from datetime import datetime, timedelta\nimport pytz\nimport calendar\ndef task_func(days_in_past=7):\n    \"\"\"\n    This function computes the date that is 'days_in_past' number of days ago from the current system time's date in UTC.\n    It then determines the weekday of this target date using calendar and returns its name as a string.\n    \"\"\"\n    if days_in_past < 0:\n        raise ValueError(\"'days_in_past' cannot be negative\")\n\n    # Get the current date in UTC\n    current_date = datetime.now(pytz.utc)\n\n    # Compute the date 'days_in_past' days ago\n    target_date = current_date - timedelta(days=days_in_past)\n\n    # Get the weekday of the target date\n    weekday = calendar.day_name[target_date.weekday()]\n\n    return weekday", "raw_solution": "from datetime import datetime, timedelta\nimport pytz\nimport calendar\n\ndef task_func(days_in_past=7):\n    \"\"\"\n    This function computes the date that is 'days_in_past' number of days ago from the current system time's date in UTC.\n    It then determines the weekday of this target date using calendar and returns its name as a string.\n    \"\"\"\n    if days_in_past < 0:\n        raise ValueError(\"'days_in_past' cannot be negative\")\n\n    # Get the current date in UTC\n    current_date = datetime.now(pytz.utc)\n\n    # Compute the date 'days_in_past' days ago\n    target_date = current_date - timedelta(days=days_in_past)\n\n    # Get the weekday of the target date\n    weekday = calendar.day_name[target_date.weekday()]\n\n    return weekday\n\n# Test the function"}
{"task_id": "BigCodeBench/498", "solution": "import xmltodict\nimport json\nimport os\ndef task_func(s, save_json=False, json_file_path=None):\n    \"\"\"\n    Converts an XML string into a dictionary representation and optionally saves it as a JSON file.\n    \"\"\"\n    if not s or s.isspace():\n        raise ValueError(\"Input XML string cannot be empty or contain only whitespace.\")\n\n    result = xmltodict.parse(s)\n\n    if save_json:\n        if not json_file_path:\n            raise ValueError(\"JSON file path must be provided when save_json is True.\")\n        elif not os.path.splitext(json_file_path)[1] == '.json':\n            raise ValueError(\"JSON file path must have a '.json' extension.\")\n\n        with open(json_file_path, 'w') as f:\n            json.dump(result, f)\n\n    return result", "raw_solution": "import xmltodict\nimport json\nimport os\n\ndef task_func(s, save_json=False, json_file_path=None):\n    \"\"\"\n    Converts an XML string into a dictionary representation and optionally saves it as a JSON file.\n    \"\"\"\n    if not s or s.isspace():\n        raise ValueError(\"Input XML string cannot be empty or contain only whitespace.\")\n\n    result = xmltodict.parse(s)\n\n    if save_json:\n        if not json_file_path:\n            raise ValueError(\"JSON file path must be provided when save_json is True.\")\n        elif not os.path.splitext(json_file_path)[1] == '.json':\n            raise ValueError(\"JSON file path must have a '.json' extension.\")\n\n        with open(json_file_path, 'w') as f:\n            json.dump(result, f)\n\n    return result"}
{"task_id": "BigCodeBench/499", "solution": "import xlwt\nimport os\nimport io\nimport csv\ndef task_func(csv_content, filename):\n    \"\"\"\n    Converts CSV content into an Excel file and saves it with the given filename.\n    \"\"\"\n    # Create a new workbook and add a worksheet\n    workbook = xlwt.Workbook()\n    worksheet = workbook.add_sheet('Sheet 1')\n\n    # Convert CSV content into a list of rows\n    csv_content = csv_content.split('\\n')\n    csv_reader = csv.reader(csv_content)\n    rows = list(csv_reader)\n\n    # Write the data into the workbook\n    for row_index, row in enumerate(rows):\n        for col_index, value in enumerate(row):\n            worksheet.write(row_index, col_index, value)\n\n    # Save the workbook as an Excel file\n    workbook.save(filename)\n\n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)\ncsv_content = 'Hello'", "raw_solution": "import xlwt\nimport os\nimport io\nimport csv\n\ndef task_func(csv_content, filename):\n    \"\"\"\n    Converts CSV content into an Excel file and saves it with the given filename.\n    \"\"\"\n    # Create a new workbook and add a worksheet\n    workbook = xlwt.Workbook()\n    worksheet = workbook.add_sheet('Sheet 1')\n\n    # Convert CSV content into a list of rows\n    csv_content = csv_content.split('\\n')\n    csv_reader = csv.reader(csv_content)\n    rows = list(csv_reader)\n\n    # Write the data into the workbook\n    for row_index, row in enumerate(rows):\n        for col_index, value in enumerate(row):\n            worksheet.write(row_index, col_index, value)\n\n    # Save the workbook as an Excel file\n    workbook.save(filename)\n\n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)\n\n# Test the function\ncsv_content = 'Hello'"}
{"task_id": "BigCodeBench/500", "solution": "import xlwt\nimport os\nfrom collections import OrderedDict\nFIELDS = ['ID', 'Name', 'Age']\ndef task_func(values, filename):\n    \"\"\"\n    Writes a list of OrderedDicts to an Excel file. Each OrderedDict in the list represents a row in the Excel sheet, and each key in the OrderedDict corresponds to a column defined in the FIELDS constant.\n    \"\"\"\n    # Create a new workbook and add a worksheet\n    workbook = xlwt.Workbook()\n    worksheet = workbook.add_sheet('Sheet 1')\n\n    # Write the column headers\n    for i, field in enumerate(FIELDS):\n        worksheet.write(0, i, field)\n\n    # Write the data\n    for i, value in enumerate(values, start=1):\n        for j, key in enumerate(FIELDS):\n            worksheet.write(i, j, value.get(key, ''))\n\n    # Save the workbook\n    workbook.save(filename)\n\n    # Return the absolute path of the created file\n    return os.path.abspath(filename)\nempty_data = []", "raw_solution": "import xlwt\nimport os\nfrom collections import OrderedDict\n\n# Constants\nFIELDS = ['ID', 'Name', 'Age']\n\ndef task_func(values, filename):\n    \"\"\"\n    Writes a list of OrderedDicts to an Excel file. Each OrderedDict in the list represents a row in the Excel sheet, and each key in the OrderedDict corresponds to a column defined in the FIELDS constant.\n    \"\"\"\n    # Create a new workbook and add a worksheet\n    workbook = xlwt.Workbook()\n    worksheet = workbook.add_sheet('Sheet 1')\n\n    # Write the column headers\n    for i, field in enumerate(FIELDS):\n        worksheet.write(0, i, field)\n\n    # Write the data\n    for i, value in enumerate(values, start=1):\n        for j, key in enumerate(FIELDS):\n            worksheet.write(i, j, value.get(key, ''))\n\n    # Save the workbook\n    workbook.save(filename)\n\n    # Return the absolute path of the created file\n    return os.path.abspath(filename)\n\n# Test the function\nempty_data = []\npath = task_func(empty_data, 'empty_data.xls')\nassert os.path.exists(path) and 'empty_data.xls' in path"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\nfrom pandas.io.json import json_normalize\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Converts JSON strings to an Excel file.\n    \"\"\"\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    try:\n        data = json.loads(json_str)\n    except ValueError:\n        raise ValueError(\"Invalid JSON\")\n\n    if not isinstance(data, list):\n        data = [data]\n\n    df = pd.json_normalize(data)\n\n    try:\n        if not df.empty:\n            writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n            df.to_excel(writer, sheet_name=sheet_name, index=False)\n            writer.save()\n        else:\n            workbook = xlwt.Workbook()\n            workbook.save(filename)\n    except Exception as e:\n        raise Exception(\"Error writing to file: {}\".format(str(e)))\n\n    return os.path.abspath(filename)", "raw_solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\nfrom pandas.io.json import json_normalize\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Converts JSON strings to an Excel file.\n    \"\"\"\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    try:\n        data = json.loads(json_str)\n    except ValueError:\n        raise ValueError(\"Invalid JSON\")\n\n    if not isinstance(data, list):\n        data = [data]\n\n    df = pd.json_normalize(data)\n\n    try:\n        if not df.empty:\n            writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n            df.to_excel(writer, sheet_name=sheet_name, index=False)\n            writer.save()\n        else:\n            workbook = xlwt.Workbook()\n            workbook.save(filename)\n    except Exception as e:\n        raise Exception(\"Error writing to file: {}\".format(str(e)))\n\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n\n    # Define activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n\n    # Generate data\n    data = []\n    for i in range(days_in_past):\n        date = datetime.now() - timedelta(days=i)\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append([date, activity, duration])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    df[\"Date\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n\n    # Create lineplot\n    ax = sns.lineplot(x=\"Date\", y=\"Duration\", hue=\"Activity\", data=df)\n\n    return (ax, df)", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    random.seed(random_seed)\n\n    # Define activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n\n    # Generate data\n    data = []\n    for i in range(days_in_past):\n        date = datetime.now() - timedelta(days=i)\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append([date, activity, duration])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    df[\"Date\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n\n    # Create lineplot\n    ax = sns.lineplot(x=\"Date\", y=\"Duration\", hue=\"Activity\", data=df)\n\n    return (ax, df)\n\n# Test the function\nax, df = task_func(7)\nplt.show()"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    np.random.seed(random_seed)\n    dates = pd.date_range(start=datetime.now() - timedelta(days=days_in_past), periods=days_in_past).tolist()\n    prices = np.random.rand(days_in_past, len(stock_names)).tolist()\n    data = {stock_names[i]: prices[j] for i in range(len(stock_names)) for j in range(days_in_past)}\n    df = pd.DataFrame(data, index=dates)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    np.random.seed(random_seed)\n    dates = pd.date_range(start=datetime.now() - timedelta(days=days_in_past), periods=days_in_past).tolist()\n    prices = np.random.rand(days_in_past, len(stock_names)).tolist()\n    data = {stock_names[i]: prices[j] for i in range(len(stock_names)) for j in range(days_in_past)}\n    df = pd.DataFrame(data, index=dates)\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/504", "solution": "import hashlib\nimport rsa\nimport base64\nfrom typing import Optional\ndef task_func(file_path: str) -> Optional[str]:\n    \"\"\"\n    Generates a signed hash of a file's contents using RSA encryption.\n    The file's contents are hashed using SHA-256, and then the hash is signed with a private RSA key stored in 'private.pem'.\n    The signed hash is encoded in base64.\n    \"\"\"\n    try:\n        with open(file_path, 'rb') as file:\n            data = file.read()\n            file_hash = hashlib.sha256(data).digest()\n\n        with open('private.pem', 'rb') as private_key_file:\n            private_key = rsa.PrivateKey.load_pkcs1(private_key_file.read())\n\n        signature = rsa.sign_hash(file_hash, private_key, 'SHA-256')\n        return base64.b64encode(signature).decode()\n    except (FileNotFoundError, rsa.VerificationError):\n        return None", "raw_solution": "import hashlib\nimport rsa\nimport base64\nfrom typing import Optional\n\ndef task_func(file_path: str) -> Optional[str]:\n    \"\"\"\n    Generates a signed hash of a file's contents using RSA encryption.\n    The file's contents are hashed using SHA-256, and then the hash is signed with a private RSA key stored in 'private.pem'.\n    The signed hash is encoded in base64.\n    \"\"\"\n    try:\n        with open(file_path, 'rb') as file:\n            data = file.read()\n            file_hash = hashlib.sha256(data).digest()\n\n        with open('private.pem', 'rb') as private_key_file:\n            private_key = rsa.PrivateKey.load_pkcs1(private_key_file.read())\n\n        signature = rsa.sign_hash(file_hash, private_key, 'SHA-256')\n        return base64.b64encode(signature).decode()\n    except (FileNotFoundError, rsa.VerificationError):\n        return None\n\n# Test the function"}
{"task_id": "BigCodeBench/505", "solution": "import hashlib\nimport hmac\ndef task_func(secret, message):\n    \"\"\"\n    Generates an HMAC (Hash-based Message Authentication Code) signature for a given message using a secret key.\n    The function uses SHA-256 as the hash function to create the HMAC signature.\n    \"\"\"\n    secret = secret.encode()\n    message = message.encode()\n    hmac_signature = hmac.new(secret, message, hashlib.sha256).hexdigest()\n    return hmac_signature", "raw_solution": "import hashlib\nimport hmac\n\ndef task_func(secret, message):\n    \"\"\"\n    Generates an HMAC (Hash-based Message Authentication Code) signature for a given message using a secret key.\n    The function uses SHA-256 as the hash function to create the HMAC signature.\n    \"\"\"\n    secret = secret.encode()\n    message = message.encode()\n    hmac_signature = hmac.new(secret, message, hashlib.sha256).hexdigest()\n    return hmac_signature\n\n# Test the function\nassert len(task_func('mysecretkey', 'Goodbye, world!')) == 64"}
{"task_id": "BigCodeBench/506", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    \"\"\"\n    Analyze and visualize statistical properties of a specified weather data column.\n    \"\"\"\n    # Convert data to pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # If data is empty, set 'mean' to np.nan, 'min' to np.inf, and 'max' to -np.inf\n    if df.empty:\n        result = {\n            'sum': np.nan,\n            'mean': np.nan,\n            'min': np.inf,\n            'max': -np.inf,\n            'plot': None\n        }\n        return result\n\n    # Calculate sum, mean, min, and max of the specified column\n    column_sum = df[column].sum()\n    column_mean = df[column].mean()\n    column_min = df[column].min()\n    column_max = df[column].max()\n\n    # Generate histogram plot of the data in the column\n    plt.figure(figsize=(10, 6))\n    plt.hist(df[column], bins='auto', color='#003f5c', edgecolor='black', alpha=0.7)\n    plt.title(f'Histogram of {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n    plot = plt.gca()\n\n    # Store results in a dictionary\n    result = {\n        'sum': column_sum,\n        'mean': column_mean,\n        'min': column_min,\n        'max': column_max,\n        'plot': plot\n    }\n\n    return result", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    \"\"\"\n    Analyze and visualize statistical properties of a specified weather data column.\n    \"\"\"\n    # Convert data to pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # If data is empty, set 'mean' to np.nan, 'min' to np.inf, and 'max' to -np.inf\n    if df.empty:\n        result = {\n            'sum': np.nan,\n            'mean': np.nan,\n            'min': np.inf,\n            'max': -np.inf,\n            'plot': None\n        }\n        return result\n\n    # Calculate sum, mean, min, and max of the specified column\n    column_sum = df[column].sum()\n    column_mean = df[column].mean()\n    column_min = df[column].min()\n    column_max = df[column].max()\n\n    # Generate histogram plot of the data in the column\n    plt.figure(figsize=(10, 6))\n    plt.hist(df[column], bins='auto', color='#003f5c', edgecolor='black', alpha=0.7)\n    plt.title(f'Histogram of {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n    plot = plt.gca()\n\n    # Store results in a dictionary\n    result = {\n        'sum': column_sum,\n        'mean': column_mean,\n        'min': column_min,\n        'max': column_max,\n        'plot': plot\n    }\n\n    return result"}
{"task_id": "BigCodeBench/507", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(column, data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data should be a pandas DataFrame\")\n    if not isinstance(column, str):\n        raise ValueError(\"Column should be a string\")\n    if column not in data.columns:\n        raise ValueError(f\"Column '{column}' not found in data\")\n\n    if data.empty:\n        return {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n\n    return {\n        'sum': data[column].sum(),\n        'mean': data[column].mean(),\n        'min': data[column].min(),\n        'max': data[column].max(),\n    }", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(column, data):\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data should be a pandas DataFrame\")\n    if not isinstance(column, str):\n        raise ValueError(\"Column should be a string\")\n    if column not in data.columns:\n        raise ValueError(f\"Column '{column}' not found in data\")\n\n    if data.empty:\n        return {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n\n    return {\n        'sum': data[column].sum(),\n        'mean': data[column].mean(),\n        'min': data[column].min(),\n        'max': data[column].max(),\n    }"}
{"task_id": "BigCodeBench/508", "solution": "import hashlib\nimport io\nimport os\ndef task_func(file_path1, file_path2):\n    \"\"\"\n    Compares two files to determine if they are identical by computing and comparing their MD5 hash values.\n    \"\"\"\n    # Check if files exist\n    if not os.path.isfile(file_path1) or not os.path.isfile(file_path2):\n        raise FileNotFoundError(\"One or both of the files do not exist.\")\n\n    # Compute MD5 hash of each file\n    hash1 = hashlib.md5()\n    hash2 = hashlib.md5()\n\n    with io.FileIO(file_path1, 'rb') as f1, io.FileIO(file_path2, 'rb') as f2:\n        for chunk in iter(lambda: f1.read(4096), b\"\"):\n            hash1.update(chunk)\n        for chunk in iter(lambda: f2.read(4096), b\"\"):\n            hash2.update(chunk)\n\n    # Compare MD5 hashes\n    return hash1.digest() == hash2.digest()", "raw_solution": "import hashlib\nimport io\nimport os\n\ndef task_func(file_path1, file_path2):\n    \"\"\"\n    Compares two files to determine if they are identical by computing and comparing their MD5 hash values.\n    \"\"\"\n    # Check if files exist\n    if not os.path.isfile(file_path1) or not os.path.isfile(file_path2):\n        raise FileNotFoundError(\"One or both of the files do not exist.\")\n\n    # Compute MD5 hash of each file\n    hash1 = hashlib.md5()\n    hash2 = hashlib.md5()\n\n    with io.FileIO(file_path1, 'rb') as f1, io.FileIO(file_path2, 'rb') as f2:\n        for chunk in iter(lambda: f1.read(4096), b\"\"):\n            hash1.update(chunk)\n        for chunk in iter(lambda: f2.read(4096), b\"\"):\n            hash2.update(chunk)\n\n    # Compare MD5 hashes\n    return hash1.digest() == hash2.digest()"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r') as file1, open(file_path2, 'r') as file2:\n            lines1 = file1.readlines()\n            lines2 = file2.readlines()\n\n        if not lines1 or not lines2:\n            raise ValueError(\"One or both of the files are empty.\")\n\n        diff_lines = []\n        for line1, line2 in zip(lines1, lines2):\n            if line1.strip() != line2.strip():\n                diff_lines.append((line1, '-'))\n                diff_lines.append((line2, '+'))\n\n        # If the files have different lengths, append the remaining lines\n        if len(lines1) > len(lines2):\n            for line in lines1[len(lines2):]:\n                diff_lines.append((line, '-'))\n        elif len(lines2) > len(lines1):\n            for line in lines2[len(lines1):]:\n                diff_lines.append((line, '+'))\n\n        df = pd.DataFrame(diff_lines, columns=['Content', 'Status'])\n        df['Line Number'] = range(1, len(df) + 1)\n\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files cannot be found.\")\n    except Exception as e:\n        raise Exception(\"An error occurred: \" + str(e))", "raw_solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r') as file1, open(file_path2, 'r') as file2:\n            lines1 = file1.readlines()\n            lines2 = file2.readlines()\n\n        if not lines1 or not lines2:\n            raise ValueError(\"One or both of the files are empty.\")\n\n        diff_lines = []\n        for line1, line2 in zip(lines1, lines2):\n            if line1.strip() != line2.strip():\n                diff_lines.append((line1, '-'))\n                diff_lines.append((line2, '+'))\n\n        # If the files have different lengths, append the remaining lines\n        if len(lines1) > len(lines2):\n            for line in lines1[len(lines2):]:\n                diff_lines.append((line, '-'))\n        elif len(lines2) > len(lines1):\n            for line in lines2[len(lines1):]:\n                diff_lines.append((line, '+'))\n\n        df = pd.DataFrame(diff_lines, columns=['Content', 'Status'])\n        df['Line Number'] = range(1, len(df) + 1)\n\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files cannot be found.\")\n    except Exception as e:\n        raise Exception(\"An error occurred: \" + str(e))"}
{"task_id": "BigCodeBench/510", "solution": "import difflib\nimport gzip\nimport io\ndef task_func(file_path1, file_path2):\n    \"\"\"\n    Compares the contents of two gzip files and returns a string describing the differences between them.\n    \"\"\"\n    # Open the gzip files and read their contents\n    with gzip.open(file_path1, 'rb') as f1, gzip.open(file_path2, 'rb') as f2:\n        content1 = f1.read().decode('utf-8')\n        content2 = f2.read().decode('utf-8')\n\n    # Use difflib to compute the differences between the two contents\n    diff = difflib.Differ()\n    diffs = diff.compare(content1.splitlines(keepends=True), content2.splitlines(keepends=True))\n\n    # Join the differences into a single string\n    diff_str = ''.join(diffs)\n\n    # If there are no differences, return an empty string\n    if diff_str == '':\n        return ''\n\n    return diff_str", "raw_solution": "import difflib\nimport gzip\nimport io\n\ndef task_func(file_path1, file_path2):\n    \"\"\"\n    Compares the contents of two gzip files and returns a string describing the differences between them.\n    \"\"\"\n    # Open the gzip files and read their contents\n    with gzip.open(file_path1, 'rb') as f1, gzip.open(file_path2, 'rb') as f2:\n        content1 = f1.read().decode('utf-8')\n        content2 = f2.read().decode('utf-8')\n\n    # Use difflib to compute the differences between the two contents\n    diff = difflib.Differ()\n    diffs = diff.compare(content1.splitlines(keepends=True), content2.splitlines(keepends=True))\n\n    # Join the differences into a single string\n    diff_str = ''.join(diffs)\n\n    # If there are no differences, return an empty string\n    if diff_str == '':\n        return ''\n\n    return diff_str"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    if not data:\n        return ({'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}, None)\n\n    df = pd.DataFrame(data)\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in data\")\n\n    column_data = df[column]\n    summary_stats = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max(),\n    }\n\n    fig, ax = plt.subplots()\n    column_data.value_counts().plot(kind='pie', ax=ax)\n    ax.set_title(f\"Distribution of '{column}'\")\n\n    return (summary_stats, ax)", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    if not data:\n        return ({'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}, None)\n\n    df = pd.DataFrame(data)\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in data\")\n\n    column_data = df[column]\n    summary_stats = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max(),\n    }\n\n    fig, ax = plt.subplots()\n    column_data.value_counts().plot(kind='pie', ax=ax)\n    ax.set_title(f\"Distribution of '{column}'\")\n\n    return (summary_stats, ax)"}
{"task_id": "BigCodeBench/512", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(column, data):\n    \"\"\"\n    Analyze a list of sales data, calculate the sum, the mean, the minimum, the maximum of a given column, and return the bar chart plot for the given column without displaying it.\n    \"\"\"\n    # Check if the data is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Data should be a pandas DataFrame\")\n\n    # Check if the column exists in the data\n    if column not in data.columns:\n        raise ValueError(f\"Column {column} not found in data\")\n\n    # Check if the column contains only numeric values\n    if not pd.api.types.is_numeric_dtype(data[column]):\n        raise ValueError(f\"Column {column} should be numeric\")\n\n    # Check if the column contains only non-negative values\n    if (data[column] < 0).any():\n        raise ValueError(\"Quantity sold or total sales cannot be negative\")\n\n    # Calculate the sum, mean, min, max\n    summary_stats = {\n        'sum': data[column].sum(),\n        'mean': data[column].mean(),\n        'min': data[column].min(),\n        'max': data[column].max()\n    }\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    data[column].plot(kind='bar', ax=ax)\n    ax.set_xlabel('Product')\n    ax.set_ylabel(column)\n    ax.set_title(f'Bar Chart of {column}')\n\n    return summary_stats, ax.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(column, data):\n    \"\"\"\n    Analyze a list of sales data, calculate the sum, the mean, the minimum, the maximum of a given column, and return the bar chart plot for the given column without displaying it.\n    \"\"\"\n    # Check if the data is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Data should be a pandas DataFrame\")\n\n    # Check if the column exists in the data\n    if column not in data.columns:\n        raise ValueError(f\"Column {column} not found in data\")\n\n    # Check if the column contains only numeric values\n    if not pd.api.types.is_numeric_dtype(data[column]):\n        raise ValueError(f\"Column {column} should be numeric\")\n\n    # Check if the column contains only non-negative values\n    if (data[column] < 0).any():\n        raise ValueError(\"Quantity sold or total sales cannot be negative\")\n\n    # Calculate the sum, mean, min, max\n    summary_stats = {\n        'sum': data[column].sum(),\n        'mean': data[column].mean(),\n        'min': data[column].min(),\n        'max': data[column].max()\n    }\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    data[column].plot(kind='bar', ax=ax)\n    ax.set_xlabel('Product')\n    ax.set_ylabel(column)\n    ax.set_title(f'Bar Chart of {column}')\n\n    return summary_stats, ax.figure"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    \"\"\"\n    Analyze a list of fitness data, calculate the sum, the mean, the minimum, the maximum of a certain column and draw a line chart.\n    Additionally, validate that the numeric values for steps, calories burned, and distance walked are non-negative.\n    \"\"\"\n    # Create a pandas DataFrame from the data\n    df = pd.DataFrame(data, columns=['Date', 'Steps', 'Calories Burned', 'Distance Walked'])\n\n    # Validate that the data list is not empty\n    if df.empty:\n        raise ValueError(\"The data list is empty.\")\n\n    # Validate that the specified column is valid\n    if column not in df.columns:\n        raise KeyError(f\"The specified column '{column}' is not valid.\")\n\n    # Validate that the numeric values for steps, calories burned, and distance walked are non-negative\n    for col in ['Steps', 'Calories Burned', 'Distance Walked']:\n        if (df[col] < 0).any():\n            raise ValueError(f\"The numeric value for '{col}' cannot be negative.\")\n\n    # Calculate the sum, mean, min, max of the column\n    summary_stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Plot a line chart\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column])\n    ax.set_title(f'Line Chart of {column}')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n\n    return summary_stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    \"\"\"\n    Analyze a list of fitness data, calculate the sum, the mean, the minimum, the maximum of a certain column and draw a line chart.\n    Additionally, validate that the numeric values for steps, calories burned, and distance walked are non-negative.\n    \"\"\"\n    # Create a pandas DataFrame from the data\n    df = pd.DataFrame(data, columns=['Date', 'Steps', 'Calories Burned', 'Distance Walked'])\n\n    # Validate that the data list is not empty\n    if df.empty:\n        raise ValueError(\"The data list is empty.\")\n\n    # Validate that the specified column is valid\n    if column not in df.columns:\n        raise KeyError(f\"The specified column '{column}' is not valid.\")\n\n    # Validate that the numeric values for steps, calories burned, and distance walked are non-negative\n    for col in ['Steps', 'Calories Burned', 'Distance Walked']:\n        if (df[col] < 0).any():\n            raise ValueError(f\"The numeric value for '{col}' cannot be negative.\")\n\n    # Calculate the sum, mean, min, max of the column\n    summary_stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Plot a line chart\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column])\n    ax.set_title(f'Line Chart of {column}')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n\n    return summary_stats, ax"}
{"task_id": "BigCodeBench/514", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(array):\n    \"\"\"\n    Create a Pandas DataFrame from a 2D list and plot the sum of each column.\n    \"\"\"\n    # Create DataFrame\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'E'])\n\n    # Plot sum of each column\n    fig, ax = plt.subplots()\n    df.sum().plot(kind='bar', ax=ax)\n    ax.set_title('Sum of each column')\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Sum')\n\n    return df, ax.figure\narray = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(array):\n    \"\"\"\n    Create a Pandas DataFrame from a 2D list and plot the sum of each column.\n    \"\"\"\n    # Create DataFrame\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'E'])\n\n    # Plot sum of each column\n    fig, ax = plt.subplots()\n    df.sum().plot(kind='bar', ax=ax)\n    ax.set_title('Sum of each column')\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Sum')\n\n    return df, ax.figure\n\n# Test the function\narray = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]\ndf, fig = task_func(array)\nplt.show()"}
{"task_id": "BigCodeBench/515", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(array):\n    if not array:\n        raise ValueError(\"Input array is empty\")\n    for sublist in array:\n        if len(sublist) != 5:\n            raise ValueError(\"Sublists of varying lengths found\")\n\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'E'])\n    print(df)\n\n    correlation_matrix = df.corr()\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.show()\narray = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(array):\n    if not array:\n        raise ValueError(\"Input array is empty\")\n    for sublist in array:\n        if len(sublist) != 5:\n            raise ValueError(\"Sublists of varying lengths found\")\n\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'E'])\n    print(df)\n\n    correlation_matrix = df.corr()\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.show()\n\n# Testing the function\narray = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]\ntask_func(array)"}
{"task_id": "BigCodeBench/516", "solution": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\ndef task_func(\n    array: list, random_seed: int = 0\n) -> (pd.DataFrame, sm.regression.linear_model.RegressionResultsWrapper):\n    \"\"\"\n    Generate a Pandas DataFrame from a 2D list and perform a multiple linear regression.\n    \"\"\"\n    # Validate the input list\n    if not all(len(row) == 5 for row in array):\n        raise ValueError(\"All rows in the input list must have exactly 5 elements.\")\n\n    # Create a DataFrame\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'Response'])\n\n    # Separate independent and dependent variables\n    X = df[['A', 'B', 'C', 'D']]\n    y = df['Response']\n\n    # Add a constant to the model\n    X = sm.add_constant(X)\n\n    # Fit a linear regression\n    model = sm.OLS(y, X)\n    results = model.fit()\n\n    return df, results", "raw_solution": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndef task_func(\n    array: list, random_seed: int = 0\n) -> (pd.DataFrame, sm.regression.linear_model.RegressionResultsWrapper):\n    \"\"\"\n    Generate a Pandas DataFrame from a 2D list and perform a multiple linear regression.\n    \"\"\"\n    # Validate the input list\n    if not all(len(row) == 5 for row in array):\n        raise ValueError(\"All rows in the input list must have exactly 5 elements.\")\n\n    # Create a DataFrame\n    df = pd.DataFrame(array, columns=['A', 'B', 'C', 'D', 'Response'])\n\n    # Separate independent and dependent variables\n    X = df[['A', 'B', 'C', 'D']]\n    y = df['Response']\n\n    # Add a constant to the model\n    X = sm.add_constant(X)\n\n    # Fit a linear regression\n    model = sm.OLS(y, X)\n    results = model.fit()\n\n    return df, results"}
{"task_id": "BigCodeBench/517", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\ndef task_func(array: list, random_seed: int = 42) -> (pd.DataFrame, np.ndarray):\n    \"\"\"\n    Converts a 2D list into a pandas DataFrame and applies PCA for dimensionality reduction.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Convert the 2D list to a DataFrame\n    df = pd.DataFrame(array)\n\n    # Apply PCA to reduce the dataset to its two main components\n    pca = PCA(n_components=2)\n    pca_data = pca.fit_transform(df)\n\n    return df, pca_data", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\ndef task_func(array: list, random_seed: int = 42) -> (pd.DataFrame, np.ndarray):\n    \"\"\"\n    Converts a 2D list into a pandas DataFrame and applies PCA for dimensionality reduction.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Convert the 2D list to a DataFrame\n    df = pd.DataFrame(array)\n\n    # Apply PCA to reduce the dataset to its two main components\n    pca = PCA(n_components=2)\n    pca_data = pca.fit_transform(df)\n\n    return df, pca_data"}
{"task_id": "BigCodeBench/518", "solution": "import pandas as pd\nfrom scipy.spatial.distance import pdist, squareform\ndef task_func(array):\n    \"\"\"\n    Generate a Pandas DataFrame from a 2D list and calculate a distance matrix.\n    \"\"\"\n    # Convert 2D list to DataFrame\n    df = pd.DataFrame(array)\n\n    # Assign alphabetical labels to columns\n    df.columns = [chr(65 + i) for i in range(df.shape[1])]\n\n    # Compute distance matrix\n    distance_matrix = pd.DataFrame(squareform(pdist(df)), index=df.index, columns=df.index)\n\n    return df, distance_matrix\narray = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]", "raw_solution": "import pandas as pd\nfrom scipy.spatial.distance import pdist, squareform\n\ndef task_func(array):\n    \"\"\"\n    Generate a Pandas DataFrame from a 2D list and calculate a distance matrix.\n    \"\"\"\n    # Convert 2D list to DataFrame\n    df = pd.DataFrame(array)\n\n    # Assign alphabetical labels to columns\n    df.columns = [chr(65 + i) for i in range(df.shape[1])]\n\n    # Compute distance matrix\n    distance_matrix = pd.DataFrame(squareform(pdist(df)), index=df.index, columns=df.index)\n\n    return df, distance_matrix\n\n# Test the function\narray = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ndf, distance_matrix = task_func(array)"}
{"task_id": "BigCodeBench/519", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Combines a list of dictionaries with the same keys (fruit names) into a single pandas dataframe.\n    Fills NA/NaN values with 0, and generates a line chart of sales.\n    \"\"\"\n    # Convert list of dictionaries to pandas dataframe\n    df = pd.DataFrame(data)\n\n    # Fill NA/NaN values with 0\n    df.fillna(0, inplace=True)\n\n    # Generate line chart of sales\n    ax = df.plot(kind='line', title='Fruit Sales over Time', xlabel='Time', ylabel='Sales Quantity')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    Combines a list of dictionaries with the same keys (fruit names) into a single pandas dataframe.\n    Fills NA/NaN values with 0, and generates a line chart of sales.\n    \"\"\"\n    # Convert list of dictionaries to pandas dataframe\n    df = pd.DataFrame(data)\n\n    # Fill NA/NaN values with 0\n    df.fillna(0, inplace=True)\n\n    # Generate line chart of sales\n    ax = df.plot(kind='line', title='Fruit Sales over Time', xlabel='Time', ylabel='Sales Quantity')\n\n    return ax"}
{"task_id": "BigCodeBench/520", "solution": "import collections\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Combines a list of dictionaries with the same keys (fruit names) into a single dictionary,\n    calculates the total turnover for each fruit, and returns a bar chart's axes with colors\n    representing different fruits.\n    \"\"\"\n    # Initialize an empty dictionary to store the total sales for each fruit\n    total_sales = {}\n\n    # Iterate over each dictionary in the data list\n    for item in data:\n        # Iterate over each key-value pair in the current dictionary\n        for fruit, sales in item.items():\n            # If the sales quantity is negative, raise a ValueError\n            if sales < 0:\n                raise ValueError(\"Sales quantity must not be negative\")\n            # If the fruit is already in the total_sales dictionary, add the sales to the existing total\n            if fruit in total_sales:\n                total_sales[fruit] += sales\n            # If the fruit is not in the total_sales dictionary, add it with the sales as the total\n            else:\n                total_sales[fruit] = sales\n\n    # Create a list of fruit names in the order they were encountered in the data\n    fruit_order = list(total_sales.keys())\n\n    # Create a list of sales in the order they were encountered in the data\n    sales_order = list(total_sales.values())\n\n    # Create a list of colors for each fruit in the order they were encountered in the data\n    colors = ['red', 'yellow', 'green', 'blue', 'purple']\n    color_order = [colors[i % len(colors)] for i in range(len(fruit_order))]\n\n    # Create a bar chart with the total sales for each fruit\n    fig, ax = plt.subplots()\n    ax.bar(fruit_order, sales_order, color=color_order)\n    ax.set_xlabel('Fruit')\n    ax.set_ylabel('Total Sales')\n    ax.set_title('Total Fruit Sales')\n\n    return total_sales, ax", "raw_solution": "import collections\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    Combines a list of dictionaries with the same keys (fruit names) into a single dictionary,\n    calculates the total turnover for each fruit, and returns a bar chart's axes with colors\n    representing different fruits.\n    \"\"\"\n    # Initialize an empty dictionary to store the total sales for each fruit\n    total_sales = {}\n\n    # Iterate over each dictionary in the data list\n    for item in data:\n        # Iterate over each key-value pair in the current dictionary\n        for fruit, sales in item.items():\n            # If the sales quantity is negative, raise a ValueError\n            if sales < 0:\n                raise ValueError(\"Sales quantity must not be negative\")\n            # If the fruit is already in the total_sales dictionary, add the sales to the existing total\n            if fruit in total_sales:\n                total_sales[fruit] += sales\n            # If the fruit is not in the total_sales dictionary, add it with the sales as the total\n            else:\n                total_sales[fruit] = sales\n\n    # Create a list of fruit names in the order they were encountered in the data\n    fruit_order = list(total_sales.keys())\n\n    # Create a list of sales in the order they were encountered in the data\n    sales_order = list(total_sales.values())\n\n    # Create a list of colors for each fruit in the order they were encountered in the data\n    colors = ['red', 'yellow', 'green', 'blue', 'purple']\n    color_order = [colors[i % len(colors)] for i in range(len(fruit_order))]\n\n    # Create a bar chart with the total sales for each fruit\n    fig, ax = plt.subplots()\n    ax.bar(fruit_order, sales_order, color=color_order)\n    ax.set_xlabel('Fruit')\n    ax.set_ylabel('Total Sales')\n    ax.set_title('Total Fruit Sales')\n\n    return total_sales, ax"}
{"task_id": "BigCodeBench/521", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_list):\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(data_list)\n    df = df.set_index('Test')\n\n    # Transpose the DataFrame to have students as columns\n    df = df.transpose()\n\n    # Plot the DataFrame\n    ax = df.plot(kind='line', legend=True)\n\n    # Set the x-axis label\n    ax.set_xlabel('Test Number')\n\n    # Set the y-axis label\n    ax.set_ylabel('Score')\n\n    # Return the Axes object\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_list):\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(data_list)\n    df = df.set_index('Test')\n\n    # Transpose the DataFrame to have students as columns\n    df = df.transpose()\n\n    # Plot the DataFrame\n    ax = df.plot(kind='line', legend=True)\n\n    # Set the x-axis label\n    ax.set_xlabel('Test Number')\n\n    # Set the y-axis label\n    ax.set_ylabel('Score')\n\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/522", "solution": "import collections\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(data):\n    if not data:\n        return None\n\n    scores = collections.defaultdict(list)\n    for student_data in data:\n        for student, score in student_data.items():\n            if score is not None:\n                if score < 0:\n                    raise ValueError(\"Negative scores are not allowed\")\n                scores[student].append(score)\n\n    averages = {student: np.mean(score_list) for student, score_list in scores.items()}\n\n    fig, ax = plt.subplots()\n    colors = ['red', 'yellow', 'green', 'blue', 'purple']\n    ax.bar(range(len(averages)), list(averages.values()), color=colors)\n    ax.set_xticks(range(len(averages)))\n    ax.set_xticklabels(list(averages.keys()))\n    ax.set_xlabel('Student')\n    ax.set_ylabel('Average Score')\n    ax.set_title('Average Student Scores')\n\n    return ax", "raw_solution": "import collections\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(data):\n    if not data:\n        return None\n\n    scores = collections.defaultdict(list)\n    for student_data in data:\n        for student, score in student_data.items():\n            if score is not None:\n                if score < 0:\n                    raise ValueError(\"Negative scores are not allowed\")\n                scores[student].append(score)\n\n    averages = {student: np.mean(score_list) for student, score_list in scores.items()}\n\n    fig, ax = plt.subplots()\n    colors = ['red', 'yellow', 'green', 'blue', 'purple']\n    ax.bar(range(len(averages)), list(averages.values()), color=colors)\n    ax.set_xticks(range(len(averages)))\n    ax.set_xticklabels(list(averages.keys()))\n    ax.set_xlabel('Student')\n    ax.set_ylabel('Average Score')\n    ax.set_title('Average Student Scores')\n\n    return ax"}
{"task_id": "BigCodeBench/523", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        return None\n    combined_dict = {}\n    for d in data:\n        for key, value in d.items():\n            if key in combined_dict:\n                combined_dict[key].append(value)\n            else:\n                combined_dict[key] = [value]\n    df = pd.DataFrame(combined_dict)\n    ax = df.plot(kind='line', x='Time', y='Data Points')\n    plt.xlabel('Time')\n    plt.ylabel('Data Points')\n    plt.title('Data over Time')\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not data:\n        return None\n    combined_dict = {}\n    for d in data:\n        for key, value in d.items():\n            if key in combined_dict:\n                combined_dict[key].append(value)\n            else:\n                combined_dict[key] = [value]\n    df = pd.DataFrame(combined_dict)\n    ax = df.plot(kind='line', x='Time', y='Data Points')\n    plt.xlabel('Time')\n    plt.ylabel('Data Points')\n    plt.title('Data over Time')\n    return ax"}
{"task_id": "BigCodeBench/524", "solution": "from collections import defaultdict\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data is empty\")\n    if not isinstance(data, list):\n        raise TypeError(\"Input is not a list of dictionaries\")\n    for item in data:\n        if not isinstance(item, dict):\n            raise TypeError(\"Input is not a list of dictionaries\")\n        for key, value in item.items():\n            if not isinstance(value, (int, float)):\n                raise TypeError(\"All values in the dictionaries should be numeric\")\n\n    stats = defaultdict(list)\n    for d in data:\n        for k, v in d.items():\n            stats[k].append(v)\n\n    mean_std_dict = {}\n    axes_list = []\n    for k, v in stats.items():\n        mean = np.mean(v)\n        std_dev = np.std(v)\n        mean_std_dict[k] = (mean, std_dev)\n\n        fig, ax = plt.subplots()\n        ax.bar(['Mean', 'Std Dev'], [mean, std_dev], color=['blue', 'orange'])\n        ax.set_title(f'{k} Mean and Standard Deviation')\n        ax.set_ylabel('Value')\n        ax.set_xticks(['Mean', 'Std Dev'])\n        axes_list.append(ax)\n\n    return mean_std_dict, axes_list", "raw_solution": "from collections import defaultdict\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    if not data:\n        raise ValueError(\"Input data is empty\")\n    if not isinstance(data, list):\n        raise TypeError(\"Input is not a list of dictionaries\")\n    for item in data:\n        if not isinstance(item, dict):\n            raise TypeError(\"Input is not a list of dictionaries\")\n        for key, value in item.items():\n            if not isinstance(value, (int, float)):\n                raise TypeError(\"All values in the dictionaries should be numeric\")\n\n    stats = defaultdict(list)\n    for d in data:\n        for k, v in d.items():\n            stats[k].append(v)\n\n    mean_std_dict = {}\n    axes_list = []\n    for k, v in stats.items():\n        mean = np.mean(v)\n        std_dev = np.std(v)\n        mean_std_dict[k] = (mean, std_dev)\n\n        fig, ax = plt.subplots()\n        ax.bar(['Mean', 'Std Dev'], [mean, std_dev], color=['blue', 'orange'])\n        ax.set_title(f'{k} Mean and Standard Deviation')\n        ax.set_ylabel('Value')\n        ax.set_xticks(['Mean', 'Std Dev'])\n        axes_list.append(ax)\n\n    return mean_std_dict, axes_list"}
{"task_id": "BigCodeBench/525", "solution": "import json\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(input_file):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    result = defaultdict(dict)\n    plots = []\n\n    for key in data[0].keys():\n        values = [d[key] for d in data if key in d]\n        mean = np.mean(values)\n        median = np.median(values)\n        result[key]['mean'] = mean\n        result[key]['median'] = median\n\n        fig, ax = plt.subplots()\n        ax.bar(['Mean', 'Median'], [mean, median], color=['b', 'r'])\n        ax.set_title(f'Mean and Median of {key}')\n        plots.append(ax)\n\n    return result, plots", "raw_solution": "import json\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(input_file):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    result = defaultdict(dict)\n    plots = []\n\n    for key in data[0].keys():\n        values = [d[key] for d in data if key in d]\n        mean = np.mean(values)\n        median = np.median(values)\n        result[key]['mean'] = mean\n        result[key]['median'] = median\n\n        fig, ax = plt.subplots()\n        ax.bar(['Mean', 'Median'], [mean, median], color=['b', 'r'])\n        ax.set_title(f'Mean and Median of {key}')\n        plots.append(ax)\n\n    return result, plots"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom typing import List, Dict, Any\ndef task_func(input_file: str = \"data.json\") -> pd.DataFrame:\n    \"\"\"\n    Read a list of dictionaries from a JSON file, calculate the mean and median for each key (ignoring non-numeric or missing values), and convert the results into a Pandas DataFrame.\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize dictionaries to store mean and median values\n    mean_dict = defaultdict(list)\n    median_dict = defaultdict(list)\n\n    # Iterate over each dictionary in the list\n    for d in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in d.items():\n            # Ignore non-numeric or missing values\n            if isinstance(value, (int, float)) and not np.isnan(value):\n                mean_dict[key].append(value)\n\n    # Calculate mean and median for each key\n    for key, values in mean_dict.items():\n        mean_dict[key] = np.mean(values)\n        median_dict[key] = np.median(values)\n\n    # Convert dictionaries to DataFrame\n    mean_df = pd.DataFrame(list(mean_dict.items()), columns=['key', 'mean'])\n    median_df = pd.DataFrame(list(median_dict.items()), columns=['key', 'median'])\n\n    # Merge DataFrames on 'key'\n    df = pd.merge(mean_df, median_df, on='key')\n\n    # Set 'key' as index and sort by 'key'\n    df.set_index('key', inplace=True)\n    df.sort_index(inplace=True)\n\n    return df", "raw_solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom typing import List, Dict, Any\n\ndef task_func(input_file: str = \"data.json\") -> pd.DataFrame:\n    \"\"\"\n    Read a list of dictionaries from a JSON file, calculate the mean and median for each key (ignoring non-numeric or missing values), and convert the results into a Pandas DataFrame.\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize dictionaries to store mean and median values\n    mean_dict = defaultdict(list)\n    median_dict = defaultdict(list)\n\n    # Iterate over each dictionary in the list\n    for d in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in d.items():\n            # Ignore non-numeric or missing values\n            if isinstance(value, (int, float)) and not np.isnan(value):\n                mean_dict[key].append(value)\n\n    # Calculate mean and median for each key\n    for key, values in mean_dict.items():\n        mean_dict[key] = np.mean(values)\n        median_dict[key] = np.median(values)\n\n    # Convert dictionaries to DataFrame\n    mean_df = pd.DataFrame(list(mean_dict.items()), columns=['key', 'mean'])\n    median_df = pd.DataFrame(list(median_dict.items()), columns=['key', 'median'])\n\n    # Merge DataFrames on 'key'\n    df = pd.merge(mean_df, median_df, on='key')\n\n    # Set 'key' as index and sort by 'key'\n    df.set_index('key', inplace=True)\n    df.sort_index(inplace=True)\n\n    return df"}
{"task_id": "BigCodeBench/527", "solution": "import json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file: str):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    results = defaultdict(dict)\n    df_list = []\n\n    for item in data:\n        for key, value in item.items():\n            df_list.append({'X': key, 'Y': value})\n\n    df = pd.DataFrame(df_list)\n\n    for key in df['X'].unique():\n        values = df[df['X'] == key]['Y'].values\n        results[key]['mean'] = np.mean(values)\n        results[key]['median'] = np.median(values)\n\n    ax = sns.boxplot(x='X', y='Y', data=df)\n\n    return results, ax.figure", "raw_solution": "import json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file: str):\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    results = defaultdict(dict)\n    df_list = []\n\n    for item in data:\n        for key, value in item.items():\n            df_list.append({'X': key, 'Y': value})\n\n    df = pd.DataFrame(df_list)\n\n    for key in df['X'].unique():\n        values = df[df['X'] == key]['Y'].values\n        results[key]['mean'] = np.mean(values)\n        results[key]['median'] = np.median(values)\n\n    ax = sns.boxplot(x='X', y='Y', data=df)\n\n    return results, ax.figure"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File should be in .csv format\")\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        rows = list(reader)\n\n    duplicates = [item for item in rows if rows.count(item) > 1]\n    duplicates_dict = dict(Counter(map(tuple, duplicates)))\n\n    df = pd.DataFrame(list(duplicates_dict.items()), columns=['Duplicates', 'Counts'])\n    df.set_index('Duplicates', inplace=True)\n\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Counts')\n    ax.set_title('Count of Duplicate Rows')\n\n    return duplicates_dict, ax", "raw_solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File should be in .csv format\")\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        rows = list(reader)\n\n    duplicates = [item for item in rows if rows.count(item) > 1]\n    duplicates_dict = dict(Counter(map(tuple, duplicates)))\n\n    df = pd.DataFrame(list(duplicates_dict.items()), columns=['Duplicates', 'Counts'])\n    df.set_index('Duplicates', inplace=True)\n\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Counts')\n    ax.set_title('Count of Duplicate Rows')\n\n    return duplicates_dict, ax"}
{"task_id": "BigCodeBench/529", "solution": "from collections import Counter\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(num_rolls, num_dice, plot_path=None, random_seed=0):\n    \"\"\"\n    Simulates rolling a certain number of a standard six-sided dice several times, then identifies and displays the distribution of the sums of the dice rolls in a bar plot.\n    \"\"\"\n    random.seed(random_seed)\n    dice_rolls = [sum(random.randint(1, 6) for _ in range(num_dice)) for _ in range(num_rolls)]\n    counter = Counter(dice_rolls)\n\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values())\n    ax.set_xlabel('Sum of Dice Roll')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Distribution of Dice Roll Sums ({num_rolls} rolls, {num_dice} dice)')\n\n    if plot_path is not None:\n        plt.savefig(plot_path)\n\n    return counter, ax", "raw_solution": "from collections import Counter\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(num_rolls, num_dice, plot_path=None, random_seed=0):\n    \"\"\"\n    Simulates rolling a certain number of a standard six-sided dice several times, then identifies and displays the distribution of the sums of the dice rolls in a bar plot.\n    \"\"\"\n    random.seed(random_seed)\n    dice_rolls = [sum(random.randint(1, 6) for _ in range(num_dice)) for _ in range(num_rolls)]\n    counter = Counter(dice_rolls)\n\n    fig, ax = plt.subplots()\n    ax.bar(counter.keys(), counter.values())\n    ax.set_xlabel('Sum of Dice Roll')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Distribution of Dice Roll Sums ({num_rolls} rolls, {num_dice} dice)')\n\n    if plot_path is not None:\n        plt.savefig(plot_path)\n\n    return counter, ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    df['age'] = df['age'].apply(np.floor)\n    df = df[df['age'] >= 0]\n    if df.empty:\n        raise ValueError(\"All ages are negative\")\n    age_counts = df[df.duplicated(subset='name', keep=False)]['age'].value_counts(sort=False)\n    age_range = (age_counts.index.min(), age_counts.index.max())\n    bins = np.arange(age_range[0], age_range[1]+1, 1)\n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(data=df, x='age', bins=bins, kde=False, color='skyblue')\n    ax.set_title('Age Distribution for Duplicate Names')\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    return age_counts, ax.figure", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    df['age'] = df['age'].apply(np.floor)\n    df = df[df['age'] >= 0]\n    if df.empty:\n        raise ValueError(\"All ages are negative\")\n    age_counts = df[df.duplicated(subset='name', keep=False)]['age'].value_counts(sort=False)\n    age_range = (age_counts.index.min(), age_counts.index.max())\n    bins = np.arange(age_range[0], age_range[1]+1, 1)\n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(data=df, x='age', bins=bins, kde=False, color='skyblue')\n    ax.set_title('Age Distribution for Duplicate Names')\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    return age_counts, ax.figure"}
{"task_id": "BigCodeBench/531", "solution": "from collections import Counter\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\ndef task_func(df, n_clusters=3, random_state=None, n_init=10):\n    \"\"\"\n    Identify duplicate points in a DataFrame, perform KMeans clustering on the unique points, and record the clusters.\n    \"\"\"\n    # Identify duplicate points\n    duplicate_rows = df[df.duplicated()]\n    duplicate_counts = Counter(map(tuple, duplicate_rows.values))\n\n    # Perform KMeans clustering on the unique points\n    unique_rows = df.drop_duplicates()\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    kmeans.fit(unique_rows)\n\n    # Record the clusters\n    unique_rows['cluster'] = kmeans.labels_\n\n    # Create a scatter plot of the clustered data\n    plt.scatter(unique_rows['x'], unique_rows['y'], c=unique_rows['cluster'])\n    plt.title('KMeans Clustering')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.show()\n\n    return duplicate_counts, unique_rows", "raw_solution": "from collections import Counter\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df, n_clusters=3, random_state=None, n_init=10):\n    \"\"\"\n    Identify duplicate points in a DataFrame, perform KMeans clustering on the unique points, and record the clusters.\n    \"\"\"\n    # Identify duplicate points\n    duplicate_rows = df[df.duplicated()]\n    duplicate_counts = Counter(map(tuple, duplicate_rows.values))\n\n    # Perform KMeans clustering on the unique points\n    unique_rows = df.drop_duplicates()\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    kmeans.fit(unique_rows)\n\n    # Record the clusters\n    unique_rows['cluster'] = kmeans.labels_\n\n    # Create a scatter plot of the clustered data\n    plt.scatter(unique_rows['x'], unique_rows['y'], c=unique_rows['cluster'])\n    plt.title('KMeans Clustering')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.show()\n\n    return duplicate_counts, unique_rows"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, bins=4):\n    # Identify and count duplicate values in the 'value' column\n    counts = Counter(df['value'])\n\n    # Create a histogram with a normal distribution curve\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, color='green', alpha=0.6, edgecolor='black')\n\n    # Fit a normal distribution to the data\n    mu, sigma = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set plot title and labels\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return counts, ax", "raw_solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, bins=4):\n    # Identify and count duplicate values in the 'value' column\n    counts = Counter(df['value'])\n\n    # Create a histogram with a normal distribution curve\n    fig, ax = plt.subplots()\n    ax.hist(df['value'], bins=bins, color='green', alpha=0.6, edgecolor='black')\n\n    # Fit a normal distribution to the data\n    mu, sigma = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set plot title and labels\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return counts, ax"}
{"task_id": "BigCodeBench/533", "solution": "import numpy as np\nimport secrets\nimport hashlib\nimport base64\ndef task_func(num, from_base, to_base, alphabet):\n    if from_base < 2 or to_base < 2:\n        raise ValueError(\"Invalid base for conversion. Base must be greater than 1.\")\n\n    try:\n        num_in_base_10 = int(num, from_base)\n    except ValueError:\n        raise ValueError(\"Invalid number format for conversion.\")\n\n    num_in_base_to = \"\"\n    while num_in_base_10 > 0:\n        digit = num_in_base_10 % to_base\n        num_in_base_10 //= to_base\n        num_in_base_to = alphabet[digit] + num_in_base_to\n\n    salt = secrets.token_hex(16)\n    hash_input = (num_in_base_to + salt).encode('utf-8')\n    hash_object = hashlib.sha256(hash_input)\n    hex_digest = hash_object.hexdigest()\n    base64_digest = base64.b64encode(hex_digest.encode('utf-8')).decode('utf-8')\n\n    return (base64_digest, salt)", "raw_solution": "import numpy as np\nimport secrets\nimport hashlib\nimport base64\n\ndef task_func(num, from_base, to_base, alphabet):\n    if from_base < 2 or to_base < 2:\n        raise ValueError(\"Invalid base for conversion. Base must be greater than 1.\")\n\n    try:\n        num_in_base_10 = int(num, from_base)\n    except ValueError:\n        raise ValueError(\"Invalid number format for conversion.\")\n\n    num_in_base_to = \"\"\n    while num_in_base_10 > 0:\n        digit = num_in_base_10 % to_base\n        num_in_base_10 //= to_base\n        num_in_base_to = alphabet[digit] + num_in_base_to\n\n    salt = secrets.token_hex(16)\n    hash_input = (num_in_base_to + salt).encode('utf-8')\n    hash_object = hashlib.sha256(hash_input)\n    hex_digest = hash_object.hexdigest()\n    base64_digest = base64.b64encode(hex_digest.encode('utf-8')).decode('utf-8')\n\n    return (base64_digest, salt)"}
{"task_id": "BigCodeBench/534", "solution": "import numpy as np\nimport base64\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.backends import default_backend\ndef task_func(num, from_base, to_base, private_key, alphabet):\n    # Convert number from one base to another\n    num = int(num, from_base)\n    num = int(str(num), to_base)\n\n    # Sign the number with RSA\n    private_key = serialization.load_pem_private_key(\n        private_key,\n        password=None,\n        backend=default_backend()\n    )\n    signature = private_key.sign(\n        str(num).encode(),\n        padding.PSS(\n            mgf=padding.MGF1(hashes.SHA256()),\n            salt_length=padding.PSS.MAX_LENGTH\n        ),\n        hashes.SHA256()\n    )\n\n    # Encode the signed number in base64 with a custom alphabet\n    encoded = base64.b64encode(signature).decode()\n    table = str.maketrans(''.join(alphabet), base64.b64encode(b'+/').decode())\n    encoded = encoded.translate(table)\n\n    return encoded", "raw_solution": "import numpy as np\nimport base64\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.backends import default_backend\n\ndef task_func(num, from_base, to_base, private_key, alphabet):\n    # Convert number from one base to another\n    num = int(num, from_base)\n    num = int(str(num), to_base)\n\n    # Sign the number with RSA\n    private_key = serialization.load_pem_private_key(\n        private_key,\n        password=None,\n        backend=default_backend()\n    )\n    signature = private_key.sign(\n        str(num).encode(),\n        padding.PSS(\n            mgf=padding.MGF1(hashes.SHA256()),\n            salt_length=padding.PSS.MAX_LENGTH\n        ),\n        hashes.SHA256()\n    )\n\n    # Encode the signed number in base64 with a custom alphabet\n    encoded = base64.b64encode(signature).decode()\n    table = str.maketrans(''.join(alphabet), base64.b64encode(b'+/').decode())\n    encoded = encoded.translate(table)\n\n    return encoded"}
{"task_id": "BigCodeBench/535", "solution": "import sqlite3\nimport numpy as np\nfrom random import choice, seed\ndef task_func(db_path, table_name, num_entries, random_seed=None):\n    \"\"\"\n    Inserts random data into an SQLite3 table that contains random names, ages, and heights.\n    If the table does not exist, it will be created.\n    \"\"\"\n    if num_entries < 0:\n        raise ValueError(\"num_entries cannot be negative\")\n\n    NAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = list(range(18, 65))\n    HEIGHTS = list(range(150, 200))\n\n    seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {table_name} (\n            name TEXT,\n            age INTEGER,\n            height INTEGER\n        )\n    \"\"\")\n\n    rows_inserted = 0\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = choice(AGES)\n        height = choice(HEIGHTS)\n        cursor.execute(f\"\"\"\n            INSERT INTO {table_name} (name, age, height)\n            VALUES (?, ?, ?)\n        \"\"\", (name, age, height))\n        rows_inserted += 1\n\n    conn.commit()\n    conn.close()\n\n    return rows_inserted", "raw_solution": "import sqlite3\nimport numpy as np\nfrom random import choice, seed\n\ndef task_func(db_path, table_name, num_entries, random_seed=None):\n    \"\"\"\n    Inserts random data into an SQLite3 table that contains random names, ages, and heights.\n    If the table does not exist, it will be created.\n    \"\"\"\n    if num_entries < 0:\n        raise ValueError(\"num_entries cannot be negative\")\n\n    NAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = list(range(18, 65))\n    HEIGHTS = list(range(150, 200))\n\n    seed(random_seed)\n\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {table_name} (\n            name TEXT,\n            age INTEGER,\n            height INTEGER\n        )\n    \"\"\")\n\n    rows_inserted = 0\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = choice(AGES)\n        height = choice(HEIGHTS)\n        cursor.execute(f\"\"\"\n            INSERT INTO {table_name} (name, age, height)\n            VALUES (?, ?, ?)\n        \"\"\", (name, age, height))\n        rows_inserted += 1\n\n    conn.commit()\n    conn.close()\n\n    return rows_inserted"}
{"task_id": "BigCodeBench/536", "solution": "import sqlite3\nimport pandas as pd\nimport os\ndef task_func(db_name, table_name, csv_path=\"data.csv\"):\n    \"\"\"\n    Read SQLite3 table via pandas and export to a CSV file.\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n\n    # Read the table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n\n    # Export the DataFrame to a CSV file\n    df.to_csv(csv_path, index=False)\n\n    # Get the absolute path of the exported CSV file\n    csv_abs_path = os.path.abspath(csv_path)\n\n    return csv_abs_path", "raw_solution": "import sqlite3\nimport pandas as pd\nimport os\n\ndef task_func(db_name, table_name, csv_path=\"data.csv\"):\n    \"\"\"\n    Read SQLite3 table via pandas and export to a CSV file.\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n\n    # Read the table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n\n    # Export the DataFrame to a CSV file\n    df.to_csv(csv_path, index=False)\n\n    # Get the absolute path of the exported CSV file\n    csv_abs_path = os.path.abspath(csv_path)\n\n    return csv_abs_path"}
{"task_id": "BigCodeBench/537", "solution": "import sqlite3\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(db_name=\"test.db\", table_name=\"People\"):\n    \"\"\"\n    Draws the age distribution of the persons in an SQLite3 table and returns the Axes object of the plot.\n    Raises a ValueError if the loaded data contains negative age values.\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n\n    # Execute a query to fetch data from the table\n    query = f\"SELECT * FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n\n    # Check for negative age values\n    if (df['age'] < 0).any():\n        raise ValueError(\"The data contains negative age values.\")\n\n    # Plot the age distribution\n    ax = sns.histplot(data=df, x='age', bins=30, kde=True)\n\n    # Close the connection\n    conn.close()\n\n    return ax", "raw_solution": "import sqlite3\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(db_name=\"test.db\", table_name=\"People\"):\n    \"\"\"\n    Draws the age distribution of the persons in an SQLite3 table and returns the Axes object of the plot.\n    Raises a ValueError if the loaded data contains negative age values.\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n\n    # Execute a query to fetch data from the table\n    query = f\"SELECT * FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n\n    # Check for negative age values\n    if (df['age'] < 0).any():\n        raise ValueError(\"The data contains negative age values.\")\n\n    # Plot the age distribution\n    ax = sns.histplot(data=df, x='age', bins=30, kde=True)\n\n    # Close the connection\n    conn.close()\n\n    return ax"}
{"task_id": "BigCodeBench/538", "solution": "import sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(db_name, table_name):\n    \"\"\"\n    Plots the relationship between the first and second numerical columns of an SQLite3 table,\n    excluding the 'id' column.\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n\n    # Read the table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n\n    # Get the column names excluding 'id'\n    cols = [col for col in df.columns if col != 'id']\n\n    # Check if there are at least two numerical columns\n    if len([col for col in cols if pd.api.types.is_numeric_dtype(df[col])]) < 2:\n        raise ValueError(\"The table has less than two numerical columns.\")\n\n    # Select the first two numerical columns\n    num_cols = cols[:2]\n\n    # Plot the relationship\n    plt.scatter(df[num_cols[0]], df[num_cols[1]])\n    plt.xlabel(num_cols[0])\n    plt.ylabel(num_cols[1])\n    plt.title(f\"Relationship between {num_cols[0]} and {num_cols[1]}\")\n    plt.show()\n\n    # Close the connection\n    conn.close()", "raw_solution": "import sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(db_name, table_name):\n    \"\"\"\n    Plots the relationship between the first and second numerical columns of an SQLite3 table,\n    excluding the 'id' column.\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_name)\n\n    # Read the table into a pandas DataFrame\n    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n\n    # Get the column names excluding 'id'\n    cols = [col for col in df.columns if col != 'id']\n\n    # Check if there are at least two numerical columns\n    if len([col for col in cols if pd.api.types.is_numeric_dtype(df[col])]) < 2:\n        raise ValueError(\"The table has less than two numerical columns.\")\n\n    # Select the first two numerical columns\n    num_cols = cols[:2]\n\n    # Plot the relationship\n    plt.scatter(df[num_cols[0]], df[num_cols[1]])\n    plt.xlabel(num_cols[0])\n    plt.ylabel(num_cols[1])\n    plt.title(f\"Relationship between {num_cols[0]} and {num_cols[1]}\")\n    plt.show()\n\n    # Close the connection\n    conn.close()"}
{"task_id": "BigCodeBench/539", "solution": "import sqlite3\nfrom random import choice, randint, seed\nimport os\ndef task_func(db_name, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries cannot be negative\")\n\n    NAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = range(18, 65)\n    HEIGHTS = range(150, 200)\n\n    if random_seed is not None:\n        seed(random_seed)\n\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n\n    cursor.execute(f'CREATE TABLE {table_name} (name text, age integer, height integer)')\n\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = randint(18, 65)\n        height = randint(150, 200)\n        cursor.execute(f'INSERT INTO {table_name} VALUES (?, ?, ?)', (name, age, height))\n\n    conn.commit()\n    conn.close()\n\n    return os.path.abspath(db_name)", "raw_solution": "import sqlite3\nfrom random import choice, randint, seed\nimport os\n\ndef task_func(db_name, table_name, num_entries, random_seed=None):\n    if num_entries < 0:\n        raise ValueError(\"num_entries cannot be negative\")\n\n    NAMES = ['John', 'Jane', 'Steve', 'Emma', 'Liam', 'Olivia']\n    AGES = range(18, 65)\n    HEIGHTS = range(150, 200)\n\n    if random_seed is not None:\n        seed(random_seed)\n\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n\n    cursor.execute(f'CREATE TABLE {table_name} (name text, age integer, height integer)')\n\n    for _ in range(num_entries):\n        name = choice(NAMES)\n        age = randint(18, 65)\n        height = randint(150, 200)\n        cursor.execute(f'INSERT INTO {table_name} VALUES (?, ?, ?)', (name, age, height))\n\n    conn.commit()\n    conn.close()\n\n    return os.path.abspath(db_name)"}
{"task_id": "BigCodeBench/540", "solution": "from collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\ndef task_func(list_of_menuitems, title=\"Menu Distribution\", color=\"blue\", width=1.0):\n    \"\"\"\n    Given a nested list of menu items, flatten the list using itertool chain, count the occurrences of each item,\n    then plot a histogram with an alphabetically sorted x-axis labeled as \"Menu Items\" and y-axis as \"Frequency\".\n    \"\"\"\n    # Flatten the list using itertools.chain\n    flat_list = list(itertools.chain(*list_of_menuitems))\n\n    # Count the occurrences of each item\n    counter = Counter(flat_list)\n\n    # Create a list of items and their counts\n    items = list(counter.keys())\n    counts = list(counter.values())\n\n    # Sort the items alphabetically\n    items.sort()\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot the histogram\n    ax.bar(items, counts, color=color, width=width)\n\n    # Set the x-axis label\n    ax.set_xlabel(\"Menu Items\")\n\n    # Set the y-axis label\n    ax.set_ylabel(\"Frequency\")\n\n    # Set the title\n    ax.set_title(title)\n\n    return ax", "raw_solution": "from collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef task_func(list_of_menuitems, title=\"Menu Distribution\", color=\"blue\", width=1.0):\n    \"\"\"\n    Given a nested list of menu items, flatten the list using itertool chain, count the occurrences of each item,\n    then plot a histogram with an alphabetically sorted x-axis labeled as \"Menu Items\" and y-axis as \"Frequency\".\n    \"\"\"\n    # Flatten the list using itertools.chain\n    flat_list = list(itertools.chain(*list_of_menuitems))\n\n    # Count the occurrences of each item\n    counter = Counter(flat_list)\n\n    # Create a list of items and their counts\n    items = list(counter.keys())\n    counts = list(counter.values())\n\n    # Sort the items alphabetically\n    items.sort()\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot the histogram\n    ax.bar(items, counts, color=color, width=width)\n\n    # Set the x-axis label\n    ax.set_xlabel(\"Menu Items\")\n\n    # Set the y-axis label\n    ax.set_ylabel(\"Frequency\")\n\n    # Set the title\n    ax.set_title(title)\n\n    return ax"}
{"task_id": "BigCodeBench/541", "solution": "import os\nimport sys\nimport importlib\nfrom pkgutil import iter_modules\ndef task_func(package_name):\n    \"\"\"\n    Adds all modules of a specified package to the system path.\n    \"\"\"\n    try:\n        package = importlib.import_module(package_name)\n    except ImportError:\n        raise ImportError(f\"Package '{package_name}' is not installed or cannot be found. Please install it using 'pip install {package_name}'.\")\n\n    package_path = os.path.dirname(package.__file__)\n    sys.path.append(package_path)\n\n    modules = [name for _, name, _ in iter_modules([package_path])]\n\n    return modules", "raw_solution": "import os\nimport sys\nimport importlib\nfrom pkgutil import iter_modules\n\ndef task_func(package_name):\n    \"\"\"\n    Adds all modules of a specified package to the system path.\n    \"\"\"\n    try:\n        package = importlib.import_module(package_name)\n    except ImportError:\n        raise ImportError(f\"Package '{package_name}' is not installed or cannot be found. Please install it using 'pip install {package_name}'.\")\n\n    package_path = os.path.dirname(package.__file__)\n    sys.path.append(package_path)\n\n    modules = [name for _, name, _ in iter_modules([package_path])]\n\n    return modules\n\n# Testing\nassert len(task_func('pandas')) >= 2, \"Test case 1 failed\"\nassert 'numpy' in task_func('numpy'), \"Test case 2 failed\"\ntry:\n    task_func('non_existent_package')\n    assert False, \"Test case 3 failed\"\nexcept ImportError:\n    pass"}
{"task_id": "BigCodeBench/542", "solution": "import hashlib\nimport random\nimport struct\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_keys=KEYS, seed=42):\n    random.seed(seed)\n    hex_key = random.choice(hex_keys)\n    try:\n        float_val = struct.unpack('f', bytes.fromhex(hex_key))[0]\n    except (ValueError, TypeError):\n        raise ValueError(\"Invalid hexadecimal string\")\n    md5_hash = hashlib.md5(struct.pack('f', float_val))\n    return md5_hash.hexdigest()", "raw_solution": "import hashlib\nimport random\nimport struct\n\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef task_func(hex_keys=KEYS, seed=42):\n    random.seed(seed)\n    hex_key = random.choice(hex_keys)\n    try:\n        float_val = struct.unpack('f', bytes.fromhex(hex_key))[0]\n    except (ValueError, TypeError):\n        raise ValueError(\"Invalid hexadecimal string\")\n    md5_hash = hashlib.md5(struct.pack('f', float_val))\n    return md5_hash.hexdigest()\n\n# Testing the function"}
{"task_id": "BigCodeBench/543", "solution": "import base64\nimport os\ndef task_func():\n    # Generate a random float number\n    random_float = os.urandom(4)\n    # Convert the float to hexadecimal\n    hex_string = random_float.hex()\n    # Encode the hexadecimal string in base64\n    base64_string = base64.b64encode(hex_string.encode()).decode()\n    return base64_string", "raw_solution": "import base64\nimport os\n\ndef task_func():\n    # Generate a random float number\n    random_float = os.urandom(4)\n    # Convert the float to hexadecimal\n    hex_string = random_float.hex()\n    # Encode the hexadecimal string in base64\n    base64_string = base64.b64encode(hex_string.encode()).decode()\n    return base64_string"}
{"task_id": "BigCodeBench/544", "solution": "import struct\nimport zlib\nKEY = '470FC614'\ndef task_func(hex_string=KEY):\n    # Convert hex string to float\n    float_num = struct.unpack('f', bytes.fromhex(hex_string))[0]\n\n    # Convert float to bytes\n    float_bytes = struct.pack('f', float_num)\n\n    # Compress the bytes\n    compressed_bytes = zlib.compress(float_bytes)\n\n    return compressed_bytes", "raw_solution": "import struct\nimport zlib\n\n# Constants\nKEY = '470FC614'\n\ndef task_func(hex_string=KEY):\n    # Convert hex string to float\n    float_num = struct.unpack('f', bytes.fromhex(hex_string))[0]\n\n    # Convert float to bytes\n    float_bytes = struct.pack('f', float_num)\n\n    # Compress the bytes\n    compressed_bytes = zlib.compress(float_bytes)\n\n    return compressed_bytes\n\n# Test the function\ncompressed_bytes = task_func(KEY)"}
{"task_id": "BigCodeBench/545", "solution": "import codecs\nimport random\nimport struct\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_keys=KEYS):\n    # Generate a random float number\n    float_num = random.uniform(0, 1)\n\n    # Convert the float number to bytes\n    float_bytes = struct.pack('f', float_num)\n\n    # Encode the bytes in utf-8\n    encoded_bytes = codecs.encode(float_bytes, 'utf-8')\n\n    return encoded_bytes", "raw_solution": "import codecs\nimport random\nimport struct\n\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef task_func(hex_keys=KEYS):\n    # Generate a random float number\n    float_num = random.uniform(0, 1)\n\n    # Convert the float number to bytes\n    float_bytes = struct.pack('f', float_num)\n\n    # Encode the bytes in utf-8\n    encoded_bytes = codecs.encode(float_bytes, 'utf-8')\n\n    return encoded_bytes\n\n# Test the function"}
{"task_id": "BigCodeBench/546", "solution": "from collections import OrderedDict\nfrom prettytable import PrettyTable\ndef task_func(my_dict):\n    # Sort the dictionary by keys\n    sorted_dict = OrderedDict(sorted(my_dict.items()))\n\n    # Create a PrettyTable object\n    pt = PrettyTable()\n\n    # Add the column names\n    pt.field_names = [\"Key\", \"Value\"]\n\n    # Add the dictionary items to the table\n    for key, value in sorted_dict.items():\n        pt.add_row([key, value])\n\n    return pt", "raw_solution": "from collections import OrderedDict\nfrom prettytable import PrettyTable\n\ndef task_func(my_dict):\n    # Sort the dictionary by keys\n    sorted_dict = OrderedDict(sorted(my_dict.items()))\n\n    # Create a PrettyTable object\n    pt = PrettyTable()\n\n    # Add the column names\n    pt.field_names = [\"Key\", \"Value\"]\n\n    # Add the dictionary items to the table\n    for key, value in sorted_dict.items():\n        pt.add_row([key, value])\n\n    return pt\n\n# Test the function"}
{"task_id": "BigCodeBench/547", "solution": "import hashlib\nimport os\nimport base64\ndef task_func(password: str, salt_length: int = 8) -> str:\n    \"\"\"\n    Encrypt a password using Salt and SHA-256, then encode the result in base64.\n    \"\"\"\n    # Generate a random salt\n    salt = os.urandom(salt_length)\n\n    # Combine the password and salt\n    password_salt = password.encode() + salt\n\n    # Hash the password with SHA-256\n    hashed_password = hashlib.sha256(password_salt).digest()\n\n    # Encode the hashed password in base64\n    base64_encoded_password = base64.b64encode(hashed_password)\n\n    return base64_encoded_password.decode()", "raw_solution": "import hashlib\nimport os\nimport base64\n\ndef task_func(password: str, salt_length: int = 8) -> str:\n    \"\"\"\n    Encrypt a password using Salt and SHA-256, then encode the result in base64.\n    \"\"\"\n    # Generate a random salt\n    salt = os.urandom(salt_length)\n\n    # Combine the password and salt\n    password_salt = password.encode() + salt\n\n    # Hash the password with SHA-256\n    hashed_password = hashlib.sha256(password_salt).digest()\n\n    # Encode the hashed password in base64\n    base64_encoded_password = base64.b64encode(hashed_password)\n\n    return base64_encoded_password.decode()\n\n# Test the function"}
{"task_id": "BigCodeBench/548", "solution": "import random\nimport string\nimport base64\nimport zlib\ndef task_func(string_length=100):\n    \"\"\"\n    Generates a random string of a specified length with uppercase letters and digits,\n    compresses it with zlib, and then encodes the compressed string in base64.\n    \"\"\"\n    # Generate a random string\n    random_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=string_length))\n\n    # Compress the string with zlib\n    compressed_string = zlib.compress(random_string.encode())\n\n    # Encode the compressed string in base64\n    base64_string = base64.b64encode(compressed_string).decode()\n\n    return base64_string", "raw_solution": "import random\nimport string\nimport base64\nimport zlib\n\ndef task_func(string_length=100):\n    \"\"\"\n    Generates a random string of a specified length with uppercase letters and digits,\n    compresses it with zlib, and then encodes the compressed string in base64.\n    \"\"\"\n    # Generate a random string\n    random_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=string_length))\n\n    # Compress the string with zlib\n    compressed_string = zlib.compress(random_string.encode())\n\n    # Encode the compressed string in base64\n    base64_string = base64.b64encode(compressed_string).decode()\n\n    return base64_string\n\n# Test the function"}
{"task_id": "BigCodeBench/549", "solution": "import base64\nimport pandas as pd\nfrom io import StringIO\ndef task_func(data):\n    \"\"\"\n    Encodes a dict of list as a Base64 string.\n    \"\"\"\n    # Convert the dict to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Convert the DataFrame to CSV format\n    csv_str = df.to_csv(index=False)\n\n    # Encode the CSV string to bytes\n    csv_bytes = csv_str.encode('utf-8')\n\n    # Encode the bytes to a Base64 string\n    base64_str = base64.b64encode(csv_bytes).decode('utf-8')\n\n    return base64_str", "raw_solution": "import base64\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(data):\n    \"\"\"\n    Encodes a dict of list as a Base64 string.\n    \"\"\"\n    # Convert the dict to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Convert the DataFrame to CSV format\n    csv_str = df.to_csv(index=False)\n\n    # Encode the CSV string to bytes\n    csv_bytes = csv_str.encode('utf-8')\n\n    # Encode the bytes to a Base64 string\n    base64_str = base64.b64encode(csv_bytes).decode('utf-8')\n\n    return base64_str"}
{"task_id": "BigCodeBench/550", "solution": "from collections import Counter\nimport pandas as pd\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Given a nested list of menu items, this function flattens the list and returns a Pandas DataFrame detailing the count of each individual menu item with index name 'MenuItem'.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in list_of_menuitems for item in sublist]\n    \n    # Count the occurrences of each menu item\n    counter = Counter(flat_list)\n    \n    # Create a DataFrame from the counter\n    df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count'])\n    df.index.name = 'MenuItem'\n    \n    return df\nlist_of_menuitems = [['Pasta', 'Pizza', 'Pasta', 'Burger'], ['Pizza', 'Burger', 'Soup'], ['Burger', 'Soup', 'Pasta']]", "raw_solution": "from collections import Counter\nimport pandas as pd\n\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Given a nested list of menu items, this function flattens the list and returns a Pandas DataFrame detailing the count of each individual menu item with index name 'MenuItem'.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in list_of_menuitems for item in sublist]\n    \n    # Count the occurrences of each menu item\n    counter = Counter(flat_list)\n    \n    # Create a DataFrame from the counter\n    df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count'])\n    df.index.name = 'MenuItem'\n    \n    return df\n\n# Test the function\nlist_of_menuitems = [['Pasta', 'Pizza', 'Pasta', 'Burger'], ['Pizza', 'Burger', 'Soup'], ['Burger', 'Soup', 'Pasta']]"}
{"task_id": "BigCodeBench/551", "solution": "from collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Given a nested list of menu items, this function flattens the list and visualizes the frequency of each menu item using a seaborn barplot.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in list_of_menuitems for item in sublist]\n\n    # Count the frequency of each menu item\n    counter = Counter(flat_list)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(list(counter.items()), columns=['Menu Item', 'Frequency'])\n\n    # Create a barplot using seaborn\n    ax = sns.barplot(x='Menu Item', y='Frequency', data=df)\n\n    return ax", "raw_solution": "from collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Given a nested list of menu items, this function flattens the list and visualizes the frequency of each menu item using a seaborn barplot.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in list_of_menuitems for item in sublist]\n\n    # Count the frequency of each menu item\n    counter = Counter(flat_list)\n\n    # Create a DataFrame from the counter\n    df = pd.DataFrame(list(counter.items()), columns=['Menu Item', 'Frequency'])\n\n    # Create a barplot using seaborn\n    ax = sns.barplot(x='Menu Item', y='Frequency', data=df)\n\n    return ax"}
{"task_id": "BigCodeBench/552", "solution": "import collections\nimport itertools\nimport matplotlib.pyplot as plt\nITEMS = ['apple', 'banana']\ndef task_func(a, b, items=ITEMS):\n    # Combine the lists\n    combined = list(itertools.chain(a, b))\n\n    # Count the frequency of each item\n    counter = collections.Counter(combined)\n\n    # Filter the counter to only include the items we're interested in\n    filtered_counter = {item: counter[item] for item in items}\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(filtered_counter.keys(), filtered_counter.values())\n    ax.set_xlabel('Item')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of items in combined list')\n\n    return ax", "raw_solution": "import collections\nimport itertools\nimport matplotlib.pyplot as plt\n\n# Constants\nITEMS = ['apple', 'banana']\n\ndef task_func(a, b, items=ITEMS):\n    # Combine the lists\n    combined = list(itertools.chain(a, b))\n\n    # Count the frequency of each item\n    counter = collections.Counter(combined)\n\n    # Filter the counter to only include the items we're interested in\n    filtered_counter = {item: counter[item] for item in items}\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(filtered_counter.keys(), filtered_counter.values())\n    ax.set_xlabel('Item')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of items in combined list')\n\n    return ax"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b', and plots it as a bar chart.\n    List 'a' sets the DataFrame's row indices, while the length of list 'b' determines the number of columns.\n    The function returns the Axes object of the plotted bar chart.\n    \"\"\"\n    # Generate random values for DataFrame\n    np.random.seed(0)\n    df = pd.DataFrame(np.random.randint(0,10,size=(len(a), len(b))), index=a, columns=COLUMNS[:len(b)])\n\n    # Plot DataFrame as bar chart\n    ax = df.plot(kind='bar', figsize=(10,5))\n\n    return ax\na = ['Row1', 'Row2', 'Row3']\nb = [1, 2, 3, 4, 5]", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b', and plots it as a bar chart.\n    List 'a' sets the DataFrame's row indices, while the length of list 'b' determines the number of columns.\n    The function returns the Axes object of the plotted bar chart.\n    \"\"\"\n    # Generate random values for DataFrame\n    np.random.seed(0)\n    df = pd.DataFrame(np.random.randint(0,10,size=(len(a), len(b))), index=a, columns=COLUMNS[:len(b)])\n\n    # Plot DataFrame as bar chart\n    ax = df.plot(kind='bar', figsize=(10,5))\n\n    return ax\n\n# Test the function\na = ['Row1', 'Row2', 'Row3']\nb = [1, 2, 3, 4, 5]\nax = task_func(a, b)\nplt.show()"}
{"task_id": "BigCodeBench/554", "solution": "import numpy as np\nimport random\ndef task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL):\n    num_words = np.random.randint(MIN_WORDS, MAX_WORDS + 1)\n    sentence = ' '.join(random.choices(WORDS_POOL, k=num_words))\n    return sentence\nMIN_WORDS = 5\nMAX_WORDS = 10\nWORDS_POOL = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']", "raw_solution": "import numpy as np\nimport random\n\ndef task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL):\n    num_words = np.random.randint(MIN_WORDS, MAX_WORDS + 1)\n    sentence = ' '.join(random.choices(WORDS_POOL, k=num_words))\n    return sentence\n\n# Test the function\nMIN_WORDS = 5\nMAX_WORDS = 10\nWORDS_POOL = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n\nsentence = task_func(MIN_WORDS, MAX_WORDS, WORDS_POOL)"}
{"task_id": "BigCodeBench/555", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\ndef task_func(a, b):\n    \"\"\"\n    Calculates the Pearson correlation coefficient of two lists, generates a Pandas DataFrame from these lists,\n    and then draws a scatter plot with a regression line.\n    \"\"\"\n    # Create a DataFrame from the lists\n    df = pd.DataFrame({'a': a, 'b': b})\n\n    # Calculate the Pearson correlation coefficient\n    pearson_coef, _ = stats.pearsonr(df['a'], df['b'])\n\n    # Create a scatter plot with a regression line\n    fig, ax = plt.subplots()\n    ax.scatter(df['a'], df['b'])\n    ax.plot(df['a'], np.poly1d(np.polyfit(df['a'], df['b'], 1))(df['a']), color='red')\n\n    return pearson_coef, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndef task_func(a, b):\n    \"\"\"\n    Calculates the Pearson correlation coefficient of two lists, generates a Pandas DataFrame from these lists,\n    and then draws a scatter plot with a regression line.\n    \"\"\"\n    # Create a DataFrame from the lists\n    df = pd.DataFrame({'a': a, 'b': b})\n\n    # Calculate the Pearson correlation coefficient\n    pearson_coef, _ = stats.pearsonr(df['a'], df['b'])\n\n    # Create a scatter plot with a regression line\n    fig, ax = plt.subplots()\n    ax.scatter(df['a'], df['b'])\n    ax.plot(df['a'], np.poly1d(np.polyfit(df['a'], df['b'], 1))(df['a']), color='red')\n\n    return pearson_coef, ax"}
{"task_id": "BigCodeBench/556", "solution": "import numpy as np\nimport random\nfrom difflib import SequenceMatcher\ndef task_func(s, min_length, max_length, letters):\n    \"\"\"\n    Generates a random string of length between `min_length` and `max_length`, inclusive,\n    using characters from `letters`, and evaluates its similarity to the provided string `s`.\n    A similarity score of 0.5 or higher considered 'similar'.\n    The function should output with:\n        tuple: A tuple containing the generated string and a boolean indicating whether it's\n        considered similar to `s` based on the similarity threshold.\n    \"\"\"\n    # Generate a random string\n    length = random.randint(min_length, max_length)\n    random_string = ''.join(random.choice(letters) for _ in range(length))\n\n    # Calculate similarity score\n    similarity = SequenceMatcher(None, s, random_string).ratio()\n\n    # Return the generated string and whether it's considered similar\n    return (random_string, similarity >= 0.5)\ns = \"hello\"\nmin_length = 5\nmax_length = 10\nletters = \"abcdefghijklmnopqrstuvwxyz\"", "raw_solution": "import numpy as np\nimport random\nfrom difflib import SequenceMatcher\n\ndef task_func(s, min_length, max_length, letters):\n    \"\"\"\n    Generates a random string of length between `min_length` and `max_length`, inclusive,\n    using characters from `letters`, and evaluates its similarity to the provided string `s`.\n    A similarity score of 0.5 or higher considered 'similar'.\n    The function should output with:\n        tuple: A tuple containing the generated string and a boolean indicating whether it's\n        considered similar to `s` based on the similarity threshold.\n    \"\"\"\n    # Generate a random string\n    length = random.randint(min_length, max_length)\n    random_string = ''.join(random.choice(letters) for _ in range(length))\n\n    # Calculate similarity score\n    similarity = SequenceMatcher(None, s, random_string).ratio()\n\n    # Return the generated string and whether it's considered similar\n    return (random_string, similarity >= 0.5)\n\n# Test the function\ns = \"hello\"\nmin_length = 5\nmax_length = 10\nletters = \"abcdefghijklmnopqrstuvwxyz\"\n"}
{"task_id": "BigCodeBench/557", "solution": "import numpy as np\nfrom difflib import SequenceMatcher\nimport matplotlib.pyplot as plt\ndef task_func(s_list, plot_path=None):\n    if not isinstance(s_list, list):\n        raise ValueError(\"`s_list` should be a list of strings\")\n    if not all(isinstance(i, str) for i in s_list):\n        raise ValueError(\"`s_list` should be a list of strings\")\n    if len(s_list) == 0:\n        return np.nan\n    if len(s_list) == 1:\n        return np.nan\n\n    scores = []\n    for i in range(len(s_list)):\n        avg_score = sum([SequenceMatcher(None, s_list[i], s_list[j]).ratio() for j in range(len(s_list)) if i != j]) / (len(s_list) - 1)\n        scores.append(avg_score)\n\n    if plot_path is not None:\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(s_list)), scores)\n        plt.xlabel('String Index')\n        plt.ylabel('Average Similarity Score')\n        plt.title('Average Similarity Scores of Strings')\n        plt.savefig(plot_path)\n\n    return scores", "raw_solution": "import numpy as np\nfrom difflib import SequenceMatcher\nimport matplotlib.pyplot as plt\n\ndef task_func(s_list, plot_path=None):\n    if not isinstance(s_list, list):\n        raise ValueError(\"`s_list` should be a list of strings\")\n    if not all(isinstance(i, str) for i in s_list):\n        raise ValueError(\"`s_list` should be a list of strings\")\n    if len(s_list) == 0:\n        return np.nan\n    if len(s_list) == 1:\n        return np.nan\n\n    scores = []\n    for i in range(len(s_list)):\n        avg_score = sum([SequenceMatcher(None, s_list[i], s_list[j]).ratio() for j in range(len(s_list)) if i != j]) / (len(s_list) - 1)\n        scores.append(avg_score)\n\n    if plot_path is not None:\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(s_list)), scores)\n        plt.xlabel('String Index')\n        plt.ylabel('Average Similarity Score')\n        plt.title('Average Similarity Scores of Strings')\n        plt.savefig(plot_path)\n\n    return scores"}
{"task_id": "BigCodeBench/558", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(a, b, columns=['A', 'B']):\n    \"\"\"\n    Standardize two lists of numbers using the StandardScaler from sklearn and visualize the standardized values using a bar plot.\n    \"\"\"\n    # Check if inputs are lists\n    if not isinstance(a, list) or not isinstance(b, list):\n        raise ValueError(\"Inputs should be lists of numbers.\")\n\n    # Check if inputs are not empty\n    if not a or not b:\n        raise ValueError(\"Inputs should not be empty.\")\n\n    # Convert lists to numpy arrays\n    a = np.array(a).reshape(-1, 1)\n    b = np.array(b).reshape(-1, 1)\n\n    # Combine the two arrays\n    X = np.hstack((a, b))\n\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Fit and transform the data\n    X_std = scaler.fit_transform(X)\n\n    # Create a DataFrame\n    df = pd.DataFrame(X_std, columns=columns)\n\n    # Create a bar plot\n    ax = df.plot(kind='bar', figsize=(10, 6), title='Standardized Values')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Standardized Value')\n\n    return df, ax.figure", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(a, b, columns=['A', 'B']):\n    \"\"\"\n    Standardize two lists of numbers using the StandardScaler from sklearn and visualize the standardized values using a bar plot.\n    \"\"\"\n    # Check if inputs are lists\n    if not isinstance(a, list) or not isinstance(b, list):\n        raise ValueError(\"Inputs should be lists of numbers.\")\n\n    # Check if inputs are not empty\n    if not a or not b:\n        raise ValueError(\"Inputs should not be empty.\")\n\n    # Convert lists to numpy arrays\n    a = np.array(a).reshape(-1, 1)\n    b = np.array(b).reshape(-1, 1)\n\n    # Combine the two arrays\n    X = np.hstack((a, b))\n\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Fit and transform the data\n    X_std = scaler.fit_transform(X)\n\n    # Create a DataFrame\n    df = pd.DataFrame(X_std, columns=columns)\n\n    # Create a bar plot\n    ax = df.plot(kind='bar', figsize=(10, 6), title='Standardized Values')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Standardized Value')\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/559", "solution": "import pandas as pd\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\ndef task_func(a, b):\n    # Calculate Euclidean distance\n    euclidean_distance = distance.euclidean(a, b)\n\n    # Create DataFrame\n    df = pd.DataFrame({'A': a, 'B': b}, index=['A', 'B', 'C', 'D', 'E'])\n\n    # Create plot\n    fig, ax = plt.subplots()\n    ax.plot(df)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend(['List A', 'List B'])\n\n    return euclidean_distance, df, ax.figure", "raw_solution": "import pandas as pd\nfrom scipy.spatial import distance\nimport matplotlib.pyplot as plt\n\ndef task_func(a, b):\n    # Calculate Euclidean distance\n    euclidean_distance = distance.euclidean(a, b)\n\n    # Create DataFrame\n    df = pd.DataFrame({'A': a, 'B': b}, index=['A', 'B', 'C', 'D', 'E'])\n\n    # Create plot\n    fig, ax = plt.subplots()\n    ax.plot(df)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend(['List A', 'List B'])\n\n    return euclidean_distance, df, ax.figure"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): A pandas DataFrame with 'date' and 'value' columns.\n\n    Returns:\n    Axes: A matplotlib.axes.Axes object representing the plot.\n    \"\"\"\n    # Ensure 'date' column is in datetime format\n    data['date'] = pd.to_datetime(data['date'])\n\n    # Extract year and month from 'date' column\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n\n    # Group by month and calculate mean value\n    monthly_data = data.groupby('month')['value'].mean().reset_index()\n\n    # Create a bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(monthly_data['month'], monthly_data['value'])\n\n    # Set title and labels\n    ax.set_title(f\"Monthly Data for {data['year'].unique()[0]}\")\n    ax.set_xlabel(\"Month\")\n    ax.set_ylabel(\"Value\")\n\n    return ax", "raw_solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): A pandas DataFrame with 'date' and 'value' columns.\n\n    Returns:\n    Axes: A matplotlib.axes.Axes object representing the plot.\n    \"\"\"\n    # Ensure 'date' column is in datetime format\n    data['date'] = pd.to_datetime(data['date'])\n\n    # Extract year and month from 'date' column\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n\n    # Group by month and calculate mean value\n    monthly_data = data.groupby('month')['value'].mean().reset_index()\n\n    # Create a bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(monthly_data['month'], monthly_data['value'])\n\n    # Set title and labels\n    ax.set_title(f\"Monthly Data for {data['year'].unique()[0]}\")\n    ax.set_xlabel(\"Month\")\n    ax.set_ylabel(\"Value\")\n\n    return ax"}
{"task_id": "BigCodeBench/561", "solution": "from datetime import datetime\nfrom dateutil import parser\nfrom pytz import timezone\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Converts a date time from one timezone to another.\n    \"\"\"\n    # Parse the date string into a datetime object\n    dt = parser.parse(date_str)\n\n    # Localize the datetime object to the from_tz timezone\n    dt = dt.replace(tzinfo=timezone(from_tz))\n\n    # Convert the datetime object to the to_tz timezone\n    dt = dt.astimezone(timezone(to_tz))\n\n    # Format the datetime object into a string in \"yyyy-mm-dd hh:mm:ss\" format\n    dt_str = dt.strftime('%Y-%m-%d %H:%M:%S')\n\n    return dt_str", "raw_solution": "from datetime import datetime\nfrom dateutil import parser\nfrom pytz import timezone\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Converts a date time from one timezone to another.\n    \"\"\"\n    # Parse the date string into a datetime object\n    dt = parser.parse(date_str)\n\n    # Localize the datetime object to the from_tz timezone\n    dt = dt.replace(tzinfo=timezone(from_tz))\n\n    # Convert the datetime object to the to_tz timezone\n    dt = dt.astimezone(timezone(to_tz))\n\n    # Format the datetime object into a string in \"yyyy-mm-dd hh:mm:ss\" format\n    dt_str = dt.strftime('%Y-%m-%d %H:%M:%S')\n\n    return dt_str\n\n# Test the function"}
{"task_id": "BigCodeBench/562", "solution": "import os\nimport ctypes\nimport sys\nimport subprocess\ndef task_func(filepath):\n    \"\"\"\n    Loads a DLL file specified by the given filepath, then retrieves and prints system information including system name, node name, release, version, machine, Python version, and PIP version.\n    \"\"\"\n    if not isinstance(filepath, str):\n        raise TypeError(\"Input filepath must be a string\")\n    if not filepath:\n        raise OSError(\"Input filepath is invalid or empty\")\n\n    # Load DLL file\n    try:\n        ctypes.CDLL(filepath)\n    except OSError:\n        raise OSError(\"Failed to load DLL file\")\n\n    # Retrieve system information\n    system_name = os.uname().sysname\n    node_name = os.uname().nodename\n    release = os.uname().release\n    version = os.uname().version\n    machine = os.uname().machine\n\n    # Retrieve Python and PIP versions\n    python_version = sys.version.split()[0]\n    pip_version = subprocess.getoutput(\"pip --version\").split()[1]\n\n    # Print system information\n    print(f\"System: {system_name} Node Name: {node_name} Release: {release} Version: {version} Machine: {machine} Python Version: {python_version} PIP Version: {pip_version}\")\n\n    # Return the name of the loaded DLL file\n    return os.path.basename(filepath)", "raw_solution": "import os\nimport ctypes\nimport sys\nimport subprocess\n\ndef task_func(filepath):\n    \"\"\"\n    Loads a DLL file specified by the given filepath, then retrieves and prints system information including system name, node name, release, version, machine, Python version, and PIP version.\n    \"\"\"\n    if not isinstance(filepath, str):\n        raise TypeError(\"Input filepath must be a string\")\n    if not filepath:\n        raise OSError(\"Input filepath is invalid or empty\")\n\n    # Load DLL file\n    try:\n        ctypes.CDLL(filepath)\n    except OSError:\n        raise OSError(\"Failed to load DLL file\")\n\n    # Retrieve system information\n    system_name = os.uname().sysname\n    node_name = os.uname().nodename\n    release = os.uname().release\n    version = os.uname().version\n    machine = os.uname().machine\n\n    # Retrieve Python and PIP versions\n    python_version = sys.version.split()[0]\n    pip_version = subprocess.getoutput(\"pip --version\").split()[1]\n\n    # Print system information\n    print(f\"System: {system_name} Node Name: {node_name} Release: {release} Version: {version} Machine: {machine} Python Version: {python_version} PIP Version: {pip_version}\")\n\n    # Return the name of the loaded DLL file\n    return os.path.basename(filepath)"}
{"task_id": "BigCodeBench/563", "solution": "import ctypes\nimport os\nimport shutil\nimport glob\ndef task_func(filepath, destination_dir):\n    \"\"\"\n    Loads a DLL file specified by the given filepath and moves all DLL files in the same directory to another specified directory.\n    \"\"\"\n    # Load the DLL file\n    dll_file = ctypes.CDLL(filepath)\n\n    # Get the directory of the DLL file\n    dll_dir = os.path.dirname(filepath)\n\n    # Get all DLL files in the same directory\n    dll_files = glob.glob(os.path.join(dll_dir, \"*.dll\"))\n\n    # Move all DLL files to the destination directory\n    for dll_file in dll_files:\n        shutil.move(dll_file, destination_dir)\n\n    # Return the name of the loaded DLL file\n    return os.path.basename(filepath)", "raw_solution": "import ctypes\nimport os\nimport shutil\nimport glob\n\ndef task_func(filepath, destination_dir):\n    \"\"\"\n    Loads a DLL file specified by the given filepath and moves all DLL files in the same directory to another specified directory.\n    \"\"\"\n    # Load the DLL file\n    dll_file = ctypes.CDLL(filepath)\n\n    # Get the directory of the DLL file\n    dll_dir = os.path.dirname(filepath)\n\n    # Get all DLL files in the same directory\n    dll_files = glob.glob(os.path.join(dll_dir, \"*.dll\"))\n\n    # Move all DLL files to the destination directory\n    for dll_file in dll_files:\n        shutil.move(dll_file, destination_dir)\n\n    # Return the name of the loaded DLL file\n    return os.path.basename(filepath)"}
{"task_id": "BigCodeBench/564", "solution": "import os\nimport ctypes\nfrom datetime import datetime\nimport pytz\ndef task_func(filepath):\n    \"\"\"\n    Loads a DLL file from the specified filepath and returns its metadata, including creation time, modification time, and file size.\n    The times are displayed in UTC format.\n    \"\"\"\n    # Load the DLL file\n    dll = ctypes.CDLL(filepath)\n\n    # Get the file size\n    size = os.path.getsize(filepath)\n\n    # Get the creation and modification times\n    creation_time = os.path.getctime(filepath)\n    modification_time = os.path.getmtime(filepath)\n\n    # Convert the times to UTC\n    creation_time_utc = datetime.fromtimestamp(creation_time, pytz.utc)\n    modification_time_utc = datetime.fromtimestamp(modification_time, pytz.utc)\n\n    # Create a dictionary to store the metadata\n    metadata = {\n        'Creation Time': creation_time_utc.strftime('%Y-%m-%d %H:%M:%S'),\n        'Modification Time': modification_time_utc.strftime('%Y-%m-%d %H:%M:%S'),\n        'Size': size\n    }\n\n    return os.path.basename(filepath), metadata", "raw_solution": "import os\nimport ctypes\nfrom datetime import datetime\nimport pytz\n\ndef task_func(filepath):\n    \"\"\"\n    Loads a DLL file from the specified filepath and returns its metadata, including creation time, modification time, and file size.\n    The times are displayed in UTC format.\n    \"\"\"\n    # Load the DLL file\n    dll = ctypes.CDLL(filepath)\n\n    # Get the file size\n    size = os.path.getsize(filepath)\n\n    # Get the creation and modification times\n    creation_time = os.path.getctime(filepath)\n    modification_time = os.path.getmtime(filepath)\n\n    # Convert the times to UTC\n    creation_time_utc = datetime.fromtimestamp(creation_time, pytz.utc)\n    modification_time_utc = datetime.fromtimestamp(modification_time, pytz.utc)\n\n    # Create a dictionary to store the metadata\n    metadata = {\n        'Creation Time': creation_time_utc.strftime('%Y-%m-%d %H:%M:%S'),\n        'Modification Time': modification_time_utc.strftime('%Y-%m-%d %H:%M:%S'),\n        'Size': size\n    }\n\n    return os.path.basename(filepath), metadata"}
{"task_id": "BigCodeBench/565", "solution": "import ctypes\nimport hashlib\nimport binascii\ndef task_func(filepath):\n    \"\"\"\n    Loads a DLL file from a given filepath, calculates its MD5 and SHA256 hashes, and prints these hashes in hexadecimal format.\n    \"\"\"\n    # Load the DLL file\n    ctypes.CDLL(filepath)\n\n    # Calculate the MD5 hash\n    md5_hash = hashlib.md5()\n    with open(filepath, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(chunk)\n    md5_hash = md5_hash.digest()\n\n    # Calculate the SHA256 hash\n    sha256_hash = hashlib.sha256()\n    with open(filepath, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(chunk)\n    sha256_hash = sha256_hash.digest()\n\n    # Convert the hashes to hexadecimal\n    md5_hash_hex = binascii.hexlify(md5_hash).decode()\n    sha256_hash_hex = binascii.hexlify(sha256_hash).decode()\n\n    # Print the hashes\n    print(f\"MD5 hash of {filepath}: {md5_hash_hex}\")\n    print(f\"SHA256 hash of {filepath}: {sha256_hash_hex}\")\n\n    # Return the name of the loaded DLL file\n    return filepath", "raw_solution": "import ctypes\nimport hashlib\nimport binascii\n\ndef task_func(filepath):\n    \"\"\"\n    Loads a DLL file from a given filepath, calculates its MD5 and SHA256 hashes, and prints these hashes in hexadecimal format.\n    \"\"\"\n    # Load the DLL file\n    ctypes.CDLL(filepath)\n\n    # Calculate the MD5 hash\n    md5_hash = hashlib.md5()\n    with open(filepath, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(chunk)\n    md5_hash = md5_hash.digest()\n\n    # Calculate the SHA256 hash\n    sha256_hash = hashlib.sha256()\n    with open(filepath, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(chunk)\n    sha256_hash = sha256_hash.digest()\n\n    # Convert the hashes to hexadecimal\n    md5_hash_hex = binascii.hexlify(md5_hash).decode()\n    sha256_hash_hex = binascii.hexlify(sha256_hash).decode()\n\n    # Print the hashes\n    print(f\"MD5 hash of {filepath}: {md5_hash_hex}\")\n    print(f\"SHA256 hash of {filepath}: {sha256_hash_hex}\")\n\n    # Return the name of the loaded DLL file\n    return filepath"}
{"task_id": "BigCodeBench/566", "solution": "import inspect\nimport types\ndef task_func(f):\n    if not inspect.isfunction(f):\n        raise ValueError(\"Input must be a function\")\n\n    spec = {\n        \"name\": f.__name__,\n        \"is_lambda\": isinstance(f, types.LambdaType),\n        \"arguments\": list(inspect.signature(f).parameters.keys()),\n        \"default_values\": {\n            k: v.default\n            for k, v in inspect.signature(f).parameters.items()\n            if v.default is not inspect.Parameter.empty\n        },\n        \"annotations\": {\n            k: v.annotation\n            for k, v in inspect.signature(f).parameters.items()\n            if v.annotation is not inspect.Parameter.empty\n        },\n    }\n\n    return spec", "raw_solution": "import inspect\nimport types\n\ndef task_func(f):\n    if not inspect.isfunction(f):\n        raise ValueError(\"Input must be a function\")\n\n    spec = {\n        \"name\": f.__name__,\n        \"is_lambda\": isinstance(f, types.LambdaType),\n        \"arguments\": list(inspect.signature(f).parameters.keys()),\n        \"default_values\": {\n            k: v.default\n            for k, v in inspect.signature(f).parameters.items()\n            if v.default is not inspect.Parameter.empty\n        },\n        \"annotations\": {\n            k: v.annotation\n            for k, v in inspect.signature(f).parameters.items()\n            if v.annotation is not inspect.Parameter.empty\n        },\n    }\n\n    return spec\n\n# Testing\ndef test_func(a, b=1, c=2, *args, d=3, **kwargs):\n    \"\"\"\n    This is a test function.\n    \"\"\"\n    return a + b + c + sum(args) + sum(kwargs.values()) + d\n"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    This function draws a histogram to visualize the frequency distribution of numeric values provided in a string format.\n    \"\"\"\n    # Convert the string data to a list of floats\n    data = [float(i) for i in data.split()]\n    \n    # Convert the list to a numpy array\n    data = np.array(data)\n    \n    # Calculate the bins\n    bins = np.arange(data.min(), data.max()+2) - 0.5\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, edgecolor='black')\n    \n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    This function draws a histogram to visualize the frequency distribution of numeric values provided in a string format.\n    \"\"\"\n    # Convert the string data to a list of floats\n    data = [float(i) for i in data.split()]\n    \n    # Convert the list to a numpy array\n    data = np.array(data)\n    \n    # Calculate the bins\n    bins = np.arange(data.min(), data.max()+2) - 0.5\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, edgecolor='black')\n    \n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    return ax"}
{"task_id": "BigCodeBench/568", "solution": "import inspect\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(f_list):\n    \"\"\"\n    Analyzes a list of functions and draws a bar chart showing the number of arguments for each function.\n    \"\"\"\n    if not all(inspect.isfunction(f) for f in f_list):\n        raise ValueError(\"All elements in the input list must be functions.\")\n\n    data = [(f.__name__, len(inspect.signature(f).parameters)) for f in f_list]\n    df = pd.DataFrame(data, columns=[\"Function Name\", \"Number of Arguments\"])\n    df.plot(x=\"Function Name\", y=\"Number of Arguments\", kind=\"bar\", legend=None)\n    plt.ylabel(\"Number of Arguments\")\n    plt.show()\n\n    return df", "raw_solution": "import inspect\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(f_list):\n    \"\"\"\n    Analyzes a list of functions and draws a bar chart showing the number of arguments for each function.\n    \"\"\"\n    if not all(inspect.isfunction(f) for f in f_list):\n        raise ValueError(\"All elements in the input list must be functions.\")\n\n    data = [(f.__name__, len(inspect.signature(f).parameters)) for f in f_list]\n    df = pd.DataFrame(data, columns=[\"Function Name\", \"Number of Arguments\"])\n    df.plot(x=\"Function Name\", y=\"Number of Arguments\", kind=\"bar\", legend=None)\n    plt.ylabel(\"Number of Arguments\")\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/569", "solution": "import inspect\nimport types\nimport math\ndef task_func(f):\n    sig = inspect.signature(f)\n    num_args = len(sig.parameters)\n    lambda_count = sum(1 for param in sig.parameters.values() if isinstance(param.default, types.LambdaType))\n    return {\n        'name': f.__name__,\n        'sqrt_num_args': math.sqrt(num_args),\n        'lambda_count': lambda_count,\n    }", "raw_solution": "import inspect\nimport types\nimport math\n\ndef task_func(f):\n    sig = inspect.signature(f)\n    num_args = len(sig.parameters)\n    lambda_count = sum(1 for param in sig.parameters.values() if isinstance(param.default, types.LambdaType))\n    return {\n        'name': f.__name__,\n        'sqrt_num_args': math.sqrt(num_args),\n        'lambda_count': lambda_count,\n    }\n\n# Testing the function\ndef test_task_func():\n    def f1(a, b, c=lambda: 1, d=lambda: 2):\n        pass\n\n    def f2(a, b, c=lambda: 1):\n        pass\n\n    assert task_func(f1) == {'name': 'f1', 'sqrt_num_args': 1.4142135623730951, 'lambda_count': 2}\n    assert task_func(f2) == {'name': 'f2', 'sqrt_num_args': 1.4142135623730951, 'lambda_count': 1}\n\n    print(\"All tests passed.\")\n\ntest_task_func()"}
{"task_id": "BigCodeBench/570", "solution": "import inspect\nimport types\nimport json\nfrom typing import Any, Callable, Dict\ndef task_func(f: Callable[..., Any]) -> str:\n    \"\"\"\n    Inspects the given function 'f' and returns its specifications as a JSON string.\n    This includes the function's name, arguments, default values, annotations in a string format, and a boolean indicating if it's a lambda function.\n    \"\"\"\n    if isinstance(f, types.LambdaType):\n        is_lambda = True\n    else:\n        is_lambda = False\n\n    arg_spec = inspect.getfullargspec(f)\n    annotations = inspect.get_annotations(f)\n\n    result = {\n        \"name\": f.__name__,\n        \"arguments\": arg_spec.args,\n        \"defaults\": {k: v for k, v in zip(arg_spec.args[::-1], arg_spec.defaults[::-1]) if v is not inspect._empty},\n        \"annotations\": {k: str(v) for k, v in annotations.items() if k != 'return'},\n        \"is_lambda\": is_lambda,\n    }\n\n    return json.dumps(result, indent=4)", "raw_solution": "import inspect\nimport types\nimport json\nfrom typing import Any, Callable, Dict\n\ndef task_func(f: Callable[..., Any]) -> str:\n    \"\"\"\n    Inspects the given function 'f' and returns its specifications as a JSON string.\n    This includes the function's name, arguments, default values, annotations in a string format, and a boolean indicating if it's a lambda function.\n    \"\"\"\n    if isinstance(f, types.LambdaType):\n        is_lambda = True\n    else:\n        is_lambda = False\n\n    arg_spec = inspect.getfullargspec(f)\n    annotations = inspect.get_annotations(f)\n\n    result = {\n        \"name\": f.__name__,\n        \"arguments\": arg_spec.args,\n        \"defaults\": {k: v for k, v in zip(arg_spec.args[::-1], arg_spec.defaults[::-1]) if v is not inspect._empty},\n        \"annotations\": {k: str(v) for k, v in annotations.items() if k != 'return'},\n        \"is_lambda\": is_lambda,\n    }\n\n    return json.dumps(result, indent=4)\n\n# Testing\ndef test_func(a: int, b: str = 'test') -> None:\n    \"\"\"\n    A test function.\n    \"\"\"\n    print(a, b)\n"}
{"task_id": "BigCodeBench/571", "solution": "import inspect\nimport pandas as pd\ndef task_func(f_list, file_path):\n    if not isinstance(f_list, list) or not all(inspect.isfunction(f) for f in f_list):\n        raise ValueError(\"'f_list' should be a list of functions\")\n    if not f_list:\n        raise ValueError(\"'f_list' is empty\")\n    if not isinstance(file_path, str):\n        raise ValueError(\"'file_path' should be a string\")\n\n    data = []\n    for f in f_list:\n        defaults = inspect.getfullargspec(f).defaults\n        if defaults is not None:\n            defaults = list(defaults)\n        annotations = inspect.getfullargspec(f).annotations\n        if annotations is not None:\n            annotations = str(annotations)\n        row = {\n            'Function Name': f.__name__,\n            'Number of Arguments': len(inspect.signature(f).parameters),\n            'Defaults': defaults,\n            'Annotations': annotations,\n            'Is Lambda': inspect.isfunction(f) and f.__name__ == '<lambda>'\n        }\n        data.append(row)\n\n    df = pd.DataFrame(data)\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"Error writing to the specified file path\")", "raw_solution": "import inspect\nimport pandas as pd\n\ndef task_func(f_list, file_path):\n    if not isinstance(f_list, list) or not all(inspect.isfunction(f) for f in f_list):\n        raise ValueError(\"'f_list' should be a list of functions\")\n    if not f_list:\n        raise ValueError(\"'f_list' is empty\")\n    if not isinstance(file_path, str):\n        raise ValueError(\"'file_path' should be a string\")\n\n    data = []\n    for f in f_list:\n        defaults = inspect.getfullargspec(f).defaults\n        if defaults is not None:\n            defaults = list(defaults)\n        annotations = inspect.getfullargspec(f).annotations\n        if annotations is not None:\n            annotations = str(annotations)\n        row = {\n            'Function Name': f.__name__,\n            'Number of Arguments': len(inspect.signature(f).parameters),\n            'Defaults': defaults,\n            'Annotations': annotations,\n            'Is Lambda': inspect.isfunction(f) and f.__name__ == '<lambda>'\n        }\n        data.append(row)\n\n    df = pd.DataFrame(data)\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"Error writing to the specified file path\")"}
{"task_id": "BigCodeBench/572", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100):\n    \"\"\"\n    Generates two arrays of random integers and draws a line diagram with the maximum values of the respective elements of the two arrays.\n    \"\"\"\n    # Generate two arrays of random integers\n    array1 = [randint(0, 100) for _ in range(array_length)]\n    array2 = [randint(0, 100) for _ in range(array_length)]\n\n    # Find the maximum values of the respective elements of the two arrays\n    max_values = [max(array1[i], array2[i]) for i in range(array_length)]\n\n    # Create a line diagram\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(array_length), max_values, label='Maximum Values')\n    plt.xlabel('Index')\n    plt.ylabel('Maximum Values')\n    plt.title('Line Diagram of Maximum Values')\n    plt.legend()\n\n    return plt.gca()", "raw_solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100):\n    \"\"\"\n    Generates two arrays of random integers and draws a line diagram with the maximum values of the respective elements of the two arrays.\n    \"\"\"\n    # Generate two arrays of random integers\n    array1 = [randint(0, 100) for _ in range(array_length)]\n    array2 = [randint(0, 100) for _ in range(array_length)]\n\n    # Find the maximum values of the respective elements of the two arrays\n    max_values = [max(array1[i], array2[i]) for i in range(array_length)]\n\n    # Create a line diagram\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(array_length), max_values, label='Maximum Values')\n    plt.xlabel('Index')\n    plt.ylabel('Maximum Values')\n    plt.title('Line Diagram of Maximum Values')\n    plt.legend()\n\n    return plt.gca()\n\n# Test the function\naxes = task_func(100)\nplt.show()"}
{"task_id": "BigCodeBench/573", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(array_length=100):\n    np.random.seed(0)\n    array1 = np.random.rand(array_length)\n    array2 = np.random.rand(array_length)\n\n    statistics = pd.DataFrame({\n        'Array1': [np.mean(array1), np.median(array1), np.std(array1)],\n        'Array2': [np.mean(array2), np.median(array2), np.std(array2)]\n    }, index=['Mean', 'Median', 'Standard Deviation'])\n\n    fig, ax = plt.subplots()\n    statistics.plot(kind='bar', ax=ax)\n    ax.set_ylabel('Value')\n    ax.set_title('Comparison of Statistics')\n\n    return statistics, ax.figure", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(array_length=100):\n    np.random.seed(0)\n    array1 = np.random.rand(array_length)\n    array2 = np.random.rand(array_length)\n\n    statistics = pd.DataFrame({\n        'Array1': [np.mean(array1), np.median(array1), np.std(array1)],\n        'Array2': [np.mean(array2), np.median(array2), np.std(array2)]\n    }, index=['Mean', 'Median', 'Standard Deviation'])\n\n    fig, ax = plt.subplots()\n    statistics.plot(kind='bar', ax=ax)\n    ax.set_ylabel('Value')\n    ax.set_title('Comparison of Statistics')\n\n    return statistics, ax.figure"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    def sine_func(x, a, b, c):\n        return a * np.sin(b * x) + c\n\n    x = np.linspace(0, 2 * np.pi, array_length)\n    y = np.sin(x) + np.random.normal(0, noise_level, array_length)\n\n    popt, _ = curve_fit(sine_func, x, y)\n\n    y_fit = sine_func(x, *popt)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, 'o', label='data')\n    ax.plot(x, y_fit, '-', label='fit')\n    ax.legend()\n    return ax", "raw_solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    def sine_func(x, a, b, c):\n        return a * np.sin(b * x) + c\n\n    x = np.linspace(0, 2 * np.pi, array_length)\n    y = np.sin(x) + np.random.normal(0, noise_level, array_length)\n\n    popt, _ = curve_fit(sine_func, x, y)\n\n    y_fit = sine_func(x, *popt)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, 'o', label='data')\n    ax.plot(x, y_fit, '-', label='fit')\n    ax.legend()\n    return ax\n\n# Test the function\nax = task_func(100, 0.2)\nplt.show()"}
{"task_id": "BigCodeBench/575", "solution": "from random import shuffle\nimport pandas as pd\nimport numpy as np\ndef task_func(l, n_groups = 5):\n    # Shuffle the list\n    shuffle(l)\n\n    # Construct a dataframe from the shuffled list\n    df = pd.DataFrame(l, columns=['Value'])\n\n    # For each row in the dataframe, move the first n_groups elements to the end of the same row\n    for i in range(len(df)):\n        df.iloc[i] = np.roll(df.iloc[i], n_groups)\n\n    return df\nl = [i for i in range(1, 21)]", "raw_solution": "from random import shuffle\nimport pandas as pd\nimport numpy as np\n\ndef task_func(l, n_groups = 5):\n    # Shuffle the list\n    shuffle(l)\n\n    # Construct a dataframe from the shuffled list\n    df = pd.DataFrame(l, columns=['Value'])\n\n    # For each row in the dataframe, move the first n_groups elements to the end of the same row\n    for i in range(len(df)):\n        df.iloc[i] = np.roll(df.iloc[i], n_groups)\n\n    return df\n\n# Test the function\nl = [i for i in range(1, 21)]"}
{"task_id": "BigCodeBench/576", "solution": "from random import shuffle, randint\nimport pandas as pd\ndef task_func(l, n_groups = 5):\n    shuffle(l)\n    l = l * 2\n    series = pd.Series(l)\n    for i in range(len(series)):\n        series[i] = series[i][n_groups:] + series[i][:n_groups]\n    return series\nl = ['abcdefghijklmnopqrstuvwxyz', '1234567890', 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', '!@#$%^&*()', 'Hello, World!']", "raw_solution": "from random import shuffle, randint\nimport pandas as pd\n\ndef task_func(l, n_groups = 5):\n    shuffle(l)\n    l = l * 2\n    series = pd.Series(l)\n    for i in range(len(series)):\n        series[i] = series[i][n_groups:] + series[i][:n_groups]\n    return series\n\n# Testing the function\nl = ['abcdefghijklmnopqrstuvwxyz', '1234567890', 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', '!@#$%^&*()', 'Hello, World!']"}
{"task_id": "BigCodeBench/577", "solution": "import os\nimport pathlib\nfrom hashlib import md5\nimport unicodedata\ndef task_func(directory):\n    \"\"\"\n    Processes all files within the specified directory, normalizes their filenames to ASCII, calculates their MD5 hashes, and retrieves their sizes.\n    Returns a dictionary where each key is the normalized file name and each value is another dictionary with the file's size and MD5 hash.\n    \"\"\"\n    result = {}\n    for filename in os.listdir(directory):\n        # Normalize filename to ASCII\n        ascii_filename = unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode()\n        # Get file size\n        file_size = os.path.getsize(os.path.join(directory, filename))\n        # Calculate MD5 hash\n        with open(os.path.join(directory, filename), 'rb') as file:\n            file_hash = md5(file.read()).hexdigest()\n        # Add to result dictionary\n        result[ascii_filename] = {'Size': file_size, 'MD5 Hash': file_hash}\n    return result", "raw_solution": "import os\nimport pathlib\nfrom hashlib import md5\nimport unicodedata\n\ndef task_func(directory):\n    \"\"\"\n    Processes all files within the specified directory, normalizes their filenames to ASCII, calculates their MD5 hashes, and retrieves their sizes.\n    Returns a dictionary where each key is the normalized file name and each value is another dictionary with the file's size and MD5 hash.\n    \"\"\"\n    result = {}\n    for filename in os.listdir(directory):\n        # Normalize filename to ASCII\n        ascii_filename = unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode()\n        # Get file size\n        file_size = os.path.getsize(os.path.join(directory, filename))\n        # Calculate MD5 hash\n        with open(os.path.join(directory, filename), 'rb') as file:\n            file_hash = md5(file.read()).hexdigest()\n        # Add to result dictionary\n        result[ascii_filename] = {'Size': file_size, 'MD5 Hash': file_hash}\n    return result"}
{"task_id": "BigCodeBench/578", "solution": "import unicodedata\nimport requests\nfrom requests.exceptions import HTTPError\nURL = 'https://api.github.com/users/'\ndef task_func(username):\n    \"\"\"\n    Retrieves user information from the GitHub API for a given username, normalizes all string data to ASCII, and returns a dictionary of the normalized data.\n    \"\"\"\n    try:\n        response = requests.get(URL + username)\n        # If the response was successful, no Exception will be raised\n        response.raise_for_status()\n    except HTTPError as http_err:\n        print(f'HTTP error occurred: {http_err}')\n    except Exception as err:\n        print(f'Other error occurred: {err}')\n    else:\n        data = response.json()\n        normalized_data = {\n            key: unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode()\n            if isinstance(value, str) else value\n            for key, value in data.items()\n        }\n        return normalized_data", "raw_solution": "import unicodedata\nimport requests\nfrom requests.exceptions import HTTPError\n\nURL = 'https://api.github.com/users/'\n\ndef task_func(username):\n    \"\"\"\n    Retrieves user information from the GitHub API for a given username, normalizes all string data to ASCII, and returns a dictionary of the normalized data.\n    \"\"\"\n    try:\n        response = requests.get(URL + username)\n        # If the response was successful, no Exception will be raised\n        response.raise_for_status()\n    except HTTPError as http_err:\n        print(f'HTTP error occurred: {http_err}')\n    except Exception as err:\n        print(f'Other error occurred: {err}')\n    else:\n        data = response.json()\n        normalized_data = {\n            key: unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode()\n            if isinstance(value, str) else value\n            for key, value in data.items()\n        }\n        return normalized_data"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r') as file:\n            reader = csv.reader(file)\n            data = [row[0] for row in reader]\n    except FileNotFoundError:\n        return \"File not found\"\n    except IOError:\n        return \"Error reading file\"\n\n    # Normalize text to ASCII\n    data = [unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode() for text in data]\n\n    # Split text into words\n    words = ' '.join(data).split()\n\n    # Count words\n    word_counts = Counter(words)\n\n    # Get 10 most common words\n    common_words = word_counts.most_common(10)\n\n    # Create bar plot\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('10 Most Common Words')\n\n    return (ax, common_words)", "raw_solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    try:\n        with open(csv_file, 'r') as file:\n            reader = csv.reader(file)\n            data = [row[0] for row in reader]\n    except FileNotFoundError:\n        return \"File not found\"\n    except IOError:\n        return \"Error reading file\"\n\n    # Normalize text to ASCII\n    data = [unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode() for text in data]\n\n    # Split text into words\n    words = ' '.join(data).split()\n\n    # Count words\n    word_counts = Counter(words)\n\n    # Get 10 most common words\n    common_words = word_counts.most_common(10)\n\n    # Create bar plot\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('10 Most Common Words')\n\n    return (ax, common_words)"}
{"task_id": "BigCodeBench/580", "solution": "import pandas as pd\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\nRANGE = 10000\nSIZE = 1000\nBIN_WIDTH = 100\ndef task_func():\n    \"\"\"\n    Generates a pandas DataFrame with two columns, \"Random Numbers\" and \"Moving Average,\" filled with random integers and their moving average, respectively.\n    Additionally, this function plots a histogram of the \"Random Numbers\" column.\n    \"\"\"\n    # Generate random numbers\n    random_numbers = [random.randint(0, RANGE) for _ in range(SIZE)]\n\n    # Calculate moving average\n    moving_avg = [sum(random_numbers[max(0, i-5):i+1])/min(5, i+1) for i in range(SIZE)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Random Numbers\": random_numbers,\n        \"Moving Average\": moving_avg\n    })\n\n    # Plot histogram\n    plt.hist(df[\"Random Numbers\"], bins=np.arange(min(random_numbers), max(random_numbers) + BIN_WIDTH, BIN_WIDTH))\n    plt.title(\"Histogram of Random Numbers\")\n    plt.xlabel(\"Random Number\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nRANGE = 10000  # The range within which random numbers are generated\nSIZE = 1000  # The number of random numbers to generate\nBIN_WIDTH = 100  # The width of bins for the histogram\n\ndef task_func():\n    \"\"\"\n    Generates a pandas DataFrame with two columns, \"Random Numbers\" and \"Moving Average,\" filled with random integers and their moving average, respectively.\n    Additionally, this function plots a histogram of the \"Random Numbers\" column.\n    \"\"\"\n    # Generate random numbers\n    random_numbers = [random.randint(0, RANGE) for _ in range(SIZE)]\n\n    # Calculate moving average\n    moving_avg = [sum(random_numbers[max(0, i-5):i+1])/min(5, i+1) for i in range(SIZE)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Random Numbers\": random_numbers,\n        \"Moving Average\": moving_avg\n    })\n\n    # Plot histogram\n    plt.hist(df[\"Random Numbers\"], bins=np.arange(min(random_numbers), max(random_numbers) + BIN_WIDTH, BIN_WIDTH))\n    plt.title(\"Histogram of Random Numbers\")\n    plt.xlabel(\"Random Number\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/581", "solution": "import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nRANGE = 10000\nSIZE = 1000\nPI = np.pi\ndef task_func(size=SIZE, frequency=1):\n    \"\"\"\n    Generates a list of random sinusoidal values and plots them in a graph.\n    \"\"\"\n    # Generate a list of random x values\n    x = np.linspace(0, 2*PI, size)\n\n    # Generate a list of random y values\n    y = [random.uniform(-RANGE, RANGE) + RANGE * np.sin(frequency * x[i]) for i in range(size)]\n\n    # Plot the sinusoidal wave\n    plt.plot(x, y)\n    plt.title('Sinusoidal Wave')\n    plt.xlabel('x')\n    plt.ylabel('y')\n\n    return plt.gca()", "raw_solution": "import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\n# Constants\nRANGE = 10000\nSIZE = 1000\nPI = np.pi\n\ndef task_func(size=SIZE, frequency=1):\n    \"\"\"\n    Generates a list of random sinusoidal values and plots them in a graph.\n    \"\"\"\n    # Generate a list of random x values\n    x = np.linspace(0, 2*PI, size)\n\n    # Generate a list of random y values\n    y = [random.uniform(-RANGE, RANGE) + RANGE * np.sin(frequency * x[i]) for i in range(size)]\n\n    # Plot the sinusoidal wave\n    plt.plot(x, y)\n    plt.title('Sinusoidal Wave')\n    plt.xlabel('x')\n    plt.ylabel('y')\n\n    return plt.gca()\n\n# Test the function\naxes = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    \"\"\"\n    Generates a list of normally distributed random numbers, plots their histogram and PDF, and returns the figure.\n    \"\"\"\n    # Generate normally distributed random numbers\n    data = np.random.normal(size=size)\n\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n\n    # Plot histogram\n    ax[0].hist(data, bins=30, density=True, edgecolor='black')\n    ax[0].set_title('Histogram')\n    ax[0].set_xlabel('Value')\n    ax[0].set_ylabel('Frequency')\n\n    # Plot PDF\n    x = np.linspace(-4, 4, 1000)\n    pdf = stats.norm.pdf(x)\n    ax[1].plot(x, pdf, label='PDF')\n    ax[1].set_title('Probability Density Function')\n    ax[1].set_xlabel('Value')\n    ax[1].set_ylabel('Probability')\n\n    # Adjust layout and display plot\n    plt.tight_layout()\n    plt.show()\n\n    return fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    \"\"\"\n    Generates a list of normally distributed random numbers, plots their histogram and PDF, and returns the figure.\n    \"\"\"\n    # Generate normally distributed random numbers\n    data = np.random.normal(size=size)\n\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n\n    # Plot histogram\n    ax[0].hist(data, bins=30, density=True, edgecolor='black')\n    ax[0].set_title('Histogram')\n    ax[0].set_xlabel('Value')\n    ax[0].set_ylabel('Frequency')\n\n    # Plot PDF\n    x = np.linspace(-4, 4, 1000)\n    pdf = stats.norm.pdf(x)\n    ax[1].plot(x, pdf, label='PDF')\n    ax[1].set_title('Probability Density Function')\n    ax[1].set_xlabel('Value')\n    ax[1].set_ylabel('Probability')\n\n    # Adjust layout and display plot\n    plt.tight_layout()\n    plt.show()\n\n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\nfrom Crypto.Util.Padding import pad, unpad\nfrom Crypto.Random import get_random_bytes\ndef task_func():\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Generate random bytes for filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Generate AES key and nonce\n    aes_key = get_random_bytes(32)\n    aes_nonce = get_random_bytes(16)\n\n    # Encrypt the private key with AES\n    cipher = AES.new(aes_key, AES.MODE_EAX)\n    encrypted_data = cipher.encrypt(rsa.key.exportKey('PEM'))\n\n    # Save the encrypted private key to file\n    with open(filename, 'wb') as f:\n        f.write(cipher.nonce)\n        f.write(encrypted_data)\n\n    return pubkey, filename, aes_key, cipher.nonce", "raw_solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\nfrom Crypto.Util.Padding import pad, unpad\nfrom Crypto.Random import get_random_bytes\n\ndef task_func():\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Generate random bytes for filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Generate AES key and nonce\n    aes_key = get_random_bytes(32)\n    aes_nonce = get_random_bytes(16)\n\n    # Encrypt the private key with AES\n    cipher = AES.new(aes_key, AES.MODE_EAX)\n    encrypted_data = cipher.encrypt(rsa.key.exportKey('PEM'))\n\n    # Save the encrypted private key to file\n    with open(filename, 'wb') as f:\n        f.write(cipher.nonce)\n        f.write(encrypted_data)\n\n    return pubkey, filename, aes_key, cipher.nonce\n\n# Testing\npubkey, filename, aes_key, aes_nonce = task_func()"}
{"task_id": "BigCodeBench/584", "solution": "import rsa\nimport urllib.request\nfrom hashlib import sha256\ndef task_func(url):\n    try:\n        # Generate RSA public and private keys\n        (pubkey, privkey) = rsa.newkeys(512)\n\n        # Retrieve content from the specified URL\n        response = urllib.request.urlopen(url)\n        content = response.read()\n\n        # Calculate SHA256 hash of the content\n        content_hash = sha256(content).digest()\n\n        # Sign the hash with the private key\n        signature = rsa.sign_hash(content_hash, privkey, 'SHA-256')\n\n        # Convert the signature to hexadecimal\n        signature_hex = signature.hex()\n\n        return pubkey, signature_hex, content_hash\n\n    except urllib.error.URLError as e:\n        print(f\"URL Error: {e.reason}\")\n        raise\n    except urllib.error.HTTPError as e:\n        print(f\"HTTP Error: {e.code} {e.reason}\")\n        raise\n    except rsa.pkcs1.VerificationError as e:\n        print(f\"Verification Error: {e.args[0]}\")\n        raise\n    except Exception as e:\n        print(f\"Unexpected Error: {e}\")\n        raise", "raw_solution": "import rsa\nimport urllib.request\nfrom hashlib import sha256\n\ndef task_func(url):\n    try:\n        # Generate RSA public and private keys\n        (pubkey, privkey) = rsa.newkeys(512)\n\n        # Retrieve content from the specified URL\n        response = urllib.request.urlopen(url)\n        content = response.read()\n\n        # Calculate SHA256 hash of the content\n        content_hash = sha256(content).digest()\n\n        # Sign the hash with the private key\n        signature = rsa.sign_hash(content_hash, privkey, 'SHA-256')\n\n        # Convert the signature to hexadecimal\n        signature_hex = signature.hex()\n\n        return pubkey, signature_hex, content_hash\n\n    except urllib.error.URLError as e:\n        print(f\"URL Error: {e.reason}\")\n        raise\n    except urllib.error.HTTPError as e:\n        print(f\"HTTP Error: {e.code} {e.reason}\")\n        raise\n    except rsa.pkcs1.VerificationError as e:\n        print(f\"Verification Error: {e.args[0]}\")\n        raise\n    except Exception as e:\n        print(f\"Unexpected Error: {e}\")\n        raise"}
{"task_id": "BigCodeBench/585", "solution": "import rsa\nimport os\nimport zipfile\nfrom base64 import b64encode\nfrom typing import Tuple\ndef task_func(directory: str) -> Tuple[rsa.PublicKey, str]:\n    \"\"\"\n    Generates RSA public and private keys, encrypts all files in the specified directory using the public key, and saves the encrypted files into a zip file.\n    Returns the public key and the name of the zip file.\n    \"\"\"\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Create a zip file\n    zip_filename = \"encrypted_files.zip\"\n    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n        # Iterate over all files in the directory\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                # Read the file\n                with open(os.path.join(root, file), 'rb') as f:\n                    file_data = f.read()\n\n                # Encrypt the file data with the public key\n                encrypted_data = rsa.encrypt(file_data, pubkey)\n\n                # Write the encrypted data to the zip file\n                zipf.writestr(file, b64encode(encrypted_data).decode())\n\n    return pubkey, zip_filename", "raw_solution": "import rsa\nimport os\nimport zipfile\nfrom base64 import b64encode\nfrom typing import Tuple\n\ndef task_func(directory: str) -> Tuple[rsa.PublicKey, str]:\n    \"\"\"\n    Generates RSA public and private keys, encrypts all files in the specified directory using the public key, and saves the encrypted files into a zip file.\n    Returns the public key and the name of the zip file.\n    \"\"\"\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Create a zip file\n    zip_filename = \"encrypted_files.zip\"\n    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n        # Iterate over all files in the directory\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                # Read the file\n                with open(os.path.join(root, file), 'rb') as f:\n                    file_data = f.read()\n\n                # Encrypt the file data with the public key\n                encrypted_data = rsa.encrypt(file_data, pubkey)\n\n                # Write the encrypted data to the zip file\n                zipf.writestr(file, b64encode(encrypted_data).decode())\n\n    return pubkey, zip_filename"}
{"task_id": "BigCodeBench/586", "solution": "import rsa\nfrom cryptography.fernet import Fernet\nfrom base64 import b64encode\nimport os\ndef generate_keys():\n    (public_key, private_key) = rsa.newkeys(512)\n    return (public_key, private_key)\ndef encrypt_file(file_path, key):\n    fernet = Fernet(key)\n    with open(file_path, 'rb') as f:\n        file_data = f.read()\n    encrypted_data = fernet.encrypt(file_data)\n    return encrypted_data\ndef encrypt_key(key, public_key):\n    encrypted_key = rsa.encrypt(key, public_key)\n    return encrypted_key\ndef save_to_file(data, filename):\n    with open(filename, 'wb') as f:\n        f.write(data)\ndef task_func(file_path):\n    (public_key, private_key) = generate_keys()\n    key = Fernet.generate_key()\n    encrypted_data = encrypt_file(file_path, key)\n    encrypted_key = encrypt_key(key, public_key)\n    filename = os.path.splitext(file_path)[0]\n    encrypted_file_path = filename + \"_encrypted.bin\"\n    encrypted_key_path = filename + \"_key_encrypted.bin\"\n    save_to_file(encrypted_data, encrypted_file_path)\n    save_to_file(encrypted_key, encrypted_key_path)\n    return public_key, encrypted_file_path, encrypted_key_path", "raw_solution": "import rsa\nfrom cryptography.fernet import Fernet\nfrom base64 import b64encode\nimport os\n\ndef generate_keys():\n    (public_key, private_key) = rsa.newkeys(512)\n    return (public_key, private_key)\n\ndef encrypt_file(file_path, key):\n    fernet = Fernet(key)\n    with open(file_path, 'rb') as f:\n        file_data = f.read()\n    encrypted_data = fernet.encrypt(file_data)\n    return encrypted_data\n\ndef encrypt_key(key, public_key):\n    encrypted_key = rsa.encrypt(key, public_key)\n    return encrypted_key\n\ndef save_to_file(data, filename):\n    with open(filename, 'wb') as f:\n        f.write(data)\n\ndef task_func(file_path):\n    (public_key, private_key) = generate_keys()\n    key = Fernet.generate_key()\n    encrypted_data = encrypt_file(file_path, key)\n    encrypted_key = encrypt_key(key, public_key)\n    filename = os.path.splitext(file_path)[0]\n    encrypted_file_path = filename + \"_encrypted.bin\"\n    encrypted_key_path = filename + \"_key_encrypted.bin\"\n    save_to_file(encrypted_data, encrypted_file_path)\n    save_to_file(encrypted_key, encrypted_key_path)\n    return public_key, encrypted_file_path, encrypted_key_path"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Read file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # AES encryption\n    aes_key = os.urandom(32)  # 256-bit key\n    cipher = Cipher(algorithms.AES(aes_key), modes.ECB())\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    padded_content = padder.update(file_content) + padder.finalize()\n    encrypted_file_content = encryptor.update(padded_content) + encryptor.finalize()\n\n    # Save encrypted file\n    encrypted_file_path = 'encrypted_file.bin'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_file_content)\n\n    # Encrypt AES key with RSA\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save encrypted AES key\n    encrypted_aes_key_path = 'encrypted_aes_key.bin'\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pubkey, encrypted_file_path, encrypted_aes_key_path", "raw_solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Read file content\n    with open(file_path, 'rb') as f:\n        file_content = f.read()\n\n    # AES encryption\n    aes_key = os.urandom(32)  # 256-bit key\n    cipher = Cipher(algorithms.AES(aes_key), modes.ECB())\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    padded_content = padder.update(file_content) + padder.finalize()\n    encrypted_file_content = encryptor.update(padded_content) + encryptor.finalize()\n\n    # Save encrypted file\n    encrypted_file_path = 'encrypted_file.bin'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(encrypted_file_content)\n\n    # Encrypt AES key with RSA\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save encrypted AES key\n    encrypted_aes_key_path = 'encrypted_aes_key.bin'\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(b64encode(encrypted_aes_key))\n\n    return pubkey, encrypted_file_path, encrypted_aes_key_path"}
{"task_id": "BigCodeBench/588", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nRANGE = 100\nSIZE = 1000\ndef task_func():\n    \"\"\"\n    Generates a DataFrame with two columns, 'X' and 'Y', each filled with random integers within a specified range,\n    and plots these points using a scatter plot. The visualization is created using Seaborn on top of Matplotlib.\n    The function is designed to be parameter-free for simplicity, utilizing constants for configuration.\n    \"\"\"\n    # Generate DataFrame with random integers\n    df = pd.DataFrame({\n        'X': np.random.randint(0, RANGE, SIZE),\n        'Y': np.random.randint(0, RANGE, SIZE)\n    })\n\n    # Plot the DataFrame using Seaborn\n    sns.scatterplot(x='X', y='Y', data=df)\n    plt.show()\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants defining the range of random integers and the size of the DataFrame\nRANGE = 100\nSIZE = 1000\n\ndef task_func():\n    \"\"\"\n    Generates a DataFrame with two columns, 'X' and 'Y', each filled with random integers within a specified range,\n    and plots these points using a scatter plot. The visualization is created using Seaborn on top of Matplotlib.\n    The function is designed to be parameter-free for simplicity, utilizing constants for configuration.\n    \"\"\"\n    # Generate DataFrame with random integers\n    df = pd.DataFrame({\n        'X': np.random.randint(0, RANGE, SIZE),\n        'Y': np.random.randint(0, RANGE, SIZE)\n    })\n\n    # Plot the DataFrame using Seaborn\n    sns.scatterplot(x='X', y='Y', data=df)\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/589", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\nRANGE = 100\nSIZE = 1000\nCLUSTERS = 5\ndef task_func():\n    # Generate random 2D points within the specified range and size\n    data = np.random.rand(SIZE, 2) * RANGE\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=CLUSTERS, random_state=0).fit(data)\n\n    # Plot the clustered points\n    plt.scatter(data[:, 0], data[:, 1], c=kmeans.labels_)\n\n    # Plot the cluster centroids\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n\n    return data, kmeans", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n# Constants for configuration\nRANGE = 100\nSIZE = 1000\nCLUSTERS = 5\n\ndef task_func():\n    # Generate random 2D points within the specified range and size\n    data = np.random.rand(SIZE, 2) * RANGE\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=CLUSTERS, random_state=0).fit(data)\n\n    # Plot the clustered points\n    plt.scatter(data[:, 0], data[:, 1], c=kmeans.labels_)\n\n    # Plot the cluster centroids\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n    plt.show()\n\n    return data, kmeans"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL is invalid or empty\")\n    try:\n        response = urllib.request.urlopen(url)\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"Network connectivity or server issue\") from e\n    html = response.read()\n    doc = pq(html)\n    data = []\n    for a in doc('a'):\n        text = a.text\n        href = a.attrib['href']\n        data.append({'text': text, 'href': href})\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    return df", "raw_solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL is invalid or empty\")\n    try:\n        response = urllib.request.urlopen(url)\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"Network connectivity or server issue\") from e\n    html = response.read()\n    doc = pq(html)\n    data = []\n    for a in doc('a'):\n        text = a.text\n        href = a.attrib['href']\n        data.append({'text': text, 'href': href})\n    df = pd.DataFrame(data)\n    df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    return df"}
{"task_id": "BigCodeBench/591", "solution": "from datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTEMP_CATEGORIES = ['Cold', 'Normal', 'Hot']\nFILE_PATH = 'custom_data.csv'\ndef task_func(hours, file_path=FILE_PATH):\n    \"\"\"\n    Generates temperature data for the specified number of hours, saves it in a CSV file, and plots the data using matplotlib.\n    Data Structure: The function uses a dictionary to manage the generated temperature data with keys: 'Time', 'Temperature', and 'Category'.\n    The function should output with:\n        tuple:\n        str: The path of the generated CSV file.\n        Axes: The plot object for further manipulation or saving.\n    \"\"\"\n    data = {'Time': [], 'Temperature': [], 'Category': []}\n    for i in range(hours):\n        time = datetime.now() + timedelta(hours=i)\n        temp = randint(0, 100)\n        category = TEMP_CATEGORIES[0] if temp < 30 else TEMP_CATEGORIES[1] if temp < 70 else TEMP_CATEGORIES[2]\n        data['Time'].append(time)\n        data['Temperature'].append(temp)\n        data['Category'].append(category)\n\n    df = pd.DataFrame(data)\n    df.to_csv(file_path, index=False)\n\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Temperature'], color='blue')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Temperature')\n    ax.set_title('Temperature over Time')\n\n    return file_path, ax", "raw_solution": "from datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nTEMP_CATEGORIES = ['Cold', 'Normal', 'Hot']\nFILE_PATH = 'custom_data.csv'\n\ndef task_func(hours, file_path=FILE_PATH):\n    \"\"\"\n    Generates temperature data for the specified number of hours, saves it in a CSV file, and plots the data using matplotlib.\n    Data Structure: The function uses a dictionary to manage the generated temperature data with keys: 'Time', 'Temperature', and 'Category'.\n    The function should output with:\n        tuple:\n        str: The path of the generated CSV file.\n        Axes: The plot object for further manipulation or saving.\n    \"\"\"\n    data = {'Time': [], 'Temperature': [], 'Category': []}\n    for i in range(hours):\n        time = datetime.now() + timedelta(hours=i)\n        temp = randint(0, 100)\n        category = TEMP_CATEGORIES[0] if temp < 30 else TEMP_CATEGORIES[1] if temp < 70 else TEMP_CATEGORIES[2]\n        data['Time'].append(time)\n        data['Temperature'].append(temp)\n        data['Category'].append(category)\n\n    df = pd.DataFrame(data)\n    df.to_csv(file_path, index=False)\n\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Temperature'], color='blue')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Temperature')\n    ax.set_title('Temperature over Time')\n\n    return file_path, ax"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates sensor data for the specified number of hours and saves it in a CSV file.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Define the filename\n    filename = os.path.join(output_dir, 'sensor_data.csv')\n\n    # Open the file in write mode\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write the header\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n\n        # Generate data for the specified number of hours\n        start_time = datetime.now()\n        for _ in range(hours):\n            # Generate random data for each sensor\n            data = [str(randint(0, 100)) for _ in SENSORS]\n\n            # Generate the timestamp for the current hour\n            timestamp = (start_time + timedelta(hours=1)).strftime('%Y-%m-%d %H:%M:%S')\n\n            # Write the data to the file\n            writer.writerow([timestamp] + data)\n\n            # Move to the next hour\n            start_time += timedelta(hours=1)\n\n    print(f'Sensor data generated and saved to {filename}')", "raw_solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates sensor data for the specified number of hours and saves it in a CSV file.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Define the filename\n    filename = os.path.join(output_dir, 'sensor_data.csv')\n\n    # Open the file in write mode\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write the header\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n\n        # Generate data for the specified number of hours\n        start_time = datetime.now()\n        for _ in range(hours):\n            # Generate random data for each sensor\n            data = [str(randint(0, 100)) for _ in SENSORS]\n\n            # Generate the timestamp for the current hour\n            timestamp = (start_time + timedelta(hours=1)).strftime('%Y-%m-%d %H:%M:%S')\n\n            # Write the data to the file\n            writer.writerow([timestamp] + data)\n\n            # Move to the next hour\n            start_time += timedelta(hours=1)\n\n    print(f'Sensor data generated and saved to {filename}')\n\n# Test the function\ntask_func(5)"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate traffic data\n    traffic_data = []\n    current_time = datetime.now()\n    for i in range(hours):\n        traffic_counts = {vehicle_type: randint(0, 100) for vehicle_type in VEHICLE_TYPES}\n        traffic_data.append({'Time': current_time, **traffic_counts})\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV file\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=['Time', *VEHICLE_TYPES])\n        writer.writeheader()\n        for data in traffic_data:\n            writer.writerow(data)\n\n    # Load data from CSV file and plot\n    df = pd.read_csv(csv_file_path)\n    df.set_index('Time', inplace=True)\n    ax = df.plot(kind='line', figsize=(10, 6))\n    plt.title('Traffic Data Over Time')\n    plt.xlabel('Time')\n    plt.ylabel('Vehicle Count')\n    plt.legend(loc='upper left')\n    plt.tight_layout()\n\n    return csv_file_path, ax.figure", "raw_solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate traffic data\n    traffic_data = []\n    current_time = datetime.now()\n    for i in range(hours):\n        traffic_counts = {vehicle_type: randint(0, 100) for vehicle_type in VEHICLE_TYPES}\n        traffic_data.append({'Time': current_time, **traffic_counts})\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV file\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=['Time', *VEHICLE_TYPES])\n        writer.writeheader()\n        for data in traffic_data:\n            writer.writerow(data)\n\n    # Load data from CSV file and plot\n    df = pd.read_csv(csv_file_path)\n    df.set_index('Time', inplace=True)\n    ax = df.plot(kind='line', figsize=(10, 6))\n    plt.title('Traffic Data Over Time')\n    plt.xlabel('Time')\n    plt.ylabel('Vehicle Count')\n    plt.legend(loc='upper left')\n    plt.tight_layout()\n\n    return csv_file_path, ax.figure"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate weather data\n    weather_data = []\n    start_time = datetime.now()\n    for i in range(hours):\n        time = (start_time + timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        weather_data.append([time, condition])\n\n    # Save weather data to CSV file\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file to a backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    shutil.copy(csv_file_path, backup_dir)\n\n    return csv_file_path", "raw_solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate weather data\n    weather_data = []\n    start_time = datetime.now()\n    for i in range(hours):\n        time = (start_time + timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS)-1)]\n        weather_data.append([time, condition])\n\n    # Save weather data to CSV file\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    with open(csv_file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file to a backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    shutil.copy(csv_file_path, backup_dir)\n\n    return csv_file_path"}
{"task_id": "BigCodeBench/595", "solution": "import random\nimport bisect\nfrom array import array\ndef task_func(n=10, total=100):\n    \"\"\"\n    Generates 'n' random integers such that their sum equals 'total',\n    sorts these numbers, and determines the position where a new random number\n    can be inserted to maintain the sorted order.\n    \"\"\"\n    # Generate 'n' random integers\n    nums = [random.randint(1, total) for _ in range(n)]\n\n    # Retry mechanism to ensure the generated numbers sum up to 'total'\n    while sum(nums) != total:\n        for i in range(len(nums)):\n            nums[i] = random.randint(1, total)\n\n    # Sort the numbers\n    nums.sort()\n\n    # Determine the position where a new random number can be inserted\n    new_num = random.randint(1, total)\n    pos = bisect.bisect_left(nums, new_num)\n\n    return (nums, pos)", "raw_solution": "import random\nimport bisect\nfrom array import array\n\ndef task_func(n=10, total=100):\n    \"\"\"\n    Generates 'n' random integers such that their sum equals 'total',\n    sorts these numbers, and determines the position where a new random number\n    can be inserted to maintain the sorted order.\n    \"\"\"\n    # Generate 'n' random integers\n    nums = [random.randint(1, total) for _ in range(n)]\n\n    # Retry mechanism to ensure the generated numbers sum up to 'total'\n    while sum(nums) != total:\n        for i in range(len(nums)):\n            nums[i] = random.randint(1, total)\n\n    # Sort the numbers\n    nums.sort()\n\n    # Determine the position where a new random number can be inserted\n    new_num = random.randint(1, total)\n    pos = bisect.bisect_left(nums, new_num)\n\n    return (nums, pos)\n\n# Test the function"}
{"task_id": "BigCodeBench/596", "solution": "import time\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\ndef task_func(duration):\n    start_time = time.time()\n    timestamps = []\n    values = []\n\n    while time.time() - start_time < duration:\n        timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]\n        value = randint(1, 100)\n        timestamps.append(timestamp)\n        values.append(value)\n        time.sleep(0.1)  # sleep for 0.1 seconds to simulate real-time data generation\n\n    return (timestamps, values)", "raw_solution": "import time\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\n\ndef task_func(duration):\n    start_time = time.time()\n    timestamps = []\n    values = []\n\n    while time.time() - start_time < duration:\n        timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]\n        value = randint(1, 100)\n        timestamps.append(timestamp)\n        values.append(value)\n        time.sleep(0.1)  # sleep for 0.1 seconds to simulate real-time data generation\n\n    return (timestamps, values)\n\n# Test the function\ntimestamps, values = task_func(5)\n\n# Plot the data\nplt.plot(timestamps, values)\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Real-time data')\nplt.show()"}
{"task_id": "BigCodeBench/597", "solution": "import pandas as pd\nimport time\ndef task_func(data, letter):\n    \"\"\"\n    Filters rows in a dictionary where the 'Name' column values start with a specified letter.\n    First, convert the dict to a DataFrame and then filter rows in this DataFrame.\n    \"\"\"\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Filter rows where the 'Name' column starts with the specified letter\n    filtered_df = df[df['Name'].str.startswith(letter)]\n\n    # Return the 'Name' column of the filtered DataFrame as a Series\n    return filtered_df['Name']\ndata = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Helen', 'Igor', 'John']}", "raw_solution": "import pandas as pd\nimport time\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef task_func(data, letter):\n    \"\"\"\n    Filters rows in a dictionary where the 'Name' column values start with a specified letter.\n    First, convert the dict to a DataFrame and then filter rows in this DataFrame.\n    \"\"\"\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Filter rows where the 'Name' column starts with the specified letter\n    filtered_df = df[df['Name'].str.startswith(letter)]\n\n    # Return the 'Name' column of the filtered DataFrame as a Series\n    return filtered_df['Name']\n\n# Test the function\ndata = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Helen', 'Igor', 'John']}"}
{"task_id": "BigCodeBench/598", "solution": "import pandas as pd\ndef task_func(df, letter):\n    \"\"\"\n    This function filters rows in a dict of list in which the values of the 'Word' column begin with a specified letter.\n    It first convert the dict to DataFrame, then calculates the length of the words in the filtered column and returns a dictionary of word lengths and their respective counts.\n    \"\"\"\n    # Convert the dict to DataFrame\n    df = pd.DataFrame(df)\n\n    # Filter rows where the 'Word' column starts with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n\n    # Calculate the length of the words in the filtered column\n    word_lengths = filtered_df['Word'].str.len()\n\n    # Return a dictionary of word lengths and their counts\n    return word_lengths.value_counts().to_dict()\ndf = {'Word': ['apple', 'banana', 'cherry', 'date', 'elderberry']}\nletter = 'a'", "raw_solution": "import pandas as pd\n\ndef task_func(df, letter):\n    \"\"\"\n    This function filters rows in a dict of list in which the values of the 'Word' column begin with a specified letter.\n    It first convert the dict to DataFrame, then calculates the length of the words in the filtered column and returns a dictionary of word lengths and their respective counts.\n    \"\"\"\n    # Convert the dict to DataFrame\n    df = pd.DataFrame(df)\n\n    # Filter rows where the 'Word' column starts with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n\n    # Calculate the length of the words in the filtered column\n    word_lengths = filtered_df['Word'].str.len()\n\n    # Return a dictionary of word lengths and their counts\n    return word_lengths.value_counts().to_dict()\n\n# Test the function\ndf = {'Word': ['apple', 'banana', 'cherry', 'date', 'elderberry']}\nletter = 'a'"}
{"task_id": "BigCodeBench/599", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df, letter):\n    \"\"\"\n    This function converts an input dictionary into a DataFrame, filters rows where 'Word' column values start with a specified letter, calculates the lengths of these words, and returns a histogram plot of the word lengths.\n    \"\"\"\n    # Convert the input dictionary into a DataFrame\n    df = pd.DataFrame(df)\n\n    # Filter rows where 'Word' column values start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n\n    # Calculate the lengths of the words\n    filtered_df['Word_Length'] = filtered_df['Word'].str.len()\n\n    # Plot a histogram of the word lengths\n    plt.figure(figsize=(10, 6))\n    sns.histplot(filtered_df['Word_Length'], bins=20, color='skyblue', kde=False)\n    plt.title(f'Histogram of word lengths for words starting with \"{letter}\"')\n    plt.xlabel('Word Length')\n    plt.ylabel('Frequency')\n    plt.show()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df, letter):\n    \"\"\"\n    This function converts an input dictionary into a DataFrame, filters rows where 'Word' column values start with a specified letter, calculates the lengths of these words, and returns a histogram plot of the word lengths.\n    \"\"\"\n    # Convert the input dictionary into a DataFrame\n    df = pd.DataFrame(df)\n\n    # Filter rows where 'Word' column values start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n\n    # Calculate the lengths of the words\n    filtered_df['Word_Length'] = filtered_df['Word'].str.len()\n\n    # Plot a histogram of the word lengths\n    plt.figure(figsize=(10, 6))\n    sns.histplot(filtered_df['Word_Length'], bins=20, color='skyblue', kde=False)\n    plt.title(f'Histogram of word lengths for words starting with \"{letter}\"')\n    plt.xlabel('Word Length')\n    plt.ylabel('Frequency')\n    plt.show()\n\n# Test the function\ntask_func({'Word': ['apple', 'banana', 'cherry', 'date', 'elderberry', 'fig', 'grape', 'honeydew', 'ice-cream', 'jackfruit', 'kiwi', 'lemon', 'mango', 'nectarine', 'orange', 'pineapple', 'quince', 'raspberry', 'strawberry', 'tangerine', 'ugli fruit', 'victoria plum', 'watermelon', 'xigua', 'yellow passionfruit', 'zucchini']}, 'a')"}
{"task_id": "BigCodeBench/600", "solution": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\ndef task_func(df, letter):\n    # Filter rows where 'Word' column values start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the lengths of the words\n    lengths = filtered_df['Word'].str.len()\n    \n    # Calculate mean, median, and mode\n    mean_length = np.mean(lengths)\n    median_length = np.median(lengths)\n    mode_length = stats.mode(lengths)[0][0]\n    \n    # Return as a dictionary\n    return {'mean': mean_length, 'median': median_length, 'mode': mode_length}\ndf = pd.DataFrame({'Word': ['apple', 'banana', 'cherry', 'date', 'elderberry']})", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndef task_func(df, letter):\n    # Filter rows where 'Word' column values start with the specified letter\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    \n    # Calculate the lengths of the words\n    lengths = filtered_df['Word'].str.len()\n    \n    # Calculate mean, median, and mode\n    mean_length = np.mean(lengths)\n    median_length = np.median(lengths)\n    mode_length = stats.mode(lengths)[0][0]\n    \n    # Return as a dictionary\n    return {'mean': mean_length, 'median': median_length, 'mode': mode_length}\n\n# Test the function\ndf = pd.DataFrame({'Word': ['apple', 'banana', 'cherry', 'date', 'elderberry']})"}
{"task_id": "BigCodeBench/601", "solution": "import seaborn as sns\nimport pandas as pd\ndef task_func(df, letter):\n    if 'Word' not in df.columns or df.empty:\n        return None\n\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    filtered_df['WordLength'] = filtered_df['Word'].str.len()\n\n    return sns.boxplot(x='WordLength', data=filtered_df)", "raw_solution": "import seaborn as sns\nimport pandas as pd\n\ndef task_func(df, letter):\n    if 'Word' not in df.columns or df.empty:\n        return None\n\n    filtered_df = df[df['Word'].str.startswith(letter)]\n    filtered_df['WordLength'] = filtered_df['Word'].str.len()\n\n    return sns.boxplot(x='WordLength', data=filtered_df)"}
{"task_id": "BigCodeBench/602", "solution": "import numpy as np\nimport pandas as pd\nimport string\nLETTERS = list(string.ascii_lowercase)\nOUTPUT_DIR = './output'\ndef task_func(file_path, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Creates a CSV file containing a 2D matrix populated exclusively with random lowercase letters.\n    \"\"\"\n    # Generate a 2D matrix of random lowercase letters\n    matrix = np.random.choice(LETTERS, size=(10, 10))\n\n    # Convert the matrix to a DataFrame\n    df = pd.DataFrame(matrix)\n\n    # Write the DataFrame to a CSV file\n    df.to_csv(f'{output_dir}/{file_path}', index=False)", "raw_solution": "import numpy as np\nimport pandas as pd\nimport string\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\nOUTPUT_DIR = './output'\n\ndef task_func(file_path, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Creates a CSV file containing a 2D matrix populated exclusively with random lowercase letters.\n    \"\"\"\n    # Generate a 2D matrix of random lowercase letters\n    matrix = np.random.choice(LETTERS, size=(10, 10))\n\n    # Convert the matrix to a DataFrame\n    df = pd.DataFrame(matrix)\n\n    # Write the DataFrame to a CSV file\n    df.to_csv(f'{output_dir}/{file_path}', index=False)\n\n# Test the function\ntask_func('test.csv')"}
{"task_id": "BigCodeBench/603", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(matrix1, matrix2):\n    \"\"\"\n    Connects two 2D numeric arrays along the second axis (columns), converts them into a Pandas DataFrame, and returns a string representation of the DataFrame.\n    \"\"\"\n    # Convert the 2D arrays into a DataFrame\n    df1 = pd.DataFrame(matrix1)\n    df2 = pd.DataFrame(matrix2)\n\n    # Concatenate the DataFrames along the second axis (columns)\n    df = pd.concat([df1, df2], axis=1)\n\n    # Return the string representation of the DataFrame without the index and header\n    return df.to_string(header=False, index=False)\nmatrix1 = np.array([[1, 2, 3], [4, 5, 6]])\nmatrix2 = np.array([[7, 8, 9], [10, 11, 12]])", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(matrix1, matrix2):\n    \"\"\"\n    Connects two 2D numeric arrays along the second axis (columns), converts them into a Pandas DataFrame, and returns a string representation of the DataFrame.\n    \"\"\"\n    # Convert the 2D arrays into a DataFrame\n    df1 = pd.DataFrame(matrix1)\n    df2 = pd.DataFrame(matrix2)\n\n    # Concatenate the DataFrames along the second axis (columns)\n    df = pd.concat([df1, df2], axis=1)\n\n    # Return the string representation of the DataFrame without the index and header\n    return df.to_string(header=False, index=False)\n\n# Test the function\nmatrix1 = np.array([[1, 2, 3], [4, 5, 6]])\nmatrix2 = np.array([[7, 8, 9], [10, 11, 12]])"}
{"task_id": "BigCodeBench/604", "solution": "import subprocess\nimport logging\nimport os\ndef task_func(filepath):\n    \"\"\"\n    Attempts to compile a C++ file specified by 'filepath'.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n\n    # Check if the compiler is available\n    try:\n        subprocess.run(['g++', '--version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n    except subprocess.CalledProcessError:\n        raise FileNotFoundError(\"The compiler 'g++' is not found.\")\n\n    # Compile the file\n    try:\n        compile_output = subprocess.run(['g++', '-o', 'output', filepath], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n    except subprocess.CalledProcessError as e:\n        logging.error(f\"Compilation failed with error: {str(e)}\")\n        raise\n    else:\n        logging.info(\"Compilation was successful.\")\n\n    return None", "raw_solution": "import subprocess\nimport logging\nimport os\n\ndef task_func(filepath):\n    \"\"\"\n    Attempts to compile a C++ file specified by 'filepath'.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n\n    # Check if the compiler is available\n    try:\n        subprocess.run(['g++', '--version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n    except subprocess.CalledProcessError:\n        raise FileNotFoundError(\"The compiler 'g++' is not found.\")\n\n    # Compile the file\n    try:\n        compile_output = subprocess.run(['g++', '-o', 'output', filepath], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n    except subprocess.CalledProcessError as e:\n        logging.error(f\"Compilation failed with error: {str(e)}\")\n        raise\n    else:\n        logging.info(\"Compilation was successful.\")\n\n    return None"}
{"task_id": "BigCodeBench/605", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(matrix):\n    \"\"\"\n    Visualizes a 2D numeric array (matrix) as a heatmap using matplotlib, specifying a cmap for the color mapping and interpolation to control the pixel rendering.\n    \"\"\"\n    # Convert the matrix to a numpy array\n    matrix = np.array(matrix)\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Create the heatmap\n    heatmap = ax.pcolormesh(matrix, cmap='hot', edgecolors='w', linewidths=1)\n\n    # Return the axis\n    return ax", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(matrix):\n    \"\"\"\n    Visualizes a 2D numeric array (matrix) as a heatmap using matplotlib, specifying a cmap for the color mapping and interpolation to control the pixel rendering.\n    \"\"\"\n    # Convert the matrix to a numpy array\n    matrix = np.array(matrix)\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Create the heatmap\n    heatmap = ax.pcolormesh(matrix, cmap='hot', edgecolors='w', linewidths=1)\n\n    # Return the axis\n    return ax"}
{"task_id": "BigCodeBench/606", "solution": "import pandas as pd\nfrom scipy import stats\ndef task_func(matrix):\n    \"\"\"\n    Normalizes a 2D numeric array (matrix) using the Z score.\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    df = (df - df.mean()) / df.std()\n    return df\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]", "raw_solution": "import pandas as pd\nfrom scipy import stats\n\ndef task_func(matrix):\n    \"\"\"\n    Normalizes a 2D numeric array (matrix) using the Z score.\n    \"\"\"\n    df = pd.DataFrame(matrix)\n    df = (df - df.mean()) / df.std()\n    return df\n\n# Test the function\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/607", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    \"\"\"\n    Removes rows from a DataFrame based on column values and generates random scatter plots.\n\n    Parameters:\n    df (pd.DataFrame): The DataFrame to remove rows from.\n    tuples (list): A list of tuples, where each tuple contains a column name and a value.\n                   Rows with values in the specified columns that match the specified values will be removed.\n    n_plots (int): The number of scatter plots to generate.\n\n    Returns:\n    tuple: A tuple containing:\n        pd.DataFrame: The DataFrame after removal of specified rows.\n        list: A list containing matplotlib Axes objects of the generated plots.\n    \"\"\"\n    # Remove rows based on column values\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Generate scatter plots\n    axes = []\n    for _ in range(n_plots):\n        # Select random columns\n        cols = sample(COLUMNS, 2)\n        # Create scatter plot\n        ax = df.plot(kind='scatter', x=cols[0], y=cols[1], alpha=0.5)\n        axes.append(ax)\n\n    return df, axes", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants for column names to use in plots\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    \"\"\"\n    Removes rows from a DataFrame based on column values and generates random scatter plots.\n\n    Parameters:\n    df (pd.DataFrame): The DataFrame to remove rows from.\n    tuples (list): A list of tuples, where each tuple contains a column name and a value.\n                   Rows with values in the specified columns that match the specified values will be removed.\n    n_plots (int): The number of scatter plots to generate.\n\n    Returns:\n    tuple: A tuple containing:\n        pd.DataFrame: The DataFrame after removal of specified rows.\n        list: A list containing matplotlib Axes objects of the generated plots.\n    \"\"\"\n    # Remove rows based on column values\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Generate scatter plots\n    axes = []\n    for _ in range(n_plots):\n        # Select random columns\n        cols = sample(COLUMNS, 2)\n        # Create scatter plot\n        ax = df.plot(kind='scatter', x=cols[0], y=cols[1], alpha=0.5)\n        axes.append(ax)\n\n    return df, axes"}
{"task_id": "BigCodeBench/608", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom random import sample\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df, tuples, n_plots):\n    \"\"\"\n    Removes rows from a dataframe based on values of multiple columns, and then \n    creates n random pairs of two columns against each other to generate pairplots.\n    \"\"\"\n    # Remove rows based on tuples\n    for col, val in tuples:\n        df = df[~df[col].isin(val)]\n\n    # Generate pairplots\n    pairplot_axes = []\n    for _ in range(n_plots):\n        pairplot_cols = sample(COLUMNS, 2)\n        g = sns.pairplot(df, vars=pairplot_cols)\n        pairplot_axes.append(g)\n\n    return df, pairplot_axes", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df, tuples, n_plots):\n    \"\"\"\n    Removes rows from a dataframe based on values of multiple columns, and then \n    creates n random pairs of two columns against each other to generate pairplots.\n    \"\"\"\n    # Remove rows based on tuples\n    for col, val in tuples:\n        df = df[~df[col].isin(val)]\n\n    # Generate pairplots\n    pairplot_axes = []\n    for _ in range(n_plots):\n        pairplot_cols = sample(COLUMNS, 2)\n        g = sns.pairplot(df, vars=pairplot_cols)\n        pairplot_axes.append(g)\n\n    return df, pairplot_axes"}
{"task_id": "BigCodeBench/609", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\nfrom random import sample\ndef task_func(df, tuples, n_plots):\n    # Remove rows from DataFrame based on list of tuples\n    for tup in tuples:\n        df = df[~df[tup].isin([True])]\n\n    # Generate scatter plots for random combinations of two columns\n    plots = []\n    for cols in sample(list(combinations(df.columns, 2)), min(n_plots, len(df.columns)**2)):\n        plt.figure()\n        plt.scatter(df[cols[0]], df[cols[1]])\n        plt.xlabel(cols[0])\n        plt.ylabel(cols[1])\n        plots.append((cols, plt.gcf()))\n\n    return df, plots", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\nfrom random import sample\n\ndef task_func(df, tuples, n_plots):\n    # Remove rows from DataFrame based on list of tuples\n    for tup in tuples:\n        df = df[~df[tup].isin([True])]\n\n    # Generate scatter plots for random combinations of two columns\n    plots = []\n    for cols in sample(list(combinations(df.columns, 2)), min(n_plots, len(df.columns)**2)):\n        plt.figure()\n        plt.scatter(df[cols[0]], df[cols[1]])\n        plt.xlabel(cols[0])\n        plt.ylabel(cols[1])\n        plots.append((cols, plt.gcf()))\n\n    return df, plots"}
{"task_id": "BigCodeBench/610", "solution": "from random import sample\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    \"\"\"\n    Remove rows from a dataframe based on values of multiple columns, and then create n random joint plots of two columns against each other if the DataFrame is not empty.\n    \"\"\"\n    # Remove rows based on conditions\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Create joint plots\n    joint_plots = []\n    if not df.empty:\n        for _ in range(n_plots):\n            cols = sample(COLUMNS, 2)\n            joint_grid = sns.jointplot(df[cols[0]], df[cols[1]])\n            joint_plots.append(joint_grid)\n\n    return df, joint_plots", "raw_solution": "from random import sample\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df: pd.DataFrame, tuples: list, n_plots: int) -> (pd.DataFrame, list):\n    \"\"\"\n    Remove rows from a dataframe based on values of multiple columns, and then create n random joint plots of two columns against each other if the DataFrame is not empty.\n    \"\"\"\n    # Remove rows based on conditions\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Create joint plots\n    joint_plots = []\n    if not df.empty:\n        for _ in range(n_plots):\n            cols = sample(COLUMNS, 2)\n            joint_grid = sns.jointplot(df[cols[0]], df[cols[1]])\n            joint_plots.append(joint_grid)\n\n    return df, joint_plots"}
{"task_id": "BigCodeBench/611", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(df, tuples, n_plots):\n    \"\"\"\n    Removes rows from a DataFrame based on values of multiple columns, and then create n random line plots of two columns against each other.\n    \"\"\"\n    # Remove rows based on tuples\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Ensure n_plots is not more than the number of unique pairs of columns\n    n_pairs = len(COLUMNS) * (len(COLUMNS) - 1) // 2\n    n_plots = min(n_plots, n_pairs)\n\n    # Randomly select n_plots unique pairs of columns\n    pairs = sample(list(set(df.columns.tolist())), n_plots)\n\n    # Create line plots\n    plots = []\n    for i in range(n_plots):\n        x_col, y_col = pairs[i]\n        plt.figure()\n        plt.plot(df[x_col], df[y_col])\n        plt.xlabel(x_col)\n        plt.ylabel(y_col)\n        plt.title(f'{x_col} vs {y_col}')\n        plots.append((x_col, y_col))\n\n    return df, plots", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import sample\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(df, tuples, n_plots):\n    \"\"\"\n    Removes rows from a DataFrame based on values of multiple columns, and then create n random line plots of two columns against each other.\n    \"\"\"\n    # Remove rows based on tuples\n    for col, val in tuples:\n        df = df[df[col] != val]\n\n    # Ensure n_plots is not more than the number of unique pairs of columns\n    n_pairs = len(COLUMNS) * (len(COLUMNS) - 1) // 2\n    n_plots = min(n_plots, n_pairs)\n\n    # Randomly select n_plots unique pairs of columns\n    pairs = sample(list(set(df.columns.tolist())), n_plots)\n\n    # Create line plots\n    plots = []\n    for i in range(n_plots):\n        x_col, y_col = pairs[i]\n        plt.figure()\n        plt.plot(df[x_col], df[y_col])\n        plt.xlabel(x_col)\n        plt.ylabel(y_col)\n        plt.title(f'{x_col} vs {y_col}')\n        plots.append((x_col, y_col))\n\n    return df, plots"}
{"task_id": "BigCodeBench/612", "solution": "from random import choice\nimport numpy as np\nimport pandas as pd\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTIES_COSTS = [100, 200, 300, 400, 500]\ndef task_func(goals, penalties, teams=TEAMS, penalties_costs=PENALTIES_COSTS):\n    \"\"\"\n    Generates a performance report DataFrame for teams, detailing goals and penalties.\n    For each team, the function fetches goal and penalty counts, calculates 'Penalties Cost' using a random multiplier from a predefined list, and computes a 'Performance Score' as the non-negative difference between goals and penalties.\n    Return a Dataframe with colomns 'Team', 'Goals', 'Penalties', 'Penalties Cost' and 'Performance Score'.\n    \"\"\"\n    # Create a DataFrame from the input data\n    df = pd.DataFrame({\n        'Team': teams,\n        'Goals': goals,\n        'Penalties': penalties\n    })\n\n    # Calculate 'Penalties Cost' and 'Performance Score'\n    df['Penalties Cost'] = df['Penalties'].apply(lambda x: choice(penalties_costs) * x)\n    df['Performance Score'] = df['Goals'] - df['Penalties']\n\n    # Ensure 'Performance Score' is non-negative\n    df['Performance Score'] = df['Performance Score'].apply(lambda x: max(0, x))\n\n    return df\ngoals = [2, 3, 1, 4, 2]\npenalties = [1, 2, 3, 1, 2]", "raw_solution": "from random import choice\nimport numpy as np\nimport pandas as pd\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTIES_COSTS = [100, 200, 300, 400, 500]\n\ndef task_func(goals, penalties, teams=TEAMS, penalties_costs=PENALTIES_COSTS):\n    \"\"\"\n    Generates a performance report DataFrame for teams, detailing goals and penalties.\n    For each team, the function fetches goal and penalty counts, calculates 'Penalties Cost' using a random multiplier from a predefined list, and computes a 'Performance Score' as the non-negative difference between goals and penalties.\n    Return a Dataframe with colomns 'Team', 'Goals', 'Penalties', 'Penalties Cost' and 'Performance Score'.\n    \"\"\"\n    # Create a DataFrame from the input data\n    df = pd.DataFrame({\n        'Team': teams,\n        'Goals': goals,\n        'Penalties': penalties\n    })\n\n    # Calculate 'Penalties Cost' and 'Performance Score'\n    df['Penalties Cost'] = df['Penalties'].apply(lambda x: choice(penalties_costs) * x)\n    df['Performance Score'] = df['Goals'] - df['Penalties']\n\n    # Ensure 'Performance Score' is non-negative\n    df['Performance Score'] = df['Performance Score'].apply(lambda x: max(0, x))\n\n    return df\n\n# Test the function\ngoals = [2, 3, 1, 4, 2]\npenalties = [1, 2, 3, 1, 2]"}
{"task_id": "BigCodeBench/613", "solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nGOALS_RANGE = (-10, 10)\ndef task_func(goals, penalties):\n    \"\"\"\n    Calculates net scores for teams ('Team A' to 'Team E') by subtracting penalties from goals and clips scores to stay within -10 to 10.\n    Visualizes results with a bar chart showing each team's adjusted scores 'Team' on the x-axis and score values 'Score' on the y-axis.\n    \"\"\"\n    # Check if the inputs are valid\n    assert len(goals) == len(penalties) == len(TEAMS), \"Inputs must have the same length as the number of teams\"\n    assert all(g >= GOALS_RANGE[0] and g <= GOALS_RANGE[1] for g in goals), f\"Goals must be within the range {GOALS_RANGE}\"\n    assert all(p >= 0 for p in penalties), \"Penalties must be non-negative\"\n\n    # Calculate net scores\n    scores = [max(GOALS_RANGE[0], g - p) for g, p in zip(goals, penalties)]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Team': TEAMS,\n        'Score': scores\n    })\n\n    # Visualize the results\n    plt.figure(figsize=(10, 5))\n    plt.bar(df['Team'], df['Score'])\n    plt.xlabel('Team')\n    plt.ylabel('Score')\n    plt.title('Net Scores for Each Team')\n    plt.show()\n\n    return df", "raw_solution": "import pandas as pd\nfrom matplotlib import pyplot as plt\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nGOALS_RANGE = (-10, 10)\n\ndef task_func(goals, penalties):\n    \"\"\"\n    Calculates net scores for teams ('Team A' to 'Team E') by subtracting penalties from goals and clips scores to stay within -10 to 10.\n    Visualizes results with a bar chart showing each team's adjusted scores 'Team' on the x-axis and score values 'Score' on the y-axis.\n    \"\"\"\n    # Check if the inputs are valid\n    assert len(goals) == len(penalties) == len(TEAMS), \"Inputs must have the same length as the number of teams\"\n    assert all(g >= GOALS_RANGE[0] and g <= GOALS_RANGE[1] for g in goals), f\"Goals must be within the range {GOALS_RANGE}\"\n    assert all(p >= 0 for p in penalties), \"Penalties must be non-negative\"\n\n    # Calculate net scores\n    scores = [max(GOALS_RANGE[0], g - p) for g, p in zip(goals, penalties)]\n\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Team': TEAMS,\n        'Score': scores\n    })\n\n    # Visualize the results\n    plt.figure(figsize=(10, 5))\n    plt.bar(df['Team'], df['Score'])\n    plt.xlabel('Team')\n    plt.ylabel('Score')\n    plt.title('Net Scores for Each Team')\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/614", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(goals, penalties):\n    \"\"\"\n    Function to create a DataFrame and pairplot visualization of goals and penalties distribution for the teams.\n    \"\"\"\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Team': ['Team ' + str(i) for i in range(1, len(goals) + 1)],\n        'Goals': goals,\n        'Penalties': penalties\n    })\n\n    # Create a pairplot\n    pairplot = sns.pairplot(df, hue='Team')\n\n    return df, pairplot\ngoals = [2, 3, 1, 5, 4]\npenalties = [1, 2, 3, 1, 2]", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(goals, penalties):\n    \"\"\"\n    Function to create a DataFrame and pairplot visualization of goals and penalties distribution for the teams.\n    \"\"\"\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Team': ['Team ' + str(i) for i in range(1, len(goals) + 1)],\n        'Goals': goals,\n        'Penalties': penalties\n    })\n\n    # Create a pairplot\n    pairplot = sns.pairplot(df, hue='Team')\n\n    return df, pairplot\n\n# Test the function\ngoals = [2, 3, 1, 5, 4]\npenalties = [1, 2, 3, 1, 2]\ndf, pairplot = task_func(goals, penalties)"}
{"task_id": "BigCodeBench/615", "solution": "from random import randint, seed\nimport pandas as pd\ndef task_func(teams, goals, penalties, penalty_cost, rng_seed=None):\n    \"\"\"\n    Generates a Pandas DataFrame with columns 'Team' and 'Match Result' of the results of football matches for multiple teams,\n    incorporating random goals and penalties. Penalties are converted into fines using a predefined cost.\n    \"\"\"\n    # Seed the random number generator if a seed is provided\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Initialize an empty list to store the match results\n    match_results = []\n\n    # Iterate over each team\n    for team in teams:\n        # Generate random goals and penalties\n        team_goals = sum(randint(0, goals) for _ in range(10))  # 10 matches\n        team_penalties = sum(randint(0, penalties) for _ in range(10))  # 10 matches\n\n        # Calculate the team's total fines\n        team_fines = team_penalties * penalty_cost\n\n        # Calculate the team's total match result\n        team_result = team_goals - team_fines\n\n        # Append the team's match result to the list\n        match_results.append({'Team': team, 'Match Result': team_result})\n\n    # Create a DataFrame from the match results\n    df = pd.DataFrame(match_results)\n\n    return df\nteams = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\ngoals = 5\npenalties = 3\npenalty_cost = 2\nrng_seed = 42", "raw_solution": "from random import randint, seed\nimport pandas as pd\n\n# Method\ndef task_func(teams, goals, penalties, penalty_cost, rng_seed=None):\n    \"\"\"\n    Generates a Pandas DataFrame with columns 'Team' and 'Match Result' of the results of football matches for multiple teams,\n    incorporating random goals and penalties. Penalties are converted into fines using a predefined cost.\n    \"\"\"\n    # Seed the random number generator if a seed is provided\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Initialize an empty list to store the match results\n    match_results = []\n\n    # Iterate over each team\n    for team in teams:\n        # Generate random goals and penalties\n        team_goals = sum(randint(0, goals) for _ in range(10))  # 10 matches\n        team_penalties = sum(randint(0, penalties) for _ in range(10))  # 10 matches\n\n        # Calculate the team's total fines\n        team_fines = team_penalties * penalty_cost\n\n        # Calculate the team's total match result\n        team_result = team_goals - team_fines\n\n        # Append the team's match result to the list\n        match_results.append({'Team': team, 'Match Result': team_result})\n\n    # Create a DataFrame from the match results\n    df = pd.DataFrame(match_results)\n\n    return df\n\n# Test the function\nteams = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\ngoals = 5\npenalties = 3\npenalty_cost = 2\nrng_seed = 42\n\ndf = task_func(teams, goals, penalties, penalty_cost, rng_seed)"}
{"task_id": "BigCodeBench/616", "solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, teams=TEAMS, penalty_cost=PENALTY_COST, rng_seed=None):\n    \"\"\"\n    Generates a DataFrame with the football match results of teams, their goals, and penalty costs.\n    Also, creates a bar plot of the results. Penalties are converted into fines according to the penalty costs.\n    \"\"\"\n    # Set the random seed for reproducibility\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Generate random goals and penalties for each team\n    data = {\n        'Team': teams,\n        'Goals': [randint(0, goals) for _ in teams],\n        'Penalty Cost': [randint(0, penalties) for _ in teams],\n    }\n\n    # Convert penalty costs to fines\n    data['Fines'] = [cost * penalty_cost for cost in data['Penalty Cost']]\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n\n    # Create bar plot\n    fig, ax = plt.subplots()\n    df.plot(x='Team', y=['Goals', 'Fines'], kind='bar', ax=ax)\n    ax.set_xlabel('Team')\n    ax.set_ylabel('Goals and Fines')\n    ax.legend(['Goals', 'Fines'])\n\n    return df, ax", "raw_solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants (they can be overridden with default parameters)\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, teams=TEAMS, penalty_cost=PENALTY_COST, rng_seed=None):\n    \"\"\"\n    Generates a DataFrame with the football match results of teams, their goals, and penalty costs.\n    Also, creates a bar plot of the results. Penalties are converted into fines according to the penalty costs.\n    \"\"\"\n    # Set the random seed for reproducibility\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Generate random goals and penalties for each team\n    data = {\n        'Team': teams,\n        'Goals': [randint(0, goals) for _ in teams],\n        'Penalty Cost': [randint(0, penalties) for _ in teams],\n    }\n\n    # Convert penalty costs to fines\n    data['Fines'] = [cost * penalty_cost for cost in data['Penalty Cost']]\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n\n    # Create bar plot\n    fig, ax = plt.subplots()\n    df.plot(x='Team', y=['Goals', 'Fines'], kind='bar', ax=ax)\n    ax.set_xlabel('Team')\n    ax.set_ylabel('Goals and Fines')\n    ax.legend(['Goals', 'Fines'])\n\n    return df, ax"}
{"task_id": "BigCodeBench/617", "solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, rng_seed=None, teams=TEAMS):\n    \"\"\"\n    Generates a pandas DataFrame of football match results for multiple teams,\n    incorporating random goals and penalties, then visualizes the analyzed data.\n    \"\"\"\n    # Set the random seed for reproducibility\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Generate random goals and penalties for each team\n    data = {\n        'Team': teams,\n        'Goals': [randint(0, goals) for _ in teams],\n        'Penalties': [randint(0, penalties) for _ in teams],\n    }\n\n    # Calculate penalty cost\n    data['Penalty Cost'] = data['Penalties'] * PENALTY_COST\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n\n    # Visualize data\n    df.plot(x='Team', y=['Goals', 'Penalty Cost'], kind='bar', figsize=(12, 8))\n    plt.ylabel('Value')\n    plt.title('Football Match Results')\n    plt.legend(loc='best')\n    plt.show()\n\n    return df", "raw_solution": "from random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport re\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, rng_seed=None, teams=TEAMS):\n    \"\"\"\n    Generates a pandas DataFrame of football match results for multiple teams,\n    incorporating random goals and penalties, then visualizes the analyzed data.\n    \"\"\"\n    # Set the random seed for reproducibility\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Generate random goals and penalties for each team\n    data = {\n        'Team': teams,\n        'Goals': [randint(0, goals) for _ in teams],\n        'Penalties': [randint(0, penalties) for _ in teams],\n    }\n\n    # Calculate penalty cost\n    data['Penalty Cost'] = data['Penalties'] * PENALTY_COST\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n\n    # Visualize data\n    df.plot(x='Team', y=['Goals', 'Penalty Cost'], kind='bar', figsize=(12, 8))\n    plt.ylabel('Value')\n    plt.title('Football Match Results')\n    plt.legend(loc='best')\n    plt.show()\n\n    return df\n\n# Test the function\ndf = task_func(10, 5, rng_seed=42)"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    \"\"\"\n    Generates a DataFrame of football match results for multiple teams, with random goals and penalties.\n    Penalties are converted into fines according to penalty costs.\n    \"\"\"\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Team': [TEAMS[randint(0, len(TEAMS)-1)] for _ in range(goals+penalties)],\n        'Result': ['Goal' if randint(0, 1) == 0 else 'Penalty' for _ in range(goals+penalties)]\n    })\n\n    # Add Goals and Penalty Costs\n    df.loc[df['Result'] == 'Goal', 'Goals'] = 1\n    df.loc[df['Result'] == 'Penalty', 'Penalty Cost'] = PENALTY_COST\n\n    # Calculate total fines\n    df['Fines'] = df['Penalty Cost'].fillna(0)\n\n    # Create plots\n    fig1, ax1 = plt.subplots()\n    sns.countplot(x='Team', data=df, hue='Result', ax=ax1)\n    ax1.set_title('Goals and Penalties by Team')\n\n    fig2, ax2 = plt.subplots()\n    df.groupby('Team')['Fines'].sum().plot(kind='bar', ax=ax2)\n    ax2.set_title('Total Fines by Team')\n\n    # Return DataFrame and plot objects\n    return df, [fig1, fig2]", "raw_solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    \"\"\"\n    Generates a DataFrame of football match results for multiple teams, with random goals and penalties.\n    Penalties are converted into fines according to penalty costs.\n    \"\"\"\n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Team': [TEAMS[randint(0, len(TEAMS)-1)] for _ in range(goals+penalties)],\n        'Result': ['Goal' if randint(0, 1) == 0 else 'Penalty' for _ in range(goals+penalties)]\n    })\n\n    # Add Goals and Penalty Costs\n    df.loc[df['Result'] == 'Goal', 'Goals'] = 1\n    df.loc[df['Result'] == 'Penalty', 'Penalty Cost'] = PENALTY_COST\n\n    # Calculate total fines\n    df['Fines'] = df['Penalty Cost'].fillna(0)\n\n    # Create plots\n    fig1, ax1 = plt.subplots()\n    sns.countplot(x='Team', data=df, hue='Result', ax=ax1)\n    ax1.set_title('Goals and Penalties by Team')\n\n    fig2, ax2 = plt.subplots()\n    df.groupby('Team')['Fines'].sum().plot(kind='bar', ax=ax2)\n    ax2.set_title('Total Fines by Team')\n\n    # Return DataFrame and plot objects\n    return df, [fig1, fig2]"}
{"task_id": "BigCodeBench/619", "solution": "from random import randint, seed\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties, rng_seed=None):\n    \"\"\"\n    Simulates football match results with random goals and penalties for multiple teams,\n    and trains a linear regression model to predict penalty costs from goals.\n    \"\"\"\n    # Set random seed for reproducibility\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Simulate match results\n    data = []\n    for team in TEAMS:\n        for _ in range(goals):\n            data.append({\n                'Team': team,\n                'Goals': randint(0, 5),\n                'Penalty Cost': randint(0, PENALTY_COST)\n            })\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n\n    # Train linear regression model\n    model = LinearRegression()\n    model.fit(df[['Goals']], df['Penalty Cost'])\n\n    return df, model", "raw_solution": "from random import randint, seed\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, rng_seed=None):\n    \"\"\"\n    Simulates football match results with random goals and penalties for multiple teams,\n    and trains a linear regression model to predict penalty costs from goals.\n    \"\"\"\n    # Set random seed for reproducibility\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    # Simulate match results\n    data = []\n    for team in TEAMS:\n        for _ in range(goals):\n            data.append({\n                'Team': team,\n                'Goals': randint(0, 5),\n                'Penalty Cost': randint(0, PENALTY_COST)\n            })\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n\n    # Train linear regression model\n    model = LinearRegression()\n    model.fit(df[['Goals']], df['Penalty Cost'])\n\n    return df, model"}
{"task_id": "BigCodeBench/620", "solution": "import numpy as np\nimport pandas as pd\nRANGE = (1, 100)\ndef task_func(L):\n    \"\"\"\n    Generates a DataFrame filled with random integers. The dimensions of the DataFrame (number of rows and columns) are determined by multiplying pairs of integers from nested lists within the input list of lists 'L'.\n    \"\"\"\n    # Initialize the DataFrame\n    df = pd.DataFrame()\n\n    # Iterate over the nested list\n    for pair in L:\n        # Check if the pair has two elements\n        if len(pair) != 2:\n            raise ValueError(\"Each pair in the list should have exactly two elements.\")\n\n        # Check if the pair elements are integers\n        if not all(isinstance(i, int) for i in pair):\n            raise ValueError(\"Each pair in the list should contain two integers.\")\n\n        # Generate a DataFrame with random integers\n        df = pd.concat([df, pd.DataFrame(np.random.randint(RANGE[0], RANGE[1], (pair[0], pair[1])))], ignore_index=True)\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\n# Constants\nRANGE = (1, 100)\n\ndef task_func(L):\n    \"\"\"\n    Generates a DataFrame filled with random integers. The dimensions of the DataFrame (number of rows and columns) are determined by multiplying pairs of integers from nested lists within the input list of lists 'L'.\n    \"\"\"\n    # Initialize the DataFrame\n    df = pd.DataFrame()\n\n    # Iterate over the nested list\n    for pair in L:\n        # Check if the pair has two elements\n        if len(pair) != 2:\n            raise ValueError(\"Each pair in the list should have exactly two elements.\")\n\n        # Check if the pair elements are integers\n        if not all(isinstance(i, int) for i in pair):\n            raise ValueError(\"Each pair in the list should contain two integers.\")\n\n        # Generate a DataFrame with random integers\n        df = pd.concat([df, pd.DataFrame(np.random.randint(RANGE[0], RANGE[1], (pair[0], pair[1])))], ignore_index=True)\n\n    return df"}
{"task_id": "BigCodeBench/621", "solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    \"\"\"\n    Converts a list of lists 'L' into a single list of integers, standardizes the integers, and plots the standardized values.\n    \"\"\"\n    # Flatten the list of lists\n    flat_list = list(chain(*L))\n\n    # Convert the list to a numpy array\n    arr = np.array(flat_list).reshape(-1, 1)\n\n    # Standardize the values\n    scaler = StandardScaler()\n    arr_standardized = scaler.fit_transform(arr)\n\n    # Plot the standardized values\n    plt.figure(figsize=(10, 6))\n    plt.plot(arr_standardized)\n    plt.title('Standardized Values')\n    plt.xlabel('Index')\n    plt.ylabel('Standardized Value')\n    plt.grid(True)\n    return plt.gca()", "raw_solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    \"\"\"\n    Converts a list of lists 'L' into a single list of integers, standardizes the integers, and plots the standardized values.\n    \"\"\"\n    # Flatten the list of lists\n    flat_list = list(chain(*L))\n\n    # Convert the list to a numpy array\n    arr = np.array(flat_list).reshape(-1, 1)\n\n    # Standardize the values\n    scaler = StandardScaler()\n    arr_standardized = scaler.fit_transform(arr)\n\n    # Plot the standardized values\n    plt.figure(figsize=(10, 6))\n    plt.plot(arr_standardized)\n    plt.title('Standardized Values')\n    plt.xlabel('Index')\n    plt.ylabel('Standardized Value')\n    plt.grid(True)\n    return plt.gca()"}
{"task_id": "BigCodeBench/622", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import chain\ndef task_func(L):\n    # Flatten the list of lists\n    flat_list = list(chain(*L))\n\n    # Fit a normal distribution to the data\n    mu, std = norm.fit(flat_list)\n\n    # Create a range of values for the normal distribution\n    x = np.linspace(min(flat_list), max(flat_list), 100)\n    p = norm.pdf(x, mu, std)\n\n    # Plot the histogram with the fitted normal distribution overlay\n    plt.hist(flat_list, bins=20, density=True, alpha=0.6, color='g')\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.title('Histogram with fitted normal distribution')\n    plt.show()\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import chain\n\ndef task_func(L):\n    # Flatten the list of lists\n    flat_list = list(chain(*L))\n\n    # Fit a normal distribution to the data\n    mu, std = norm.fit(flat_list)\n\n    # Create a range of values for the normal distribution\n    x = np.linspace(min(flat_list), max(flat_list), 100)\n    p = norm.pdf(x, mu, std)\n\n    # Plot the histogram with the fitted normal distribution overlay\n    plt.hist(flat_list, bins=20, density=True, alpha=0.6, color='g')\n    plt.plot(x, p, 'k', linewidth=2)\n    plt.title('Histogram with fitted normal distribution')\n    plt.show()\n\n# Test the function\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ntask_func(L)"}
{"task_id": "BigCodeBench/623", "solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(L):\n    \"\"\"\n    Converts a list of lists into a list of integers, applies KMeans clustering, and returns a scatter plot with data points color-coded by their cluster.\n    \"\"\"\n    # Flatten the list of lists\n    flat_list = list(chain(*L))\n\n    # Convert the list to a numpy array\n    data = np.array(flat_list).reshape(-1, 1)\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(data)\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(data)), data, c=kmeans.labels_)\n\n    return ax", "raw_solution": "from itertools import chain\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    \"\"\"\n    Converts a list of lists into a list of integers, applies KMeans clustering, and returns a scatter plot with data points color-coded by their cluster.\n    \"\"\"\n    # Flatten the list of lists\n    flat_list = list(chain(*L))\n\n    # Convert the list to a numpy array\n    data = np.array(flat_list).reshape(-1, 1)\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(data)\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(data)), data, c=kmeans.labels_)\n\n    return ax"}
{"task_id": "BigCodeBench/624", "solution": "from sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\nN_COMPONENTS = 2\ndef task_func(L):\n    \"\"\"\n    Converts a list of lists 'L' into a 2D numeric array, applies PCA to it and returns the PCA result and scatter plot.\n    \"\"\"\n    # Convert list of lists to numpy array\n    arr = np.array(L)\n\n    # Apply PCA\n    pca = PCA(n_components=N_COMPONENTS)\n    pca_result = pca.fit_transform(arr)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n\n    return pca_result, ax", "raw_solution": "from sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nN_COMPONENTS = 2\n\ndef task_func(L):\n    \"\"\"\n    Converts a list of lists 'L' into a 2D numeric array, applies PCA to it and returns the PCA result and scatter plot.\n    \"\"\"\n    # Convert list of lists to numpy array\n    arr = np.array(L)\n\n    # Apply PCA\n    pca = PCA(n_components=N_COMPONENTS)\n    pca_result = pca.fit_transform(arr)\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(pca_result[:, 0], pca_result[:, 1])\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n\n    return pca_result, ax"}
{"task_id": "BigCodeBench/625", "solution": "import math\nfrom random import randint\nimport pandas as pd\ndef task_func(cities_list):\n    \"\"\"\n    Generates a DataFrame with population data for a list of cities.\n    The population is generated randomly and rounded up to the next thousand.\n    \"\"\"\n    population_list = [math.ceil(randint(1, 100) * 1000) for _ in range(len(cities_list))]\n    df = pd.DataFrame({'City': cities_list, 'Population': population_list})\n    return df\ncities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']", "raw_solution": "import math\nfrom random import randint\nimport pandas as pd\n\ndef task_func(cities_list):\n    \"\"\"\n    Generates a DataFrame with population data for a list of cities.\n    The population is generated randomly and rounded up to the next thousand.\n    \"\"\"\n    population_list = [math.ceil(randint(1, 100) * 1000) for _ in range(len(cities_list))]\n    df = pd.DataFrame({'City': cities_list, 'Population': population_list})\n    return df\n\n# Test the function\ncities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\ndf = task_func(cities)"}
{"task_id": "BigCodeBench/626", "solution": "from random import choice\nimport pytz\nfrom dateutil.parser import parse\nfrom dateutil.relativedelta import relativedelta\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\ndef task_func(date_str, from_tz):\n    \"\"\"\n    Converts a datetime string from a given timezone to a datetime string in a randomly chosen timezone.\n    \"\"\"\n    # Parse the input datetime string to a datetime object\n    dt = parse(date_str)\n\n    # Convert the datetime object to the given timezone\n    from_tz_obj = pytz.timezone(from_tz)\n    dt = from_tz_obj.localize(dt)\n\n    # Randomly choose a timezone\n    to_tz = choice(TIMEZONES)\n    to_tz_obj = pytz.timezone(to_tz)\n\n    # Convert the datetime object to the chosen timezone\n    dt = dt.astimezone(to_tz_obj)\n\n    # Format the datetime object to a string\n    date_str_converted = dt.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n\n    return (date_str_converted, to_tz)\ndate_str = '2022-01-01 12:00:00'\nfrom_tz = 'Asia/Tokyo'", "raw_solution": "from random import choice\nimport pytz\nfrom dateutil.parser import parse\nfrom dateutil.relativedelta import relativedelta\n\n# Constants\nTIMEZONES = ['America/New_York', 'Europe/London', 'Asia/Shanghai', 'Asia/Tokyo', 'Australia/Sydney']\n\ndef task_func(date_str, from_tz):\n    \"\"\"\n    Converts a datetime string from a given timezone to a datetime string in a randomly chosen timezone.\n    \"\"\"\n    # Parse the input datetime string to a datetime object\n    dt = parse(date_str)\n\n    # Convert the datetime object to the given timezone\n    from_tz_obj = pytz.timezone(from_tz)\n    dt = from_tz_obj.localize(dt)\n\n    # Randomly choose a timezone\n    to_tz = choice(TIMEZONES)\n    to_tz_obj = pytz.timezone(to_tz)\n\n    # Convert the datetime object to the chosen timezone\n    dt = dt.astimezone(to_tz_obj)\n\n    # Format the datetime object to a string\n    date_str_converted = dt.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n\n    return (date_str_converted, to_tz)\n\n# Test the function\ndate_str = '2022-01-01 12:00:00'\nfrom_tz = 'Asia/Tokyo'"}
{"task_id": "BigCodeBench/627", "solution": "from random import randint\nfrom statistics import mean\nimport pandas as pd\ndef task_func(products_list):\n    data = {'Product': [], 'Month 1': [], 'Month 2': [], 'Month 3': [], 'Month 4': [], 'Month 5': [], 'Month 6': [], 'Month 7': [], 'Month 8': [], 'Month 9': [], 'Month 10': [], 'Month 11': [], 'Month 12': [], 'Average Sales': []}\n    for product in products_list:\n        sales = [randint(1, 100) for _ in range(12)]\n        data['Product'].append(product)\n        data['Average Sales'].append(mean(sales))\n        for i, sale in enumerate(sales, start=1):\n            data[f'Month {i}'].append(sale)\n    return pd.DataFrame(data)", "raw_solution": "from random import randint\nfrom statistics import mean\nimport pandas as pd\n\ndef task_func(products_list):\n    data = {'Product': [], 'Month 1': [], 'Month 2': [], 'Month 3': [], 'Month 4': [], 'Month 5': [], 'Month 6': [], 'Month 7': [], 'Month 8': [], 'Month 9': [], 'Month 10': [], 'Month 11': [], 'Month 12': [], 'Average Sales': []}\n    for product in products_list:\n        sales = [randint(1, 100) for _ in range(12)]\n        data['Product'].append(product)\n        data['Average Sales'].append(mean(sales))\n        for i, sale in enumerate(sales, start=1):\n            data[f'Month {i}'].append(sale)\n    return pd.DataFrame(data)\n\n# Test the function\nproducts = ['Product A', 'Product B', 'Product C']\ndf = task_func(products)"}
{"task_id": "BigCodeBench/628", "solution": "import math\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np", "raw_solution": "import math\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_sine_wave(frequency=None, amplitude=None, phase_shift=None):\n    \"\"\"\n    Creates a sine wave with random frequency, amplitude and phase shift.\n    \"\"\"\n    if frequency is None:\n        frequency = randint(1, 10)\n    if amplitude is None:\n        amplitude = randint(1, 10)\n    if phase_shift is None:\n        phase_shift = randint(1, 10)\n\n    time = np.arange(0, 10, 0.1)\n    sine_wave = amplitude * np.sin(2 * np.pi * frequency * time + phase_shift)\n\n    fig, ax = plt.subplots()\n    ax.plot(time, sine_wave)\n    ax.set_title('Random Sine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n\n    return ax"}
{"task_id": "BigCodeBench/629", "solution": "import pandas as pd\nimport os\nOUTPUT_DIR = './output'\ndef task_func(datasets, filenames, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Writes multiple Pandas DataFrames to a single CSV file, separating each DataFrame by a line of hyphens (\"------\").\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    with open(os.path.join(output_dir, filenames), 'w') as f:\n        for i, df in enumerate(datasets):\n            df.to_csv(f, index=False)\n            if i < len(datasets) - 1:  # Do not add a separator after the last DataFrame\n                f.write('------\\n')", "raw_solution": "import pandas as pd\nimport os\n\nOUTPUT_DIR = './output'\n\ndef task_func(datasets, filenames, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Writes multiple Pandas DataFrames to a single CSV file, separating each DataFrame by a line of hyphens (\"------\").\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    with open(os.path.join(output_dir, filenames), 'w') as f:\n        for i, df in enumerate(datasets):\n            df.to_csv(f, index=False)\n            if i < len(datasets) - 1:  # Do not add a separator after the last DataFrame\n                f.write('------\\n')\n\n# Test the function\ndf1 = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'c']})\ndf2 = pd.DataFrame({'C': [4, 5, 6], 'D': ['d', 'e', 'f']})\ntask_func([df1, df2], 'test.csv')"}
{"task_id": "BigCodeBench/630", "solution": "import pandas as pd\nimport os\nOUTPUT_DIR = './output'\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Save a Pandas DataFrame to a JSON file in a specified directory.\n    \"\"\"\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Construct the full file path\n    filepath = os.path.join(output_dir, filename)\n\n    # Save the DataFrame to the JSON file\n    df.to_json(filepath, orient='records')\n\n    return filepath", "raw_solution": "import pandas as pd\nimport os\n\nOUTPUT_DIR = './output'\n\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Save a Pandas DataFrame to a JSON file in a specified directory.\n    \"\"\"\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Construct the full file path\n    filepath = os.path.join(output_dir, filename)\n\n    # Save the DataFrame to the JSON file\n    df.to_json(filepath, orient='records')\n\n    return filepath"}
{"task_id": "BigCodeBench/631", "solution": "import pandas as pd\nimport os\nOUTPUT_DIR = './output'\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Saves a Pandas DataFrame to a CSV file in a specified directory.\n    \"\"\"\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Construct the full path to the output file\n    filepath = os.path.join(output_dir, filename)\n\n    # Save the DataFrame to the CSV file\n    df.to_csv(filepath, index=False)\n\n    # Return the absolute path of the saved CSV file\n    return os.path.abspath(filepath)\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': ['a', 'b', 'c']\n})", "raw_solution": "import pandas as pd\nimport os\n\nOUTPUT_DIR = './output'\n\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Saves a Pandas DataFrame to a CSV file in a specified directory.\n    \"\"\"\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Construct the full path to the output file\n    filepath = os.path.join(output_dir, filename)\n\n    # Save the DataFrame to the CSV file\n    df.to_csv(filepath, index=False)\n\n    # Return the absolute path of the saved CSV file\n    return os.path.abspath(filepath)\n\n# Test the function\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': ['a', 'b', 'c']\n})"}
{"task_id": "BigCodeBench/632", "solution": "import pandas as pd\nimport json\nimport os\nOUTPUT_DIR = './output'\ndef task_func(df: pd.DataFrame, filename: str) -> str:\n    \"\"\"\n    Write a Pandas DataFrame into a JSON Lines file and save it in a specified directory.\n    \"\"\"\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    # Construct the full path of the output file\n    file_path = os.path.join(OUTPUT_DIR, filename)\n\n    # Write the DataFrame to a JSON Lines file\n    df.to_json(file_path, orient='records', lines=True)\n\n    return file_path", "raw_solution": "import pandas as pd\nimport json\nimport os\n\nOUTPUT_DIR = './output'\n\ndef task_func(df: pd.DataFrame, filename: str) -> str:\n    \"\"\"\n    Write a Pandas DataFrame into a JSON Lines file and save it in a specified directory.\n    \"\"\"\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    # Construct the full path of the output file\n    file_path = os.path.join(OUTPUT_DIR, filename)\n\n    # Write the DataFrame to a JSON Lines file\n    df.to_json(file_path, orient='records', lines=True)\n\n    return file_path"}
{"task_id": "BigCodeBench/633", "solution": "import re\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom nltk import word_tokenize\ndef task_func(text: str) -> dict:\n    \"\"\"\n    Analyzes a given text string by removing duplicate words and stopwords defined by nltk.corpus,\n    and then returns a frequency distribution of the remaining words.\n    \"\"\"\n    # Define a set of common English stopwords\n    stop_words = set(stopwords.words('english'))\n\n    # Tokenize the input text\n    words = word_tokenize(text)\n\n    # Remove stopwords and duplicate words\n    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n\n    # Create a frequency distribution of the remaining words\n    freq_dist = FreqDist(words)\n\n    return dict(freq_dist)", "raw_solution": "import re\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom nltk import word_tokenize\n\ndef task_func(text: str) -> dict:\n    \"\"\"\n    Analyzes a given text string by removing duplicate words and stopwords defined by nltk.corpus,\n    and then returns a frequency distribution of the remaining words.\n    \"\"\"\n    # Define a set of common English stopwords\n    stop_words = set(stopwords.words('english'))\n\n    # Tokenize the input text\n    words = word_tokenize(text)\n\n    # Remove stopwords and duplicate words\n    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n\n    # Create a frequency distribution of the remaining words\n    freq_dist = FreqDist(words)\n\n    return dict(freq_dist)\n\n# Test the function"}
{"task_id": "BigCodeBench/634", "solution": "import itertools\nfrom typing import Any\nfrom scipy import stats\ndef task_func(input_list: list, repetitions: int) -> Any:\n    \"\"\"\n    Takes a list and a repetition count as input.\n    Flattens the list with multiple repetitions.\n    Calculates the mode of the flattened list.\n    \"\"\"\n    # Flatten the list with multiple repetitions\n    flattened_list = list(itertools.chain.from_iterable([input_list]*repetitions))\n    \n    # Calculate the mode of the flattened list\n    mode_result = stats.mode(flattened_list)\n    \n    return mode_result", "raw_solution": "import itertools\nfrom typing import Any\nfrom scipy import stats\n\ndef task_func(input_list: list, repetitions: int) -> Any:\n    \"\"\"\n    Takes a list and a repetition count as input.\n    Flattens the list with multiple repetitions.\n    Calculates the mode of the flattened list.\n    \"\"\"\n    # Flatten the list with multiple repetitions\n    flattened_list = list(itertools.chain.from_iterable([input_list]*repetitions))\n    \n    # Calculate the mode of the flattened list\n    mode_result = stats.mode(flattened_list)\n    \n    return mode_result"}
{"task_id": "BigCodeBench/635", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\ndef task_func(text, n=2):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', ' ', text).lower()\n\n    # Remove duplicate consecutive words\n    text = re.sub(r'\\b(\\w+\\b\\s+\\b\\1\\b)+\\b', r'\\1', text)\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n\n    # Generate a square co-occurrence matrix of words\n    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n    matrix = vectorizer.fit_transform([text])\n    matrix = matrix.toarray()\n\n    # Create a DataFrame from the matrix\n    df = pd.DataFrame(matrix, columns=vectorizer.get_feature_names())\n\n    # Plot the co-occurrence matrix\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.matshow(df, cmap='viridis')\n    ax.set_xticks(range(len(df.columns)))\n    ax.set_yticks(range(len(df.index)))\n    ax.set_xticklabels(df.columns, rotation=90)\n    ax.set_yticklabels(df.index)\n    plt.show()\n\n    return df, ax", "raw_solution": "# Importing the required libraries\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\n\ndef task_func(text, n=2):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', ' ', text).lower()\n\n    # Remove duplicate consecutive words\n    text = re.sub(r'\\b(\\w+\\b\\s+\\b\\1\\b)+\\b', r'\\1', text)\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n\n    # Generate a square co-occurrence matrix of words\n    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n    matrix = vectorizer.fit_transform([text])\n    matrix = matrix.toarray()\n\n    # Create a DataFrame from the matrix\n    df = pd.DataFrame(matrix, columns=vectorizer.get_feature_names())\n\n    # Plot the co-occurrence matrix\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.matshow(df, cmap='viridis')\n    ax.set_xticks(range(len(df.columns)))\n    ax.set_yticks(range(len(df.index)))\n    ax.set_xticklabels(df.columns, rotation=90)\n    ax.set_yticklabels(df.index)\n    plt.show()\n\n    return df, ax"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    \"\"\"\n    Generates a DataFrame with random integer values between 0 and 9 for a given number of rows.\n    Counts the non-zero values in each column and visualizes this information using a bar plot.\n    \"\"\"\n    # Generate DataFrame with random integer values\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.ne(0).sum()\n\n    # Create bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Non-zero count')\n    ax.set_title('Count of non-zero values in each column')\n\n    return (df, ax)", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    \"\"\"\n    Generates a DataFrame with random integer values between 0 and 9 for a given number of rows.\n    Counts the non-zero values in each column and visualizes this information using a bar plot.\n    \"\"\"\n    # Generate DataFrame with random integer values\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.ne(0).sum()\n\n    # Create bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Non-zero count')\n    ax.set_title('Count of non-zero values in each column')\n\n    return (df, ax)\n\n# Test the function\ndf, ax = task_func(100)\nplt.show()"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students):\n    np.random.seed(0)\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    grades = np.random.randint(50, 100, (num_students, len(courses)))\n    students = [f'Student_{i}' for i in range(1, num_students+1)]\n    df = pd.DataFrame(grades, index=students, columns=courses)\n\n    # Calculate average grade in each course\n    avg_grades = df.mean()\n\n    # Calculate number of students with passing grade (>=60)\n    passing_grades = df[df >= 60].count()\n\n    # Create DataFrame for visualization\n    df_vis = pd.DataFrame({'Average Grade': avg_grades, 'Passing Grade Count': passing_grades})\n\n    # Plotting\n    fig, ax = plt.subplots()\n    df_vis.plot(kind='bar', ax=ax)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.legend(loc='upper left')\n\n    return df_vis, ax", "raw_solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students):\n    np.random.seed(0)\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    grades = np.random.randint(50, 100, (num_students, len(courses)))\n    students = [f'Student_{i}' for i in range(1, num_students+1)]\n    df = pd.DataFrame(grades, index=students, columns=courses)\n\n    # Calculate average grade in each course\n    avg_grades = df.mean()\n\n    # Calculate number of students with passing grade (>=60)\n    passing_grades = df[df >= 60].count()\n\n    # Create DataFrame for visualization\n    df_vis = pd.DataFrame({'Average Grade': avg_grades, 'Passing Grade Count': passing_grades})\n\n    # Plotting\n    fig, ax = plt.subplots()\n    df_vis.plot(kind='bar', ax=ax)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.legend(loc='upper left')\n\n    return df_vis, ax"}
{"task_id": "BigCodeBench/638", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(num_teams=5, num_games=100):\n    teams = [f'Team_{i}' for i in range(1, num_teams+1)]\n    games = [f'Game_{i}' for i in range(1, num_games+1)]\n    scores = np.random.randint(0, 100, size=(num_teams, num_games))\n    df = pd.DataFrame(scores, index=teams, columns=games)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(num_teams=5, num_games=100):\n    teams = [f'Team_{i}' for i in range(1, num_teams+1)]\n    games = [f'Game_{i}' for i in range(1, num_games+1)]\n    scores = np.random.randint(0, 100, size=(num_teams, num_games))\n    df = pd.DataFrame(scores, index=teams, columns=games)\n    return df\n\n# Test the function\ndf = task_func(5, 100)"}
{"task_id": "BigCodeBench/639", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(num_samples=100, num_features=5):\n    \"\"\"\n    Generates a DataFrame with random values, calculates the correlation between the features,\n    and visualizes this information using a heatmap.\n    \"\"\"\n    # Generate a DataFrame with random values\n    np.random.seed(0)\n    df = pd.DataFrame(np.random.randn(num_samples, num_features),\n                      columns=[f'Feature_{i}' for i in range(1, num_features + 1)])\n\n    # Calculate the correlation matrix\n    corr = df.corr()\n\n    # Create a heatmap\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, fmt=\".2f\", linewidths=.5, cmap='coolwarm', ax=ax)\n    plt.show()\n\n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(num_samples=100, num_features=5):\n    \"\"\"\n    Generates a DataFrame with random values, calculates the correlation between the features,\n    and visualizes this information using a heatmap.\n    \"\"\"\n    # Generate a DataFrame with random values\n    np.random.seed(0)\n    df = pd.DataFrame(np.random.randn(num_samples, num_features),\n                      columns=[f'Feature_{i}' for i in range(1, num_features + 1)])\n\n    # Calculate the correlation matrix\n    corr = df.corr()\n\n    # Create a heatmap\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, fmt=\".2f\", linewidths=.5, cmap='coolwarm', ax=ax)\n    plt.show()\n\n    return df, ax\n\n# Test the function\ndf, ax = task_func(num_samples=100, num_features=5)\nassert isinstance(df, pd.DataFrame), \"The function should return a DataFrame\"\nassert isinstance(ax, matplotlib.axes._subplots.Axes), \"The function should return a matplotlib Axes object\""}
{"task_id": "BigCodeBench/640", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nPRODUCTS = ['Product' + str(i) for i in range(1, 6)]\nMONTHS = ['Month' + str(i) for i in range(1, 13)]\ndef task_func():\n    # Generate random sales figures\n    np.random.seed(0)\n    sales_data = np.random.randint(100, 500, size=(12, 5))\n\n    # Create DataFrame\n    df = pd.DataFrame(sales_data, columns=PRODUCTS, index=MONTHS)\n\n    # Plot total sales per product\n    df.sum().plot(kind='line', title='Total Sales per Product')\n    plt.ylabel('Sales')\n    plt.show()\n\n    # Plot heatmap\n    sns.heatmap(df, annot=True, cmap='YlGnBu', fmt='d')\n    plt.title('Sales Heatmap')\n    plt.show()\n\n    return df", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nPRODUCTS = ['Product' + str(i) for i in range(1, 6)]\nMONTHS = ['Month' + str(i) for i in range(1, 13)]\n\ndef task_func():\n    # Generate random sales figures\n    np.random.seed(0)\n    sales_data = np.random.randint(100, 500, size=(12, 5))\n\n    # Create DataFrame\n    df = pd.DataFrame(sales_data, columns=PRODUCTS, index=MONTHS)\n\n    # Plot total sales per product\n    df.sum().plot(kind='line', title='Total Sales per Product')\n    plt.ylabel('Sales')\n    plt.show()\n\n    # Plot heatmap\n    sns.heatmap(df, annot=True, cmap='YlGnBu', fmt='d')\n    plt.title('Sales Heatmap')\n    plt.show()\n\n    return df\n\n# Test the function\ndf = task_func()"}
{"task_id": "BigCodeBench/641", "solution": "import os\nimport re\nimport pandas as pd\nfrom typing import List\ndef task_func(pattern: str, directory: str, output_csv: str) -> pd.DataFrame:\n    \"\"\"\n    Searches for files in the specified directory that match a given regex pattern.\n    This function walks through the directory, matches filenames against the pattern,\n    and saves the matched file paths to a CSV file. It returns a DataFrame of these\n    paths with column 'File Path'.\n    \"\"\"\n    matched_files = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(pattern, file):\n                matched_files.append(os.path.join(root, file))\n\n    df = pd.DataFrame(matched_files, columns=['File Path'])\n    df.to_csv(output_csv, index=False)\n    return df", "raw_solution": "import os\nimport re\nimport pandas as pd\nfrom typing import List\n\ndef task_func(pattern: str, directory: str, output_csv: str) -> pd.DataFrame:\n    \"\"\"\n    Searches for files in the specified directory that match a given regex pattern.\n    This function walks through the directory, matches filenames against the pattern,\n    and saves the matched file paths to a CSV file. It returns a DataFrame of these\n    paths with column 'File Path'.\n    \"\"\"\n    matched_files = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(pattern, file):\n                matched_files.append(os.path.join(root, file))\n\n    df = pd.DataFrame(matched_files, columns=['File Path'])\n    df.to_csv(output_csv, index=False)\n    return df"}
{"task_id": "BigCodeBench/642", "solution": "import os\nimport re\nimport hashlib\ndef task_func(directory: str, pattern: str = r\"(?<!Distillr)\\\\AcroTray\\.exe\") -> dict:\n    \"\"\"\n    Searches for files within the specified directory matching a given regex pattern and computes a SHA256 hash of each file's content.\n    \"\"\"\n    # Initialize an empty dictionary to store file paths and their SHA256 hashes\n    file_hashes = {}\n\n    # Walk through the directory and its subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file matches the regex pattern\n            if re.search(pattern, file):\n                # Compute the SHA256 hash of the file's content\n                with open(os.path.join(root, file), 'rb') as f:\n                    file_hash = hashlib.sha256(f.read()).hexdigest()\n                # Store the file path and its hash in the dictionary\n                file_hashes[os.path.join(root, file)] = file_hash\n\n    return file_hashes", "raw_solution": "import os\nimport re\nimport hashlib\n\ndef task_func(directory: str, pattern: str = r\"(?<!Distillr)\\\\AcroTray\\.exe\") -> dict:\n    \"\"\"\n    Searches for files within the specified directory matching a given regex pattern and computes a SHA256 hash of each file's content.\n    \"\"\"\n    # Initialize an empty dictionary to store file paths and their SHA256 hashes\n    file_hashes = {}\n\n    # Walk through the directory and its subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file matches the regex pattern\n            if re.search(pattern, file):\n                # Compute the SHA256 hash of the file's content\n                with open(os.path.join(root, file), 'rb') as f:\n                    file_hash = hashlib.sha256(f.read()).hexdigest()\n                # Store the file path and its hash in the dictionary\n                file_hashes[os.path.join(root, file)] = file_hash\n\n    return file_hashes"}
{"task_id": "BigCodeBench/643", "solution": "import re\nimport pandas as pd\nimport numpy as np\nDATA_PATTERN = r'>\\d+\\.\\d+<'\ndef task_func(dataframe, data_pattern=DATA_PATTERN):\n    \"\"\"\n    Extract numeric data from a Pandas DataFrame based on a specific pattern.\n    The function searches each cell for occurrences of the regex pattern '>number<number>' (e.g., '>1.23<') and replaces the cell content with the extracted numeric value. If no match is found, the cell is replaced with NaN.\n    \"\"\"\n    # Apply function to each cell in the DataFrame\n    dataframe = dataframe.applymap(lambda x: np.nan if not re.search(data_pattern, str(x)) else float(re.search(data_pattern, str(x)).group().strip('><')))\n    return dataframe", "raw_solution": "import re\nimport pandas as pd\nimport numpy as np\n\n# Constants\nDATA_PATTERN = r'>\\d+\\.\\d+<'\n\ndef task_func(dataframe, data_pattern=DATA_PATTERN):\n    \"\"\"\n    Extract numeric data from a Pandas DataFrame based on a specific pattern.\n    The function searches each cell for occurrences of the regex pattern '>number<number>' (e.g., '>1.23<') and replaces the cell content with the extracted numeric value. If no match is found, the cell is replaced with NaN.\n    \"\"\"\n    # Apply function to each cell in the DataFrame\n    dataframe = dataframe.applymap(lambda x: np.nan if not re.search(data_pattern, str(x)) else float(re.search(data_pattern, str(x)).group().strip('><')))\n    return dataframe\n\n# Test the function\ndf = pd.DataFrame({\n    'A': ['>1.23<4', '5>6.78<', '>910.11<12', '1314>1516<1718', '1920>2122<'],\n    'B': ['23>24.25<', '26>2728<', '29>3031<', '32>3334<35', '36>3738<'],\n    'C': ['39>4041<', '42>4344<', '45>4647<', '48>4950<', '51>5253<']\n})\n"}
{"task_id": "BigCodeBench/644", "solution": "import os\nimport hashlib\nimport base64\nfrom Crypto.Cipher import AES\nfrom Crypto import Random\ndef task_func(filename, data, password):\n    # Create directory if it doesn't exist\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Encrypt the data\n    encrypted_data = encrypt(data, password)\n\n    # Write the encrypted data to the file\n    with open(filename, 'w') as file:\n        file.write(encrypted_data)\n\n    return encrypted_data\ndef encrypt(data, password):\n    # Generate a salt\n    salt = Random.new().read(AES.block_size)\n\n    # Create a key from the password and the salt\n    key = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, 100000)\n\n    # Create a cipher object\n    cipher = AES.new(key, AES.MODE_CBC)\n\n    # Pad the data\n    padding_length = AES.block_size - len(data) % AES.block_size\n    data += chr(padding_length) * padding_length\n\n    # Encrypt the data\n    encrypted_data = base64.b64encode(cipher.encrypt(data.encode('utf-8')))\n\n    return encrypted_data.decode('utf-8')", "raw_solution": "import os\nimport hashlib\nimport base64\nfrom Crypto.Cipher import AES\nfrom Crypto import Random\n\ndef task_func(filename, data, password):\n    # Create directory if it doesn't exist\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Encrypt the data\n    encrypted_data = encrypt(data, password)\n\n    # Write the encrypted data to the file\n    with open(filename, 'w') as file:\n        file.write(encrypted_data)\n\n    return encrypted_data\n\ndef encrypt(data, password):\n    # Generate a salt\n    salt = Random.new().read(AES.block_size)\n\n    # Create a key from the password and the salt\n    key = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, 100000)\n\n    # Create a cipher object\n    cipher = AES.new(key, AES.MODE_CBC)\n\n    # Pad the data\n    padding_length = AES.block_size - len(data) % AES.block_size\n    data += chr(padding_length) * padding_length\n\n    # Encrypt the data\n    encrypted_data = base64.b64encode(cipher.encrypt(data.encode('utf-8')))\n\n    return encrypted_data.decode('utf-8')\n\n# Test the function"}
{"task_id": "BigCodeBench/645", "solution": "import os\nimport pandas as pd\ndef task_func(filename: str) -> pd.DataFrame:\n    \"\"\"\n    Reads a CSV file into a pandas DataFrame and then deletes the entire contents of the original file.\n    If the CSV file does not exist, it raises a FileNotFoundError.\n    \"\"\"\n    if not os.path.isfile(filename):\n        raise FileNotFoundError(f\"The file {filename} does not exist.\")\n\n    df = pd.read_csv(filename)\n    os.remove(filename)\n\n    return df", "raw_solution": "import os\nimport pandas as pd\n\ndef task_func(filename: str) -> pd.DataFrame:\n    \"\"\"\n    Reads a CSV file into a pandas DataFrame and then deletes the entire contents of the original file.\n    If the CSV file does not exist, it raises a FileNotFoundError.\n    \"\"\"\n    if not os.path.isfile(filename):\n        raise FileNotFoundError(f\"The file {filename} does not exist.\")\n\n    df = pd.read_csv(filename)\n    os.remove(filename)\n\n    return df"}
{"task_id": "BigCodeBench/646", "solution": "import os\nimport pandas as pd\nfrom dateutil.parser import parse\nimport matplotlib.pyplot as plt\nimport datetime\nOUTPUT_DIR = './output'\ndef task_func(csv_path=os.path.join(OUTPUT_DIR, 'data.csv'), date_column='date'):\n    # Read the CSV file\n    df = pd.read_csv(csv_path)\n\n    # Convert the date strings to datetime objects\n    df[date_column] = df[date_column].apply(lambda x: parse(x))\n\n    # Extract the year from the datetime objects\n    df['year'] = df[date_column].apply(lambda x: x.year)\n\n    # Draw a histogram of the year distribution\n    plt.hist(df['year'], bins=10, edgecolor='black')\n    plt.title('Year Distribution')\n    plt.xlabel('Year')\n    plt.ylabel('Frequency')\n\n    return plt.gca()", "raw_solution": "import os\nimport pandas as pd\nfrom dateutil.parser import parse\nimport matplotlib.pyplot as plt\nimport datetime\n\nOUTPUT_DIR = './output'\n\ndef task_func(csv_path=os.path.join(OUTPUT_DIR, 'data.csv'), date_column='date'):\n    # Read the CSV file\n    df = pd.read_csv(csv_path)\n\n    # Convert the date strings to datetime objects\n    df[date_column] = df[date_column].apply(lambda x: parse(x))\n\n    # Extract the year from the datetime objects\n    df['year'] = df[date_column].apply(lambda x: x.year)\n\n    # Draw a histogram of the year distribution\n    plt.hist(df['year'], bins=10, edgecolor='black')\n    plt.title('Year Distribution')\n    plt.xlabel('Year')\n    plt.ylabel('Frequency')\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/647", "solution": "from datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Converts a date string from one time zone to another and returns the time difference in seconds to the current time in the destination time zone.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Convert the datetime object to the source timezone\n    source_tz = pytz.timezone(from_tz)\n    source_date = source_tz.localize(date)\n\n    # Convert the datetime object to the destination timezone\n    dest_tz = pytz.timezone(to_tz)\n    dest_date = source_date.astimezone(dest_tz)\n\n    # Calculate the time difference in seconds\n    time_diff = (dest_date - datetime.now(dest_tz)).total_seconds()\n\n    return int(time_diff)", "raw_solution": "from datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\n\ndef task_func(date_str, from_tz, to_tz):\n    \"\"\"\n    Converts a date string from one time zone to another and returns the time difference in seconds to the current time in the destination time zone.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = parse(date_str)\n\n    # Convert the datetime object to the source timezone\n    source_tz = pytz.timezone(from_tz)\n    source_date = source_tz.localize(date)\n\n    # Convert the datetime object to the destination timezone\n    dest_tz = pytz.timezone(to_tz)\n    dest_date = source_date.astimezone(dest_tz)\n\n    # Calculate the time difference in seconds\n    time_diff = (dest_date - datetime.now(dest_tz)).total_seconds()\n\n    return int(time_diff)\n\n# Test the function"}
{"task_id": "BigCodeBench/648", "solution": "from dateutil.parser import parse\nfrom datetime import timedelta\ndef task_func(date_str):\n    \"\"\"\n    Get the next business day (Mon-Fri) after a certain date string.\n    \"\"\"\n    date = parse(date_str)\n    next_day = date + timedelta(days=1)\n    while next_day.weekday() > 4:  # Mon-Fri are 0-4\n        next_day += timedelta(days=1)\n    return next_day", "raw_solution": "from dateutil.parser import parse\nfrom datetime import timedelta\n\ndef task_func(date_str):\n    \"\"\"\n    Get the next business day (Mon-Fri) after a certain date string.\n    \"\"\"\n    date = parse(date_str)\n    next_day = date + timedelta(days=1)\n    while next_day.weekday() > 4:  # Mon-Fri are 0-4\n        next_day += timedelta(days=1)\n    return next_day\n\n# Test the function"}
{"task_id": "BigCodeBench/649", "solution": "import numpy as np\nimport pandas as pd\nfrom dateutil.parser import parse\ndef task_func(dates_str_list):\n    \"\"\"\n    Analyze the weekday distribution in a list of date strings.\n    \"\"\"\n    # Parse the dates and get weekdays\n    dates = [parse(date_str) for date_str in dates_str_list]\n    weekdays = [date.weekday() for date in dates]\n\n    # Create a Series from the weekdays\n    weekday_series = pd.Series(weekdays)\n\n    # Calculate the distribution of weekdays\n    distribution = weekday_series.value_counts().sort_index()\n\n    # Map the index to weekday names\n    index_to_day = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n    distribution.index = distribution.index.map(index_to_day)\n\n    return distribution", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom dateutil.parser import parse\n\ndef task_func(dates_str_list):\n    \"\"\"\n    Analyze the weekday distribution in a list of date strings.\n    \"\"\"\n    # Parse the dates and get weekdays\n    dates = [parse(date_str) for date_str in dates_str_list]\n    weekdays = [date.weekday() for date in dates]\n\n    # Create a Series from the weekdays\n    weekday_series = pd.Series(weekdays)\n\n    # Calculate the distribution of weekdays\n    distribution = weekday_series.value_counts().sort_index()\n\n    # Map the index to weekday names\n    index_to_day = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n    distribution.index = distribution.index.map(index_to_day)\n\n    return distribution"}
{"task_id": "BigCodeBench/650", "solution": "from datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\ndef task_func(date_str, tz_str):\n    \"\"\"\n    Determines the time in seconds until the next New Year in a certain timezone from a given date string.\n    \"\"\"\n    # Parse the input date string\n    date = parse(date_str)\n\n    # Create a timezone object\n    tz = pytz.timezone(tz_str)\n\n    # Localize the date to the specified timezone\n    date = date.replace(tzinfo=tz)\n\n    # Calculate the time until the next New Year\n    next_year = date.year + 1\n    next_year_date = datetime(next_year, 1, 1, 0, 0, 0, tzinfo=tz)\n    time_until_next_year = (next_year_date - date).total_seconds()\n\n    return int(time_until_next_year)", "raw_solution": "from datetime import datetime\nimport pytz\nfrom dateutil.parser import parse\n\ndef task_func(date_str, tz_str):\n    \"\"\"\n    Determines the time in seconds until the next New Year in a certain timezone from a given date string.\n    \"\"\"\n    # Parse the input date string\n    date = parse(date_str)\n\n    # Create a timezone object\n    tz = pytz.timezone(tz_str)\n\n    # Localize the date to the specified timezone\n    date = date.replace(tzinfo=tz)\n\n    # Calculate the time until the next New Year\n    next_year = date.year + 1\n    next_year_date = datetime(next_year, 1, 1, 0, 0, 0, tzinfo=tz)\n    time_until_next_year = (next_year_date - date).total_seconds()\n\n    return int(time_until_next_year)\n\n# Test the function"}
{"task_id": "BigCodeBench/651", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df, target_value):\n    \"\"\"\n    Converts the input dictionary of lists to a DataFrame, searches for rows with cells equal to the provided target_value,\n    and plots the count of such rows per column.\n    \"\"\"\n    # Convert the input dictionary of lists to a DataFrame\n    df = pd.DataFrame(df)\n\n    # Search for rows with cells equal to the target_value\n    target_rows = df[df == target_value]\n\n    # Count the number of such rows per column\n    counts = target_rows.count()\n\n    # Plot the counts\n    if not df.empty:\n        counts.plot(kind='bar')\n        plt.ylabel('Count')\n        plt.title('Count of Rows with Target Value per Column')\n        ax = plt.gca()\n    else:\n        ax = None\n\n    return counts, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_value):\n    \"\"\"\n    Converts the input dictionary of lists to a DataFrame, searches for rows with cells equal to the provided target_value,\n    and plots the count of such rows per column.\n    \"\"\"\n    # Convert the input dictionary of lists to a DataFrame\n    df = pd.DataFrame(df)\n\n    # Search for rows with cells equal to the target_value\n    target_rows = df[df == target_value]\n\n    # Count the number of such rows per column\n    counts = target_rows.count()\n\n    # Plot the counts\n    if not df.empty:\n        counts.plot(kind='bar')\n        plt.ylabel('Count')\n        plt.title('Count of Rows with Target Value per Column')\n        ax = plt.gca()\n    else:\n        ax = None\n\n    return counts, ax"}
{"task_id": "BigCodeBench/652", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nTARGET_VALUE = '332'\nARRAY = np.array([['0', '1', '2'], ['a', 'bb', 'ccc'], ['332', '33', '2'], ['33', '22', '332']])\ndef task_func(target_value=TARGET_VALUE, array=ARRAY):\n    # Find the row indices where the first cell matches target_value\n    indices = np.where(array[:, 0] == target_value)[0]\n\n    if indices.size == 0:\n        # If no target value found, return 'N/A' for all stats\n        return 'N/A', 'N/A', 'N/A', 'N/A'\n    else:\n        # Perform statistical analysis on the indices\n        mean = np.mean(indices)\n        variance = np.var(indices)\n        skewness = stats.skew(indices)\n        kurtosis = stats.kurtosis(indices)\n\n        # Plot the distribution of the indices\n        plt.hist(indices, bins='auto', alpha=0.75)\n        plt.title('Distribution of Indices')\n        plt.xlabel('Index')\n        plt.ylabel('Frequency')\n        plt.show()\n\n        return mean, variance, skewness, kurtosis", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Constants\nTARGET_VALUE = '332'\nARRAY = np.array([['0', '1', '2'], ['a', 'bb', 'ccc'], ['332', '33', '2'], ['33', '22', '332']])\n\ndef task_func(target_value=TARGET_VALUE, array=ARRAY):\n    # Find the row indices where the first cell matches target_value\n    indices = np.where(array[:, 0] == target_value)[0]\n\n    if indices.size == 0:\n        # If no target value found, return 'N/A' for all stats\n        return 'N/A', 'N/A', 'N/A', 'N/A'\n    else:\n        # Perform statistical analysis on the indices\n        mean = np.mean(indices)\n        variance = np.var(indices)\n        skewness = stats.skew(indices)\n        kurtosis = stats.kurtosis(indices)\n\n        # Plot the distribution of the indices\n        plt.hist(indices, bins='auto', alpha=0.75)\n        plt.title('Distribution of Indices')\n        plt.xlabel('Index')\n        plt.ylabel('Frequency')\n        plt.show()\n\n        return mean, variance, skewness, kurtosis\n\n# Test the function"}
{"task_id": "BigCodeBench/653", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(dataframe, target_value='332'):\n    \"\"\"\n    Searches a given DataFrame for occurrences of a specified target value and visualizes these occurrences using a heatmap.\n    \"\"\"\n    # Create a Boolean DataFrame indicating the presence of the target value\n    presence_df = dataframe == target_value\n\n    # Create a heatmap using seaborn\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(presence_df, cmap='coolwarm', cbar=False)\n\n    return (presence_df, ax.get_figure())", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(dataframe, target_value='332'):\n    \"\"\"\n    Searches a given DataFrame for occurrences of a specified target value and visualizes these occurrences using a heatmap.\n    \"\"\"\n    # Create a Boolean DataFrame indicating the presence of the target value\n    presence_df = dataframe == target_value\n\n    # Create a heatmap using seaborn\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(presence_df, cmap='coolwarm', cbar=False)\n\n    return (presence_df, ax.get_figure())\n\n# Test the function\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500],\n    'D': [1000, 2000, 3000, 4000, 5000]\n})\n\npresence_df, ax = task_func(df, target_value=200)"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef exponential_decay(t, a, b, c):\n    \"\"\"\n    Exponential decay function of the form:\n    f(t) = a * exp(-b * t) + c\n    \"\"\"\n    return a * np.exp(-b * t) + c\ndef task_func(array, target_value):\n    \"\"\"\n    Fits an exponential decay function to the indices in the array where the first column matches the target value.\n    \"\"\"\n    # Extract the indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n\n    # Fit the exponential decay function to the indices\n    popt, _ = optimize.curve_fit(exponential_decay, indices, array[indices, 1])\n\n    # Create a new figure and plot the data and the fitted function\n    fig, ax = plt.subplots()\n    ax.plot(indices, array[indices, 1], 'o', label='data')\n    ax.plot(indices, exponential_decay(indices, *popt), '-', label='fit')\n    ax.legend()\n\n    return popt, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef exponential_decay(t, a, b, c):\n    \"\"\"\n    Exponential decay function of the form:\n    f(t) = a * exp(-b * t) + c\n    \"\"\"\n    return a * np.exp(-b * t) + c\n\ndef task_func(array, target_value):\n    \"\"\"\n    Fits an exponential decay function to the indices in the array where the first column matches the target value.\n    \"\"\"\n    # Extract the indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n\n    # Fit the exponential decay function to the indices\n    popt, _ = optimize.curve_fit(exponential_decay, indices, array[indices, 1])\n\n    # Create a new figure and plot the data and the fitted function\n    fig, ax = plt.subplots()\n    ax.plot(indices, array[indices, 1], 'o', label='data')\n    ax.plot(indices, exponential_decay(indices, *popt), '-', label='fit')\n    ax.legend()\n\n    return popt, ax"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = set(stopwords.words('english'))\ndef preprocess(text):\n    \"\"\"Preprocess the input text by removing non-alphanumeric characters,\n    converting to lowercase, and removing stopwords.\"\"\"\n    text = ALPHANUMERIC.sub(' ', text)\n    text = text.lower()\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n    return ' '.join(words)\ndef task_func(texts, num_topics):\n    \"\"\"Perform topic extraction from the given texts using NMF.\"\"\"\n    # Preprocess the texts\n    texts = [preprocess(text) for text in texts]\n\n    # Vectorize the texts using TF-IDF\n    vectorizer = TfidfVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(texts)\n\n    # Apply NMF to extract topics\n    nmf = NMF(n_components=num_topics, random_state=42, alpha=.1, l1_ratio=.5)\n    nmf.fit(X)\n\n    # Extract the most significant words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_features = [feature_names[i] for i in topic.argsort()[:-num_topics - 1:-1]]\n        topics.append(top_features)\n\n    return topics", "raw_solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef preprocess(text):\n    \"\"\"Preprocess the input text by removing non-alphanumeric characters,\n    converting to lowercase, and removing stopwords.\"\"\"\n    text = ALPHANUMERIC.sub(' ', text)\n    text = text.lower()\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n    return ' '.join(words)\n\ndef task_func(texts, num_topics):\n    \"\"\"Perform topic extraction from the given texts using NMF.\"\"\"\n    # Preprocess the texts\n    texts = [preprocess(text) for text in texts]\n\n    # Vectorize the texts using TF-IDF\n    vectorizer = TfidfVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(texts)\n\n    # Apply NMF to extract topics\n    nmf = NMF(n_components=num_topics, random_state=42, alpha=.1, l1_ratio=.5)\n    nmf.fit(X)\n\n    # Extract the most significant words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_features = [feature_names[i] for i in topic.argsort()[:-num_topics - 1:-1]]\n        topics.append(top_features)\n\n    return topics"}
{"task_id": "BigCodeBench/656", "solution": "import re\nimport string\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nALPHANUMERIC = re.compile('[\\W_]+')\nPUNCTUATIONS = string.punctuation\ndef task_func(text: str, sia: SentimentIntensityAnalyzer) -> dict:\n    \"\"\"\n    Analyze the sentiment of a text using the provided SentimentIntensityAnalyzer.\n    The text is first cleaned by:\n    - Removing all non-alphanumeric characters except spaces.\n    - Converting to lowercase.\n    - Removing punctuation.\n    \"\"\"\n    # Clean the text\n    text = text.lower()\n    text = ALPHANUMERIC.sub(' ', text)\n    text = ''.join(ch for ch in text if ch not in PUNCTUATIONS)\n\n    # Analyze the sentiment\n    sentiment = sia.polarity_scores(text)\n\n    return sentiment", "raw_solution": "import re\nimport string\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nnltk.download('vader_lexicon')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nPUNCTUATIONS = string.punctuation\n\ndef task_func(text: str, sia: SentimentIntensityAnalyzer) -> dict:\n    \"\"\"\n    Analyze the sentiment of a text using the provided SentimentIntensityAnalyzer.\n    The text is first cleaned by:\n    - Removing all non-alphanumeric characters except spaces.\n    - Converting to lowercase.\n    - Removing punctuation.\n    \"\"\"\n    # Clean the text\n    text = text.lower()\n    text = ALPHANUMERIC.sub(' ', text)\n    text = ''.join(ch for ch in text if ch not in PUNCTUATIONS)\n\n    # Analyze the sentiment\n    sentiment = sia.polarity_scores(text)\n\n    return sentiment"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nALPHANUMERIC = re.compile('[\\W_]+')\ndef clean_text(text, stopwords=None):\n    \"\"\"\n    Clean a text by removing all non-alphanumeric characters except space, lowercased, and stop words are removed.\n    \"\"\"\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove non-alphanumeric characters\n    text = ALPHANUMERIC.sub(' ', text)\n\n    # Tokenize the text\n    words = text.split()\n\n    # Remove stopwords\n    if stopwords is not None:\n        words = [word for word in words if word not in stopwords]\n\n    return words\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generate word vectors from a list of texts using the gensim Word2Vec model and nltk.corpus.stopwords.\n    \"\"\"\n    # Clean the texts\n    cleaned_texts = [clean_text(text, stopwords) for text in texts]\n\n    # Train the Word2Vec model\n    model = Word2Vec(cleaned_texts)\n\n    return model", "raw_solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef clean_text(text, stopwords=None):\n    \"\"\"\n    Clean a text by removing all non-alphanumeric characters except space, lowercased, and stop words are removed.\n    \"\"\"\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove non-alphanumeric characters\n    text = ALPHANUMERIC.sub(' ', text)\n\n    # Tokenize the text\n    words = text.split()\n\n    # Remove stopwords\n    if stopwords is not None:\n        words = [word for word in words if word not in stopwords]\n\n    return words\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generate word vectors from a list of texts using the gensim Word2Vec model and nltk.corpus.stopwords.\n    \"\"\"\n    # Clean the texts\n    cleaned_texts = [clean_text(text, stopwords) for text in texts]\n\n    # Train the Word2Vec model\n    model = Word2Vec(cleaned_texts)\n\n    return model"}
{"task_id": "BigCodeBench/658", "solution": "import re\nimport nltk\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(texts):\n    \"\"\"\n    Creates a document-term matrix (DTM) from a list of text documents using CountVectorizer from Scikit-learn.\n    Texts are preprocessed by removing non-alphanumeric characters (excluding spaces), converting to lowercase, and excluding English stop words defined in NLTK.\n    \"\"\"\n    # Preprocess the texts\n    texts = [ALPHANUMERIC.sub(' ', text.lower()) for text in texts]\n    texts = [' '.join([word for word in text.split() if word not in STOPWORDS]) for text in texts]\n\n    # Create a CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the texts\n    dtm = vectorizer.fit_transform(texts)\n\n    # Convert the sparse matrix to a DataFrame\n    dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return dtm_df", "raw_solution": "import re\nimport nltk\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\n\n# Make sure to download NLTK stopwords\nnltk.download('stopwords')\n\n# Define a regex pattern for matching all non-alphanumeric characters\nALPHANUMERIC = re.compile('[\\W_]+')\n\n# Load NLTK's list of English stop words\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(texts):\n    \"\"\"\n    Creates a document-term matrix (DTM) from a list of text documents using CountVectorizer from Scikit-learn.\n    Texts are preprocessed by removing non-alphanumeric characters (excluding spaces), converting to lowercase, and excluding English stop words defined in NLTK.\n    \"\"\"\n    # Preprocess the texts\n    texts = [ALPHANUMERIC.sub(' ', text.lower()) for text in texts]\n    texts = [' '.join([word for word in text.split() if word not in STOPWORDS]) for text in texts]\n\n    # Create a CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the texts\n    dtm = vectorizer.fit_transform(texts)\n\n    # Convert the sparse matrix to a DataFrame\n    dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return dtm_df"}
{"task_id": "BigCodeBench/659", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\ndef task_func(x, y, labels):\n    \"\"\"\n    Draw normal distributions for multiple 'x' and 'y' arrays with labels. Each pair (x, y) represents a different chemical compound in the 'labels' list.\n    \"\"\"\n    # Create a new figure\n    fig, ax = plt.subplots(1, 1)\n\n    # Draw normal distributions for each pair of x and y arrays\n    for i in range(len(x)):\n        mu, sigma = x[i], y[i]\n        x_axis = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n        ax.plot(x_axis, stats.norm.pdf(x_axis, mu, sigma), label=labels[i])\n\n    # Add legend\n    ax.legend()\n\n    return fig", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\ndef task_func(x, y, labels):\n    \"\"\"\n    Draw normal distributions for multiple 'x' and 'y' arrays with labels. Each pair (x, y) represents a different chemical compound in the 'labels' list.\n    \"\"\"\n    # Create a new figure\n    fig, ax = plt.subplots(1, 1)\n\n    # Draw normal distributions for each pair of x and y arrays\n    for i in range(len(x)):\n        mu, sigma = x[i], y[i]\n        x_axis = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n        ax.plot(x_axis, stats.norm.pdf(x_axis, mu, sigma), label=labels[i])\n\n    # Add legend\n    ax.legend()\n\n    return fig"}
{"task_id": "BigCodeBench/660", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(x, y, labels):\n    \"\"\"\n    Scales the \"x\" and \"y\" arrays using the standard scaler of sklearn and plots them with given labels.\n    Each pair of x and y arrays are scaled independently and plotted as a separate series with a label.\n    \"\"\"\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Scale x and y arrays\n    x_scaled = scaler.fit_transform(x)\n    y_scaled = scaler.fit_transform(y)\n\n    # Create figure\n    fig, ax = plt.subplots()\n\n    # Plot scaled x and y arrays with corresponding labels\n    ax.plot(x_scaled, label=labels[0])\n    ax.plot(y_scaled, label=labels[1])\n\n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Scaled Value')\n    ax.set_title('Scaled x and y arrays')\n\n    # Add legend\n    ax.legend()\n\n    return fig", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(x, y, labels):\n    \"\"\"\n    Scales the \"x\" and \"y\" arrays using the standard scaler of sklearn and plots them with given labels.\n    Each pair of x and y arrays are scaled independently and plotted as a separate series with a label.\n    \"\"\"\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Scale x and y arrays\n    x_scaled = scaler.fit_transform(x)\n    y_scaled = scaler.fit_transform(y)\n\n    # Create figure\n    fig, ax = plt.subplots()\n\n    # Plot scaled x and y arrays with corresponding labels\n    ax.plot(x_scaled, label=labels[0])\n    ax.plot(y_scaled, label=labels[1])\n\n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Scaled Value')\n    ax.set_title('Scaled x and y arrays')\n\n    # Add legend\n    ax.legend()\n\n    return fig"}
{"task_id": "BigCodeBench/661", "solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nLABELS = ['H\\u2082O', 'O\\u2082', 'CO\\u2082', 'N\\u2082', 'Ar']\ndef task_func(x, y, labels):\n    \"\"\"\n    Creates a heatmap using the seaborn library for \"x\" as x-values and \"y\" as y-values with labels.\n    \"\"\"\n    # Create a DataFrame from the input data\n    df = pd.DataFrame({'x': x, 'y': y, 'label': labels})\n\n    # Pivot the DataFrame to create a 2D array for the heatmap\n    pivot_df = df.pivot('x', 'y', 'label')\n\n    # Create the heatmap\n    ax = sns.heatmap(pivot_df)\n\n    return ax, df\nx = np.random.randint(0, 10, 100)\ny = np.random.randint(0, 10, 100)\nlabels = np.random.choice(LABELS, 100)", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n# Constants\nLABELS = ['H\\u2082O', 'O\\u2082', 'CO\\u2082', 'N\\u2082', 'Ar']\n\ndef task_func(x, y, labels):\n    \"\"\"\n    Creates a heatmap using the seaborn library for \"x\" as x-values and \"y\" as y-values with labels.\n    \"\"\"\n    # Create a DataFrame from the input data\n    df = pd.DataFrame({'x': x, 'y': y, 'label': labels})\n\n    # Pivot the DataFrame to create a 2D array for the heatmap\n    pivot_df = df.pivot('x', 'y', 'label')\n\n    # Create the heatmap\n    ax = sns.heatmap(pivot_df)\n\n    return ax, df\n\n# Test the function\nx = np.random.randint(0, 10, 100)\ny = np.random.randint(0, 10, 100)\nlabels = np.random.choice(LABELS, 100)\nax, df = task_func(x, y, labels)"}
{"task_id": "BigCodeBench/662", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(x, y, labels):\n    # Combine x and y into a single array\n    data = np.column_stack((x, y))\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    pca.fit(data)\n\n    # Transform the data\n    transformed_data = pca.transform(data)\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_data[:, 0], transformed_data[:, 1], c=labels)\n\n    return fig", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(x, y, labels):\n    # Combine x and y into a single array\n    data = np.column_stack((x, y))\n\n    # Perform PCA\n    pca = PCA(n_components=2)\n    pca.fit(data)\n\n    # Transform the data\n    transformed_data = pca.transform(data)\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_data[:, 0], transformed_data[:, 1], c=labels)\n\n    return fig"}
{"task_id": "BigCodeBench/663", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\ndef exponential_func(x, a, b, c):\n    return a * np.exp(-b * x) + c\ndef task_func(x, y, labels):\n    # Create a new figure\n    fig = plt.figure()\n\n    # Fit an exponential curve to each set of data\n    for i in range(len(x)):\n        popt, _ = curve_fit(exponential_func, x[i], y[i])\n        a, b, c = popt\n\n        # Generate x values for the fitted curve\n        x_fit = np.linspace(min(x[i]), max(x[i]), 100)\n\n        # Calculate y values for the fitted curve\n        y_fit = exponential_func(x_fit, a, b, c)\n\n        # Plot the original data and the fitted curve\n        plt.plot(x[i], y[i], 'o', label=labels[i])\n        plt.plot(x_fit, y_fit, '-', label=f'{labels[i]} Fit')\n\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Exponential Curve Fits')\n    return fig", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\ndef exponential_func(x, a, b, c):\n    return a * np.exp(-b * x) + c\n\ndef task_func(x, y, labels):\n    # Create a new figure\n    fig = plt.figure()\n\n    # Fit an exponential curve to each set of data\n    for i in range(len(x)):\n        popt, _ = curve_fit(exponential_func, x[i], y[i])\n        a, b, c = popt\n\n        # Generate x values for the fitted curve\n        x_fit = np.linspace(min(x[i]), max(x[i]), 100)\n\n        # Calculate y values for the fitted curve\n        y_fit = exponential_func(x_fit, a, b, c)\n\n        # Plot the original data and the fitted curve\n        plt.plot(x[i], y[i], 'o', label=labels[i])\n        plt.plot(x_fit, y_fit, '-', label=f'{labels[i]} Fit')\n\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Exponential Curve Fits')\n    return fig"}
{"task_id": "BigCodeBench/664", "solution": "import statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(sales_data):\n    \"\"\"\n    Plots sales trends for five products over a year, highlighting variability with standard deviation shading.\n    \"\"\"\n    # Extract product names and sales data\n    product_names = list(sales_data.keys())\n    sales_values = list(sales_data.values())\n\n    # Create a figure and an axes\n    fig, ax = plt.subplots()\n\n    # Plot sales data\n    for i, product in enumerate(product_names):\n        sales = sales_values[i]\n        ax.plot(range(1, 13), sales, label=product)\n\n        # Calculate standard deviation and plot shading\n        std_dev = np.std(sales)\n        ax.fill_between(range(1, 13), sales, alpha=0.2)\n\n    # Set x-axis labels to represent months\n    ax.set_xticks(range(1, 13))\n    ax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n\n    # Set x-axis and y-axis labels\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n\n    # Add legend\n    ax.legend()\n\n    return ax", "raw_solution": "import statistics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(sales_data):\n    \"\"\"\n    Plots sales trends for five products over a year, highlighting variability with standard deviation shading.\n    \"\"\"\n    # Extract product names and sales data\n    product_names = list(sales_data.keys())\n    sales_values = list(sales_data.values())\n\n    # Create a figure and an axes\n    fig, ax = plt.subplots()\n\n    # Plot sales data\n    for i, product in enumerate(product_names):\n        sales = sales_values[i]\n        ax.plot(range(1, 13), sales, label=product)\n\n        # Calculate standard deviation and plot shading\n        std_dev = np.std(sales)\n        ax.fill_between(range(1, 13), sales, alpha=0.2)\n\n    # Set x-axis labels to represent months\n    ax.set_xticks(range(1, 13))\n    ax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n\n    # Set x-axis and y-axis labels\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n\n    # Add legend\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/665", "solution": "import shutil\nimport os\nimport fnmatch\nimport itertools\ndef task_func(src_dir, dst_dir):\n    \"\"\"\n    Copies all files from 'src_dir' to 'dst_dir' that match any pattern in ['*.txt', '*.docx'].\n    \"\"\"\n    # Define the file patterns\n    patterns = ['*.txt', '*.docx']\n\n    # Iterate over each pattern\n    for pattern in patterns:\n        # Find all files in the source directory that match the pattern\n        for file_name in fnmatch.filter(os.listdir(src_dir), pattern):\n            # Construct the full file path\n            src_file = os.path.join(src_dir, file_name)\n            # Copy the file to the destination directory\n            shutil.copy2(src_file, dst_dir)\n\n    # Return the destination directory\n    return dst_dir", "raw_solution": "import shutil\nimport os\nimport fnmatch\nimport itertools\n\ndef task_func(src_dir, dst_dir):\n    \"\"\"\n    Copies all files from 'src_dir' to 'dst_dir' that match any pattern in ['*.txt', '*.docx'].\n    \"\"\"\n    # Define the file patterns\n    patterns = ['*.txt', '*.docx']\n\n    # Iterate over each pattern\n    for pattern in patterns:\n        # Find all files in the source directory that match the pattern\n        for file_name in fnmatch.filter(os.listdir(src_dir), pattern):\n            # Construct the full file path\n            src_file = os.path.join(src_dir, file_name)\n            # Copy the file to the destination directory\n            shutil.copy2(src_file, dst_dir)\n\n    # Return the destination directory\n    return dst_dir"}
{"task_id": "BigCodeBench/666", "solution": "from itertools import combinations\nimport math\ndef task_func(seq, letter_weight_dict):\n    max_weight = 0\n    max_subseq = ''\n    for L in range(0, len(seq)+1):\n        for subset in combinations(seq, L):\n            subset_weight = sum(letter_weight_dict[i] for i in subset)\n            if subset_weight > max_weight:\n                max_weight = subset_weight\n                max_subseq = ''.join(subset)\n    return max_subseq\nletter_weight_dict = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\nseq = 'abcde'", "raw_solution": "from itertools import combinations\nimport math\n\ndef task_func(seq, letter_weight_dict):\n    max_weight = 0\n    max_subseq = ''\n    for L in range(0, len(seq)+1):\n        for subset in combinations(seq, L):\n            subset_weight = sum(letter_weight_dict[i] for i in subset)\n            if subset_weight > max_weight:\n                max_weight = subset_weight\n                max_subseq = ''.join(subset)\n    return max_subseq\n\n# Test the function\nletter_weight_dict = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\nseq = 'abcde'"}
{"task_id": "BigCodeBench/667", "solution": "import heapq\nimport collections\ndef task_func(x, n):\n    \"\"\"\n    Finds the n most common letters in a dictionary x, where the key letters and the values are their frequencies.\n    \"\"\"\n    # Create a priority queue to store the letters and their frequencies\n    pq = []\n    for letter, freq in x.items():\n        # Use negative frequency to make it a max heap\n        heapq.heappush(pq, (-freq, letter))\n\n    # Get the n most common letters\n    most_common_letters = []\n    for _ in range(n):\n        if pq:\n            _, letter = heapq.heappop(pq)\n            most_common_letters.append(letter)\n        else:\n            break\n\n    return most_common_letters\nx = {'a': 5, 'b': 3, 'c': 2, 'd': 10, 'e': 8, 'f': 6}\nn = 3", "raw_solution": "import heapq\nimport collections\n\ndef task_func(x, n):\n    \"\"\"\n    Finds the n most common letters in a dictionary x, where the key letters and the values are their frequencies.\n    \"\"\"\n    # Create a priority queue to store the letters and their frequencies\n    pq = []\n    for letter, freq in x.items():\n        # Use negative frequency to make it a max heap\n        heapq.heappush(pq, (-freq, letter))\n\n    # Get the n most common letters\n    most_common_letters = []\n    for _ in range(n):\n        if pq:\n            _, letter = heapq.heappop(pq)\n            most_common_letters.append(letter)\n        else:\n            break\n\n    return most_common_letters\n\n# Test the function\nx = {'a': 5, 'b': 3, 'c': 2, 'd': 10, 'e': 8, 'f': 6}\nn = 3"}
{"task_id": "BigCodeBench/668", "solution": "import itertools\nimport math\ndef task_func(x):\n    \"\"\"\n    Finds the sub-sequence of a dictionary, x, with the minimum total length, where the keys are letters and the values are their lengths.\n    \"\"\"\n    # Sort the dictionary by values\n    sorted_x = dict(sorted(x.items(), key=lambda item: item[1]))\n\n    # Initialize minimum length and subsequence\n    min_length = math.inf\n    min_subsequence = []\n\n    # Iterate over all possible subsequences of the sorted dictionary\n    for length in range(1, len(sorted_x) + 1):\n        for subsequence in itertools.combinations(sorted_x.items(), length):\n            # Calculate total length of the subsequence\n            total_length = sum(value for key, value in subsequence)\n\n            # If the total length is less than the current minimum, update minimum and subsequence\n            if total_length < min_length:\n                min_length = total_length\n                min_subsequence = [key for key, value in subsequence]\n\n    return min_subsequence\nx = {'a': 3, 'b': 2, 'c': 1, 'd': 4, 'e': 5}", "raw_solution": "import itertools\nimport math\n\ndef task_func(x):\n    \"\"\"\n    Finds the sub-sequence of a dictionary, x, with the minimum total length, where the keys are letters and the values are their lengths.\n    \"\"\"\n    # Sort the dictionary by values\n    sorted_x = dict(sorted(x.items(), key=lambda item: item[1]))\n\n    # Initialize minimum length and subsequence\n    min_length = math.inf\n    min_subsequence = []\n\n    # Iterate over all possible subsequences of the sorted dictionary\n    for length in range(1, len(sorted_x) + 1):\n        for subsequence in itertools.combinations(sorted_x.items(), length):\n            # Calculate total length of the subsequence\n            total_length = sum(value for key, value in subsequence)\n\n            # If the total length is less than the current minimum, update minimum and subsequence\n            if total_length < min_length:\n                min_length = total_length\n                min_subsequence = [key for key, value in subsequence]\n\n    return min_subsequence\n\n# Test the function\nx = {'a': 3, 'b': 2, 'c': 1, 'd': 4, 'e': 5}"}
{"task_id": "BigCodeBench/669", "solution": "import itertools\nimport math\nfrom typing import Dict, Tuple\ndef task_func(x: Dict[str, Dict[str, float]]) -> Tuple[str, str]:\n    \"\"\"\n    Finds the key pair in a dictionary, x, which has the highest sum of the cosine of each of its values.\n    \"\"\"\n    max_cos_sum = -1\n    max_keys = None\n    for keys in itertools.combinations(x.keys(), 2):\n        cos_sum = sum(math.cos(v) for v in x[keys[0]].values()) + sum(math.cos(v) for v in x[keys[1]].values())\n        if cos_sum > max_cos_sum:\n            max_cos_sum = cos_sum\n            max_keys = keys\n    return max_keys", "raw_solution": "import itertools\nimport math\nfrom typing import Dict, Tuple\n\ndef task_func(x: Dict[str, Dict[str, float]]) -> Tuple[str, str]:\n    \"\"\"\n    Finds the key pair in a dictionary, x, which has the highest sum of the cosine of each of its values.\n    \"\"\"\n    max_cos_sum = -1\n    max_keys = None\n    for keys in itertools.combinations(x.keys(), 2):\n        cos_sum = sum(math.cos(v) for v in x[keys[0]].values()) + sum(math.cos(v) for v in x[keys[1]].values())\n        if cos_sum > max_cos_sum:\n            max_cos_sum = cos_sum\n            max_keys = keys\n    return max_keys"}
{"task_id": "BigCodeBench/670", "solution": "from itertools import combinations\nimport math\ndef task_func(x, w):\n    \"\"\"\n    Finds the continuous substring of x, which has the maximum total weight, given a dictionary where the keys are characters and the values are their weights.\n    \"\"\"\n    max_weight = -math.inf\n    max_substr = ''\n\n    for i in range(len(x)):\n        for j in range(i + 1, len(x) + 1):\n            substr = x[i:j]\n            weight = sum(w[c] for c in substr)\n            if weight > max_weight:\n                max_weight = weight\n                max_substr = substr\n\n    return max_substr\nw = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\nx = 'abcde'", "raw_solution": "from itertools import combinations\nimport math\n\ndef task_func(x, w):\n    \"\"\"\n    Finds the continuous substring of x, which has the maximum total weight, given a dictionary where the keys are characters and the values are their weights.\n    \"\"\"\n    max_weight = -math.inf\n    max_substr = ''\n\n    for i in range(len(x)):\n        for j in range(i + 1, len(x) + 1):\n            substr = x[i:j]\n            weight = sum(w[c] for c in substr)\n            if weight > max_weight:\n                max_weight = weight\n                max_substr = substr\n\n    return max_substr\n\n# Test the function\nw = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\nx = 'abcde'"}
{"task_id": "BigCodeBench/671", "solution": "import os\nimport random\nimport json\nimport tempfile\ndef task_func(directory, n):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n):\n        file_path = os.path.join(directory, f'file_{i}.json')\n        with open(file_path, 'w') as file:\n            data = {'number': random.randint(1, 100)}\n            json.dump(data, file)\n\n    return directory", "raw_solution": "import os\nimport random\nimport json\nimport tempfile\n\ndef task_func(directory, n):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n):\n        file_path = os.path.join(directory, f'file_{i}.json')\n        with open(file_path, 'w') as file:\n            data = {'number': random.randint(1, 100)}\n            json.dump(data, file)\n\n    return directory"}
{"task_id": "BigCodeBench/672", "solution": "import csv\nimport sys\ndef task_func(filename):\n    \"\"\"\n    Read a CSV file, inverse the order of the lines and write the inverted lines back into the file.\n    Then reset the cursor to the beginning of the file.\n    \"\"\"\n    with open(filename, 'r') as file:\n        lines = file.readlines()\n\n    with open(filename, 'w') as file:\n        for line in reversed(lines):\n            file.write(line)\n\n    with open(filename, 'r+') as file:\n        file.seek(0)", "raw_solution": "import csv\nimport sys\n\ndef task_func(filename):\n    \"\"\"\n    Read a CSV file, inverse the order of the lines and write the inverted lines back into the file.\n    Then reset the cursor to the beginning of the file.\n    \"\"\"\n    with open(filename, 'r') as file:\n        lines = file.readlines()\n\n    with open(filename, 'w') as file:\n        for line in reversed(lines):\n            file.write(line)\n\n    with open(filename, 'r+') as file:\n        file.seek(0)\n\n# Test the function\ntask_func('test.csv')"}
{"task_id": "BigCodeBench/673", "solution": "import os\nimport random\ndef task_func(directory, n_files):\n    \"\"\"\n    Creates n random txt files in a specific directory, writes a single digit random integer into each file, and then resets the cursor to the beginning of each file.\n    The file names start from 'file_1.txt' and increment by 1 for each file.\n    \"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        file_name = os.path.join(directory, f'file_{i+1}.txt')\n        with open(file_name, 'w') as f:\n            f.write(str(random.randint(0, 9)))\n        f.seek(0)\n\n    return n_files", "raw_solution": "import os\nimport random\n\ndef task_func(directory, n_files):\n    \"\"\"\n    Creates n random txt files in a specific directory, writes a single digit random integer into each file, and then resets the cursor to the beginning of each file.\n    The file names start from 'file_1.txt' and increment by 1 for each file.\n    \"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        file_name = os.path.join(directory, f'file_{i+1}.txt')\n        with open(file_name, 'w') as f:\n            f.write(str(random.randint(0, 9)))\n        f.seek(0)\n\n    return n_files\n\n# Test the function\nn_files = task_func('test_directory', 5)"}
{"task_id": "BigCodeBench/674", "solution": "import pandas as pd\nimport os\ndef task_func(filename):\n    \"\"\"\n    Read a CSV file of pandas, reverse the order of the lines and write the inverted lines back into the file.\n    Then move the cursor back to the beginning of the file. The header should not be inverted and the file may be empty.\n    \"\"\"\n    if os.path.exists(filename):\n        df = pd.read_csv(filename)\n        df = df.iloc[::-1]\n        df.to_csv(filename, index=False)\n        os.lseek(os.open(filename, os.O_RDONLY), 0, os.SEEK_SET)\n    else:\n        print(f\"File {filename} does not exist.\")", "raw_solution": "import pandas as pd\nimport os\n\ndef task_func(filename):\n    \"\"\"\n    Read a CSV file of pandas, reverse the order of the lines and write the inverted lines back into the file.\n    Then move the cursor back to the beginning of the file. The header should not be inverted and the file may be empty.\n    \"\"\"\n    if os.path.exists(filename):\n        df = pd.read_csv(filename)\n        df = df.iloc[::-1]\n        df.to_csv(filename, index=False)\n        os.lseek(os.open(filename, os.O_RDONLY), 0, os.SEEK_SET)\n    else:\n        print(f\"File {filename} does not exist.\")\n\n# Test the function\ntask_func('test.csv')"}
{"task_id": "BigCodeBench/675", "solution": "import os\nimport random\nimport string\ndef task_func(directory, n_files):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        filename = os.path.join(directory, f'file_{i}.txt')\n        with open(filename, 'w') as f:\n            f.write(''.join(random.choices(string.ascii_letters, k=100)))\n\n    return directory", "raw_solution": "import os\nimport random\nimport string\n\ndef task_func(directory, n_files):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    for i in range(n_files):\n        filename = os.path.join(directory, f'file_{i}.txt')\n        with open(filename, 'w') as f:\n            f.write(''.join(random.choices(string.ascii_letters, k=100)))\n\n    return directory"}
{"task_id": "BigCodeBench/676", "solution": "import pandas as pd\nimport random\ndef task_func(df):\n    \"\"\"\n    Adds a 'winner' column to the DataFrame, which is the team with the highest score in each match.\n    If the scores are equal, the winner is randomly decided.\n    \"\"\"\n    df['winner'] = df.apply(lambda row: random.choice([row['team1'], row['team2']]) if row['score1'] == row['score2'] else (row['team1'] if row['score1'] > row['score2'] else row['team2']), axis=1)\n    return df\ndf = pd.DataFrame({\n    'team1': ['Team A', 'Team B', 'Team C', 'Team D'],\n    'score1': [5, 7, 1, 3],\n    'team2': ['Team E', 'Team F', 'Team G', 'Team H'],\n    'score2': [2, 3, 6, 4]\n})", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(df):\n    \"\"\"\n    Adds a 'winner' column to the DataFrame, which is the team with the highest score in each match.\n    If the scores are equal, the winner is randomly decided.\n    \"\"\"\n    df['winner'] = df.apply(lambda row: random.choice([row['team1'], row['team2']]) if row['score1'] == row['score2'] else (row['team1'] if row['score1'] > row['score2'] else row['team2']), axis=1)\n    return df\n\n# Test the function\ndf = pd.DataFrame({\n    'team1': ['Team A', 'Team B', 'Team C', 'Team D'],\n    'score1': [5, 7, 1, 3],\n    'team2': ['Team E', 'Team F', 'Team G', 'Team H'],\n    'score2': [2, 3, 6, 4]\n})\n\ndf = task_func(df)"}
{"task_id": "BigCodeBench/677", "solution": "import numpy as np\nimport pandas as pd\nfrom scipy.stats import linregress\ndef task_func(df):\n    \"\"\"\n    Analyze the relationship between two variables in a DataFrame.\n    The function performs a linear regression on the two variables and adds a 'predicted' column to the DataFrame.\n    \"\"\"\n    if 'predicted' in df.columns:\n        df = df.drop(columns='predicted')\n\n    if len(df.columns) < 2:\n        raise ValueError(\"DataFrame must have at least two columns for regression.\")\n\n    x = df.columns[0]\n    y = df.columns[1]\n\n    slope, intercept, r_value, p_value, std_err = linregress(df[x], df[y])\n    df['predicted'] = intercept + slope * df[x]\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom scipy.stats import linregress\n\ndef task_func(df):\n    \"\"\"\n    Analyze the relationship between two variables in a DataFrame.\n    The function performs a linear regression on the two variables and adds a 'predicted' column to the DataFrame.\n    \"\"\"\n    if 'predicted' in df.columns:\n        df = df.drop(columns='predicted')\n\n    if len(df.columns) < 2:\n        raise ValueError(\"DataFrame must have at least two columns for regression.\")\n\n    x = df.columns[0]\n    y = df.columns[1]\n\n    slope, intercept, r_value, p_value, std_err = linregress(df[x], df[y])\n    df['predicted'] = intercept + slope * df[x]\n\n    return df"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in a directory. The function reads each JSON file alphabetically into a DataFrame and inserts a \"Source\" column that specifies the filename. The processed files are then moved to a \"processed\" subdirectory. The path may not exist initially.\n    \"\"\"\n    # Create the \"processed\" subdirectory if it doesn't exist\n    if not os.path.exists(os.path.join(path, \"processed\")):\n        os.makedirs(os.path.join(path, \"processed\"))\n\n    # Get a list of all JSON files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    json_files.sort()  # Sort the files alphabetically\n\n    # Initialize an empty DataFrame to store the data from all files\n    df = pd.DataFrame()\n\n    # Process each JSON file\n    for file in json_files:\n        # Read the JSON file into a DataFrame\n        with open(os.path.join(path, file), 'r') as f:\n            data = json.load(f)\n        temp_df = pd.json_normalize(data)\n\n        # Add a \"Source\" column that specifies the filename\n        temp_df['Source'] = file\n\n        # Append the data from the current file to the DataFrame\n        df = df.append(temp_df, ignore_index=True)\n\n        # Move the processed file to the \"processed\" subdirectory\n        shutil.move(os.path.join(path, file), os.path.join(path, \"processed\", file))\n\n    return df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in a directory. The function reads each JSON file alphabetically into a DataFrame and inserts a \"Source\" column that specifies the filename. The processed files are then moved to a \"processed\" subdirectory. The path may not exist initially.\n    \"\"\"\n    # Create the \"processed\" subdirectory if it doesn't exist\n    if not os.path.exists(os.path.join(path, \"processed\")):\n        os.makedirs(os.path.join(path, \"processed\"))\n\n    # Get a list of all JSON files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    json_files.sort()  # Sort the files alphabetically\n\n    # Initialize an empty DataFrame to store the data from all files\n    df = pd.DataFrame()\n\n    # Process each JSON file\n    for file in json_files:\n        # Read the JSON file into a DataFrame\n        with open(os.path.join(path, file), 'r') as f:\n            data = json.load(f)\n        temp_df = pd.json_normalize(data)\n\n        # Add a \"Source\" column that specifies the filename\n        temp_df['Source'] = file\n\n        # Append the data from the current file to the DataFrame\n        df = df.append(temp_df, ignore_index=True)\n\n        # Move the processed file to the \"processed\" subdirectory\n        shutil.move(os.path.join(path, file), os.path.join(path, \"processed\", file))\n\n    return df"}
{"task_id": "BigCodeBench/679", "solution": "import pandas as pd\nfrom collections import Counter\nfrom itertools import combinations\ndef task_func(df):\n    \"\"\"\n    Calculates the frequency of combinations of elements in a DataFrame.\n    \"\"\"\n    # Add a 'combination' column to the DataFrame\n    df['combination'] = df.apply(lambda row: tuple(sorted(combinations(row.drop('combination').dropna(), 2))), axis=1)\n\n    # Calculate the frequency of each combination\n    combination_freq = Counter()\n    for comb in df['combination']:\n        combination_freq.update(comb)\n\n    return combination_freq", "raw_solution": "import pandas as pd\nfrom collections import Counter\nfrom itertools import combinations\n\ndef task_func(df):\n    \"\"\"\n    Calculates the frequency of combinations of elements in a DataFrame.\n    \"\"\"\n    # Add a 'combination' column to the DataFrame\n    df['combination'] = df.apply(lambda row: tuple(sorted(combinations(row.drop('combination').dropna(), 2))), axis=1)\n\n    # Calculate the frequency of each combination\n    combination_freq = Counter()\n    for comb in df['combination']:\n        combination_freq.update(comb)\n\n    return combination_freq"}
{"task_id": "BigCodeBench/680", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, features):\n    \"\"\"\n    Standardize the features in a DataFrame.\n    \"\"\"\n    # Create a StandardScaler object\n    scaler = StandardScaler()\n\n    # Fit and transform the features\n    df[features] = scaler.fit_transform(df[features])\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, features):\n    \"\"\"\n    Standardize the features in a DataFrame.\n    \"\"\"\n    # Create a StandardScaler object\n    scaler = StandardScaler()\n\n    # Fit and transform the features\n    df[features] = scaler.fit_transform(df[features])\n\n    return df"}
{"task_id": "BigCodeBench/681", "solution": "import pandas as pd\nimport json\ndef task_func(file_path, key):\n    \"\"\"\n    Loads a JSON file into a Pandas DataFrame, removes a specific key from each object,\n    and writes the processed DataFrame back into a JSON file.\n    \"\"\"\n    # Load JSON file into DataFrame\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    df = pd.DataFrame(data)\n\n    # Remove specific key from each object\n    df = df.apply(lambda x: x.drop(key) if key in x else x)\n\n    # Write DataFrame back into JSON file\n    df.to_json(file_path, orient='records', indent=4)\n\n    return df", "raw_solution": "import pandas as pd\nimport json\n\ndef task_func(file_path, key):\n    \"\"\"\n    Loads a JSON file into a Pandas DataFrame, removes a specific key from each object,\n    and writes the processed DataFrame back into a JSON file.\n    \"\"\"\n    # Load JSON file into DataFrame\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    df = pd.DataFrame(data)\n\n    # Remove specific key from each object\n    df = df.apply(lambda x: x.drop(key) if key in x else x)\n\n    # Write DataFrame back into JSON file\n    df.to_json(file_path, orient='records', indent=4)\n\n    return df"}
{"task_id": "BigCodeBench/682", "solution": "from collections import Counter\nimport math\ndef task_func(nested_dict):\n    \"\"\"\n    Aggregate the values of the same keys from a nested dictionary and remove the \"ele\" key.\n    For each remaining key take the sine.\n    \"\"\"\n    # Flatten the dictionary and aggregate the values of the same keys\n    flat_dict = flatten_dict(nested_dict)\n    aggregated_dict = dict(Counter(flat_dict))\n\n    # Remove the \"ele\" key\n    if \"ele\" in aggregated_dict:\n        del aggregated_dict[\"ele\"]\n\n    # Take the sine for each remaining key\n    for key in aggregated_dict.keys():\n        aggregated_dict[key] = math.sin(aggregated_dict[key])\n\n    return aggregated_dict\ndef flatten_dict(dd, separator='_', prefix=''):\n    \"\"\"\n    Flatten a nested dictionary.\n    \"\"\"\n    return { prefix + separator + k if prefix else k : v\n             for kk, vv in dd.items()\n             for k, v in flatten_dict(vv, separator, kk).items()\n             } if isinstance(dd, dict) else { prefix : dd }\nnested_dict = {\n    \"a\": {\"b\": {\"c\": 1, \"ele\": 2}},\n    \"d\": {\"e\": {\"f\": 3, \"ele\": 4}},\n    \"g\": {\"h\": {\"i\": 5, \"ele\": 6}}\n}", "raw_solution": "from collections import Counter\nimport math\n\ndef task_func(nested_dict):\n    \"\"\"\n    Aggregate the values of the same keys from a nested dictionary and remove the \"ele\" key.\n    For each remaining key take the sine.\n    \"\"\"\n    # Flatten the dictionary and aggregate the values of the same keys\n    flat_dict = flatten_dict(nested_dict)\n    aggregated_dict = dict(Counter(flat_dict))\n\n    # Remove the \"ele\" key\n    if \"ele\" in aggregated_dict:\n        del aggregated_dict[\"ele\"]\n\n    # Take the sine for each remaining key\n    for key in aggregated_dict.keys():\n        aggregated_dict[key] = math.sin(aggregated_dict[key])\n\n    return aggregated_dict\n\ndef flatten_dict(dd, separator='_', prefix=''):\n    \"\"\"\n    Flatten a nested dictionary.\n    \"\"\"\n    return { prefix + separator + k if prefix else k : v\n             for kk, vv in dd.items()\n             for k, v in flatten_dict(vv, separator, kk).items()\n             } if isinstance(dd, dict) else { prefix : dd }\n\n# Test the function\nnested_dict = {\n    \"a\": {\"b\": {\"c\": 1, \"ele\": 2}},\n    \"d\": {\"e\": {\"f\": 3, \"ele\": 4}},\n    \"g\": {\"h\": {\"i\": 5, \"ele\": 6}}\n}"}
{"task_id": "BigCodeBench/683", "solution": "import math\nimport yaml\ndef task_func(yaml_path, key):\n    with open(yaml_path, 'r') as file:\n        data = yaml.safe_load(file)\n\n    if key in data:\n        data[key] = math.cos(data[key])\n\n    with open(yaml_path, 'w') as file:\n        yaml.safe_dump(data, file)\n\n    return data", "raw_solution": "import math\nimport yaml\n\ndef task_func(yaml_path, key):\n    with open(yaml_path, 'r') as file:\n        data = yaml.safe_load(file)\n\n    if key in data:\n        data[key] = math.cos(data[key])\n\n    with open(yaml_path, 'w') as file:\n        yaml.safe_dump(data, file)\n\n    return data"}
{"task_id": "BigCodeBench/684", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(df, col):\n    if col in df.columns:\n        df = df.drop(columns=col)\n    df['IsEvenIndex'] = df.index % 2 == 0\n    return df\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(df, col):\n    if col in df.columns:\n        df = df.drop(columns=col)\n    df['IsEvenIndex'] = df.index % 2 == 0\n    return df\n\n# Testing the function\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n})\n\ndf = task_func(df, 'B')"}
{"task_id": "BigCodeBench/685", "solution": "from collections import Counter\nfrom itertools import chain\ndef task_func(list_of_lists):\n    \"\"\"\n    Merges all sublists from a list of lists into a list and returns a count of the elements.\n    \"\"\"\n    flat_list = list(chain(*list_of_lists))\n    count_dict = Counter(flat_list)\n    return count_dict", "raw_solution": "from collections import Counter\nfrom itertools import chain\n\ndef task_func(list_of_lists):\n    \"\"\"\n    Merges all sublists from a list of lists into a list and returns a count of the elements.\n    \"\"\"\n    flat_list = list(chain(*list_of_lists))\n    count_dict = Counter(flat_list)\n    return count_dict\n\n# Test the function"}
{"task_id": "BigCodeBench/686", "solution": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\ndef task_func(list_of_lists):\n    \"\"\"\n    Merges a predefined set of lists into a list and one-hot-encodes the elements of the list.\n    \"\"\"\n    # Merge the lists\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n    \n    # Convert the merged list to a numpy array\n    merged_array = np.array(merged_list).reshape(-1, 1)\n    \n    # Initialize the OneHotEncoder\n    enc = OneHotEncoder(sparse=False)\n    \n    # Fit and transform the data\n    one_hot = enc.fit_transform(merged_array)\n    \n    return one_hot\nlist_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef task_func(list_of_lists):\n    \"\"\"\n    Merges a predefined set of lists into a list and one-hot-encodes the elements of the list.\n    \"\"\"\n    # Merge the lists\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n    \n    # Convert the merged list to a numpy array\n    merged_array = np.array(merged_list).reshape(-1, 1)\n    \n    # Initialize the OneHotEncoder\n    enc = OneHotEncoder(sparse=False)\n    \n    # Fit and transform the data\n    one_hot = enc.fit_transform(merged_array)\n    \n    return one_hot\n\n# Test the function\nlist_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/687", "solution": "import numpy as np\nfrom scipy.stats import mode\nfrom typing import List, Tuple\ndef task_func(list_of_lists: List[List[int]]) -> Tuple[Tuple[int, int], np.ndarray, int]:\n    \"\"\"\n    Merges a predefined set of lists into a list and finds the mode of the elements in the list.\n    \"\"\"\n    # Merge the lists\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n\n    # Convert the list to a numpy array for efficient computation\n    merged_array = np.array(merged_list)\n\n    # Find the mode\n    mode_value, mode_count = mode(merged_array)\n\n    # Return the mode and count of the mode, the value that appears most frequently, and its count\n    return (mode_value[0].item(), mode_count[0]), mode_value[0], mode_count[0]", "raw_solution": "import numpy as np\nfrom scipy.stats import mode\nfrom typing import List, Tuple\n\ndef task_func(list_of_lists: List[List[int]]) -> Tuple[Tuple[int, int], np.ndarray, int]:\n    \"\"\"\n    Merges a predefined set of lists into a list and finds the mode of the elements in the list.\n    \"\"\"\n    # Merge the lists\n    merged_list = [item for sublist in list_of_lists for item in sublist]\n\n    # Convert the list to a numpy array for efficient computation\n    merged_array = np.array(merged_list)\n\n    # Find the mode\n    mode_value, mode_count = mode(merged_array)\n\n    # Return the mode and count of the mode, the value that appears most frequently, and its count\n    return (mode_value[0].item(), mode_count[0]), mode_value[0], mode_count[0]\n\n# Test the function"}
{"task_id": "BigCodeBench/688", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df):\n    \"\"\"\n    Standardizes a given pandas DataFrame using sklearn's StandardScaler.\n    \"\"\"\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    return df_standardized", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    \"\"\"\n    Standardizes a given pandas DataFrame using sklearn's StandardScaler.\n    \"\"\"\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    return df_standardized"}
{"task_id": "BigCodeBench/689", "solution": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\ndef task_func(df):\n    \"\"\"\n    Given a Pandas DataFrame with random numeric values, test if the data in each column is normally distributed using the Shapiro-Wilk test.\n    The function should output with:\n        dict: A dictionary with p-values from the Shapiro-Wilk test for each column.\n    \"\"\"\n    p_values = {}\n    for col in df.columns:\n        _, p_value = stats.shapiro(df[col])\n        p_values[col] = p_value\n    return p_values", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndef task_func(df):\n    \"\"\"\n    Given a Pandas DataFrame with random numeric values, test if the data in each column is normally distributed using the Shapiro-Wilk test.\n    The function should output with:\n        dict: A dictionary with p-values from the Shapiro-Wilk test for each column.\n    \"\"\"\n    p_values = {}\n    for col in df.columns:\n        _, p_value = stats.shapiro(df[col])\n        p_values[col] = p_value\n    return p_values"}
{"task_id": "BigCodeBench/690", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nROWS = 100\nCOLUMNS = ['X', 'Y']\ndef task_func(df):\n    \"\"\"\n    Given a Pandas DataFrame with random numeric values and columns X & Y, use sklearn's linear regression to match the data to a linear model.\n    \"\"\"\n    # Create a Linear Regression model\n    model = LinearRegression()\n\n    # Fit the model to the data\n    model.fit(df[['X']], df['Y'])\n\n    return model\ndf = pd.DataFrame(data=[{'X': x, 'Y': x * 2 + (pd.np.random.random() * 10)} for x in range(ROWS)], columns=COLUMNS)", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nROWS = 100\nCOLUMNS = ['X', 'Y']\n\ndef task_func(df):\n    \"\"\"\n    Given a Pandas DataFrame with random numeric values and columns X & Y, use sklearn's linear regression to match the data to a linear model.\n    \"\"\"\n    # Create a Linear Regression model\n    model = LinearRegression()\n\n    # Fit the model to the data\n    model.fit(df[['X']], df['Y'])\n\n    return model\n\n# Create a DataFrame with random numeric values\ndf = pd.DataFrame(data=[{'X': x, 'Y': x * 2 + (pd.np.random.random() * 10)} for x in range(ROWS)], columns=COLUMNS)\n\n# Test the function\nmodel = task_func(df)"}
{"task_id": "BigCodeBench/691", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\ndef task_func(df):\n    \"\"\"\n    Given a pandas DataFrame with random numeric values, run KMeans clusters on the data and return the labels.\n    \"\"\"\n    # Preprocessing\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=0)\n    labels = kmeans.fit_predict(df_scaled)\n\n    return labels", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\ndef task_func(df):\n    \"\"\"\n    Given a pandas DataFrame with random numeric values, run KMeans clusters on the data and return the labels.\n    \"\"\"\n    # Preprocessing\n    scaler = StandardScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=0)\n    labels = kmeans.fit_predict(df_scaled)\n\n    return labels"}
{"task_id": "BigCodeBench/692", "solution": "import math\nimport pandas as pd\nfrom typing import List, Tuple\ndef task_func(tuples_list: List[Tuple[float, float]]) -> pd.DataFrame:\n    \"\"\"\n    Given a list of tuples, turn them into a Pandas DataFrame with math.sin applied to each number.\n    \"\"\"\n    df = pd.DataFrame(tuples_list, columns=['x', 'y'])\n    df['x'] = df['x'].apply(math.sin)\n    df['y'] = df['y'].apply(math.sin)\n    return df\ntuples_list = [(0, 0), (30, 60), (45, 90)]", "raw_solution": "import math\nimport pandas as pd\nfrom typing import List, Tuple\n\ndef task_func(tuples_list: List[Tuple[float, float]]) -> pd.DataFrame:\n    \"\"\"\n    Given a list of tuples, turn them into a Pandas DataFrame with math.sin applied to each number.\n    \"\"\"\n    df = pd.DataFrame(tuples_list, columns=['x', 'y'])\n    df['x'] = df['x'].apply(math.sin)\n    df['y'] = df['y'].apply(math.sin)\n    return df\n\n# Test the function\ntuples_list = [(0, 0), (30, 60), (45, 90)]\ndf = task_func(tuples_list)"}
{"task_id": "BigCodeBench/693", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(tuples_list, columns):\n    \"\"\"\n    Converts a list of tuples into a Pandas DataFrame, performs a default scaling in each column, and returns the transformed DataFrame.\n    \"\"\"\n    # Convert list of tuples into DataFrame\n    df = pd.DataFrame(tuples_list, columns=columns)\n\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Perform default scaling in each column\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=columns)\n\n    return df_scaled\ntuples_list = [(1, 2), (2, 3), (3, 4)]\ncolumns = ['A', 'B']", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(tuples_list, columns):\n    \"\"\"\n    Converts a list of tuples into a Pandas DataFrame, performs a default scaling in each column, and returns the transformed DataFrame.\n    \"\"\"\n    # Convert list of tuples into DataFrame\n    df = pd.DataFrame(tuples_list, columns=columns)\n\n    # Initialize StandardScaler\n    scaler = StandardScaler()\n\n    # Perform default scaling in each column\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=columns)\n\n    return df_scaled\n\n# Test the function\ntuples_list = [(1, 2), (2, 3), (3, 4)]\ncolumns = ['A', 'B']"}
{"task_id": "BigCodeBench/694", "solution": "import itertools\nimport random\ndef task_func(t, n):\n    \"\"\"\n    Generate all combinations from a tuple with length n and return a random combination of length n.\n    \"\"\"\n    # Generate all combinations\n    combinations = list(itertools.product(t, repeat=n))\n\n    # Return a random combination\n    return random.choice(combinations)\nt = ('a', 'b', 'c')\nn = 2", "raw_solution": "import itertools\nimport random\n\ndef task_func(t, n):\n    \"\"\"\n    Generate all combinations from a tuple with length n and return a random combination of length n.\n    \"\"\"\n    # Generate all combinations\n    combinations = list(itertools.product(t, repeat=n))\n\n    # Return a random combination\n    return random.choice(combinations)\n\n# Test the function\nt = ('a', 'b', 'c')\nn = 2"}
{"task_id": "BigCodeBench/695", "solution": "import numpy as np\nfrom sklearn.decomposition import PCA\ndef task_func(tuples_list, n_components):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a list of tuples.\n    \"\"\"\n    # Convert the list of tuples to a numpy array\n    data = np.array(tuples_list)\n\n    # Initialize PCA with the desired number of components\n    pca = PCA(n_components=n_components)\n\n    # Fit and transform the data\n    transformed_data = pca.fit_transform(data)\n\n    return transformed_data", "raw_solution": "import numpy as np\nfrom sklearn.decomposition import PCA\n\ndef task_func(tuples_list, n_components):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a list of tuples.\n    \"\"\"\n    # Convert the list of tuples to a numpy array\n    data = np.array(tuples_list)\n\n    # Initialize PCA with the desired number of components\n    pca = PCA(n_components=n_components)\n\n    # Fit and transform the data\n    transformed_data = pca.fit_transform(data)\n\n    return transformed_data"}
{"task_id": "BigCodeBench/696", "solution": "import numpy as np\nimport math\nimport random\nfrom random import uniform\ndef task_func(radius, num_points):\n    points_in_circle = []\n    for _ in range(num_points):\n        r = radius * math.sqrt(random.uniform(0, 1))\n        theta = random.uniform(0, 2*math.pi)\n        x = r * math.cos(theta)\n        y = r * math.sin(theta)\n        points_in_circle.append((x, y))\n    return points_in_circle", "raw_solution": "import numpy as np\nimport math\nimport random\nfrom random import uniform\n\ndef task_func(radius, num_points):\n    points_in_circle = []\n    for _ in range(num_points):\n        r = radius * math.sqrt(random.uniform(0, 1))\n        theta = random.uniform(0, 2*math.pi)\n        x = r * math.cos(theta)\n        y = r * math.sin(theta)\n        points_in_circle.append((x, y))\n    return points_in_circle\n\n# Test the function"}
{"task_id": "BigCodeBench/697", "solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    \"\"\"\n    Use a linear regression model to predict the \"value\" of \"feature\" in the given dataframe and return the coefficients and intercept.\n    \"\"\"\n    X = df['feature'].values.reshape(-1, 1)\n    y = df['value'].values.reshape(-1, 1)\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    coef = model.coef_[0][0]\n    intercept = model.intercept_[0]\n\n    result = {\n        'coefficient': coef,\n        'intercept': intercept\n    }\n\n    return result", "raw_solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    \"\"\"\n    Use a linear regression model to predict the \"value\" of \"feature\" in the given dataframe and return the coefficients and intercept.\n    \"\"\"\n    X = df['feature'].values.reshape(-1, 1)\n    y = df['value'].values.reshape(-1, 1)\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    coef = model.coef_[0][0]\n    intercept = model.intercept_[0]\n\n    result = {\n        'coefficient': coef,\n        'intercept': intercept\n    }\n\n    return result"}
{"task_id": "BigCodeBench/698", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef task_func(df):\n    \"\"\"\n    Divides the given DataFrame into a training set and a test set (70%: 30% split),\n    separates the \"target\" column and returns the four resulting DataFrames.\n    \"\"\"\n    # Separate the \"target\" column from the rest of the DataFrame\n    y = df['target']\n    X = df.drop('target', axis=1)\n\n    # Split the DataFrame into a training set and a test set\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    return X_train, X_test, y_train, y_test", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(df):\n    \"\"\"\n    Divides the given DataFrame into a training set and a test set (70%: 30% split),\n    separates the \"target\" column and returns the four resulting DataFrames.\n    \"\"\"\n    # Separate the \"target\" column from the rest of the DataFrame\n    y = df['target']\n    X = df.drop('target', axis=1)\n\n    # Split the DataFrame into a training set and a test set\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    return X_train, X_test, y_train, y_test"}
{"task_id": "BigCodeBench/699", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\ndef task_func(x_list, y_list, n_clusters=2, random_state=0):\n    \"\"\"\n    Perform K-Means clustering on the given data by first turning it into a DataFrame with two columns \"x\" and \"y\" and then return the labels and centroids.\n    \"\"\"\n    # Create a DataFrame from the input lists\n    data = pd.DataFrame({\n        'x': x_list,\n        'y': y_list\n    })\n\n    # Perform K-Means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(data)\n\n    # Return the labels and centroids\n    return kmeans.labels_, kmeans.cluster_centers_", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\n\ndef task_func(x_list, y_list, n_clusters=2, random_state=0):\n    \"\"\"\n    Perform K-Means clustering on the given data by first turning it into a DataFrame with two columns \"x\" and \"y\" and then return the labels and centroids.\n    \"\"\"\n    # Create a DataFrame from the input lists\n    data = pd.DataFrame({\n        'x': x_list,\n        'y': y_list\n    })\n\n    # Perform K-Means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(data)\n\n    # Return the labels and centroids\n    return kmeans.labels_, kmeans.cluster_centers_"}
{"task_id": "BigCodeBench/700", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data, cols):\n    \"\"\"\n    Turn the provided data into a DataFrame and then calculate the correlation matrix of numeric columns.\n    \"\"\"\n    # Convert the data into a DataFrame\n    df = pd.DataFrame(data)\n\n    # Select only the numeric columns\n    df_num = df[cols]\n\n    # Calculate the correlation matrix\n    correlation_matrix = df_num.corr()\n\n    return correlation_matrix\ndata = {\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n}\ncols = ['A', 'B', 'C']", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data, cols):\n    \"\"\"\n    Turn the provided data into a DataFrame and then calculate the correlation matrix of numeric columns.\n    \"\"\"\n    # Convert the data into a DataFrame\n    df = pd.DataFrame(data)\n\n    # Select only the numeric columns\n    df_num = df[cols]\n\n    # Calculate the correlation matrix\n    correlation_matrix = df_num.corr()\n\n    return correlation_matrix\n\n# Test the function\ndata = {\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n}\ncols = ['A', 'B', 'C']"}
{"task_id": "BigCodeBench/701", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\ndef task_func(df, target):\n    \"\"\"\n    Perform a linear regression analysis on a given DataFrame.\n    \"\"\"\n    # Split the data into features and target\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Fit the model to the training data\n    model.fit(X_train, y_train)\n\n    # Make predictions on the testing set\n    y_pred = model.predict(X_test)\n\n    # Calculate the R-squared score of the model\n    score = r2_score(y_test, y_pred)\n\n    return score", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\ndef task_func(df, target):\n    \"\"\"\n    Perform a linear regression analysis on a given DataFrame.\n    \"\"\"\n    # Split the data into features and target\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Fit the model to the training data\n    model.fit(X_train, y_train)\n\n    # Make predictions on the testing set\n    y_pred = model.predict(X_test)\n\n    # Calculate the R-squared score of the model\n    score = r2_score(y_test, y_pred)\n\n    return score"}
{"task_id": "BigCodeBench/702", "solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\ndef task_func(df):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the DataFrame and record the first two main components.\n    \"\"\"\n    pca = PCA(n_components=2)\n    df_pca = pca.fit_transform(df)\n    df_pca = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\n    return df_pca", "raw_solution": "import pandas as pd\nfrom sklearn.decomposition import PCA\n\ndef task_func(df):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the DataFrame and record the first two main components.\n    \"\"\"\n    pca = PCA(n_components=2)\n    df_pca = pca.fit_transform(df)\n    df_pca = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\n    return df_pca"}
{"task_id": "BigCodeBench/703", "solution": "import pandas as pd\nfrom sklearn.cluster import DBSCAN\ndef task_func(data, cols):\n    \"\"\"\n    Perform DBSCAN clustering on the data by transforming it into a DataFrame and recording the clusters in a new column named 'Cluster'.\n    \"\"\"\n    # Convert data to DataFrame\n    df = pd.DataFrame(data, columns=cols)\n\n    # Initialize DBSCAN\n    dbscan = DBSCAN(eps=3, min_samples=2)\n\n    # Fit and predict clusters\n    df['Cluster'] = dbscan.fit_predict(df[cols])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import DBSCAN\n\ndef task_func(data, cols):\n    \"\"\"\n    Perform DBSCAN clustering on the data by transforming it into a DataFrame and recording the clusters in a new column named 'Cluster'.\n    \"\"\"\n    # Convert data to DataFrame\n    df = pd.DataFrame(data, columns=cols)\n\n    # Initialize DBSCAN\n    dbscan = DBSCAN(eps=3, min_samples=2)\n\n    # Fit and predict clusters\n    df['Cluster'] = dbscan.fit_predict(df[cols])\n\n    return df"}
{"task_id": "BigCodeBench/704", "solution": "import pandas as pd\nfrom itertools import combinations\ndef task_func(data, cols, percentage):\n    \"\"\"\n    Find all combinations of columns from a given DataFrame so that the absolute correlation between them is greater than a certain threshold.\n    \"\"\"\n    # Calculate correlation matrix\n    corr_matrix = data[cols].corr().abs()\n\n    # Find combinations of columns with correlation greater than the threshold\n    corr_combinations = []\n    for col_combination in combinations(corr_matrix.columns, 2):\n        if corr_matrix.loc[col_combination].max() > percentage:\n            corr_combinations.append(col_combination)\n\n    return corr_combinations\ndata = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [2, 3, 4, 5, 6],\n    'C': [3, 4, 5, 6, 7],\n    'D': [4, 5, 6, 7, 8],\n    'E': [5, 6, 7, 8, 9]\n})\ncols = ['A', 'B', 'C', 'D', 'E']\npercentage = 0.75", "raw_solution": "import pandas as pd\nfrom itertools import combinations\n\ndef task_func(data, cols, percentage):\n    \"\"\"\n    Find all combinations of columns from a given DataFrame so that the absolute correlation between them is greater than a certain threshold.\n    \"\"\"\n    # Calculate correlation matrix\n    corr_matrix = data[cols].corr().abs()\n\n    # Find combinations of columns with correlation greater than the threshold\n    corr_combinations = []\n    for col_combination in combinations(corr_matrix.columns, 2):\n        if corr_matrix.loc[col_combination].max() > percentage:\n            corr_combinations.append(col_combination)\n\n    return corr_combinations\n\n# Test the function\ndata = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [2, 3, 4, 5, 6],\n    'C': [3, 4, 5, 6, 7],\n    'D': [4, 5, 6, 7, 8],\n    'E': [5, 6, 7, 8, 9]\n})\n\ncols = ['A', 'B', 'C', 'D', 'E']\npercentage = 0.75\n"}
{"task_id": "BigCodeBench/705", "solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\ndef task_func(df, column, alpha):\n    \"\"\"\n    Tests the normality of a particular numeric column from a DataFrame using the Shapiro-Wilk test.\n    The function returns True if the column passes the normality test, False otherwise.\n    \"\"\"\n    # Check if the column exists in the DataFrame\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in DataFrame\")\n\n    # Check if the column is numeric\n    if pd.api.types.is_numeric_dtype(df[column]):\n        # Perform Shapiro-Wilk test\n        stat, p = stats.shapiro(df[column])\n        return p > alpha\n    else:\n        raise ValueError(f\"Column '{column}' is not numeric\")\ndf = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [1, 2, 3, 4, 5]})", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\n\ndef task_func(df, column, alpha):\n    \"\"\"\n    Tests the normality of a particular numeric column from a DataFrame using the Shapiro-Wilk test.\n    The function returns True if the column passes the normality test, False otherwise.\n    \"\"\"\n    # Check if the column exists in the DataFrame\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in DataFrame\")\n\n    # Check if the column is numeric\n    if pd.api.types.is_numeric_dtype(df[column]):\n        # Perform Shapiro-Wilk test\n        stat, p = stats.shapiro(df[column])\n        return p > alpha\n    else:\n        raise ValueError(f\"Column '{column}' is not numeric\")\n\n# Test the function\ndf = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [1, 2, 3, 4, 5]})"}
{"task_id": "BigCodeBench/706", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\ndef task_func(data, columns, target_column):\n    \"\"\"\n    Perform a logistic regression on a DataFrame to predict a specific target column.\n    \"\"\"\n    # Split the data into features and target\n    X = data[columns]\n    y = data[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create a logistic regression model\n    model = LogisticRegression()\n\n    # Fit the model to the training data\n    model.fit(X_train, y_train)\n\n    # Make predictions on the testing data\n    y_pred = model.predict(X_test)\n\n    # Calculate the accuracy of the model\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef task_func(data, columns, target_column):\n    \"\"\"\n    Perform a logistic regression on a DataFrame to predict a specific target column.\n    \"\"\"\n    # Split the data into features and target\n    X = data[columns]\n    y = data[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create a logistic regression model\n    model = LogisticRegression()\n\n    # Fit the model to the training data\n    model.fit(X_train, y_train)\n\n    # Make predictions on the testing data\n    y_pred = model.predict(X_test)\n\n    # Calculate the accuracy of the model\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy"}
{"task_id": "BigCodeBench/707", "solution": "import json\nimport numpy as np\nimport pandas as pd\ndef task_func(df):\n    \"\"\"\n    Given a DataFrame with random values and an 'IntCol' column, transform the 'IntCol' column by a logarithm (base 10) and write it to a `IntCol.json` file as a list. Also return the DataFrame.\n    \"\"\"\n    df['IntCol'] = np.log10(df['IntCol'])\n    df['IntCol'].to_json('IntCol.json', orient='records')\n    return df", "raw_solution": "import json\nimport numpy as np\nimport pandas as pd\n\ndef task_func(df):\n    \"\"\"\n    Given a DataFrame with random values and an 'IntCol' column, transform the 'IntCol' column by a logarithm (base 10) and write it to a `IntCol.json` file as a list. Also return the DataFrame.\n    \"\"\"\n    df['IntCol'] = np.log10(df['IntCol'])\n    df['IntCol'].to_json('IntCol.json', orient='records')\n    return df"}
{"task_id": "BigCodeBench/708", "solution": "import json\nimport csv\nimport os\nimport base64\ndef task_func(raw_string, filename, output_dir):\n    \"\"\"\n    Processes a base64-encoded JSON string, stores the data in a CSV file, and returns the path of the file.\n    \"\"\"\n    # Decode the base64 string\n    decoded_string = base64.b64decode(raw_string).decode('utf-8')\n\n    # Convert the decoded string to a JSON object\n    json_data = json.loads(decoded_string)\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create the file path\n    file_path = os.path.join(output_dir, filename)\n\n    # Write the JSON data to a CSV file\n    with open(file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        for key in json_data:\n            writer.writerow([key, json_data[key]])\n\n    return file_path", "raw_solution": "import json\nimport csv\nimport os\nimport base64\n\ndef task_func(raw_string, filename, output_dir):\n    \"\"\"\n    Processes a base64-encoded JSON string, stores the data in a CSV file, and returns the path of the file.\n    \"\"\"\n    # Decode the base64 string\n    decoded_string = base64.b64decode(raw_string).decode('utf-8')\n\n    # Convert the decoded string to a JSON object\n    json_data = json.loads(decoded_string)\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create the file path\n    file_path = os.path.join(output_dir, filename)\n\n    # Write the JSON data to a CSV file\n    with open(file_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        for key in json_data:\n            writer.writerow([key, json_data[key]])\n\n    return file_path"}
{"task_id": "BigCodeBench/709", "solution": "import base64\nimport re\nfrom html import unescape\nimport textwrap\ndef task_func(raw_string, line_length):\n    # Decode the base64 string\n    decoded_string = base64.b64decode(raw_string).decode()\n\n    # Decouple HTML entities\n    decoupled_string = unescape(decoded_string)\n\n    # Replace multiple spaces with a single space\n    single_space_string = re.sub(' +', ' ', decoupled_string)\n\n    # Strip leading and subsequent spaces\n    stripped_string = single_space_string.strip()\n\n    # Wrap text to a certain line length\n    wrapped_text = textwrap.fill(stripped_string, line_length)\n\n    return wrapped_text\nraw_string = 'WW91ciBlbmNvZGVkIG1lc3NhZ2U='", "raw_solution": "import base64\nimport re\nfrom html import unescape\nimport textwrap\n\ndef task_func(raw_string, line_length):\n    # Decode the base64 string\n    decoded_string = base64.b64decode(raw_string).decode()\n\n    # Decouple HTML entities\n    decoupled_string = unescape(decoded_string)\n\n    # Replace multiple spaces with a single space\n    single_space_string = re.sub(' +', ' ', decoupled_string)\n\n    # Strip leading and subsequent spaces\n    stripped_string = single_space_string.strip()\n\n    # Wrap text to a certain line length\n    wrapped_text = textwrap.fill(stripped_string, line_length)\n\n    return wrapped_text\n\n# Test the function\nraw_string = 'WW91ciBlbmNvZGVkIG1lc3NhZ2U='  # base64 encoded string for 'This is a test'"}
{"task_id": "BigCodeBench/710", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data_path):\n    \"\"\"\n    Normalizes a dataset from a .csv file.\n    \"\"\"\n    # Load the dataset\n    df = pd.read_csv(data_path)\n\n    # Initialize a scaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    df[df.columns] = scaler.fit_transform(df[df.columns])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data_path):\n    \"\"\"\n    Normalizes a dataset from a .csv file.\n    \"\"\"\n    # Load the dataset\n    df = pd.read_csv(data_path)\n\n    # Initialize a scaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    df[df.columns] = scaler.fit_transform(df[df.columns])\n\n    return df"}
{"task_id": "BigCodeBench/711", "solution": "import json\nimport csv\nimport os\ndef task_func(json_file, csv_file):\n    \"\"\"\n    Converts a JSON file to a CSV file.\n    \"\"\"\n    # Open the JSON file\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    # Open the CSV file\n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n\n        # Write the header\n        writer.writerow(data[0].keys())\n\n        # Write the data\n        for row in data:\n            writer.writerow(row.values())\n\n    return csv_file", "raw_solution": "import json\nimport csv\nimport os\n\ndef task_func(json_file, csv_file):\n    \"\"\"\n    Converts a JSON file to a CSV file.\n    \"\"\"\n    # Open the JSON file\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n\n    # Open the CSV file\n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n\n        # Write the header\n        writer.writerow(data[0].keys())\n\n        # Write the data\n        for row in data:\n            writer.writerow(row.values())\n\n    return csv_file"}
{"task_id": "BigCodeBench/712", "solution": "import os\nimport shutil\nimport glob\ndef task_func(source_dir, dest_dir, extension):\n    \"\"\"\n    Moves all files with a particular extension from one directory to another.\n    \"\"\"\n    if not os.path.exists(source_dir) or not os.path.exists(dest_dir):\n        raise ValueError(\"Source or destination directory does not exist.\")\n\n    source_files = glob.glob(f\"{source_dir}/*.{extension}\")\n    for file in source_files:\n        shutil.move(file, dest_dir)\n\n    return len(source_files)\nsource_dir = \"/path/to/source\"\ndest_dir = \"/path/to/destination\"\nextension = \"txt\"", "raw_solution": "import os\nimport shutil\nimport glob\n\ndef task_func(source_dir, dest_dir, extension):\n    \"\"\"\n    Moves all files with a particular extension from one directory to another.\n    \"\"\"\n    if not os.path.exists(source_dir) or not os.path.exists(dest_dir):\n        raise ValueError(\"Source or destination directory does not exist.\")\n\n    source_files = glob.glob(f\"{source_dir}/*.{extension}\")\n    for file in source_files:\n        shutil.move(file, dest_dir)\n\n    return len(source_files)\n\n# Test the function\nsource_dir = \"/path/to/source\"\ndest_dir = \"/path/to/destination\"\nextension = \"txt\""}
{"task_id": "BigCodeBench/713", "solution": "import os\nimport re\ndef task_func(log_file_path: str, keywords: list):\n    \"\"\"\n    This function reads a log file and formats lines containing certain keywords.\n    \"\"\"\n    if not os.path.exists(log_file_path):\n        return \"Log file does not exist.\"\n\n    formatted_lines = []\n    with open(log_file_path, 'r') as log_file:\n        for line in log_file:\n            for keyword in keywords:\n                if keyword in line:\n                    timestamp_match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', line)\n                    if timestamp_match:\n                        timestamp = timestamp_match.group()\n                        formatted_line = f\"{keyword} {timestamp} {line.strip()}\"\n                        formatted_lines.append(formatted_line)\n                    else:\n                        formatted_line = f\"{keyword} No timestamp found {line.strip()}\"\n                        formatted_lines.append(formatted_line)\n    return formatted_lines", "raw_solution": "import os\nimport re\n\ndef task_func(log_file_path: str, keywords: list):\n    \"\"\"\n    This function reads a log file and formats lines containing certain keywords.\n    \"\"\"\n    if not os.path.exists(log_file_path):\n        return \"Log file does not exist.\"\n\n    formatted_lines = []\n    with open(log_file_path, 'r') as log_file:\n        for line in log_file:\n            for keyword in keywords:\n                if keyword in line:\n                    timestamp_match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', line)\n                    if timestamp_match:\n                        timestamp = timestamp_match.group()\n                        formatted_line = f\"{keyword} {timestamp} {line.strip()}\"\n                        formatted_lines.append(formatted_line)\n                    else:\n                        formatted_line = f\"{keyword} No timestamp found {line.strip()}\"\n                        formatted_lines.append(formatted_line)\n    return formatted_lines"}
{"task_id": "BigCodeBench/714", "solution": "import sys\nfrom pathlib import Path\nPATH_TO_APPEND = '/path/to/whatever'\ndef task_func(path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Adds a specific path to sys.path and creates a directory in that path if it does not exist.\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Create a directory in the path if it does not exist\n    Path(path_to_append).mkdir(parents=True, exist_ok=True)\n\n    return path_to_append", "raw_solution": "import sys\nfrom pathlib import Path\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\n\ndef task_func(path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Adds a specific path to sys.path and creates a directory in that path if it does not exist.\n    \"\"\"\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Create a directory in the path if it does not exist\n    Path(path_to_append).mkdir(parents=True, exist_ok=True)\n\n    return path_to_append\n\n# Test the function"}
{"task_id": "BigCodeBench/715", "solution": "import sys\nimport subprocess\nPYTHON_VERSION = '3.8'\nPATH_TO_APPEND = '/path/to/whatever'\ndef task_func(python_version=PYTHON_VERSION, path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Switches to a specific version of Python and adds a specific path to sys.path.\n    \"\"\"\n    # Check if the Python version is already the one we want\n    if sys.version_info.major != int(python_version.split('.')[0]):\n        # If not, switch to the desired Python version\n        subprocess.run(['pyenv', 'local', python_version], check=True)\n\n    # Check if the path is already in sys.path\n    if path_to_append not in sys.path:\n        # If not, add the path to sys.path\n        sys.path.append(path_to_append)\n\n    return sys.version_info.major", "raw_solution": "import sys\nimport subprocess\n\n# Constants\nPYTHON_VERSION = '3.8'\nPATH_TO_APPEND = '/path/to/whatever'\n\ndef task_func(python_version=PYTHON_VERSION, path_to_append=PATH_TO_APPEND):\n    \"\"\"\n    Switches to a specific version of Python and adds a specific path to sys.path.\n    \"\"\"\n    # Check if the Python version is already the one we want\n    if sys.version_info.major != int(python_version.split('.')[0]):\n        # If not, switch to the desired Python version\n        subprocess.run(['pyenv', 'local', python_version], check=True)\n\n    # Check if the path is already in sys.path\n    if path_to_append not in sys.path:\n        # If not, add the path to sys.path\n        sys.path.append(path_to_append)\n\n    return sys.version_info.major\n\n# Test the function"}
{"task_id": "BigCodeBench/716", "solution": "import sys\nimport json\nfrom datetime import datetime\nPATH_TO_APPEND = '/path/to/whatever'\nJSON_FILE = '/path/to/json_file.json'\ndef task_func(path_to_append=PATH_TO_APPEND, json_file=JSON_FILE):\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Load the JSON file\n    with open(json_file, 'r') as file:\n        json_data = json.load(file)\n\n    # Update the 'last_updated' key with the current date and time\n    json_data['last_updated'] = str(datetime.now())\n\n    # Write the updated JSON data back to the file\n    with open(json_file, 'w') as file:\n        json.dump(json_data, file)\n\n    return json_data", "raw_solution": "import sys\nimport json\nfrom datetime import datetime\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\nJSON_FILE = '/path/to/json_file.json'\n\ndef task_func(path_to_append=PATH_TO_APPEND, json_file=JSON_FILE):\n    # Append the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Load the JSON file\n    with open(json_file, 'r') as file:\n        json_data = json.load(file)\n\n    # Update the 'last_updated' key with the current date and time\n    json_data['last_updated'] = str(datetime.now())\n\n    # Write the updated JSON data back to the file\n    with open(json_file, 'w') as file:\n        json.dump(json_data, file)\n\n    return json_data"}
{"task_id": "BigCodeBench/717", "solution": "import sys\nfrom configparser import ConfigParser\nimport os\nPATH_TO_APPEND = '/path/to/whatever'\nCONFIG_FILE = '/path/to/config.ini'\ndef task_func(path_to_append=PATH_TO_APPEND, config_file=CONFIG_FILE):\n    \"\"\"\n    Adds a specific path to sys.path and updates a configuration file with this path.\n    \"\"\"\n    # Add the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Create a ConfigParser object\n    config = ConfigParser()\n\n    # Read the configuration file\n    config.read(config_file)\n\n    # Update the configuration file with the new path\n    config.set('DEFAULT', 'path', path_to_append)\n\n    # Write the changes to the configuration file\n    with open(config_file, 'w') as file:\n        config.write(file)\n\n    return config, config_file", "raw_solution": "import sys\nfrom configparser import ConfigParser\nimport os\n\n# Constants\nPATH_TO_APPEND = '/path/to/whatever'\nCONFIG_FILE = '/path/to/config.ini'\n\ndef task_func(path_to_append=PATH_TO_APPEND, config_file=CONFIG_FILE):\n    \"\"\"\n    Adds a specific path to sys.path and updates a configuration file with this path.\n    \"\"\"\n    # Add the path to sys.path\n    sys.path.append(path_to_append)\n\n    # Create a ConfigParser object\n    config = ConfigParser()\n\n    # Read the configuration file\n    config.read(config_file)\n\n    # Update the configuration file with the new path\n    config.set('DEFAULT', 'path', path_to_append)\n\n    # Write the changes to the configuration file\n    with open(config_file, 'w') as file:\n        config.write(file)\n\n    return config, config_file\n\n# Test the function\nconfig, config_file = task_func()"}
{"task_id": "BigCodeBench/718", "solution": "import re\nimport numpy as np\nfrom scipy.stats import ttest_rel\ndef task_func(text1, text2):\n    # Split the text into words\n    words1 = re.findall(r'\\b\\w+\\b', text1)\n    words2 = re.findall(r'\\b\\w+\\b', text2)\n\n    # Check if the lengths of the words lists are equal\n    if len(words1) != len(words2):\n        return np.nan, np.nan\n\n    # Convert the word lists to numpy arrays\n    words1 = np.array(words1)\n    words2 = np.array(words2)\n\n    # Perform the paired t-test\n    t_statistic, p_value = ttest_rel(words1, words2)\n\n    return t_statistic, p_value", "raw_solution": "import re\nimport numpy as np\nfrom scipy.stats import ttest_rel\n\ndef task_func(text1, text2):\n    # Split the text into words\n    words1 = re.findall(r'\\b\\w+\\b', text1)\n    words2 = re.findall(r'\\b\\w+\\b', text2)\n\n    # Check if the lengths of the words lists are equal\n    if len(words1) != len(words2):\n        return np.nan, np.nan\n\n    # Convert the word lists to numpy arrays\n    words1 = np.array(words1)\n    words2 = np.array(words2)\n\n    # Perform the paired t-test\n    t_statistic, p_value = ttest_rel(words1, words2)\n\n    return t_statistic, p_value"}
{"task_id": "BigCodeBench/719", "solution": "import re\nimport os\nimport glob\ndef task_func(directory, word):\n    \"\"\"\n    Count the number of files in a directory that contain a specific word.\n    \"\"\"\n    count = 0\n    for filename in glob.glob(os.path.join(directory, '*')):\n        if os.path.isfile(filename):\n            with open(filename, 'r') as file:\n                if re.search(word, file.read()):\n                    count += 1\n    return count", "raw_solution": "import re\nimport os\nimport glob\n\ndef task_func(directory, word):\n    \"\"\"\n    Count the number of files in a directory that contain a specific word.\n    \"\"\"\n    count = 0\n    for filename in glob.glob(os.path.join(directory, '*')):\n        if os.path.isfile(filename):\n            with open(filename, 'r') as file:\n                if re.search(word, file.read()):\n                    count += 1\n    return count\n\n# Test the function"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Create the directory if it doesn't exist\n    if not os.path.exists('task_func_data'):\n        os.makedirs('task_func_data')\n\n    # Create the CSV file\n    file_path = 'task_func_data/Output.txt'\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Timestamp\", \"Temperature\", \"Humidity\"])\n\n        # Generate random data and write it to the CSV file\n        for _ in range(10):\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            temperature = random.uniform(0, 100)\n            humidity = random.uniform(0, 100)\n            writer.writerow([timestamp, temperature, humidity])\n\n    # Return the path to the CSV file\n    print(f\"CSV file created at: {file_path}\")\n\n    # Delete the CSV file\n    os.remove(file_path)\n    print(f\"CSV file deleted at: {file_path}\")\n\n    return file_path", "raw_solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Create the directory if it doesn't exist\n    if not os.path.exists('task_func_data'):\n        os.makedirs('task_func_data')\n\n    # Create the CSV file\n    file_path = 'task_func_data/Output.txt'\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Timestamp\", \"Temperature\", \"Humidity\"])\n\n        # Generate random data and write it to the CSV file\n        for _ in range(10):\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            temperature = random.uniform(0, 100)\n            humidity = random.uniform(0, 100)\n            writer.writerow([timestamp, temperature, humidity])\n\n    # Return the path to the CSV file\n    print(f\"CSV file created at: {file_path}\")\n\n    # Delete the CSV file\n    os.remove(file_path)\n    print(f\"CSV file deleted at: {file_path}\")\n\n    return file_path\n\n# Test the function\nfile_path = task_func()"}
{"task_id": "BigCodeBench/721", "solution": "import os\nimport csv\nfrom collections import Counter\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return None\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        words = [word for row in reader for word in row]\n\n    if not words:\n        return None\n\n    word_counts = Counter(words)\n    most_common_word = word_counts.most_common(1)\n\n    if not most_common_word:\n        return None\n\n    return most_common_word[0]", "raw_solution": "import os\nimport csv\nfrom collections import Counter\n\ndef task_func(file_path):\n    if not os.path.exists(file_path):\n        return None\n\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        words = [word for row in reader for word in row]\n\n    if not words:\n        return None\n\n    word_counts = Counter(words)\n    most_common_word = word_counts.most_common(1)\n\n    if not most_common_word:\n        return None\n\n    return most_common_word[0]"}
{"task_id": "BigCodeBench/722", "solution": "import urllib.request\nimport os\nimport re\nTARGET_FILE = 'downloaded_file.txt'\nSEARCH_PATTERN = r'\\bERROR\\b'\ndef task_func(url):\n    \"\"\"\n    Downloads a text file from the given URL and counts the occurrences of the word 'ERROR'.\n    \"\"\"\n    # Download the file\n    urllib.request.urlretrieve(url, TARGET_FILE)\n\n    # Read the file\n    with open(TARGET_FILE, 'r') as file:\n        content = file.read()\n\n    # Count the occurrences of 'ERROR'\n    occurrences = len(re.findall(SEARCH_PATTERN, content, re.IGNORECASE))\n\n    # Remove the downloaded file\n    os.remove(TARGET_FILE)\n\n    return occurrences", "raw_solution": "import urllib.request\nimport os\nimport re\n\n# Constants\nTARGET_FILE = 'downloaded_file.txt'\nSEARCH_PATTERN = r'\\bERROR\\b'\n\ndef task_func(url):\n    \"\"\"\n    Downloads a text file from the given URL and counts the occurrences of the word 'ERROR'.\n    \"\"\"\n    # Download the file\n    urllib.request.urlretrieve(url, TARGET_FILE)\n\n    # Read the file\n    with open(TARGET_FILE, 'r') as file:\n        content = file.read()\n\n    # Count the occurrences of 'ERROR'\n    occurrences = len(re.findall(SEARCH_PATTERN, content, re.IGNORECASE))\n\n    # Remove the downloaded file\n    os.remove(TARGET_FILE)\n\n    return occurrences"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Scrapes data from a given URL and saves the scraped data to a CSV file.\n    \"\"\"\n    # Send a request to the website\n    response = urllib.request.urlopen(url)\n    # Parse the HTML content of the page with BeautifulSoup\n    soup = BeautifulSoup(response, 'html.parser')\n\n    # Find the table in the HTML\n    table = soup.find('table')\n\n    # Find all rows in the table\n    rows = table.find_all('tr')\n\n    # Open the CSV file for writing\n    with open(CSV_FILE_PATH, 'w', newline='') as csvfile:\n        # Create a CSV writer\n        writer = csv.writer(csvfile)\n\n        # Write the header row\n        headers = [header.text for header in rows[0].find_all('th')]\n        writer.writerow(headers)\n\n        # Write the data rows\n        for row in rows[1:]:\n            data = [cell.text for cell in row.find_all('td')]\n            writer.writerow(data)\n\n    # Return the path of the CSV file\n    return CSV_FILE_PATH", "raw_solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    Scrapes data from a given URL and saves the scraped data to a CSV file.\n    \"\"\"\n    # Send a request to the website\n    response = urllib.request.urlopen(url)\n    # Parse the HTML content of the page with BeautifulSoup\n    soup = BeautifulSoup(response, 'html.parser')\n\n    # Find the table in the HTML\n    table = soup.find('table')\n\n    # Find all rows in the table\n    rows = table.find_all('tr')\n\n    # Open the CSV file for writing\n    with open(CSV_FILE_PATH, 'w', newline='') as csvfile:\n        # Create a CSV writer\n        writer = csv.writer(csvfile)\n\n        # Write the header row\n        headers = [header.text for header in rows[0].find_all('th')]\n        writer.writerow(headers)\n\n        # Write the data rows\n        for row in rows[1:]:\n            data = [cell.text for cell in row.find_all('td')]\n            writer.writerow(data)\n\n    # Return the path of the CSV file\n    return CSV_FILE_PATH"}
{"task_id": "BigCodeBench/724", "solution": "import os\nimport json\ndef task_func(config_path: str) -> dict:\n    \"\"\"\n    Loads a JSON configuration file and returns the configuration dictionary.\n    Raises FileNotFoundError if the provided configuration file does not exist.\n    \"\"\"\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\"The configuration file {config_path} does not exist.\")\n\n    with open(config_path, 'r') as file:\n        config = json.load(file)\n\n    return config", "raw_solution": "import os\nimport json\n\ndef task_func(config_path: str) -> dict:\n    \"\"\"\n    Loads a JSON configuration file and returns the configuration dictionary.\n    Raises FileNotFoundError if the provided configuration file does not exist.\n    \"\"\"\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\"The configuration file {config_path} does not exist.\")\n\n    with open(config_path, 'r') as file:\n        config = json.load(file)\n\n    return config"}
{"task_id": "BigCodeBench/725", "solution": "import codecs\nimport os\nimport glob\nDIRECTORY_PATH = './files/'\ndef task_func(directory=DIRECTORY_PATH, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    Converts the encoding of all text files in a specified directory from one encoding to another.\n    The function modifies the files in-place.\n    \"\"\"\n    # Get all text files in the directory\n    for filename in glob.glob(os.path.join(directory, '*.txt')):\n        # Read the file in the original encoding\n        with codecs.open(filename, 'r', from_encoding) as f:\n            content = f.read()\n\n        # Write the file in the new encoding\n        with codecs.open(filename, 'w', to_encoding) as f:\n            f.write(content)", "raw_solution": "import codecs\nimport os\nimport glob\n\n# Constants\nDIRECTORY_PATH = './files/'\n\ndef task_func(directory=DIRECTORY_PATH, from_encoding='cp1251', to_encoding='utf8'):\n    \"\"\"\n    Converts the encoding of all text files in a specified directory from one encoding to another.\n    The function modifies the files in-place.\n    \"\"\"\n    # Get all text files in the directory\n    for filename in glob.glob(os.path.join(directory, '*.txt')):\n        # Read the file in the original encoding\n        with codecs.open(filename, 'r', from_encoding) as f:\n            content = f.read()\n\n        # Write the file in the new encoding\n        with codecs.open(filename, 'w', to_encoding) as f:\n            f.write(content)\n\n# Test the function\ntask_func(DIRECTORY_PATH, 'cp1251', 'utf8')"}
{"task_id": "BigCodeBench/726", "solution": "import re\nimport random\nfrom nltk.corpus import words\nfrom random import sample\nimport nltk\nSAMPLE_ENGLISH_WORDS = set(words.words())\ndef task_func(s, n):\n    \"\"\"\n    Extracts up to n different English words from a string, ignoring case.\n    \"\"\"\n    # Convert the string to lowercase and split it into words\n    s = s.lower()\n    words = re.findall(r'\\b\\w+\\b', s)\n\n    # Filter out non-English words and get the distinct ones\n    english_words = [word for word in words if word in SAMPLE_ENGLISH_WORDS]\n    distinct_english_words = list(set(english_words))\n\n    # If there are fewer than n different English words, return all of them\n    if len(distinct_english_words) < n:\n        return distinct_english_words\n\n    # Otherwise, return a random sample of n different English words\n    return random.sample(distinct_english_words, n)", "raw_solution": "import re\nimport random\nfrom nltk.corpus import words\nfrom random import sample\n# Ensure the words corpus is downloaded\nimport nltk\nnltk.download('words')\n# Constants\nSAMPLE_ENGLISH_WORDS = set(words.words())  # Correct initialization\n\ndef task_func(s, n):\n    \"\"\"\n    Extracts up to n different English words from a string, ignoring case.\n    \"\"\"\n    # Convert the string to lowercase and split it into words\n    s = s.lower()\n    words = re.findall(r'\\b\\w+\\b', s)\n\n    # Filter out non-English words and get the distinct ones\n    english_words = [word for word in words if word in SAMPLE_ENGLISH_WORDS]\n    distinct_english_words = list(set(english_words))\n\n    # If there are fewer than n different English words, return all of them\n    if len(distinct_english_words) < n:\n        return distinct_english_words\n\n    # Otherwise, return a random sample of n different English words\n    return random.sample(distinct_english_words, n)"}
{"task_id": "BigCodeBench/727", "solution": "import re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nSENTENCES = ['This is a sentence', 'Another sentence here', 'More sentences']\ndef task_func(s: str) -> np.ndarray:\n    \"\"\"\n    Vectorize a string using the Bag-of-Words model.\n    \"\"\"\n    # Create a CountVectorizer object\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    vectorizer.fit(SENTENCES)\n    vector = vectorizer.transform([s])\n\n    # Return the vector as a numpy array\n    return vector.toarray()", "raw_solution": "import re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Constants\nSENTENCES = ['This is a sentence', 'Another sentence here', 'More sentences']\n\ndef task_func(s: str) -> np.ndarray:\n    \"\"\"\n    Vectorize a string using the Bag-of-Words model.\n    \"\"\"\n    # Create a CountVectorizer object\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the sentences\n    vectorizer.fit(SENTENCES)\n    vector = vectorizer.transform([s])\n\n    # Return the vector as a numpy array\n    return vector.toarray()\n\n# Test the function"}
{"task_id": "BigCodeBench/728", "solution": "import csv\nimport io\ndef task_func(filename='sample.csv', from_encoding='cp1251', to_encoding='utf8', delimiter=','):\n    with open(filename, 'r', encoding=from_encoding) as file:\n        csv_data = file.read()\n\n    csv_data_utf8 = csv_data.encode(to_encoding).decode(to_encoding)\n\n    csv_io = io.StringIO(csv_data_utf8)\n    csv_reader = csv.reader(csv_io, delimiter=delimiter)\n    headers = next(csv_reader)\n    data = [dict(zip(headers, row)) for row in csv_reader]\n\n    return data, csv_data_utf8", "raw_solution": "import csv\nimport io\n\ndef task_func(filename='sample.csv', from_encoding='cp1251', to_encoding='utf8', delimiter=','):\n    with open(filename, 'r', encoding=from_encoding) as file:\n        csv_data = file.read()\n\n    csv_data_utf8 = csv_data.encode(to_encoding).decode(to_encoding)\n\n    csv_io = io.StringIO(csv_data_utf8)\n    csv_reader = csv.reader(csv_io, delimiter=delimiter)\n    headers = next(csv_reader)\n    data = [dict(zip(headers, row)) for row in csv_reader]\n\n    return data, csv_data_utf8"}
{"task_id": "BigCodeBench/729", "solution": "import pickle\nimport os\nimport random\nimport string\ndef task_func(strings, filename=None):\n    \"\"\"\n    Saves the list of strings in a pickle file and then reads it back for validation.\n    If a filename is not provided, a unique filename is generated.\n    \"\"\"\n    if filename is None:\n        filename = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)) + '.pkl'\n\n    # Save the list of strings to a pickle file\n    with open(filename, 'wb') as f:\n        pickle.dump(strings, f)\n\n    # Load the list of strings from the pickle file\n    with open(filename, 'rb') as f:\n        loaded_strings = pickle.load(f)\n\n    # Remove the pickle file\n    os.remove(filename)\n\n    return loaded_strings", "raw_solution": "import pickle\nimport os\nimport random\nimport string\n\ndef task_func(strings, filename=None):\n    \"\"\"\n    Saves the list of strings in a pickle file and then reads it back for validation.\n    If a filename is not provided, a unique filename is generated.\n    \"\"\"\n    if filename is None:\n        filename = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)) + '.pkl'\n\n    # Save the list of strings to a pickle file\n    with open(filename, 'wb') as f:\n        pickle.dump(strings, f)\n\n    # Load the list of strings from the pickle file\n    with open(filename, 'rb') as f:\n        loaded_strings = pickle.load(f)\n\n    # Remove the pickle file\n    os.remove(filename)\n\n    return loaded_strings"}
{"task_id": "BigCodeBench/730", "solution": "import pickle\nimport os\nfrom datetime import datetime\nFILE_NAME = 'save.pkl'\ndef task_func(dt):\n    \"\"\"\n    Saves the datetime object `dt` in a pickle file named 'save.pkl' and then reads it back.\n    Returns the loaded datetime object.\n    \"\"\"\n    # Save the datetime object to a pickle file\n    with open(FILE_NAME, 'wb') as f:\n        pickle.dump(dt, f)\n\n    # Load the datetime object from the pickle file\n    with open(FILE_NAME, 'rb') as f:\n        loaded_dt = pickle.load(f)\n\n    # Remove the pickle file\n    os.remove(FILE_NAME)\n\n    return loaded_dt\ndt = datetime.now()", "raw_solution": "import pickle\nimport os\nfrom datetime import datetime\n\n# Constants\nFILE_NAME = 'save.pkl'\n\ndef task_func(dt):\n    \"\"\"\n    Saves the datetime object `dt` in a pickle file named 'save.pkl' and then reads it back.\n    Returns the loaded datetime object.\n    \"\"\"\n    # Save the datetime object to a pickle file\n    with open(FILE_NAME, 'wb') as f:\n        pickle.dump(dt, f)\n\n    # Load the datetime object from the pickle file\n    with open(FILE_NAME, 'rb') as f:\n        loaded_dt = pickle.load(f)\n\n    # Remove the pickle file\n    os.remove(FILE_NAME)\n\n    return loaded_dt\n\n# Test the function\ndt = datetime.now()\nloaded_dt = task_func(dt)"}
{"task_id": "BigCodeBench/731", "solution": "import pickle\nimport os\nfrom sklearn.datasets import make_classification\nFILE_NAME = 'save.pkl'\ndef task_func(data, target):\n    # Save data and target to pickle file\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump((data, target), file)\n\n    # Load data and target from pickle file\n    with open(FILE_NAME, 'rb') as file:\n        loaded_data, loaded_target = pickle.load(file)\n\n    # Remove the pickle file\n    os.remove(FILE_NAME)\n\n    # Return loaded data and target\n    return (loaded_data, loaded_target)", "raw_solution": "import pickle\nimport os\nfrom sklearn.datasets import make_classification\n\n# Constants\nFILE_NAME = 'save.pkl'\nDATA, TARGET = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=1)\n\ndef task_func(data, target):\n    # Save data and target to pickle file\n    with open(FILE_NAME, 'wb') as file:\n        pickle.dump((data, target), file)\n\n    # Load data and target from pickle file\n    with open(FILE_NAME, 'rb') as file:\n        loaded_data, loaded_target = pickle.load(file)\n\n    # Remove the pickle file\n    os.remove(FILE_NAME)\n\n    # Return loaded data and target\n    return (loaded_data, loaded_target)\n\n# Test the function\ndata, target = task_func(DATA, TARGET)"}
{"task_id": "BigCodeBench/732", "solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nSTEMMER = PorterStemmer()\ndef task_func(content):\n    # Remove punctuation\n    content = re.sub('['+string.punctuation+']', '', content)\n    # Split the content into words\n    words = content.split()\n    # Stem every word except the last one\n    stems = [STEMMER.stem(word) if i != len(words) - 1 else word for i, word in enumerate(words)]\n    # Count the frequency of each stem\n    freq_dict = dict(Counter(stems))\n    return freq_dict\ncontent = \"This is a test sentence. It is used to test the function.\"", "raw_solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\n\nSTEMMER = PorterStemmer()\n\ndef task_func(content):\n    # Remove punctuation\n    content = re.sub('['+string.punctuation+']', '', content)\n    # Split the content into words\n    words = content.split()\n    # Stem every word except the last one\n    stems = [STEMMER.stem(word) if i != len(words) - 1 else word for i, word in enumerate(words)]\n    # Count the frequency of each stem\n    freq_dict = dict(Counter(stems))\n    return freq_dict\n\n# Test the function\ncontent = \"This is a test sentence. It is used to test the function.\""}
{"task_id": "BigCodeBench/733", "solution": "import re\nimport string\ndef task_func(content):\n    # Define stop words\n    stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n                  \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n                  \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \n                  \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \n                  \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n                  \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \n                  \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \n                  \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \n                  \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \n                  \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \n                  \"now\"]\n\n    # Remove punctuation\n    content = re.sub(r'[^\\w\\s]', '', content)\n\n    # Split the sentence into words\n    words = content.split()\n\n    # Count the non-stop words\n    count = sum(1 for word in words if word.lower() not in stop_words)\n\n    return count", "raw_solution": "import re\nimport string\n\ndef task_func(content):\n    # Define stop words\n    stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n                  \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n                  \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \n                  \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \n                  \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n                  \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \n                  \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \n                  \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \n                  \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \n                  \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \n                  \"now\"]\n\n    # Remove punctuation\n    content = re.sub(r'[^\\w\\s]', '', content)\n\n    # Split the sentence into words\n    words = content.split()\n\n    # Count the non-stop words\n    count = sum(1 for word in words if word.lower() not in stop_words)\n\n    return count\n\n# Test the function"}
{"task_id": "BigCodeBench/734", "solution": "import nltk\nfrom collections import Counter\ndef task_func(content):\n    # Tokenize the sentence\n    tokens = nltk.word_tokenize(content)\n    # Tag the tokens with POS tags\n    tags = nltk.pos_tag(tokens)\n    # Count the POS tags\n    pos_counts = Counter(tag for word, tag in tags)\n    # Remove the last word from the count\n    pos_counts.pop(tokens[-1], None)\n    return pos_counts", "raw_solution": "import nltk\n# Download necessary NLTK data (if not already present)\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom collections import Counter\n\ndef task_func(content):\n    # Tokenize the sentence\n    tokens = nltk.word_tokenize(content)\n    # Tag the tokens with POS tags\n    tags = nltk.pos_tag(tokens)\n    # Count the POS tags\n    pos_counts = Counter(tag for word, tag in tags)\n    # Remove the last word from the count\n    pos_counts.pop(tokens[-1], None)\n    return pos_counts\n\n# Test the function"}
{"task_id": "BigCodeBench/735", "solution": "import numpy as np\nfrom itertools import chain\ndef task_func(L):\n    flat_list = list(chain(*L))\n    mean = np.mean(flat_list)\n    variance = np.var(flat_list)\n    return {'mean': mean, 'variance': variance}\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]", "raw_solution": "import numpy as np\nfrom itertools import chain\n\ndef task_func(L):\n    flat_list = list(chain(*L))\n    mean = np.mean(flat_list)\n    variance = np.var(flat_list)\n    return {'mean': mean, 'variance': variance}\n\n# Testing the function\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/736", "solution": "import numpy as np\nfrom scipy import stats\ndef task_func(L):\n    \"\"\"\n    Calculates the mode of all elements in a nested list 'L'.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the mode\n    mode = stats.mode(flat_list)\n    \n    return mode.mode[0]\nL = [[1, 2, 2, 3], [4, 5, 5, 5], [6, 7, 8, 9]]", "raw_solution": "import numpy as np\nfrom scipy import stats\n\ndef task_func(L):\n    \"\"\"\n    Calculates the mode of all elements in a nested list 'L'.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the mode\n    mode = stats.mode(flat_list)\n    \n    return mode.mode[0]\n\n# Test the function\nL = [[1, 2, 2, 3], [4, 5, 5, 5], [6, 7, 8, 9]]"}
{"task_id": "BigCodeBench/737", "solution": "import numpy as np\nimport math\ndef task_func(L):\n    \"\"\"\n    Calculates the median of all elements in a nested list 'L'.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n\n    # Calculate the median\n    median = np.median(flat_list)\n\n    return median\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]", "raw_solution": "import numpy as np\nimport math\n\ndef task_func(L):\n    \"\"\"\n    Calculates the median of all elements in a nested list 'L'.\n    \"\"\"\n    # Flatten the list\n    flat_list = [item for sublist in L for item in sublist]\n\n    # Calculate the median\n    median = np.median(flat_list)\n\n    return median\n\n# Test the function\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/738", "solution": "import numpy as np\nfrom scipy.stats import iqr\ndef task_func(L):\n    \"\"\"\n    Calculates the interquartile range of all elements in a nested list 'L'.\n    \"\"\"\n    # Flatten the nested list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the interquartile range\n    iqr_value = iqr(flat_list)\n    \n    return iqr_value\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]", "raw_solution": "import numpy as np\nfrom scipy.stats import iqr\n\ndef task_func(L):\n    \"\"\"\n    Calculates the interquartile range of all elements in a nested list 'L'.\n    \"\"\"\n    # Flatten the nested list\n    flat_list = [item for sublist in L for item in sublist]\n    \n    # Calculate the interquartile range\n    iqr_value = iqr(flat_list)\n    \n    return iqr_value\n\n# Test the function\nL = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": "BigCodeBench/739", "solution": "import struct\nimport random\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\ndef task_func(hex_key=None):\n    if hex_key is None:\n        hex_key = random.choice(KEYS)\n    float_val = struct.unpack('f', bytes.fromhex(hex_key))[0]\n    rounded_float = round(float_val, 2)\n    return rounded_float", "raw_solution": "import struct\nimport random\n\n# Constants\nKEYS = ['470FC614', '4A0FC614', '4B9FC614', '4C8FC614', '4D7FC614']\n\ndef task_func(hex_key=None):\n    if hex_key is None:\n        hex_key = random.choice(KEYS)\n    float_val = struct.unpack('f', bytes.fromhex(hex_key))[0]\n    rounded_float = round(float_val, 2)\n    return rounded_float\n\n# Test the function"}
{"task_id": "BigCodeBench/740", "solution": "from collections import Counter\nimport heapq\nimport random\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\ndef task_func(my_dict):\n    # Generate a random integer for each letter in the dictionary\n    for letter in LETTERS:\n        if letter not in my_dict:\n            my_dict[letter] = random.randint(1, 100)\n\n    # Count the frequency of each letter\n    counter = Counter(my_dict)\n\n    # Find the 3 most common letters\n    most_common_letters = heapq.nlargest(3, counter.items(), key=lambda x: x[1])\n    most_common_letters = [letter for letter, count in most_common_letters]\n\n    return most_common_letters\nmy_dict = {'a': 5, 'b': 10, 'c': 15, 'd': 20, 'e': 25, 'f': 30}", "raw_solution": "from collections import Counter\nimport heapq\nimport random\n\n# Constants\nLETTERS = list('abcdefghijklmnopqrstuvwxyz')\n\ndef task_func(my_dict):\n    # Generate a random integer for each letter in the dictionary\n    for letter in LETTERS:\n        if letter not in my_dict:\n            my_dict[letter] = random.randint(1, 100)\n\n    # Count the frequency of each letter\n    counter = Counter(my_dict)\n\n    # Find the 3 most common letters\n    most_common_letters = heapq.nlargest(3, counter.items(), key=lambda x: x[1])\n    most_common_letters = [letter for letter, count in most_common_letters]\n\n    return most_common_letters\n\n# Test the function\nmy_dict = {'a': 5, 'b': 10, 'c': 15, 'd': 20, 'e': 25, 'f': 30}"}
{"task_id": "BigCodeBench/741", "solution": "from itertools import groupby\nfrom operator import itemgetter\nKEY_FUNC = itemgetter(0)\ndef task_func(my_dict):\n    \"\"\"\n    Group the dictionary entries after the first character of the key and add the values for each group.\n    \"\"\"\n    # Sort the dictionary by key\n    sorted_dict = dict(sorted(my_dict.items(), key=KEY_FUNC))\n\n    # Group the sorted dictionary by the first character of the key\n    grouped_dict = {key: list(group) for key, group in groupby(sorted_dict.items(), lambda x: x[0][1:])}\n\n    # Aggregate the values for each group\n    aggregated_dict = {key: sum(int(item[1]) for item in group) for key, group in grouped_dict.items()}\n\n    return aggregated_dict", "raw_solution": "from itertools import groupby\nfrom operator import itemgetter\n\n# Constants\nKEY_FUNC = itemgetter(0)\n\ndef task_func(my_dict):\n    \"\"\"\n    Group the dictionary entries after the first character of the key and add the values for each group.\n    \"\"\"\n    # Sort the dictionary by key\n    sorted_dict = dict(sorted(my_dict.items(), key=KEY_FUNC))\n\n    # Group the sorted dictionary by the first character of the key\n    grouped_dict = {key: list(group) for key, group in groupby(sorted_dict.items(), lambda x: x[0][1:])}\n\n    # Aggregate the values for each group\n    aggregated_dict = {key: sum(int(item[1]) for item in group) for key, group in grouped_dict.items()}\n\n    return aggregated_dict\n\n# Test the function"}
{"task_id": "BigCodeBench/742", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(list_of_pairs):\n    if not list_of_pairs:\n        raise Exception(\"Input array is empty\")\n    for pair in list_of_pairs:\n        if not isinstance(pair[1], (int, float)):\n            raise ValueError(\"Values are not numeric\")\n    data = [(pair[0], pair[1]) for pair in list_of_pairs]\n    df = pd.DataFrame(data, columns=['Category', 'Value'])\n    scaler = MinMaxScaler()\n    df[['Value']] = scaler.fit_transform(df[['Value']])\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(list_of_pairs):\n    if not list_of_pairs:\n        raise Exception(\"Input array is empty\")\n    for pair in list_of_pairs:\n        if not isinstance(pair[1], (int, float)):\n            raise ValueError(\"Values are not numeric\")\n    data = [(pair[0], pair[1]) for pair in list_of_pairs]\n    df = pd.DataFrame(data, columns=['Category', 'Value'])\n    scaler = MinMaxScaler()\n    df[['Value']] = scaler.fit_transform(df[['Value']])\n    return df"}
{"task_id": "BigCodeBench/743", "solution": "import json\nimport os\nPREFIXES = [\"is_\", \"has_\", \"can_\", \"should_\"]\ndef task_func(directory):\n    \"\"\"\n    Read all JSON files from the specified directory, count the occurrence of keys starting with certain prefixes,\n    and return a dictionary of statistics.\n    \"\"\"\n    stats = {prefix: 0 for prefix in PREFIXES}\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                data = json.load(file)\n                for key in data.keys():\n                    for prefix in PREFIXES:\n                        if key.startswith(prefix):\n                            stats[prefix] += 1\n\n    return stats", "raw_solution": "import json\nimport os\n\n# Constants\nPREFIXES = [\"is_\", \"has_\", \"can_\", \"should_\"]\n\ndef task_func(directory):\n    \"\"\"\n    Read all JSON files from the specified directory, count the occurrence of keys starting with certain prefixes,\n    and return a dictionary of statistics.\n    \"\"\"\n    stats = {prefix: 0 for prefix in PREFIXES}\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                data = json.load(file)\n                for key in data.keys():\n                    for prefix in PREFIXES:\n                        if key.startswith(prefix):\n                            stats[prefix] += 1\n\n    return stats"}
{"task_id": "BigCodeBench/744", "solution": "import nltk\nfrom string import punctuation\nimport pandas as pd\ndef task_func(text):\n    if not isinstance(text, str):\n        raise ValueError(\"Input should be a string\")\n\n    # Remove punctuation from text\n    text = ''.join(ch for ch in text if ch not in punctuation)\n\n    # Tokenize text into words\n    words = nltk.word_tokenize(text)\n\n    # Filter words that start with '$'\n    dollar_words = [word for word in words if word.startswith('$')]\n\n    # Compute frequency of each word\n    freq_dist = nltk.FreqDist(dollar_words)\n\n    # Create a DataFrame from the frequency distribution\n    df = pd.DataFrame(list(freq_dist.items()), columns=[\"Word\", \"Frequency\"])\n\n    return df", "raw_solution": "import nltk\nfrom string import punctuation\nimport pandas as pd\n\ndef task_func(text):\n    if not isinstance(text, str):\n        raise ValueError(\"Input should be a string\")\n\n    # Remove punctuation from text\n    text = ''.join(ch for ch in text if ch not in punctuation)\n\n    # Tokenize text into words\n    words = nltk.word_tokenize(text)\n\n    # Filter words that start with '$'\n    dollar_words = [word for word in words if word.startswith('$')]\n\n    # Compute frequency of each word\n    freq_dist = nltk.FreqDist(dollar_words)\n\n    # Create a DataFrame from the frequency distribution\n    df = pd.DataFrame(list(freq_dist.items()), columns=[\"Word\", \"Frequency\"])\n\n    return df"}
{"task_id": "BigCodeBench/745", "solution": "import subprocess\nimport random\nimport os\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\nSCRIPTS_DIR = '/path/to/scripts'\ndef task_func():\n    # Select a random script\n    script = random.choice(SCRIPTS)\n    script_path = os.path.join(SCRIPTS_DIR, script)\n\n    # Run the script\n    subprocess.run(['bash', script_path])\n\n    return script_path", "raw_solution": "import subprocess\nimport random\nimport os\n\n# Constants\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\nSCRIPTS_DIR = '/path/to/scripts'  # replace with your scripts directory\n\ndef task_func():\n    # Select a random script\n    script = random.choice(SCRIPTS)\n    script_path = os.path.join(SCRIPTS_DIR, script)\n\n    # Run the script\n    subprocess.run(['bash', script_path])\n\n    return script_path"}
{"task_id": "BigCodeBench/746", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, target_column, target_values=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df should be a pandas DataFrame\")\n    if not isinstance(target_column, str):\n        raise ValueError(\"target_column should be a string\")\n    if target_values is not None and not hasattr(target_values, '__iter__'):\n        raise ValueError(\"target_values should be an array-like object\")\n\n    if target_values is not None:\n        df = df[df[target_column].isin(target_values)]\n    else:\n        df = df.dropna(subset=[target_column])\n\n    df.fillna(0, inplace=True)\n\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, target_column, target_values=None):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df should be a pandas DataFrame\")\n    if not isinstance(target_column, str):\n        raise ValueError(\"target_column should be a string\")\n    if target_values is not None and not hasattr(target_values, '__iter__'):\n        raise ValueError(\"target_values should be an array-like object\")\n\n    if target_values is not None:\n        df = df[df[target_column].isin(target_values)]\n    else:\n        df = df.dropna(subset=[target_column])\n\n    df.fillna(0, inplace=True)\n\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    return model"}
{"task_id": "BigCodeBench/747", "solution": "import re\nimport math\ndef task_func(s):\n    \"\"\"\n    Count the number of integers and floating-point numbers in a comma-separated string and calculate the sum of their square roots.\n    \"\"\"\n    # Extract all numbers from the string\n    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|[-+]?\\d+\", s)\n\n    # Initialize counters\n    count = 0\n    sqrt_sum = 0\n\n    # Process each number\n    for num in numbers:\n        # Increment count\n        count += 1\n\n        # Add square root to sum\n        sqrt_sum += math.sqrt(float(num))\n\n    return count, sqrt_sum\ns = \"1, 2.0, 3, 4.5, -5.5, 6\"", "raw_solution": "import re\nimport math\n\ndef task_func(s):\n    \"\"\"\n    Count the number of integers and floating-point numbers in a comma-separated string and calculate the sum of their square roots.\n    \"\"\"\n    # Extract all numbers from the string\n    numbers = re.findall(r\"[-+]?\\d*\\.\\d+|[-+]?\\d+\", s)\n\n    # Initialize counters\n    count = 0\n    sqrt_sum = 0\n\n    # Process each number\n    for num in numbers:\n        # Increment count\n        count += 1\n\n        # Add square root to sum\n        sqrt_sum += math.sqrt(float(num))\n\n    return count, sqrt_sum\n\n# Test the function\ns = \"1, 2.0, 3, 4.5, -5.5, 6\""}
{"task_id": "BigCodeBench/748", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, age, weight):\n    \"\"\"\n    Filters and standardizes a given DataFrame based on specified age and weight criteria.\n    \"\"\"\n    # Check if the DataFrame contains the required columns\n    if 'Age' not in df.columns or 'Weight' not in df.columns:\n        raise KeyError(\"The input DataFrame does not contain the required columns 'Age' and 'Weight'.\")\n\n    # Filter the DataFrame based on the age and weight criteria\n    filtered_df = df[(df['Age'] < age) & (df['Weight'] > weight)]\n\n    # If the filtered DataFrame is not empty, standardize the numerical values\n    if not filtered_df.empty:\n        scaler = StandardScaler()\n        filtered_df[filtered_df.columns] = scaler.fit_transform(filtered_df[filtered_df.columns])\n\n    return filtered_df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, age, weight):\n    \"\"\"\n    Filters and standardizes a given DataFrame based on specified age and weight criteria.\n    \"\"\"\n    # Check if the DataFrame contains the required columns\n    if 'Age' not in df.columns or 'Weight' not in df.columns:\n        raise KeyError(\"The input DataFrame does not contain the required columns 'Age' and 'Weight'.\")\n\n    # Filter the DataFrame based on the age and weight criteria\n    filtered_df = df[(df['Age'] < age) & (df['Weight'] > weight)]\n\n    # If the filtered DataFrame is not empty, standardize the numerical values\n    if not filtered_df.empty:\n        scaler = StandardScaler()\n        filtered_df[filtered_df.columns] = scaler.fit_transform(filtered_df[filtered_df.columns])\n\n    return filtered_df"}
{"task_id": "BigCodeBench/749", "solution": "from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\ndef task_func(myList):\n    \"\"\"\n    Normalize a list of numeric values to the range [0, 1] using min-max scaling.\n    \"\"\"\n    # Convert the list to a numpy array\n    myArray = np.array(myList).reshape(-1, 1)\n\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    normalized_data = scaler.fit_transform(myArray)\n\n    # Return the normalized data as a numpy array\n    return np.array(normalized_data).flatten()", "raw_solution": "from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\ndef task_func(myList):\n    \"\"\"\n    Normalize a list of numeric values to the range [0, 1] using min-max scaling.\n    \"\"\"\n    # Convert the list to a numpy array\n    myArray = np.array(myList).reshape(-1, 1)\n\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    normalized_data = scaler.fit_transform(myArray)\n\n    # Return the normalized data as a numpy array\n    return np.array(normalized_data).flatten()"}
{"task_id": "BigCodeBench/750", "solution": "import pandas as pd\nimport statsmodels.api as sm\ndef task_func(df: pd.DataFrame, height: int, weight: int, columns: list) -> sm.regression.linear_model.RegressionResultsWrapper:\n    if df.empty or df.shape[0] == 0:\n        return None\n    else:\n        filtered_df = df[(df[columns[1]] > height) & (df[columns[2]] < weight)]\n        if filtered_df.empty or filtered_df.shape[0] == 0:\n            return None\n        else:\n            X = filtered_df[columns[0]]\n            y = filtered_df[columns[1]]\n            X = sm.add_constant(X)\n            model = sm.OLS(y, X)\n            results = model.fit()\n            return results", "raw_solution": "import pandas as pd\nimport statsmodels.api as sm\n\ndef task_func(df: pd.DataFrame, height: int, weight: int, columns: list) -> sm.regression.linear_model.RegressionResultsWrapper:\n    if df.empty or df.shape[0] == 0:\n        return None\n    else:\n        filtered_df = df[(df[columns[1]] > height) & (df[columns[2]] < weight)]\n        if filtered_df.empty or filtered_df.shape[0] == 0:\n            return None\n        else:\n            X = filtered_df[columns[0]]\n            y = filtered_df[columns[1]]\n            X = sm.add_constant(X)\n            model = sm.OLS(y, X)\n            results = model.fit()\n            return results"}
{"task_id": "BigCodeBench/751", "solution": "import random\nfrom collections import Counter\ndef task_func(values, weights, n_samples):\n    \"\"\"\n    Sample random numbers based on a given weighted distribution and return a histogram of the samples.\n    \"\"\"\n    # Create a list of values with corresponding weights\n    weighted_values = [value for value in values for _ in range(weights[value])]\n    \n    # Sample n_samples from the weighted list\n    samples = random.choices(weighted_values, k=n_samples)\n    \n    # Count the occurrences of each sample\n    histogram = dict(Counter(samples))\n    \n    return histogram\nvalues = [1, 2, 3, 4, 5]\nweights = [1, 2, 3, 2, 1]\nn_samples = 1000", "raw_solution": "import random\nfrom collections import Counter\n\ndef task_func(values, weights, n_samples):\n    \"\"\"\n    Sample random numbers based on a given weighted distribution and return a histogram of the samples.\n    \"\"\"\n    # Create a list of values with corresponding weights\n    weighted_values = [value for value in values for _ in range(weights[value])]\n    \n    # Sample n_samples from the weighted list\n    samples = random.choices(weighted_values, k=n_samples)\n    \n    # Count the occurrences of each sample\n    histogram = dict(Counter(samples))\n    \n    return histogram\n\n# Test the function\nvalues = [1, 2, 3, 4, 5]\nweights = [1, 2, 3, 2, 1]\nn_samples = 1000\n\nhistogram = task_func(values, weights, n_samples)"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data should be a pandas DataFrame\")\n    if data.empty:\n        raise ValueError(\"Data should not be empty\")\n    if target_column not in data.columns:\n        raise ValueError(f\"{target_column} is not a column of data\")\n    if not np.issubdtype(data.dtypes.object, np.number).all():\n        raise ValueError(\"Data should only contain numeric values\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state should be an integer\")\n    if not 0 <= test_size <= 1:\n        raise ValueError(\"Test size should be between 0 and 1\")\n\n    X = data.drop([target_column], axis=1)\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.score(X_test, y_test)", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data should be a pandas DataFrame\")\n    if data.empty:\n        raise ValueError(\"Data should not be empty\")\n    if target_column not in data.columns:\n        raise ValueError(f\"{target_column} is not a column of data\")\n    if not np.issubdtype(data.dtypes.object, np.number).all():\n        raise ValueError(\"Data should only contain numeric values\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state should be an integer\")\n    if not 0 <= test_size <= 1:\n        raise ValueError(\"Test size should be between 0 and 1\")\n\n    X = data.drop([target_column], axis=1)\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/753", "solution": "import math\nimport random\nimport statistics\nRADIUS = 5\ndef task_func(n):\n    \"\"\"\n    Generate n random points within a circle of radius RADIUS (default value is 5) and return their average distance from the center.\n    \"\"\"\n    # Generate n random points within a circle\n    points = [(random.uniform(-RADIUS, RADIUS), random.uniform(-RADIUS, RADIUS)) for _ in range(n)]\n\n    # Calculate the distance of each point from the center\n    distances = [math.sqrt(x**2 + y**2) for x, y in points]\n\n    # Return the average distance\n    return statistics.mean(distances)", "raw_solution": "import math\nimport random\nimport statistics\n\n# Constants\nRADIUS = 5\n\ndef task_func(n):\n    \"\"\"\n    Generate n random points within a circle of radius RADIUS (default value is 5) and return their average distance from the center.\n    \"\"\"\n    # Generate n random points within a circle\n    points = [(random.uniform(-RADIUS, RADIUS), random.uniform(-RADIUS, RADIUS)) for _ in range(n)]\n\n    # Calculate the distance of each point from the center\n    distances = [math.sqrt(x**2 + y**2) for x, y in points]\n\n    # Return the average distance\n    return statistics.mean(distances)\n\n# Test the function"}
{"task_id": "BigCodeBench/754", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(result):\n    \"\"\"\n    Calculates the mean, median, min, max, and standard deviation of the \"from_user\" values in \"result\" and adds the current date and time in the format YYYY-mm-dd HH:MM:SS to the summary.\n    \"\"\"\n    # Check if \"from_user\" values are numeric\n    if not all(pd.to_numeric(result[\"from_user\"], errors='coerce') == result[\"from_user\"]):\n        raise ValueError(\"All 'from_user' values must be numeric.\")\n\n    # Calculate statistics\n    stats = result[\"from_user\"].describe()\n    stats = stats.append(pd.Series([np.nan]*5, index=['current_time']))\n\n    # Add current date and time\n    current_time = datetime.now().strftime(DATE_FORMAT)\n    stats['current_time'] = current_time\n\n    return stats", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(result):\n    \"\"\"\n    Calculates the mean, median, min, max, and standard deviation of the \"from_user\" values in \"result\" and adds the current date and time in the format YYYY-mm-dd HH:MM:SS to the summary.\n    \"\"\"\n    # Check if \"from_user\" values are numeric\n    if not all(pd.to_numeric(result[\"from_user\"], errors='coerce') == result[\"from_user\"]):\n        raise ValueError(\"All 'from_user' values must be numeric.\")\n\n    # Calculate statistics\n    stats = result[\"from_user\"].describe()\n    stats = stats.append(pd.Series([np.nan]*5, index=['current_time']))\n\n    # Add current date and time\n    current_time = datetime.now().strftime(DATE_FORMAT)\n    stats['current_time'] = current_time\n\n    return stats"}
{"task_id": "BigCodeBench/755", "solution": "import os\nimport glob\ndef task_func(directory_path):\n    \"\"\"\n    Reverse the order of words in all the filenames of a directory, where words are separated by periods.\n    \"\"\"\n    # Get all file names in the directory\n    filenames = glob.glob(os.path.join(directory_path, '*'))\n\n    new_filenames = []\n    for filename in filenames:\n        # Split the filename into words by periods\n        words = filename.split('.')\n        # Reverse the order of words\n        words = words[::-1]\n        # Join the words back into a filename\n        new_filename = '.'.join(words)\n        # Append the new filename to the list\n        new_filenames.append(new_filename)\n\n    return new_filenames", "raw_solution": "import os\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    Reverse the order of words in all the filenames of a directory, where words are separated by periods.\n    \"\"\"\n    # Get all file names in the directory\n    filenames = glob.glob(os.path.join(directory_path, '*'))\n\n    new_filenames = []\n    for filename in filenames:\n        # Split the filename into words by periods\n        words = filename.split('.')\n        # Reverse the order of words\n        words = words[::-1]\n        # Join the words back into a filename\n        new_filename = '.'.join(words)\n        # Append the new filename to the list\n        new_filenames.append(new_filename)\n\n    return new_filenames"}
{"task_id": "BigCodeBench/756", "solution": "import shutil\nfrom pathlib import Path\nfrom typing import List\ndef task_func(source_dir: str, target_dir: str, extensions: List[str]) -> int:\n    \"\"\"\n    Moves all files with certain extensions from one directory to another.\n    Raises ValueError if source_dir or target_dir does not exist.\n    Returns the number of moved files.\n    \"\"\"\n    source_dir = Path(source_dir)\n    target_dir = Path(target_dir)\n\n    if not source_dir.exists():\n        raise ValueError(f\"Source directory {source_dir} does not exist.\")\n    if not target_dir.exists():\n        raise ValueError(f\"Target directory {target_dir} does not exist.\")\n\n    moved_files = 0\n    for ext in extensions:\n        for file in source_dir.glob(f\"**/*.{ext}\"):\n            shutil.move(str(file), target_dir)\n            moved_files += 1\n\n    return moved_files", "raw_solution": "import shutil\nfrom pathlib import Path\nfrom typing import List\n\ndef task_func(source_dir: str, target_dir: str, extensions: List[str]) -> int:\n    \"\"\"\n    Moves all files with certain extensions from one directory to another.\n    Raises ValueError if source_dir or target_dir does not exist.\n    Returns the number of moved files.\n    \"\"\"\n    source_dir = Path(source_dir)\n    target_dir = Path(target_dir)\n\n    if not source_dir.exists():\n        raise ValueError(f\"Source directory {source_dir} does not exist.\")\n    if not target_dir.exists():\n        raise ValueError(f\"Target directory {target_dir} does not exist.\")\n\n    moved_files = 0\n    for ext in extensions:\n        for file in source_dir.glob(f\"**/*.{ext}\"):\n            shutil.move(str(file), target_dir)\n            moved_files += 1\n\n    return moved_files"}
{"task_id": "BigCodeBench/757", "solution": "import numpy as np\ndef task_func(arr):\n    \"\"\"\n    Reverse the order of words separated by '.' in all strings of a numpy array.\n    \"\"\"\n    # Convert the numpy array to a list\n    arr_list = arr.tolist()\n\n    # Reverse the order of words in each string\n    for i in range(len(arr_list)):\n        arr_list[i] = '. '.join(arr_list[i].split('. ')[::-1])\n\n    # Convert the list back to a numpy array\n    arr = np.array(arr_list)\n\n    return arr\narr = np.array(['This. is. a. test.', 'Hello. World.'])", "raw_solution": "import numpy as np\n\ndef task_func(arr):\n    \"\"\"\n    Reverse the order of words separated by '.' in all strings of a numpy array.\n    \"\"\"\n    # Convert the numpy array to a list\n    arr_list = arr.tolist()\n\n    # Reverse the order of words in each string\n    for i in range(len(arr_list)):\n        arr_list[i] = '. '.join(arr_list[i].split('. ')[::-1])\n\n    # Convert the list back to a numpy array\n    arr = np.array(arr_list)\n\n    return arr\n\n# Test the function\narr = np.array(['This. is. a. test.', 'Hello. World.'])"}
{"task_id": "BigCodeBench/758", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(num_samples, countries=['Russia', 'China', 'USA', 'India', 'Brazil'], \n           ages=np.arange(18, 60), genders=['Male', 'Female'], rng_seed=None):\n    if not isinstance(num_samples, int):\n        raise ValueError(\"num_samples should be an integer\")\n\n    rng = np.random.default_rng(rng_seed)\n\n    country_samples = rng.choice(countries, num_samples)\n    age_samples = rng.choice(ages, num_samples)\n    gender_samples = rng.choice(genders, num_samples)\n\n    le = LabelEncoder()\n    gender_samples = le.fit_transform(gender_samples)\n\n    data = {\n        'Country': country_samples,\n        'Age': age_samples,\n        'Gender': gender_samples\n    }\n\n    df = pd.DataFrame(data)\n    df.index = np.arange(1, num_samples+1)\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(num_samples, countries=['Russia', 'China', 'USA', 'India', 'Brazil'], \n           ages=np.arange(18, 60), genders=['Male', 'Female'], rng_seed=None):\n    if not isinstance(num_samples, int):\n        raise ValueError(\"num_samples should be an integer\")\n\n    rng = np.random.default_rng(rng_seed)\n\n    country_samples = rng.choice(countries, num_samples)\n    age_samples = rng.choice(ages, num_samples)\n    gender_samples = rng.choice(genders, num_samples)\n\n    le = LabelEncoder()\n    gender_samples = le.fit_transform(gender_samples)\n\n    data = {\n        'Country': country_samples,\n        'Age': age_samples,\n        'Gender': gender_samples\n    }\n\n    df = pd.DataFrame(data)\n    df.index = np.arange(1, num_samples+1)\n\n    return df\n\n# Testing the function\ndemographics = task_func(5, countries=['Austria', 'Germany'], rng_seed=3)"}
{"task_id": "BigCodeBench/759", "solution": "import os\nimport shutil\nimport fnmatch\ndef task_func(source_directory, destination_directory, file_pattern):\n    \"\"\"\n    Moves all files that match a particular pattern from one directory to another.\n    \"\"\"\n    # Create destination directory if it doesn't exist\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    # Get a list of files in the source directory that match the file pattern\n    files_to_move = fnmatch.filter(os.listdir(source_directory), file_pattern)\n\n    # Move each file to the destination directory\n    moved_files = []\n    for file in files_to_move:\n        shutil.move(os.path.join(source_directory, file), destination_directory)\n        moved_files.append(file)\n\n    return moved_files", "raw_solution": "import os\nimport shutil\nimport fnmatch\n\ndef task_func(source_directory, destination_directory, file_pattern):\n    \"\"\"\n    Moves all files that match a particular pattern from one directory to another.\n    \"\"\"\n    # Create destination directory if it doesn't exist\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    # Get a list of files in the source directory that match the file pattern\n    files_to_move = fnmatch.filter(os.listdir(source_directory), file_pattern)\n\n    # Move each file to the destination directory\n    moved_files = []\n    for file in files_to_move:\n        shutil.move(os.path.join(source_directory, file), destination_directory)\n        moved_files.append(file)\n\n    return moved_files"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport random\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    random.seed(rng_seed)\n    np.random.seed(rng_seed)\n\n    # Generate random data\n    ids = list(range(1, 101))\n    names = latin_names + other_names\n    random.shuffle(names)\n    dobs = [datetime.datetime(random.randint(start_year, end_year), random.randint(1, 12), random.randint(1, 28)) for _ in range(100)]\n    emails = [f\"{name.lower()}{dob.year}@{email_domain}\" for name, dob in zip(names, dobs)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dobs,\n        'Email': emails\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport random\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    random.seed(rng_seed)\n    np.random.seed(rng_seed)\n\n    # Generate random data\n    ids = list(range(1, 101))\n    names = latin_names + other_names\n    random.shuffle(names)\n    dobs = [datetime.datetime(random.randint(start_year, end_year), random.randint(1, 12), random.randint(1, 28)) for _ in range(100)]\n    emails = [f\"{name.lower()}{dob.year}@{email_domain}\" for name, dob in zip(names, dobs)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dobs,\n        'Email': emails\n    })\n\n    return df\n\n# Test the function\ndf = task_func(start_year=1980, end_year=2000, email_domain='test.at', rng_seed=3)"}
{"task_id": "BigCodeBench/761", "solution": "import json\nimport re\nfrom collections import Counter\nREPLACE_NONE = \"None\"\nEMAIL_REGEX = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\ndef task_func(json_str):\n    # 1. Removing None values\n    json_str = json_str.replace(\"None\", \"null\")\n\n    # 2. Counting the frequency of each unique value\n    data = json.loads(json_str)\n    value_counts = Counter(str(data))\n\n    # 3. Replacing all email addresses with the placeholder \"None\"\n    json_str = re.sub(EMAIL_REGEX, REPLACE_NONE, json_str)\n\n    return {\n        \"data\": json.loads(json_str),\n        \"value_counts\": value_counts\n    }\njson_str = '{\"name\": \"John\", \"age\": 30, \"email\": \"john@example.com\", \"country\": \"USA\", \"None\": None}'", "raw_solution": "import json\nimport re\nfrom collections import Counter\n\n# Constants\nREPLACE_NONE = \"None\"\nEMAIL_REGEX = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n\ndef task_func(json_str):\n    # 1. Removing None values\n    json_str = json_str.replace(\"None\", \"null\")\n\n    # 2. Counting the frequency of each unique value\n    data = json.loads(json_str)\n    value_counts = Counter(str(data))\n\n    # 3. Replacing all email addresses with the placeholder \"None\"\n    json_str = re.sub(EMAIL_REGEX, REPLACE_NONE, json_str)\n\n    return {\n        \"data\": json.loads(json_str),\n        \"value_counts\": value_counts\n    }\n\n# Testing the function\njson_str = '{\"name\": \"John\", \"age\": 30, \"email\": \"john@example.com\", \"country\": \"USA\", \"None\": None}'\nresult = task_func(json_str)"}
{"task_id": "BigCodeBench/762", "solution": "import os\nimport zipfile\ndef task_func(directory_name=\"latin_files\",\n              content='Sopet\u00f3n',\n              file_names=['file1.txt', 'file2.txt', 'file3.txt'],\n              encoding=\"latin-1\"):\n    \"\"\"\n    Creates a directory with the given name, creates specified .txt files,\n    encodes the content using the specified encoding and writes it into all .txt files,\n    then zips the directory.\n    \"\"\"\n    # Create directory\n    if not os.path.exists(directory_name):\n        os.makedirs(directory_name)\n\n    # Create .txt files\n    for file_name in file_names:\n        with open(os.path.join(directory_name, file_name), 'w', encoding=encoding) as file:\n            file.write(content)\n\n    # Zip directory\n    zip_file_name = f\"{directory_name}.zip\"\n    with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n        for root, dirs, files in os.walk(directory_name):\n            for file in files:\n                zipf.write(os.path.join(root, file), arcname=file)\n\n    return zip_file_name", "raw_solution": "import os\nimport zipfile\n\ndef task_func(directory_name=\"latin_files\",\n              content='Sopet\u00f3n',\n              file_names=['file1.txt', 'file2.txt', 'file3.txt'],\n              encoding=\"latin-1\"):\n    \"\"\"\n    Creates a directory with the given name, creates specified .txt files,\n    encodes the content using the specified encoding and writes it into all .txt files,\n    then zips the directory.\n    \"\"\"\n    # Create directory\n    if not os.path.exists(directory_name):\n        os.makedirs(directory_name)\n\n    # Create .txt files\n    for file_name in file_names:\n        with open(os.path.join(directory_name, file_name), 'w', encoding=encoding) as file:\n            file.write(content)\n\n    # Zip directory\n    zip_file_name = f\"{directory_name}.zip\"\n    with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n        for root, dirs, files in os.walk(directory_name):\n            for file in files:\n                zipf.write(os.path.join(root, file), arcname=file)\n\n    return zip_file_name\n\n# Test the function\nzipped_file = task_func(directory_name=\"directorio\", content='hi', file_names=[\"custom1.txt\", \"custom2.txt\"], encoding='utf-8')"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Load data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize dictionaries to store mean and median values\n    mean_dict = defaultdict(float)\n    median_dict = defaultdict(float)\n\n    # Calculate mean and median for each key\n    for d in data:\n        for key, value in d.items():\n            mean_dict[key] += value\n            median_dict[key] += value\n\n    # Calculate mean and median\n    for key in mean_dict.keys():\n        mean_dict[key] /= len(data)\n        median_dict[key] = np.median([d[key] for d in data])\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for key in mean_dict.keys():\n            writer.writerow([key, mean_dict[key], median_dict[key]])\n\n    # Return dictionary with mean and median values\n    return {'mean': dict(mean_dict), 'median': dict(median_dict)}", "raw_solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\ndef task_func(input_file, output_file):\n    # Load data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize dictionaries to store mean and median values\n    mean_dict = defaultdict(float)\n    median_dict = defaultdict(float)\n\n    # Calculate mean and median for each key\n    for d in data:\n        for key, value in d.items():\n            mean_dict[key] += value\n            median_dict[key] += value\n\n    # Calculate mean and median\n    for key in mean_dict.keys():\n        mean_dict[key] /= len(data)\n        median_dict[key] = np.median([d[key] for d in data])\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for key in mean_dict.keys():\n            writer.writerow([key, mean_dict[key], median_dict[key]])\n\n    # Return dictionary with mean and median values\n    return {'mean': dict(mean_dict), 'median': dict(median_dict)}"}
{"task_id": "BigCodeBench/764", "solution": "", "raw_solution": ""}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(file_dict, target_dir=\"non_none_files\"):\n    \"\"\"\n    Process files from a dictionary by checking if the file exists, and if it has content, then copies it to a target directory.\n    \"\"\"\n    # Create target directory if it doesn't exist\n    Path(target_dir).mkdir(parents=True, exist_ok=True)\n\n    copied_files = []\n\n    for file_name, file_path in file_dict.items():\n        # Check if file exists\n        if os.path.isfile(file_path):\n            # Check if file is not empty\n            if os.path.getsize(file_path) > 0:\n                # Copy file to target directory\n                shutil.copy2(file_path, target_dir)\n                # Append full file path to list\n                copied_files.append(os.path.join(target_dir, file_name))\n\n    return copied_files", "raw_solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(file_dict, target_dir=\"non_none_files\"):\n    \"\"\"\n    Process files from a dictionary by checking if the file exists, and if it has content, then copies it to a target directory.\n    \"\"\"\n    # Create target directory if it doesn't exist\n    Path(target_dir).mkdir(parents=True, exist_ok=True)\n\n    copied_files = []\n\n    for file_name, file_path in file_dict.items():\n        # Check if file exists\n        if os.path.isfile(file_path):\n            # Check if file is not empty\n            if os.path.getsize(file_path) > 0:\n                # Copy file to target directory\n                shutil.copy2(file_path, target_dir)\n                # Append full file path to list\n                copied_files.append(os.path.join(target_dir, file_name))\n\n    return copied_files"}
{"task_id": "BigCodeBench/766", "solution": "import re\nimport collections\ndef task_func(string, patterns=['nnn', 'aaa', 'sss', 'ddd', 'fff']):\n    if not isinstance(string, str):\n        raise TypeError(\"string must be a str\")\n    if not all(isinstance(pattern, str) for pattern in patterns):\n        raise TypeError(\"patterns must be a list of str\")\n\n    pattern_counts = collections.defaultdict(int)\n    for pattern in patterns:\n        pattern_counts[pattern] = len(re.findall(pattern, string))\n\n    return pattern_counts", "raw_solution": "import re\nimport collections\n\ndef task_func(string, patterns=['nnn', 'aaa', 'sss', 'ddd', 'fff']):\n    if not isinstance(string, str):\n        raise TypeError(\"string must be a str\")\n    if not all(isinstance(pattern, str) for pattern in patterns):\n        raise TypeError(\"patterns must be a list of str\")\n\n    pattern_counts = collections.defaultdict(int)\n    for pattern in patterns:\n        pattern_counts[pattern] = len(re.findall(pattern, string))\n\n    return pattern_counts"}
{"task_id": "BigCodeBench/767", "solution": "from collections import Counter\nimport random\nimport string\nLETTERS = string.ascii_letters\ndef task_func(list_of_lists):\n    \"\"\"\n    Replaces each sublist with a random letter and returns a count of each letter in the final list.\n    \"\"\"\n    # Replace each sublist with a random letter\n    replaced_list = [random.choice(LETTERS) for _ in list_of_lists]\n    \n    # Count the occurrences of each letter\n    letter_counts = Counter(replaced_list)\n    \n    return letter_counts", "raw_solution": "from collections import Counter\nimport random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef task_func(list_of_lists):\n    \"\"\"\n    Replaces each sublist with a random letter and returns a count of each letter in the final list.\n    \"\"\"\n    # Replace each sublist with a random letter\n    replaced_list = [random.choice(LETTERS) for _ in list_of_lists]\n    \n    # Count the occurrences of each letter\n    letter_counts = Counter(replaced_list)\n    \n    return letter_counts"}
{"task_id": "BigCodeBench/768", "solution": "import re\nimport os\nimport glob\ndef task_func(dir_path):\n    if not os.path.exists(dir_path):\n        raise ValueError(\"Directory does not exist\")\n\n    file_dict = {}\n    for filename in glob.iglob(dir_path + '/**/*.txt', recursive=True):\n        with open(filename, 'r') as file:\n            count = len(re.findall(r'\\berror\\b', file.read().lower()))\n            file_dict[os.path.relpath(filename, dir_path)] = count\n\n    return file_dict", "raw_solution": "import re\nimport os\nimport glob\n\ndef task_func(dir_path):\n    if not os.path.exists(dir_path):\n        raise ValueError(\"Directory does not exist\")\n\n    file_dict = {}\n    for filename in glob.iglob(dir_path + '/**/*.txt', recursive=True):\n        with open(filename, 'r') as file:\n            count = len(re.findall(r'\\berror\\b', file.read().lower()))\n            file_dict[os.path.relpath(filename, dir_path)] = count\n\n    return file_dict"}
{"task_id": "BigCodeBench/769", "solution": "from collections import Counter\nimport itertools\nimport operator\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Takes a nested list of menu items and returns the most common menu item.\n    \"\"\"\n    # Flatten the list using itertools.chain\n    flat_list = list(itertools.chain.from_iterable(list_of_menuitems))\n\n    # Count the frequency of each item using collections.Counter\n    counter = Counter(flat_list)\n\n    # Find the most common item using the most_common method of collections.Counter\n    most_common_item = counter.most_common(1)[0][0]\n\n    return most_common_item", "raw_solution": "from collections import Counter\nimport itertools\nimport operator\n\ndef task_func(list_of_menuitems):\n    \"\"\"\n    Takes a nested list of menu items and returns the most common menu item.\n    \"\"\"\n    # Flatten the list using itertools.chain\n    flat_list = list(itertools.chain.from_iterable(list_of_menuitems))\n\n    # Count the frequency of each item using collections.Counter\n    counter = Counter(flat_list)\n\n    # Find the most common item using the most_common method of collections.Counter\n    most_common_item = counter.most_common(1)[0][0]\n\n    return most_common_item\n\n# Test the function\nmenu_items = [[\"Pasta\", \"Pizza\", \"Pasta\"], [\"Soup\", \"Soup\", \"Salad\"], [\"Pizza\", \"Pasta\", \"Soup\"]]"}
{"task_id": "BigCodeBench/770", "solution": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\ndef task_func(num_samples=500, noise_strength=1, random_seed=None, test_size=0.2):\n    \"\"\"\n    Generates a dataset with a single feature and a target variable. The target is computed from the feature using a linear relation.\n    In addition some gaussian noise (random samples from normal distributioin), scaled by noise_strength, is added to the target.\n    The dataset is split into training and test sets. Then a linear regression model is adjusted to the training set and the R-squared score is calculated on the test set.\n    \"\"\"\n    if test_size < 0 or test_size > 1:\n        raise ValueError(\"Test set size must be between 0 and 1\")\n    if num_samples < 2:\n        raise ValueError(\"Number of samples must be at least 2\")\n\n    np.random.seed(random_seed)\n\n    # Generate a single feature\n    X = np.random.rand(num_samples, 1)\n\n    # Compute the target variable using a linear relation with some noise\n    y = 3 * X.flatten() + 4 + np.random.normal(0, noise_strength, size=num_samples)\n\n    # Split the dataset into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n\n    # Create a linear regression model and fit it to the training data\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the target values for the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the R-squared score of the model on the test set\n    score = r2_score(y_test, y_pred)\n\n    return score, model", "raw_solution": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ndef task_func(num_samples=500, noise_strength=1, random_seed=None, test_size=0.2):\n    \"\"\"\n    Generates a dataset with a single feature and a target variable. The target is computed from the feature using a linear relation.\n    In addition some gaussian noise (random samples from normal distributioin), scaled by noise_strength, is added to the target.\n    The dataset is split into training and test sets. Then a linear regression model is adjusted to the training set and the R-squared score is calculated on the test set.\n    \"\"\"\n    if test_size < 0 or test_size > 1:\n        raise ValueError(\"Test set size must be between 0 and 1\")\n    if num_samples < 2:\n        raise ValueError(\"Number of samples must be at least 2\")\n\n    np.random.seed(random_seed)\n\n    # Generate a single feature\n    X = np.random.rand(num_samples, 1)\n\n    # Compute the target variable using a linear relation with some noise\n    y = 3 * X.flatten() + 4 + np.random.normal(0, noise_strength, size=num_samples)\n\n    # Split the dataset into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n\n    # Create a linear regression model and fit it to the training data\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the target values for the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the R-squared score of the model on the test set\n    score = r2_score(y_test, y_pred)\n\n    return score, model"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in a directory based on a specified pattern and creates new files with altered names while preserving the content.\n    \"\"\"\n    # Initialize an empty list to store the names of new files\n    new_files = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        match = re.match(pattern, filename)\n        if match:\n            # If the file matches the pattern, create a new filename\n            new_filename = match.group(1) + '.csv'\n            new_filepath = os.path.join(directory, new_filename)\n\n            # If a file with the new name already exists, append a number to the new name\n            i = 1\n            while os.path.exists(new_filepath):\n                new_filename = match.group(1) + '-' + str(i) + '.csv'\n                new_filepath = os.path.join(directory, new_filename)\n                i += 1\n\n            # Copy the content of the original file to the new file\n            with open(os.path.join(directory, filename), 'r') as f_in, open(new_filepath, 'w', newline='') as f_out:\n                reader = csv.reader(f_in)\n                writer = csv.writer(f_out)\n                for row in reader:\n                    writer.writerow(row)\n\n            # Add the new filename to the list\n            new_files.append(new_filename)\n\n    # Return the list of new filenames\n    return new_files", "raw_solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in a directory based on a specified pattern and creates new files with altered names while preserving the content.\n    \"\"\"\n    # Initialize an empty list to store the names of new files\n    new_files = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        match = re.match(pattern, filename)\n        if match:\n            # If the file matches the pattern, create a new filename\n            new_filename = match.group(1) + '.csv'\n            new_filepath = os.path.join(directory, new_filename)\n\n            # If a file with the new name already exists, append a number to the new name\n            i = 1\n            while os.path.exists(new_filepath):\n                new_filename = match.group(1) + '-' + str(i) + '.csv'\n                new_filepath = os.path.join(directory, new_filename)\n                i += 1\n\n            # Copy the content of the original file to the new file\n            with open(os.path.join(directory, filename), 'r') as f_in, open(new_filepath, 'w', newline='') as f_out:\n                reader = csv.reader(f_in)\n                writer = csv.writer(f_out)\n                for row in reader:\n                    writer.writerow(row)\n\n            # Add the new filename to the list\n            new_files.append(new_filename)\n\n    # Return the list of new filenames\n    return new_files"}
{"task_id": "BigCodeBench/772", "solution": "import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\ndef task_func(num_samples=1000, k=5, d=2, random_seed=None):\n    \"\"\"\n    Generates a dataset of `num_samples` random numbers sampled from a Gaussian\n    distribution, applies a linear transformation `k*x + d`, and standardizes it\n    using the StandardScaler from sklearn. Computes and returns the mean squared\n    error between the original and standardized datasets.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    # Generate a dataset of `num_samples` random numbers sampled from a Gaussian\n    # distribution\n    X = np.random.normal(size=num_samples)\n    # Apply a linear transformation `k*x + d`\n    X_transformed = k * X + d\n    # Standardize the dataset using the StandardScaler from sklearn\n    scaler = StandardScaler()\n    X_standardized = scaler.fit_transform(X_transformed.reshape(-1, 1))\n    # Compute and return the mean squared error between the original and\n    # standardized datasets\n    mse = mean_squared_error(X_transformed, X_standardized)\n    return mse", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\ndef task_func(num_samples=1000, k=5, d=2, random_seed=None):\n    \"\"\"\n    Generates a dataset of `num_samples` random numbers sampled from a Gaussian\n    distribution, applies a linear transformation `k*x + d`, and standardizes it\n    using the StandardScaler from sklearn. Computes and returns the mean squared\n    error between the original and standardized datasets.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    # Generate a dataset of `num_samples` random numbers sampled from a Gaussian\n    # distribution\n    X = np.random.normal(size=num_samples)\n    # Apply a linear transformation `k*x + d`\n    X_transformed = k * X + d\n    # Standardize the dataset using the StandardScaler from sklearn\n    scaler = StandardScaler()\n    X_standardized = scaler.fit_transform(X_transformed.reshape(-1, 1))\n    # Compute and return the mean squared error between the original and\n    # standardized datasets\n    mse = mean_squared_error(X_transformed, X_standardized)\n    return mse\n\n# Test the function\nmse = task_func()"}
{"task_id": "BigCodeBench/773", "solution": "import os\nimport re\nimport shutil\nSOURCE_DIR = '/source/dir'\nTARGET_DIR = '/target/dir'\nFILE_PATTERN = re.compile(r'^(.*?)-\\d+\\.json$')\ndef task_func():\n    \"\"\"\n    Moves all json files in the source directory to the target directory,\n    and renames them by splitting the filename the last time \"-\" occurs and\n    keeping the prefix part of the filename.\n    \"\"\"\n    for filename in os.listdir(SOURCE_DIR):\n        if filename.endswith('.json'):\n            match = FILE_PATTERN.match(filename)\n            if match:\n                prefix = match.group(1)\n                new_filename = f'{prefix}.json'\n                shutil.move(os.path.join(SOURCE_DIR, filename), os.path.join(TARGET_DIR, new_filename))", "raw_solution": "import os\nimport re\nimport shutil\n\n# Constants\nSOURCE_DIR = '/source/dir'\nTARGET_DIR = '/target/dir'\nFILE_PATTERN = re.compile(r'^(.*?)-\\d+\\.json$')\n\ndef task_func():\n    \"\"\"\n    Moves all json files in the source directory to the target directory,\n    and renames them by splitting the filename the last time \"-\" occurs and\n    keeping the prefix part of the filename.\n    \"\"\"\n    for filename in os.listdir(SOURCE_DIR):\n        if filename.endswith('.json'):\n            match = FILE_PATTERN.match(filename)\n            if match:\n                prefix = match.group(1)\n                new_filename = f'{prefix}.json'\n                shutil.move(os.path.join(SOURCE_DIR, filename), os.path.join(TARGET_DIR, new_filename))\n\n# Call the function\ntask_func()"}
{"task_id": "BigCodeBench/774", "solution": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\ndef task_func(num_samples=100, n_estimators=100, random_seed=None, cv=5):\n    \"\"\"\n    Generate a dataset with five features sampled from the standard normal distribution and a target variable.\n    The target value is created by computing the sum of the features and adding random numbers sampled from the standard normal distribution.\n    Then cross-validate the dataset using a RandomForestRegressor model and return the mean cross-validation score.\n    \"\"\"\n    if num_samples / cv < 2:\n        raise ValueError(\"num_samples / cv should be greater than 2\")\n\n    np.random.seed(random_seed)\n    X = np.random.normal(size=(num_samples, 5))\n    y = np.sum(X, axis=1) + np.random.normal(size=num_samples)\n\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_seed)\n    scores = cross_val_score(model, X, y, cv=cv)\n\n    return np.mean(scores), model.fit(X, y)", "raw_solution": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(num_samples=100, n_estimators=100, random_seed=None, cv=5):\n    \"\"\"\n    Generate a dataset with five features sampled from the standard normal distribution and a target variable.\n    The target value is created by computing the sum of the features and adding random numbers sampled from the standard normal distribution.\n    Then cross-validate the dataset using a RandomForestRegressor model and return the mean cross-validation score.\n    \"\"\"\n    if num_samples / cv < 2:\n        raise ValueError(\"num_samples / cv should be greater than 2\")\n\n    np.random.seed(random_seed)\n    X = np.random.normal(size=(num_samples, 5))\n    y = np.sum(X, axis=1) + np.random.normal(size=num_samples)\n\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_seed)\n    scores = cross_val_score(model, X, y, cv=cv)\n\n    return np.mean(scores), model.fit(X, y)\n\nresults = task_func(random_seed=1)"}
{"task_id": "BigCodeBench/775", "solution": "from string import ascii_lowercase\nimport re\nfrom collections import Counter\nLETTERS_PATTERN = re.compile(r'^(.*?)-[a-z]$')\ndef task_func(string):\n    \"\"\"\n    Divides the string at the last occurrence of \"-\" and counts the frequency of each lowercase letter in the prefix.\n    \"\"\"\n    match = LETTERS_PATTERN.match(string)\n    if match:\n        prefix = match.group(1)\n        letter_counts = Counter(prefix)\n        return dict(letter_counts)\n    else:\n        return {}", "raw_solution": "from string import ascii_lowercase\nimport re\nfrom collections import Counter\n\n# Constants\nLETTERS_PATTERN = re.compile(r'^(.*?)-[a-z]$')\nLETTERS = ascii_lowercase\n\ndef task_func(string):\n    \"\"\"\n    Divides the string at the last occurrence of \"-\" and counts the frequency of each lowercase letter in the prefix.\n    \"\"\"\n    match = LETTERS_PATTERN.match(string)\n    if match:\n        prefix = match.group(1)\n        letter_counts = Counter(prefix)\n        return dict(letter_counts)\n    else:\n        return {}\n\n# Test cases"}
{"task_id": "BigCodeBench/776", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(file_path, output_path=None, sort_key='title', linear_regression=False, x_column=None, y_column=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        raise Exception(f\"Error in reading data: {str(e)}\")\n\n    try:\n        # Sort data by key\n        df = df.sort_values(by=sort_key)\n    except Exception as e:\n        raise Exception(f\"Error in sorting data: {str(e)}\")\n\n    if output_path is not None:\n        try:\n            # Write sorted data to CSV file\n            df.to_csv(output_path, index=False)\n        except Exception as e:\n            raise Exception(f\"Error in writing data: {str(e)}\")\n\n    if linear_regression:\n        if x_column is None or y_column is None:\n            raise ValueError(\"x_column and y_column must be specified for linear regression\")\n        if x_column not in df.columns or y_column not in df.columns:\n            raise ValueError(\"Specified columns for linear regression do not exist in the dataframe\")\n\n        try:\n            # Fit linear regression model\n            model = LinearRegression()\n            model.fit(df[[x_column]], df[y_column])\n            return model\n        except Exception as e:\n            raise Exception(f\"Error in fitting model: {str(e)}\")\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(file_path, output_path=None, sort_key='title', linear_regression=False, x_column=None, y_column=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        raise Exception(f\"Error in reading data: {str(e)}\")\n\n    try:\n        # Sort data by key\n        df = df.sort_values(by=sort_key)\n    except Exception as e:\n        raise Exception(f\"Error in sorting data: {str(e)}\")\n\n    if output_path is not None:\n        try:\n            # Write sorted data to CSV file\n            df.to_csv(output_path, index=False)\n        except Exception as e:\n            raise Exception(f\"Error in writing data: {str(e)}\")\n\n    if linear_regression:\n        if x_column is None or y_column is None:\n            raise ValueError(\"x_column and y_column must be specified for linear regression\")\n        if x_column not in df.columns or y_column not in df.columns:\n            raise ValueError(\"Specified columns for linear regression do not exist in the dataframe\")\n\n        try:\n            # Fit linear regression model\n            model = LinearRegression()\n            model.fit(df[[x_column]], df[y_column])\n            return model\n        except Exception as e:\n            raise Exception(f\"Error in fitting model: {str(e)}\")\n\n    return df"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    for filename in os.listdir(directory):\n        match = re.match(pattern, filename)\n        if match:\n            prefix = match.group(1)\n            if not os.path.exists(os.path.join(directory, prefix)):\n                os.mkdir(os.path.join(directory, prefix))\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(os.path.join(directory, prefix))\n            extracted_dirs.append(prefix)\n            os.remove(os.path.join(directory, filename))\n    return extracted_dirs", "raw_solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    for filename in os.listdir(directory):\n        match = re.match(pattern, filename)\n        if match:\n            prefix = match.group(1)\n            if not os.path.exists(os.path.join(directory, prefix)):\n                os.mkdir(os.path.join(directory, prefix))\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(os.path.join(directory, prefix))\n            extracted_dirs.append(prefix)\n            os.remove(os.path.join(directory, filename))\n    return extracted_dirs"}
{"task_id": "BigCodeBench/778", "solution": "from collections import defaultdict\nfrom operator import itemgetter\nfrom itertools import groupby\ndef task_func(news_articles):\n    \"\"\"\n    Sorts a list of news articles by 'category' and 'title'.\n    The news articles are then grouped by 'category'.\n    \"\"\"\n    if not all(isinstance(article, dict) for article in news_articles):\n        raise ValueError(\"All elements in the list must be dictionaries.\")\n    if not all(set(article.keys()) == {'title', 'title_url', 'id', 'category'} for article in news_articles):\n        raise ValueError(\"Dictionary keys do not match the requirements.\")\n\n    sorted_articles = sorted(news_articles, key=itemgetter('category', 'title'))\n    grouped_articles = defaultdict(list)\n    for category, articles in groupby(sorted_articles, key=itemgetter('category')):\n        grouped_articles[category].extend(articles)\n\n    return grouped_articles", "raw_solution": "from collections import defaultdict\nfrom operator import itemgetter\nfrom itertools import groupby\n\ndef task_func(news_articles):\n    \"\"\"\n    Sorts a list of news articles by 'category' and 'title'.\n    The news articles are then grouped by 'category'.\n    \"\"\"\n    if not all(isinstance(article, dict) for article in news_articles):\n        raise ValueError(\"All elements in the list must be dictionaries.\")\n    if not all(set(article.keys()) == {'title', 'title_url', 'id', 'category'} for article in news_articles):\n        raise ValueError(\"Dictionary keys do not match the requirements.\")\n\n    sorted_articles = sorted(news_articles, key=itemgetter('category', 'title'))\n    grouped_articles = defaultdict(list)\n    for category, articles in groupby(sorted_articles, key=itemgetter('category')):\n        grouped_articles[category].extend(articles)\n\n    return grouped_articles"}
{"task_id": "BigCodeBench/779", "solution": "import os\nimport shutil\nimport time\ndef get_unique_backup_dir():\n    return \"/fake/backup/path\"\ndef task_func(directory):\n    \"\"\"\n    Creates a backup of a directory and cleans the directory afterwards.\n    \"\"\"\n    backup_dir = get_unique_backup_dir()\n    errors = []\n\n    # Create backup\n    try:\n        shutil.copytree(directory, backup_dir)\n    except Exception as e:\n        errors.append(f\"Error creating backup: {str(e)}\")\n\n    # Clean directory\n    try:\n        for filename in os.listdir(directory):\n            file_path = os.path.join(directory, filename)\n            try:\n                if os.path.isfile(file_path) or os.path.islink(file_path):\n                    os.unlink(file_path)\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n            except Exception as e:\n                errors.append(f\"Error cleaning directory: {str(e)}\")\n    except Exception as e:\n        errors.append(f\"Error cleaning directory: {str(e)}\")\n\n    return backup_dir, errors", "raw_solution": "import os\nimport shutil\nimport time\n\n# Constants\nBACKUP_DIR = '/tmp/backup'\n\ndef get_unique_backup_dir():\n    return \"/fake/backup/path\"\n\ndef task_func(directory):\n    \"\"\"\n    Creates a backup of a directory and cleans the directory afterwards.\n    \"\"\"\n    backup_dir = get_unique_backup_dir()\n    errors = []\n\n    # Create backup\n    try:\n        shutil.copytree(directory, backup_dir)\n    except Exception as e:\n        errors.append(f\"Error creating backup: {str(e)}\")\n\n    # Clean directory\n    try:\n        for filename in os.listdir(directory):\n            file_path = os.path.join(directory, filename)\n            try:\n                if os.path.isfile(file_path) or os.path.islink(file_path):\n                    os.unlink(file_path)\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n            except Exception as e:\n                errors.append(f\"Error cleaning directory: {str(e)}\")\n    except Exception as e:\n        errors.append(f\"Error cleaning directory: {str(e)}\")\n\n    return backup_dir, errors"}
{"task_id": "BigCodeBench/780", "solution": "import pandas as pd\nimport pytz\nfrom typing import List, Dict\ndef task_func(articles: List[Dict[str, str]], timezone: str) -> pd.DataFrame:\n    \"\"\"\n    Analyze the publication times of a list of articles.\n    \"\"\"\n    if not isinstance(articles, list):\n        raise TypeError(\"articles should be a list of dictionaries\")\n    if not articles:\n        raise ValueError(\"articles list should not be empty\")\n    for article in articles:\n        if not isinstance(article, dict):\n            raise TypeError(\"articles should be a list of dictionaries\")\n        if not set(article.keys()).issuperset({'published_time', 'category'}):\n            raise ValueError(\"dictionary keys do not match the requirements\")\n\n    # Convert 'published_time' to a specified timezone\n    for article in articles:\n        published_time = pd.to_datetime(article['published_time'])\n        article['published_time'] = published_time.tz_localize(pytz.utc).tz_convert(pytz.timezone(timezone))\n\n    # Group articles by 'category'\n    df = pd.DataFrame(articles)\n    grouped = df.groupby('category')\n\n    # For each category, calculate the count, mean, min, max publication times only considering the hour\n    result = grouped['published_time'].agg([\n        ('count', 'count'),\n        ('mean', lambda x: x.dt.hour.mean()),\n        ('min', lambda x: x.dt.hour.min()),\n        ('max', lambda x: x.dt.hour.max())\n    ]).reset_index()\n\n    return result", "raw_solution": "import pandas as pd\nimport pytz\nfrom typing import List, Dict\n\ndef task_func(articles: List[Dict[str, str]], timezone: str) -> pd.DataFrame:\n    \"\"\"\n    Analyze the publication times of a list of articles.\n    \"\"\"\n    if not isinstance(articles, list):\n        raise TypeError(\"articles should be a list of dictionaries\")\n    if not articles:\n        raise ValueError(\"articles list should not be empty\")\n    for article in articles:\n        if not isinstance(article, dict):\n            raise TypeError(\"articles should be a list of dictionaries\")\n        if not set(article.keys()).issuperset({'published_time', 'category'}):\n            raise ValueError(\"dictionary keys do not match the requirements\")\n\n    # Convert 'published_time' to a specified timezone\n    for article in articles:\n        published_time = pd.to_datetime(article['published_time'])\n        article['published_time'] = published_time.tz_localize(pytz.utc).tz_convert(pytz.timezone(timezone))\n\n    # Group articles by 'category'\n    df = pd.DataFrame(articles)\n    grouped = df.groupby('category')\n\n    # For each category, calculate the count, mean, min, max publication times only considering the hour\n    result = grouped['published_time'].agg([\n        ('count', 'count'),\n        ('mean', lambda x: x.dt.hour.mean()),\n        ('min', lambda x: x.dt.hour.min()),\n        ('max', lambda x: x.dt.hour.max())\n    ]).reset_index()\n\n    return result"}
{"task_id": "BigCodeBench/781", "solution": "import os\nfrom datetime import datetime\ndef task_func(filepath: str) -> dict:\n    \"\"\"\n    Determines the size and date of the last modification of a file.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n\n    file_info = os.stat(filepath)\n    file_size = file_info.st_size\n    last_modified_time = datetime.fromtimestamp(file_info.st_mtime)\n    last_modified_date = last_modified_time.strftime('%Y-%m-%d %H:%M:%S')\n\n    return {\"size\": file_size, \"last_modified\": last_modified_date}", "raw_solution": "import os\nfrom datetime import datetime\n\ndef task_func(filepath: str) -> dict:\n    \"\"\"\n    Determines the size and date of the last modification of a file.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n\n    file_info = os.stat(filepath)\n    file_size = file_info.st_size\n    last_modified_time = datetime.fromtimestamp(file_info.st_mtime)\n    last_modified_date = last_modified_time.strftime('%Y-%m-%d %H:%M:%S')\n\n    return {\"size\": file_size, \"last_modified\": last_modified_date}"}
{"task_id": "BigCodeBench/782", "solution": "import random\nimport pandas as pd\nimport numpy as np\ndef task_func(n,\n              domain=\"samplewebsite.com\",\n              categories=['Sports', 'Technology', 'Health', 'Science', 'Business'],\n              random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for i in range(n):\n        title = f\"Article {i}\"\n        title_url = f\"{domain}/{title}\"\n        id = i\n        category = random.choice(categories)\n        views = np.random.poisson(1000)\n        data.append([title, title_url, id, category, views])\n\n    df = pd.DataFrame(data, columns=['title', 'title_url', 'id', 'category', 'views'])\n    return df", "raw_solution": "import random\nimport pandas as pd\nimport numpy as np\n\ndef task_func(n,\n              domain=\"samplewebsite.com\",\n              categories=['Sports', 'Technology', 'Health', 'Science', 'Business'],\n              random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for i in range(n):\n        title = f\"Article {i}\"\n        title_url = f\"{domain}/{title}\"\n        id = i\n        category = random.choice(categories)\n        views = np.random.poisson(1000)\n        data.append([title, title_url, id, category, views])\n\n    df = pd.DataFrame(data, columns=['title', 'title_url', 'id', 'category', 'views'])\n    return df\n\ndf = task_func(3, categories=['A', 'B'], domain='test.de', random_seed=12)"}
{"task_id": "BigCodeBench/783", "solution": "import os\nimport shutil\ndef task_func(src_dir, dest_dir, extension):\n    \"\"\"\n    Moves all files with a particular extension from one directory to another.\n    \"\"\"\n    if not os.path.exists(src_dir) or not os.path.exists(dest_dir):\n        raise ValueError(\"Source or destination directory does not exist.\")\n\n    files_moved = 0\n    for filename in os.listdir(src_dir):\n        if filename.endswith(extension):\n            shutil.move(os.path.join(src_dir, filename), dest_dir)\n            files_moved += 1\n\n    return files_moved", "raw_solution": "import os\nimport shutil\n\ndef task_func(src_dir, dest_dir, extension):\n    \"\"\"\n    Moves all files with a particular extension from one directory to another.\n    \"\"\"\n    if not os.path.exists(src_dir) or not os.path.exists(dest_dir):\n        raise ValueError(\"Source or destination directory does not exist.\")\n\n    files_moved = 0\n    for filename in os.listdir(src_dir):\n        if filename.endswith(extension):\n            shutil.move(os.path.join(src_dir, filename), dest_dir)\n            files_moved += 1\n\n    return files_moved"}
{"task_id": "BigCodeBench/784", "solution": "import pandas as pd\nimport random\nimport csv\ndef task_func(n, \n           categories=['Sports', 'Technology', 'Business', 'Politics', 'Entertainment'],\n           news_sites=['New York Times', 'USA Today', 'Apple News', 'CNN', 'BBC'],\n           likert_scale=['Strongly Disagree', 'Disagree', 'Neither Agree nor Disagree', 'Agree', 'Strongly Agree'],\n           file_path='news_survey_data.csv',\n           random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        site = random.choice(news_sites)\n        category = random.choice(categories)\n        response = random.choice(likert_scale)\n        value = likert_scale.index(response) + 1\n        data.append([site, category, response, value])\n\n    df = pd.DataFrame(data, columns=['Site', 'Category', 'Response', 'Value'])\n    df.to_csv(file_path, index=False)\n\n    df = pd.read_csv(file_path)\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport csv\n\ndef task_func(n, \n           categories=['Sports', 'Technology', 'Business', 'Politics', 'Entertainment'],\n           news_sites=['New York Times', 'USA Today', 'Apple News', 'CNN', 'BBC'],\n           likert_scale=['Strongly Disagree', 'Disagree', 'Neither Agree nor Disagree', 'Agree', 'Strongly Agree'],\n           file_path='news_survey_data.csv',\n           random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = []\n    for _ in range(n):\n        site = random.choice(news_sites)\n        category = random.choice(categories)\n        response = random.choice(likert_scale)\n        value = likert_scale.index(response) + 1\n        data.append([site, category, response, value])\n\n    df = pd.DataFrame(data, columns=['Site', 'Category', 'Response', 'Value'])\n    df.to_csv(file_path, index=False)\n\n    df = pd.read_csv(file_path)\n    return df\n\ndf = task_func(8, ['test', 'fun'], likert_scale=['true', 'false'], news_sites=['cat', 'dog'], random_seed=12)"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nimport shutil\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    \"\"\"\n    Archive all files that match a particular pattern and then delete the original files.\n    \"\"\"\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Get a list of files that match the pattern\n    files = glob.glob(pattern)\n\n    # Archive each file\n    for file in files:\n        # Get the base name of the file\n        base_name = os.path.basename(file)\n\n        # Create the archive file path\n        archive_file = os.path.join(ARCHIVE_DIR, base_name)\n\n        # Archive the file\n        shutil.move(file, archive_file)\n\n    # Return the archive file path\n    return archive_file", "raw_solution": "import subprocess\nimport os\nimport glob\nimport shutil\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    Archive all files that match a particular pattern and then delete the original files.\n    \"\"\"\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Get a list of files that match the pattern\n    files = glob.glob(pattern)\n\n    # Archive each file\n    for file in files:\n        # Get the base name of the file\n        base_name = os.path.basename(file)\n\n        # Create the archive file path\n        archive_file = os.path.join(ARCHIVE_DIR, base_name)\n\n        # Archive the file\n        shutil.move(file, archive_file)\n\n    # Return the archive file path\n    return archive_file"}
{"task_id": "BigCodeBench/786", "solution": "import pandas as pd\nimport random\ndef task_func(\n    n, \n    countries=['USA', 'UK', 'China', 'India', 'Germany'], \n    products=['Product A', 'Product B', 'Product C', 'Product D', 'Product E'], \n    output_path=None,\n    random_seed=None):\n    \n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = {\n        'Country': [random.choice(countries) for _ in range(n)],\n        'Product': [random.choice(products) for _ in range(n)],\n        'Sales': [random.randint(1, 100) for _ in range(n)]\n    }\n\n    df = pd.DataFrame(data)\n\n    if output_path is not None:\n        df.to_csv(output_path, index=False)\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(\n    n, \n    countries=['USA', 'UK', 'China', 'India', 'Germany'], \n    products=['Product A', 'Product B', 'Product C', 'Product D', 'Product E'], \n    output_path=None,\n    random_seed=None):\n    \n    if random_seed is not None:\n        random.seed(random_seed)\n\n    data = {\n        'Country': [random.choice(countries) for _ in range(n)],\n        'Product': [random.choice(products) for _ in range(n)],\n        'Sales': [random.randint(1, 100) for _ in range(n)]\n    }\n\n    df = pd.DataFrame(data)\n\n    if output_path is not None:\n        df.to_csv(output_path, index=False)\n\n    return df"}
{"task_id": "BigCodeBench/787", "solution": "import numpy as np\nfrom itertools import combinations\ndef task_func(array1, array2):\n    if len(array1) != len(array2):\n        raise ValueError(\"Input arrays must have the same length\")\n    if len(array1) == 0:\n        return 0\n    array1 = np.array(array1)\n    array2 = np.array(array2)\n    points = list(zip(array1, array2))\n    max_distance = 0\n    for point1, point2 in combinations(points, 2):\n        distance = np.linalg.norm(np.array(point1) - np.array(point2))\n        if distance > max_distance:\n            max_distance = distance\n    return max_distance", "raw_solution": "import numpy as np\nfrom itertools import combinations\n\ndef task_func(array1, array2):\n    if len(array1) != len(array2):\n        raise ValueError(\"Input arrays must have the same length\")\n    if len(array1) == 0:\n        return 0\n    array1 = np.array(array1)\n    array2 = np.array(array2)\n    points = list(zip(array1, array2))\n    max_distance = 0\n    for point1, point2 in combinations(points, 2):\n        distance = np.linalg.norm(np.array(point1) - np.array(point2))\n        if distance > max_distance:\n            max_distance = distance\n    return max_distance"}
{"task_id": "BigCodeBench/788", "solution": "import pandas as pd\nimport heapq\nfrom scipy import stats\ndef task_func(df, col1, col2, N=10):\n    \"\"\"\n    Finds the N largest absolute differences between the corresponding elements of two specified columns in a DataFrame,\n    performs a t-Test on the elements with these differences, and returns the calculated p-value.\n    \"\"\"\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame.\")\n    if N <= 1:\n        raise ValueError(\"N should be greater than 1.\")\n\n    # Calculate the absolute differences\n    df['diff'] = df[col1].sub(df[col2]).abs()\n\n    # Find the N largest differences\n    largest_diffs = heapq.nlargest(N, df['diff'])\n\n    # Perform the t-Test\n    t_stat, p_value = stats.ttest_1samp(largest_diffs, 0)\n\n    return p_value", "raw_solution": "import pandas as pd\nimport heapq\nfrom scipy import stats\n\ndef task_func(df, col1, col2, N=10):\n    \"\"\"\n    Finds the N largest absolute differences between the corresponding elements of two specified columns in a DataFrame,\n    performs a t-Test on the elements with these differences, and returns the calculated p-value.\n    \"\"\"\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame.\")\n    if N <= 1:\n        raise ValueError(\"N should be greater than 1.\")\n\n    # Calculate the absolute differences\n    df['diff'] = df[col1].sub(df[col2]).abs()\n\n    # Find the N largest differences\n    largest_diffs = heapq.nlargest(N, df['diff'])\n\n    # Perform the t-Test\n    t_stat, p_value = stats.ttest_1samp(largest_diffs, 0)\n\n    return p_value"}
{"task_id": "BigCodeBench/789", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nARRAY_LENGTH = 10\ndef task_func():\n    # Generate a random array\n    array = np.random.rand(ARRAY_LENGTH)\n\n    # Reshape array to 2D for MinMaxScaler\n    array = array.reshape(-1, 1)\n\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the array\n    scaled_array = scaler.fit_transform(array)\n\n    # Flatten the array back to 1D\n    scaled_array = scaled_array.flatten()\n\n    return scaled_array", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nARRAY_LENGTH = 10\n\ndef task_func():\n    # Generate a random array\n    array = np.random.rand(ARRAY_LENGTH)\n\n    # Reshape array to 2D for MinMaxScaler\n    array = array.reshape(-1, 1)\n\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the array\n    scaled_array = scaler.fit_transform(array)\n\n    # Flatten the array back to 1D\n    scaled_array = scaled_array.flatten()\n\n    return scaled_array"}
{"task_id": "BigCodeBench/790", "solution": "import pandas as pd\nimport heapq\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(df, col1, col2, N=10):\n    \"\"\"\n    Standardize two columns ('col1' and 'col2') in the DataFrame, find the biggest differences between the individual elements of the standardized columns, and return the indices of the N largest differences.\n    \"\"\"\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame.\")\n\n    scaler = StandardScaler()\n    df[col1] = scaler.fit_transform(df[[col1]]).flatten()\n    df[col2] = scaler.fit_transform(df[[col2]]).flatten()\n\n    diffs = df[[col1, col2]].apply(lambda row: abs(row[col1] - row[col2]), axis=1)\n    indices = heapq.nlargest(N, range(len(diffs)), diffs.take)\n\n    return indices\ndf = pd.DataFrame({\n    'a': [1, 2, 3, 4],\n    'b': [1, 2, 3, 5]\n})", "raw_solution": "import pandas as pd\nimport heapq\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, col1, col2, N=10):\n    \"\"\"\n    Standardize two columns ('col1' and 'col2') in the DataFrame, find the biggest differences between the individual elements of the standardized columns, and return the indices of the N largest differences.\n    \"\"\"\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame.\")\n\n    scaler = StandardScaler()\n    df[col1] = scaler.fit_transform(df[[col1]]).flatten()\n    df[col2] = scaler.fit_transform(df[[col2]]).flatten()\n\n    diffs = df[[col1, col2]].apply(lambda row: abs(row[col1] - row[col2]), axis=1)\n    indices = heapq.nlargest(N, range(len(diffs)), diffs.take)\n\n    return indices\n\n# Testing the function\ndf = pd.DataFrame({\n    'a': [1, 2, 3, 4],\n    'b': [1, 2, 3, 5]\n})\n\nindices = task_func(df, 'a', 'b')"}
{"task_id": "BigCodeBench/791", "solution": "from collections import Counter\nimport random\nfrom itertools import cycle\ndef task_func(l):\n    # Create a cycled, shuffled list from the input list\n    shuffled_l = cycle(l)\n    # Create a list with the first 30 elements from the cycled, shuffled list\n    first_30_elements = [next(shuffled_l) for _ in range(30)]\n    # Move the first 3 elements to the end of the list\n    first_30_elements = first_30_elements[3:] + first_30_elements[:3]\n    # Create a frequency counter from the list\n    counter = Counter(first_30_elements)\n    return counter\nl = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']", "raw_solution": "from collections import Counter\nimport random\nfrom itertools import cycle\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\ndef task_func(l):\n    # Create a cycled, shuffled list from the input list\n    shuffled_l = cycle(l)\n    # Create a list with the first 30 elements from the cycled, shuffled list\n    first_30_elements = [next(shuffled_l) for _ in range(30)]\n    # Move the first 3 elements to the end of the list\n    first_30_elements = first_30_elements[3:] + first_30_elements[:3]\n    # Create a frequency counter from the list\n    counter = Counter(first_30_elements)\n    return counter\n\n# Test the function\nl = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']"}
{"task_id": "BigCodeBench/792", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, feature, target, n=10):\n    if feature not in df.columns or target not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame.\")\n\n    X = df[[feature]]\n    y = df[target]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    residuals = np.abs(y - model.predict(X))\n    indices = np.argsort(residuals)[-n:]\n\n    return indices.tolist(), model", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, feature, target, n=10):\n    if feature not in df.columns or target not in df.columns:\n        raise ValueError(\"Specified columns are not in the provided DataFrame.\")\n\n    X = df[[feature]]\n    y = df[target]\n\n    model = LinearRegression()\n    model.fit(X, y)\n\n    residuals = np.abs(y - model.predict(X))\n    indices = np.argsort(residuals)[-n:]\n\n    return indices.tolist(), model"}
{"task_id": "BigCodeBench/793", "solution": "import numpy as np\nimport random\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\ndef task_func(l=None):\n    if l is None:\n        l = ELEMENTS\n    l = np.array(l)\n    l = np.roll(l, -3)\n    return l", "raw_solution": "import numpy as np\nimport random\n\n# Constants\nELEMENTS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\ndef task_func(l=None):\n    if l is None:\n        l = ELEMENTS\n    l = np.array(l)\n    l = np.roll(l, -3)\n    return l\n\n# Testing"}
{"task_id": "BigCodeBench/794", "solution": "import string\nimport random\ndef task_func(length, random_seed=None):\n    BRACKETS = \"(){}[]\"\n    ALL_CHARS = string.ascii_lowercase + BRACKETS\n\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    return ''.join(random.choice(ALL_CHARS) for _ in range(length))", "raw_solution": "import string\nimport random\n\ndef task_func(length, random_seed=None):\n    BRACKETS = \"(){}[]\"\n    ALL_CHARS = string.ascii_lowercase + BRACKETS\n\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    return ''.join(random.choice(ALL_CHARS) for _ in range(length))\n\n# Testing the function"}
{"task_id": "BigCodeBench/795", "solution": "from collections import deque\nimport math\ndef task_func(l):\n    dq = deque(l)\n    dq.rotate(3)\n    sum_of_elements = sum(i for i in dq if isinstance(i, (int, float)))\n    if sum_of_elements:\n        print(f\"The square root of the sum of numeric elements: {math.sqrt(sum_of_elements)}\")\n    return dq", "raw_solution": "from collections import deque\nimport math\n\ndef task_func(l):\n    dq = deque(l)\n    dq.rotate(3)\n    sum_of_elements = sum(i for i in dq if isinstance(i, (int, float)))\n    if sum_of_elements:\n        print(f\"The square root of the sum of numeric elements: {math.sqrt(sum_of_elements)}\")\n    return dq\n\n# Testing the function"}
{"task_id": "BigCodeBench/796", "solution": "import os\nimport re\nBRACKET_PATTERN = '[(){}\\[\\]]'\ndef task_func(directory):\n    \"\"\"\n    Finds all files in the specified directory whose names contain any type of bracket (round, curly, or square).\n    Uses an internal constant BRACKET_PATTERN = '[(){}\\\\[\\\\]]', which specifies the brackets that are looked for.\n    \"\"\"\n    # Initialize an empty list to store the file paths\n    file_paths = []\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file name contains any bracket\n            if re.search(BRACKET_PATTERN, file):\n                # If so, append the file path to the list\n                file_paths.append(os.path.join(root, file))\n\n    return file_paths", "raw_solution": "import os\nimport re\n\nBRACKET_PATTERN = '[(){}\\[\\]]'\n\ndef task_func(directory):\n    \"\"\"\n    Finds all files in the specified directory whose names contain any type of bracket (round, curly, or square).\n    Uses an internal constant BRACKET_PATTERN = '[(){}\\\\[\\\\]]', which specifies the brackets that are looked for.\n    \"\"\"\n    # Initialize an empty list to store the file paths\n    file_paths = []\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file name contains any bracket\n            if re.search(BRACKET_PATTERN, file):\n                # If so, append the file path to the list\n                file_paths.append(os.path.join(root, file))\n\n    return file_paths"}
{"task_id": "BigCodeBench/797", "solution": "import re\nimport pandas as pd\nfrom typing import Union\ndef task_func(df: pd.DataFrame) -> Union[int, TypeError]:\n    \"\"\"\n    Counts the total number of brackets in a pandas DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input should be a pandas DataFrame\")\n\n    brackets = '[(){}[\\]]'\n    pattern = '[{}]'.format(brackets)\n\n    total_count = 0\n    for col in df.columns:\n        series = df[col]\n        for value in series:\n            if isinstance(value, str):\n                count = sum(1 for char in value if char in brackets)\n                total_count += count\n\n    return total_count\ndf = pd.DataFrame({'Test': ['(a)', 'b', '[[[[))c']})", "raw_solution": "import re\nimport pandas as pd\nfrom typing import Union\n\ndef task_func(df: pd.DataFrame) -> Union[int, TypeError]:\n    \"\"\"\n    Counts the total number of brackets in a pandas DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input should be a pandas DataFrame\")\n\n    brackets = '[(){}[\\]]'\n    pattern = '[{}]'.format(brackets)\n\n    total_count = 0\n    for col in df.columns:\n        series = df[col]\n        for value in series:\n            if isinstance(value, str):\n                count = sum(1 for char in value if char in brackets)\n                total_count += count\n\n    return total_count\n\n# Testing the function\ndf = pd.DataFrame({'Test': ['(a)', 'b', '[[[[))c']})"}
{"task_id": "BigCodeBench/798", "solution": "import os\nimport shutil\nBACKUP_DIR = '/tmp/backup'\ndef task_func(directory):\n    \"\"\"\n    Rollback the update of a directory by restoring it from a backup.\n    \"\"\"\n    backup_path = os.path.join(BACKUP_DIR, directory)\n    if not os.path.exists(backup_path):\n        return f\"Backup directory {BACKUP_DIR} does not exist. Cannot rollback update.\"\n\n    # Remove the existing directory\n    if os.path.exists(directory):\n        shutil.rmtree(directory)\n\n    # Restore the backup\n    shutil.copytree(backup_path, directory)\n\n    return directory", "raw_solution": "import os\nimport shutil\n\n# Constants\nBACKUP_DIR = '/tmp/backup'\n\ndef task_func(directory):\n    \"\"\"\n    Rollback the update of a directory by restoring it from a backup.\n    \"\"\"\n    backup_path = os.path.join(BACKUP_DIR, directory)\n    if not os.path.exists(backup_path):\n        return f\"Backup directory {BACKUP_DIR} does not exist. Cannot rollback update.\"\n\n    # Remove the existing directory\n    if os.path.exists(directory):\n        shutil.rmtree(directory)\n\n    # Restore the backup\n    shutil.copytree(backup_path, directory)\n\n    return directory"}
{"task_id": "BigCodeBench/799", "solution": "import pandas as pd\nfrom random import seed, choices\nfrom string import ascii_lowercase\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if not L:\n        return pd.DataFrame(), []\n\n    column_names = [''.join(choices(ascii_lowercase, k=3)) for _ in range(3)]\n    df_list = []\n    for _ in range(num_dataframes):\n        df = pd.DataFrame(columns=column_names)\n        for i in range(3):\n            df = df.append(pd.Series(choices(L, k=3), index=column_names), ignore_index=True)\n        df_list.append(df)\n\n    common_rows = df_list[0]\n    for df in df_list[1:]:\n        common_rows = pd.merge(common_rows, df, how='inner')\n\n    return common_rows, df_list", "raw_solution": "import pandas as pd\nfrom random import seed, choices\nfrom string import ascii_lowercase\n\ndef task_func(L, num_dataframes=5, random_seed=None):\n    if random_seed is not None:\n        seed(random_seed)\n\n    if not L:\n        return pd.DataFrame(), []\n\n    column_names = [''.join(choices(ascii_lowercase, k=3)) for _ in range(3)]\n    df_list = []\n    for _ in range(num_dataframes):\n        df = pd.DataFrame(columns=column_names)\n        for i in range(3):\n            df = df.append(pd.Series(choices(L, k=3), index=column_names), ignore_index=True)\n        df_list.append(df)\n\n    common_rows = df_list[0]\n    for df in df_list[1:]:\n        common_rows = pd.merge(common_rows, df, how='inner')\n\n    return common_rows, df_list"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Count the total number of goals and penalties from a CSV file and update it with the given goals and penalties.\n    The function should output with:\n        count (Counter.collections): A Counter object with total counts of goals and penalties.\n    \"\"\"\n    # Initialize counters\n    total_goals = 0\n    total_penalties = 0\n\n    # Open the CSV file\n    with open(csv_file_path, 'r') as csv_file:\n        reader = csv.reader(csv_file)\n        next(reader)  # Skip the header\n\n        # Iterate over the rows in the CSV file\n        for row in reader:\n            # Add the goals and penalties from the current row to the total counts\n            total_goals += int(row[1])\n            total_penalties += int(row[2])\n\n    # Add the given goals and penalties to the total counts\n    total_goals += goals\n    total_penalties += penalties\n\n    # Create a Counter object with the total counts of goals and penalties\n    count = Counter({'goals': total_goals, 'penalties': total_penalties})\n\n    return count", "raw_solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Count the total number of goals and penalties from a CSV file and update it with the given goals and penalties.\n    The function should output with:\n        count (Counter.collections): A Counter object with total counts of goals and penalties.\n    \"\"\"\n    # Initialize counters\n    total_goals = 0\n    total_penalties = 0\n\n    # Open the CSV file\n    with open(csv_file_path, 'r') as csv_file:\n        reader = csv.reader(csv_file)\n        next(reader)  # Skip the header\n\n        # Iterate over the rows in the CSV file\n        for row in reader:\n            # Add the goals and penalties from the current row to the total counts\n            total_goals += int(row[1])\n            total_penalties += int(row[2])\n\n    # Add the given goals and penalties to the total counts\n    total_goals += goals\n    total_penalties += penalties\n\n    # Create a Counter object with the total counts of goals and penalties\n    count = Counter({'goals': total_goals, 'penalties': total_penalties})\n\n    return count"}
{"task_id": "BigCodeBench/801", "solution": "import csv\nimport collections\ndef task_func(file_name):\n    \"\"\"\n    Finds the most common value in each column of a csv file with column names.\n    If some values occur the same number of times, the values are sorted alphabetically\n    and the first is considered most common. If an empty csv is passed, an empty\n    dictionary is returned.\n    \"\"\"\n    try:\n        with open(file_name, 'r') as file:\n            reader = csv.reader(file)\n            data = list(reader)\n    except FileNotFoundError:\n        return {}\n\n    if not data:\n        return {}\n\n    # Transpose the data to get columns\n    data_t = list(map(list, zip(*data)))\n\n    # Remove the first row (column names)\n    data_t = data_t[1:]\n\n    result = {}\n    for i, column in enumerate(data_t):\n        counter = collections.Counter(column)\n        most_common = counter.most_common(1)\n        result[data[0][i]] = most_common[0][0]\n\n    return result", "raw_solution": "import csv\nimport collections\n\ndef task_func(file_name):\n    \"\"\"\n    Finds the most common value in each column of a csv file with column names.\n    If some values occur the same number of times, the values are sorted alphabetically\n    and the first is considered most common. If an empty csv is passed, an empty\n    dictionary is returned.\n    \"\"\"\n    try:\n        with open(file_name, 'r') as file:\n            reader = csv.reader(file)\n            data = list(reader)\n    except FileNotFoundError:\n        return {}\n\n    if not data:\n        return {}\n\n    # Transpose the data to get columns\n    data_t = list(map(list, zip(*data)))\n\n    # Remove the first row (column names)\n    data_t = data_t[1:]\n\n    result = {}\n    for i, column in enumerate(data_t):\n        counter = collections.Counter(column)\n        most_common = counter.most_common(1)\n        result[data[0][i]] = most_common[0][0]\n\n    return result"}
{"task_id": "BigCodeBench/802", "solution": "import numpy as np\ndef task_func(dimension, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.randint(1, 101, dimension)\n    flat_list = list(itertools.chain.from_iterable(matrix))\n    return matrix, flat_list\ndimension = (5, 5)", "raw_solution": "import numpy as np\n\ndef task_func(dimension, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.randint(1, 101, dimension)\n    flat_list = list(itertools.chain.from_iterable(matrix))\n    return matrix, flat_list\n\n# Test the function\ndimension = (5, 5)\nmatrix, flat_list = task_func(dimension)"}
{"task_id": "BigCodeBench/803", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom typing import Union\ndef task_func(file_name: str) -> Union[pd.DataFrame, ValueError]:\n    \"\"\"\n    Normalize data in a csv file using MinMaxScaler from sklearn. Only numeric columns are normalized.\n    Columns with other dtypes are left as they are.\n    \"\"\"\n    try:\n        # Load the data\n        df = pd.read_csv(file_name)\n\n        # Identify numeric columns\n        numeric_cols = df.select_dtypes(include=[pd.np.number]).columns\n\n        # If there are no numeric columns, raise ValueError\n        if not numeric_cols.any():\n            raise ValueError(\"Input does not have numeric columns.\")\n\n        # Initialize MinMaxScaler\n        scaler = MinMaxScaler()\n\n        # Fit and transform the data\n        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n        return df\n\n    except ValueError as e:\n        print(e)\n        return e", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom typing import Union\n\ndef task_func(file_name: str) -> Union[pd.DataFrame, ValueError]:\n    \"\"\"\n    Normalize data in a csv file using MinMaxScaler from sklearn. Only numeric columns are normalized.\n    Columns with other dtypes are left as they are.\n    \"\"\"\n    try:\n        # Load the data\n        df = pd.read_csv(file_name)\n\n        # Identify numeric columns\n        numeric_cols = df.select_dtypes(include=[pd.np.number]).columns\n\n        # If there are no numeric columns, raise ValueError\n        if not numeric_cols.any():\n            raise ValueError(\"Input does not have numeric columns.\")\n\n        # Initialize MinMaxScaler\n        scaler = MinMaxScaler()\n\n        # Fit and transform the data\n        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n        return df\n\n    except ValueError as e:\n        print(e)\n        return e"}
{"task_id": "BigCodeBench/804", "solution": "import os\nfrom datetime import datetime\nLOG_DIR = './logs'\ndef task_func(metrics, filename, log_dir=LOG_DIR):\n    \"\"\"\n    Writes a dictionary of metrics to a specified log file, appending a timestamp to each entry.\n    \"\"\"\n    # Create the log directory if it doesn't exist\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    # Construct the full path to the log file\n    filepath = os.path.join(log_dir, filename)\n\n    # Open the log file in append mode\n    try:\n        with open(filepath, 'a') as log_file:\n            # Write the metrics to the log file, appending a timestamp\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            log_file.write(f'{timestamp} {str(metrics)}\\n')\n    except IOError:\n        # If an IOError occurs, return False\n        return False\n\n    # If no exception was raised, return True\n    return True\nmetrics = {'precision': 0.75, 'recall': 0.80}", "raw_solution": "import os\nfrom datetime import datetime\n\n# Constants\nLOG_DIR = './logs'\n\ndef task_func(metrics, filename, log_dir=LOG_DIR):\n    \"\"\"\n    Writes a dictionary of metrics to a specified log file, appending a timestamp to each entry.\n    \"\"\"\n    # Create the log directory if it doesn't exist\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    # Construct the full path to the log file\n    filepath = os.path.join(log_dir, filename)\n\n    # Open the log file in append mode\n    try:\n        with open(filepath, 'a') as log_file:\n            # Write the metrics to the log file, appending a timestamp\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            log_file.write(f'{timestamp} {str(metrics)}\\n')\n    except IOError:\n        # If an IOError occurs, return False\n        return False\n\n    # If no exception was raised, return True\n    return True\n\n# Test the function\nmetrics = {'precision': 0.75, 'recall': 0.80}"}
{"task_id": "BigCodeBench/805", "solution": "import pandas as pd\nimport random\nfrom typing import Dict, List, Tuple\ndef task_func(dictionary: Dict[str, List[str]], item: str, seed: int) -> Tuple[List[Tuple[int, str]], int, pd.DataFrame]:\n    \"\"\"\n    Converts a dictionary to a pandas DataFrame, finds the locations of a particular item in the DataFrame, counts the number of\n    occurrences and adds a random integer x, where 0 <=x < 10, to it.\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(dictionary)\n\n    # Find the locations of the item\n    locations = [(i, col) for col in df.columns for i in df[col][df[col] == item].index.tolist()]\n\n    # Count the number of occurrences\n    count = len(locations)\n\n    # Add a random integer to the DataFrame\n    df[str(random.randint(0, 9))] = df.apply(lambda row: random.randint(0, 9), axis=1)\n\n    return locations, count, df", "raw_solution": "import pandas as pd\nimport random\nfrom typing import Dict, List, Tuple\n\ndef task_func(dictionary: Dict[str, List[str]], item: str, seed: int) -> Tuple[List[Tuple[int, str]], int, pd.DataFrame]:\n    \"\"\"\n    Converts a dictionary to a pandas DataFrame, finds the locations of a particular item in the DataFrame, counts the number of\n    occurrences and adds a random integer x, where 0 <=x < 10, to it.\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(dictionary)\n\n    # Find the locations of the item\n    locations = [(i, col) for col in df.columns for i in df[col][df[col] == item].index.tolist()]\n\n    # Count the number of occurrences\n    count = len(locations)\n\n    # Add a random integer to the DataFrame\n    df[str(random.randint(0, 9))] = df.apply(lambda row: random.randint(0, 9), axis=1)\n\n    return locations, count, df"}
{"task_id": "BigCodeBench/806", "solution": "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom collections import Counter\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(text, n=2):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Remove stopwords\n    words = [word for word in text.split() if word not in STOPWORDS]\n\n    # Generate n-grams\n    n_grams = ngrams(words, n)\n\n    # Count n-grams\n    n_grams_count = Counter(n_grams)\n\n    return n_grams_count\ntext = \"This is a sample text. It contains several words, some of which are repeated. The function should remove these words and count the remaining n-grams.\"", "raw_solution": "import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom collections import Counter\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(text, n=2):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Remove stopwords\n    words = [word for word in text.split() if word not in STOPWORDS]\n\n    # Generate n-grams\n    n_grams = ngrams(words, n)\n\n    # Count n-grams\n    n_grams_count = Counter(n_grams)\n\n    return n_grams_count\n\n# Test the function\ntext = \"This is a sample text. It contains several words, some of which are repeated. The function should remove these words and count the remaining n-grams.\""}
{"task_id": "BigCodeBench/807", "solution": "import numpy as np\nfrom scipy.stats import norm\ndef task_func(data: np.ndarray, threshold: float = 2.0) -> list:\n    \"\"\"\n    Determines the outlier indices in a 1D numpy array based on the Z score.\n    \"\"\"\n    # Calculate mean and standard deviation\n    mean = np.mean(data)\n    std_dev = np.std(data)\n\n    # If standard deviation is 0, return empty list\n    if std_dev == 0:\n        return [], mean, std_dev**2\n\n    # Calculate Z scores\n    z_scores = (data - mean) / std_dev\n\n    # Find outliers\n    outliers = [i for i, z in enumerate(z_scores) if np.abs(z) > threshold]\n\n    # Return outliers, mean, and variance\n    return outliers, mean, std_dev**2\ndata = np.array([-10, 3, 5, 5, 5, 5, 5, 7, 20])", "raw_solution": "import numpy as np\nfrom scipy.stats import norm\n\ndef task_func(data: np.ndarray, threshold: float = 2.0) -> list:\n    \"\"\"\n    Determines the outlier indices in a 1D numpy array based on the Z score.\n    \"\"\"\n    # Calculate mean and standard deviation\n    mean = np.mean(data)\n    std_dev = np.std(data)\n\n    # If standard deviation is 0, return empty list\n    if std_dev == 0:\n        return [], mean, std_dev**2\n\n    # Calculate Z scores\n    z_scores = (data - mean) / std_dev\n\n    # Find outliers\n    outliers = [i for i, z in enumerate(z_scores) if np.abs(z) > threshold]\n\n    # Return outliers, mean, and variance\n    return outliers, mean, std_dev**2\n\n# Test the function\ndata = np.array([-10, 3, 5, 5, 5, 5, 5, 7, 20])\noutliers, mean, var = task_func(data, threshold=4)"}
{"task_id": "BigCodeBench/808", "solution": "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Remove stopwords\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n    text = ' '.join(words)\n\n    # Remove duplicate words\n    words = list(set(text.split()))\n    text = ' '.join(words)\n\n    # Analyze sentiment\n    blob = TextBlob(text)\n    sentiment = blob.sentiment.polarity\n\n    return sentiment\ntext = \"This is a sample text. It contains duplicate words and stopwords.\"", "raw_solution": "import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(text):\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n\n    # Remove stopwords\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n    text = ' '.join(words)\n\n    # Remove duplicate words\n    words = list(set(text.split()))\n    text = ' '.join(words)\n\n    # Analyze sentiment\n    blob = TextBlob(text)\n    sentiment = blob.sentiment.polarity\n\n    return sentiment\n\n# Test the function\ntext = \"This is a sample text. It contains duplicate words and stopwords.\""}
{"task_id": "BigCodeBench/809", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters):\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n    labels = kmeans.labels_\n    result = {}\n    for i, label in enumerate(labels):\n        if label not in result:\n            result[label] = []\n        result[label].append(i)\n    return result\ndata = np.array([[1, 1], [2, 2]])", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef task_func(data, n_clusters):\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n    labels = kmeans.labels_\n    result = {}\n    for i, label in enumerate(labels):\n        if label not in result:\n            result[label] = []\n        result[label].append(i)\n    return result\n\n# Testing the function\ndata = np.array([[1, 1], [2, 2]])\ncluster = task_func(data, 2)\ncluster_list = list(cluster.values())\ncluster_list.sort(key=lambda x: x[0])"}
{"task_id": "BigCodeBench/810", "solution": "import re\nimport os\nimport subprocess\ndef task_func(dir_path, exe_pattern, execute_files=True):\n    \"\"\"\n    Searches for executable files in a specified directory that match a given regular expression pattern.\n    Optionally executes any matching files and returns a list of standard outputs from the executed files or the paths of the found files.\n    \"\"\"\n    # Compile the regular expression pattern\n    pattern = re.compile(exe_pattern)\n\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(dir_path):\n        # Check if the file is an executable file\n        if os.access(os.path.join(dir_path, filename), os.X_OK) and pattern.match(filename):\n            if execute_files:\n                # Execute the file and capture its standard output\n                output = subprocess.check_output([os.path.join(dir_path, filename)]).decode('utf-8')\n                results.append(output)\n            else:\n                # Add the file path to the results\n                results.append(os.path.join(dir_path, filename))\n\n    return results", "raw_solution": "import re\nimport os\nimport subprocess\n\ndef task_func(dir_path, exe_pattern, execute_files=True):\n    \"\"\"\n    Searches for executable files in a specified directory that match a given regular expression pattern.\n    Optionally executes any matching files and returns a list of standard outputs from the executed files or the paths of the found files.\n    \"\"\"\n    # Compile the regular expression pattern\n    pattern = re.compile(exe_pattern)\n\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(dir_path):\n        # Check if the file is an executable file\n        if os.access(os.path.join(dir_path, filename), os.X_OK) and pattern.match(filename):\n            if execute_files:\n                # Execute the file and capture its standard output\n                output = subprocess.check_output([os.path.join(dir_path, filename)]).decode('utf-8')\n                results.append(output)\n            else:\n                # Add the file path to the results\n                results.append(os.path.join(dir_path, filename))\n\n    return results"}
{"task_id": "BigCodeBench/811", "solution": "import pandas as pd\nfrom random import randint, seed\ndef task_func(dictionary, item, sample_size=None, random_seed=None):\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(dictionary)\n\n    # Find positions of item\n    positions = [(i, col) for i, row in df.iterrows() for col, val in row.items() if val == item]\n\n    # If sample_size is provided, return a random sample of positions\n    if sample_size is not None:\n        if random_seed is not None:\n            seed(random_seed)\n        positions = [positions[randint(0, len(positions) - 1)] for _ in range(sample_size)]\n\n    return positions, df\ndictionary = {1: ['road', 'car', 'traffic'], 2: ['car', 'light', 'candle']}", "raw_solution": "import pandas as pd\nfrom random import randint, seed\n\ndef task_func(dictionary, item, sample_size=None, random_seed=None):\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(dictionary)\n\n    # Find positions of item\n    positions = [(i, col) for i, row in df.iterrows() for col, val in row.items() if val == item]\n\n    # If sample_size is provided, return a random sample of positions\n    if sample_size is not None:\n        if random_seed is not None:\n            seed(random_seed)\n        positions = [positions[randint(0, len(positions) - 1)] for _ in range(sample_size)]\n\n    return positions, df\n\n# Test the function\ndictionary = {1: ['road', 'car', 'traffic'], 2: ['car', 'light', 'candle']}\npositions, df = task_func(dictionary, 'car', sample_size=2, random_seed=42)"}
{"task_id": "BigCodeBench/812", "solution": "import re\nfrom pathlib import Path\nimport tarfile\nPATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIRECTORY = r\"C:\\\\SomeDir\\\\\"\ndef task_func(directory=DIRECTORY, file_pattern=PATTERN):\n    \"\"\"\n    Searches for files matching the given pattern in the given directory,\n    and archives them in a tar file.\n    \"\"\"\n    # Create a regex object from the pattern\n    regex = re.compile(file_pattern)\n\n    # Create a Path object for the directory\n    dir_path = Path(directory)\n\n    # Find all files in the directory that match the pattern\n    matching_files = [file for file in dir_path.iterdir() if regex.match(file.name)]\n\n    # If no matching files are found, return an error message\n    if not matching_files:\n        return \"No files found matching the pattern.\"\n\n    # Create a tar file\n    tar_path = dir_path / \"archive.tar\"\n    with tarfile.open(tar_path, \"w\") as tar:\n        # Add each matching file to the tar file\n        for file in matching_files:\n            tar.add(file, arcname=file.name)\n\n    # Return the path to the created tar file\n    return str(tar_path)", "raw_solution": "import re\nfrom pathlib import Path\nimport tarfile\n\n# Constants\nPATTERN = r\"(?<!Distillr)\\\\\\\\AcroTray\\.exe\"\nDIRECTORY = r\"C:\\\\SomeDir\\\\\"\n\ndef task_func(directory=DIRECTORY, file_pattern=PATTERN):\n    \"\"\"\n    Searches for files matching the given pattern in the given directory,\n    and archives them in a tar file.\n    \"\"\"\n    # Create a regex object from the pattern\n    regex = re.compile(file_pattern)\n\n    # Create a Path object for the directory\n    dir_path = Path(directory)\n\n    # Find all files in the directory that match the pattern\n    matching_files = [file for file in dir_path.iterdir() if regex.match(file.name)]\n\n    # If no matching files are found, return an error message\n    if not matching_files:\n        return \"No files found matching the pattern.\"\n\n    # Create a tar file\n    tar_path = dir_path / \"archive.tar\"\n    with tarfile.open(tar_path, \"w\") as tar:\n        # Add each matching file to the tar file\n        for file in matching_files:\n            tar.add(file, arcname=file.name)\n\n    # Return the path to the created tar file\n    return str(tar_path)"}
{"task_id": "BigCodeBench/813", "solution": "from itertools import combinations\nimport pandas as pd\ndef task_func(number_list, element):\n    if not number_list:\n        return pd.DataFrame(columns=['Combinations'])\n    result = [combo for combo in combinations(number_list, 3) if sum(combo) == element]\n    return pd.DataFrame(result, columns=['Combinations'])", "raw_solution": "from itertools import combinations\nimport pandas as pd\n\ndef task_func(number_list, element):\n    if not number_list:\n        return pd.DataFrame(columns=['Combinations'])\n    result = [combo for combo in combinations(number_list, 3) if sum(combo) == element]\n    return pd.DataFrame(result, columns=['Combinations'])\n\n# Testing the function\nresult = task_func([-1, 1, 0, -2, 2, 3], 0)"}
{"task_id": "BigCodeBench/814", "solution": "import re\nimport os\ndef task_func(source_dir, target_dir, file_pattern=r'(?<!Distillr)\\\\ AcroTray\\\\.exe'):\n    \"\"\"\n    Searches for files matching the given pattern in the source directory and writes their paths to a configuration file in the target directory.\n    \"\"\"\n    # Create target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Create configuration file path\n    config_file_path = os.path.join(target_dir, 'config.txt')\n\n    # Open configuration file for writing\n    with open(config_file_path, 'w') as config_file:\n        # Iterate over files in source directory\n        for filename in os.listdir(source_dir):\n            # Check if file matches pattern\n            if re.search(file_pattern, filename):\n                # Get file path\n                file_path = os.path.join(source_dir, filename)\n                # Write file path to configuration file\n                config_file.write(f'{file_path}\\n')\n\n    return config_file_path", "raw_solution": "import re\nimport os\n\ndef task_func(source_dir, target_dir, file_pattern=r'(?<!Distillr)\\\\ AcroTray\\\\.exe'):\n    \"\"\"\n    Searches for files matching the given pattern in the source directory and writes their paths to a configuration file in the target directory.\n    \"\"\"\n    # Create target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Create configuration file path\n    config_file_path = os.path.join(target_dir, 'config.txt')\n\n    # Open configuration file for writing\n    with open(config_file_path, 'w') as config_file:\n        # Iterate over files in source directory\n        for filename in os.listdir(source_dir):\n            # Check if file matches pattern\n            if re.search(file_pattern, filename):\n                # Get file path\n                file_path = os.path.join(source_dir, filename)\n                # Write file path to configuration file\n                config_file.write(f'{file_path}\\n')\n\n    return config_file_path"}
{"task_id": "BigCodeBench/815", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(test_scores, student):\n    df = pd.DataFrame(test_scores)\n    if student not in df['Student'].values:\n        raise ValueError('student is not present in the test_scores dataframe')\n    else:\n        student_scores = df.loc[df['Student'] == student, 'Score']\n        return np.array([student_scores.mean(), student_scores.std()]), df", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(test_scores, student):\n    df = pd.DataFrame(test_scores)\n    if student not in df['Student'].values:\n        raise ValueError('student is not present in the test_scores dataframe')\n    else:\n        student_scores = df.loc[df['Student'] == student, 'Score']\n        return np.array([student_scores.mean(), student_scores.std()]), df"}
{"task_id": "BigCodeBench/816", "solution": "from collections import Counter\nimport random", "raw_solution": "from collections import Counter\nimport random\n\n# Constants\nHAND_RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\nSUITS = ['H', 'D', 'C', 'S']\n\ndef generate_poker_hand():\n    \"\"\"\n    Generates a random poker hand consisting of five cards, and counts the frequency of each card rank.\n    \"\"\"\n    hand = [random.choice(HAND_RANKS) + random.choice(SUITS) for _ in range(5)]\n    rank_count = Counter(card[0] for card in hand)\n    return (hand, rank_count)\n\ndef test_generate_poker_hand():\n    \"\"\"\n    Tests the generate_poker_hand function.\n    \"\"\"\n    hand, rank_count = generate_poker_hand()\n    assert len(hand) == 5\n    assert all(card[0] in HAND_RANKS for card in hand)\n    assert all(card[1] in SUITS for card in hand)\n    assert len(rank_count) == 5\n    assert all(count >= 1 and count <= 4 for count in rank_count.values())\n\n# Run the test\ntest_generate_poker_hand()"}
{"task_id": "BigCodeBench/817", "solution": "from collections import Counter\nimport logging\ndef task_func(letter_list, element, log_path='./'):\n    \"\"\"\n    Count the frequency of a particular letter in a given list of letters.\n    \"\"\"\n    # Set up logging\n    logging.basicConfig(filename=f'{log_path}task_func.log', filemode='w', format='%(levelname)s:%(message)s', level=logging.DEBUG, encoding='utf-8')\n\n    # Log function call\n    logging.info(f\"Function called with list: {letter_list} and element: {element}\")\n\n    # Check if element is in letter_list\n    if element not in letter_list:\n        logging.error(f\"The element '{element}' is not in the letter list.\")\n        raise ValueError(\"The element is not in the letter list.\")\n\n    # Count frequency of element\n    frequency = Counter(letter_list)[element]\n\n    # Log frequency\n    logging.info(f\"Frequency of '{element}' is {frequency}\")\n\n    # Shutdown logging\n    logging.shutdown()\n\n    return frequency", "raw_solution": "from collections import Counter\nimport logging\n\ndef task_func(letter_list, element, log_path='./'):\n    \"\"\"\n    Count the frequency of a particular letter in a given list of letters.\n    \"\"\"\n    # Set up logging\n    logging.basicConfig(filename=f'{log_path}task_func.log', filemode='w', format='%(levelname)s:%(message)s', level=logging.DEBUG, encoding='utf-8')\n\n    # Log function call\n    logging.info(f\"Function called with list: {letter_list} and element: {element}\")\n\n    # Check if element is in letter_list\n    if element not in letter_list:\n        logging.error(f\"The element '{element}' is not in the letter list.\")\n        raise ValueError(\"The element is not in the letter list.\")\n\n    # Count frequency of element\n    frequency = Counter(letter_list)[element]\n\n    # Log frequency\n    logging.info(f\"Frequency of '{element}' is {frequency}\")\n\n    # Shutdown logging\n    logging.shutdown()\n\n    return frequency"}
{"task_id": "BigCodeBench/818", "solution": "import re\nimport string\nPUNCTUATION = string.punctuation\ndef task_func(text):\n    \"\"\"\n    Divides a string into words, removes punctuation marks and converts them to lowercase.\n    \"\"\"\n    # Remove punctuation\n    text = re.sub(f'[{PUNCTUATION}]', '', text)\n    # Split into words and convert to lowercase\n    cleaned_words = text.lower().split()\n    return cleaned_words\ntext = \"Hello, world! How's it going? Good, I hope.\"", "raw_solution": "import re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef task_func(text):\n    \"\"\"\n    Divides a string into words, removes punctuation marks and converts them to lowercase.\n    \"\"\"\n    # Remove punctuation\n    text = re.sub(f'[{PUNCTUATION}]', '', text)\n    # Split into words and convert to lowercase\n    cleaned_words = text.lower().split()\n    return cleaned_words\n\n# Test the function\ntext = \"Hello, world! How's it going? Good, I hope.\""}
{"task_id": "BigCodeBench/819", "solution": "import time\nimport random\ndef task_func(iterations=5, min_delay=1.0, max_delay=2.0, seed=None):\n    if not isinstance(iterations, int) or iterations <= 0:\n        raise ValueError(\"Iterations must be a positive integer\")\n    if not isinstance(min_delay, (int, float)) or min_delay <= 0:\n        raise ValueError(\"Min_delay must be a positive number\")\n    if not isinstance(max_delay, (int, float)) or max_delay <= 0:\n        raise ValueError(\"Max_delay must be a positive number\")\n    if seed is not None:\n        random.seed(seed)\n    messages = []\n    total_delay = 0\n    for i in range(iterations):\n        delay = random.uniform(min_delay, max_delay)\n        time.sleep(delay)\n        messages.append(f\"{delay:.2f} seconds have passed\")\n        total_delay += delay\n    return messages, total_delay", "raw_solution": "import time\nimport random\n\ndef task_func(iterations=5, min_delay=1.0, max_delay=2.0, seed=None):\n    if not isinstance(iterations, int) or iterations <= 0:\n        raise ValueError(\"Iterations must be a positive integer\")\n    if not isinstance(min_delay, (int, float)) or min_delay <= 0:\n        raise ValueError(\"Min_delay must be a positive number\")\n    if not isinstance(max_delay, (int, float)) or max_delay <= 0:\n        raise ValueError(\"Max_delay must be a positive number\")\n    if seed is not None:\n        random.seed(seed)\n    messages = []\n    total_delay = 0\n    for i in range(iterations):\n        delay = random.uniform(min_delay, max_delay)\n        time.sleep(delay)\n        messages.append(f\"{delay:.2f} seconds have passed\")\n        total_delay += delay\n    return messages, total_delay"}
{"task_id": "BigCodeBench/820", "solution": "import random\nimport string\nLETTERS = string.ascii_letters\ndef task_func(num_words, word_length):\n    \"\"\"\n    Generates a list of random words of a certain length.\n    \"\"\"\n    if num_words < 0 or word_length < 0:\n        raise ValueError(\"num_words and word_length should be non-negative\")\n\n    words = []\n    for _ in range(num_words):\n        word = ''.join(random.choice(LETTERS) for _ in range(word_length))\n        words.append(word)\n\n    return words", "raw_solution": "import random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\n\ndef task_func(num_words, word_length):\n    \"\"\"\n    Generates a list of random words of a certain length.\n    \"\"\"\n    if num_words < 0 or word_length < 0:\n        raise ValueError(\"num_words and word_length should be non-negative\")\n\n    words = []\n    for _ in range(num_words):\n        word = ''.join(random.choice(LETTERS) for _ in range(word_length))\n        words.append(word)\n\n    return words\n\n# Test the function"}
{"task_id": "BigCodeBench/821", "solution": "import time\nimport threading\ndef task_func(delay_time: float = 1.0, num_threads: int = 5):\n    def thread_func(i):\n        time.sleep(delay_time)\n        print(f'Delay in thread {i} completed')\n\n    threads = []\n    for i in range(num_threads):\n        thread = threading.Thread(target=thread_func, args=(i,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return [f'Delay in thread {i} completed' for i in range(num_threads)]", "raw_solution": "import time\nimport threading\n\ndef task_func(delay_time: float = 1.0, num_threads: int = 5):\n    def thread_func(i):\n        time.sleep(delay_time)\n        print(f'Delay in thread {i} completed')\n\n    threads = []\n    for i in range(num_threads):\n        thread = threading.Thread(target=thread_func, args=(i,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return [f'Delay in thread {i} completed' for i in range(num_threads)]\n"}
{"task_id": "BigCodeBench/822", "solution": "import random\nimport string\nLETTERS = string.ascii_letters\nDIGITS = string.digits\ndef task_func(length, num_digits):\n    if num_digits > length:\n        raise ValueError(\"Number of digits cannot be greater than the length of the password\")\n    password_digits = ''.join(random.choice(DIGITS) for _ in range(num_digits))\n    password_letters = ''.join(random.choice(LETTERS) for _ in range(length - num_digits))\n    password = list(password_digits + password_letters)\n    random.shuffle(password)\n    return ''.join(password)", "raw_solution": "import random\nimport string\n\n# Constants\nLETTERS = string.ascii_letters\nDIGITS = string.digits\n\ndef task_func(length, num_digits):\n    if num_digits > length:\n        raise ValueError(\"Number of digits cannot be greater than the length of the password\")\n    password_digits = ''.join(random.choice(DIGITS) for _ in range(num_digits))\n    password_letters = ''.join(random.choice(LETTERS) for _ in range(length - num_digits))\n    password = list(password_digits + password_letters)\n    random.shuffle(password)\n    return ''.join(password)\n\n# Testing the function"}
{"task_id": "BigCodeBench/823", "solution": "import time\nimport numpy as np\ndef task_func(samples=10, delay=0.1):\n    \"\"\"\n    This function simulates a delay for a given amount of time for a specified number of samples,\n    measures the actual delay and calculates the statistical properties of the delay times.\n    \"\"\"\n    # Initialize an empty list to store the delay times\n    delay_times = []\n\n    # Start the timer\n    start_time = time.time()\n\n    # Simulate the delay for the specified number of samples\n    for _ in range(samples):\n        time.sleep(delay)\n        # Measure the actual delay and append it to the list\n        delay_times.append(time.time() - start_time)\n\n    # Calculate the mean and standard deviation of the delay times\n    mean_delay = np.mean(delay_times)\n    std_dev_delay = np.std(delay_times)\n\n    # Return the mean and standard deviation as a tuple\n    return (mean_delay, std_dev_delay)", "raw_solution": "import time\nimport numpy as np\n\ndef task_func(samples=10, delay=0.1):\n    \"\"\"\n    This function simulates a delay for a given amount of time for a specified number of samples,\n    measures the actual delay and calculates the statistical properties of the delay times.\n    \"\"\"\n    # Initialize an empty list to store the delay times\n    delay_times = []\n\n    # Start the timer\n    start_time = time.time()\n\n    # Simulate the delay for the specified number of samples\n    for _ in range(samples):\n        time.sleep(delay)\n        # Measure the actual delay and append it to the list\n        delay_times.append(time.time() - start_time)\n\n    # Calculate the mean and standard deviation of the delay times\n    mean_delay = np.mean(delay_times)\n    std_dev_delay = np.std(delay_times)\n\n    # Return the mean and standard deviation as a tuple\n    return (mean_delay, std_dev_delay)\n\n# Test the function\nmean_delay, std_dev_delay = task_func(samples=10, delay=0.1)"}
{"task_id": "BigCodeBench/824", "solution": "import re\nimport string\nPUNCTUATION = string.punctuation\ndef task_func(text):\n    words = re.findall(r'\\b\\w+\\b', text)\n    punctuation_marks = sum(text.count(ch) for ch in PUNCTUATION if ch in text)\n    return (len(words), punctuation_marks)\ntext = \"Hello, world! How's it going? Good, I hope.\"", "raw_solution": "import re\nimport string\n\n# Constants\nPUNCTUATION = string.punctuation\n\ndef task_func(text):\n    words = re.findall(r'\\b\\w+\\b', text)\n    punctuation_marks = sum(text.count(ch) for ch in PUNCTUATION if ch in text)\n    return (len(words), punctuation_marks)\n\n# Testing the function\ntext = \"Hello, world! How's it going? Good, I hope.\""}
{"task_id": "BigCodeBench/825", "solution": "import numpy as np\nfrom itertools import product\nimport string\ndef task_func(length, seed=None, alphabets=list(string.ascii_lowercase)):\n    np.random.seed(seed)\n    return [''.join(np.random.choice(alphabets, size=length)) for _ in range(10)]", "raw_solution": "import numpy as np\nfrom itertools import product\nimport string\n\ndef task_func(length, seed=None, alphabets=list(string.ascii_lowercase)):\n    np.random.seed(seed)\n    return [''.join(np.random.choice(alphabets, size=length)) for _ in range(10)]\n"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Moves files from the source directory to the target directory based on a specified pattern.\n    \"\"\"\n    if not os.path.exists(source_dir) or not os.path.exists(target_dir):\n        raise ValueError(\"Source or target directory does not exist.\")\n\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Moves files from the source directory to the target directory based on a specified pattern.\n    \"\"\"\n    if not os.path.exists(source_dir) or not os.path.exists(target_dir):\n        raise ValueError(\"Source or target directory does not exist.\")\n\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            shutil.move(os.path.join(source_dir, filename), target_dir)\n            moved_files_count += 1\n\n    return moved_files_count"}
{"task_id": "BigCodeBench/827", "solution": "import math\nfrom sympy import isprime\nfrom typing import List\ndef task_func(input_list: List[int]) -> List[int]:\n    \"\"\"\n    Filters the prime numbers from the specified list, sorts the prime numbers ascending based on their radian value converted to degrees, and returns the sorted list.\n    \"\"\"\n    # Filter prime numbers\n    prime_list = [num for num in input_list if isprime(num)]\n\n    # Convert radian to degrees and sort\n    prime_list.sort(key=lambda x: math.degrees(math.atan(x)))\n\n    return prime_list", "raw_solution": "import math\nfrom sympy import isprime\nfrom typing import List\n\ndef task_func(input_list: List[int]) -> List[int]:\n    \"\"\"\n    Filters the prime numbers from the specified list, sorts the prime numbers ascending based on their radian value converted to degrees, and returns the sorted list.\n    \"\"\"\n    # Filter prime numbers\n    prime_list = [num for num in input_list if isprime(num)]\n\n    # Convert radian to degrees and sort\n    prime_list.sort(key=lambda x: math.degrees(math.atan(x)))\n\n    return prime_list\n\n# Test the function"}
{"task_id": "BigCodeBench/828", "solution": "import os\nimport errno\nimport shutil\ndef task_func(filename, dest_dir):\n    \"\"\"\n    Copies a file to a specified destination directory and clears its contents.\n    \"\"\"\n    try:\n        # Check if the destination directory exists, if not create it\n        if not os.path.exists(dest_dir):\n            os.makedirs(dest_dir)\n\n        # Construct full paths\n        dest_file = os.path.join(dest_dir, os.path.basename(filename))\n\n        # Copy the file\n        shutil.copy2(filename, dest_dir)\n\n        # Clear the content of the original file\n        with open(dest_file, 'w'):\n            pass\n\n        return os.path.abspath(dest_file)\n\n    except OSError as e:\n        if e.errno == errno.ENOENT:\n            raise OSError(\"Destination directory does not exist or file cannot be copied.\")\n        else:\n            raise OSError(\"An unknown error occurred.\")", "raw_solution": "import os\nimport errno\nimport shutil\n\ndef task_func(filename, dest_dir):\n    \"\"\"\n    Copies a file to a specified destination directory and clears its contents.\n    \"\"\"\n    try:\n        # Check if the destination directory exists, if not create it\n        if not os.path.exists(dest_dir):\n            os.makedirs(dest_dir)\n\n        # Construct full paths\n        dest_file = os.path.join(dest_dir, os.path.basename(filename))\n\n        # Copy the file\n        shutil.copy2(filename, dest_dir)\n\n        # Clear the content of the original file\n        with open(dest_file, 'w'):\n            pass\n\n        return os.path.abspath(dest_file)\n\n    except OSError as e:\n        if e.errno == errno.ENOENT:\n            raise OSError(\"Destination directory does not exist or file cannot be copied.\")\n        else:\n            raise OSError(\"An unknown error occurred.\")"}
{"task_id": "BigCodeBench/829", "solution": "import pandas as pd\nfrom statistics import mean\nfrom typing import Dict, Generator\ndef task_func(df: pd.DataFrame) -> Dict[str, Generator[str, float, None]]:\n    \"\"\"\n    Converts a Pandas DataFrame into a dictionary of generator objects.\n    Each generator generates a sequence of tuples that contain a unique name and the corresponding average score for that name.\n    \"\"\"\n    if 'Name' not in df.columns or 'Score' not in df.columns:\n        raise ValueError(\"The DataFrame must have 'Name' and 'Score' columns.\")\n\n    df_grouped = df.groupby('Name')['Score'].apply(list).reset_index()\n    gen_dict = {name: ((name, score) for score in scores) for name, scores in zip(df_grouped['Name'], df_grouped['Score'])}\n\n    return gen_dict\ndf_sample = pd.DataFrame({\n    'Name': ['Micky', 'Donald', 'Girl'],\n    'Score': [25.2, 9, -1]\n})", "raw_solution": "import pandas as pd\nfrom statistics import mean\nfrom typing import Dict, Generator\n\ndef task_func(df: pd.DataFrame) -> Dict[str, Generator[str, float, None]]:\n    \"\"\"\n    Converts a Pandas DataFrame into a dictionary of generator objects.\n    Each generator generates a sequence of tuples that contain a unique name and the corresponding average score for that name.\n    \"\"\"\n    if 'Name' not in df.columns or 'Score' not in df.columns:\n        raise ValueError(\"The DataFrame must have 'Name' and 'Score' columns.\")\n\n    df_grouped = df.groupby('Name')['Score'].apply(list).reset_index()\n    gen_dict = {name: ((name, score) for score in scores) for name, scores in zip(df_grouped['Name'], df_grouped['Score'])}\n\n    return gen_dict\n\n# Testing the function\ndf_sample = pd.DataFrame({\n    'Name': ['Micky', 'Donald', 'Girl'],\n    'Score': [25.2, 9, -1]\n})\n\ngen_dict = task_func(df_sample)\n\n# Printing the first tuple from each generator"}
{"task_id": "BigCodeBench/830", "solution": "import json\nimport os\ndef task_func(filename, data):\n    # Check if file exists\n    file_exists = os.path.isfile(filename)\n\n    # Write data to file\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n\n    # Read data from file\n    with open(filename, 'r') as f:\n        read_data = json.load(f)\n\n    # Verify data\n    if read_data == data:\n        success = True\n        written_data = data\n    else:\n        success = False\n        written_data = None\n\n    return (success, written_data)\nfilename = 'test.json'\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}", "raw_solution": "import json\nimport os\n\ndef task_func(filename, data):\n    # Check if file exists\n    file_exists = os.path.isfile(filename)\n\n    # Write data to file\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n\n    # Read data from file\n    with open(filename, 'r') as f:\n        read_data = json.load(f)\n\n    # Verify data\n    if read_data == data:\n        success = True\n        written_data = data\n    else:\n        success = False\n        written_data = None\n\n    return (success, written_data)\n\n# Test the function\nfilename = 'test.json'\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}"}
{"task_id": "BigCodeBench/831", "solution": "import random\nimport math\ndef task_func(range_start=1, range_end=100, pairs_count=10, random_seed=None):\n    \"\"\"\n    Generates a sequence of tuples. Each tuple contains two random numbers and the square root of their absolute difference.\n    \"\"\"\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    for _ in range(pairs_count):\n        num1 = random.randint(range_start, range_end)\n        num2 = random.randint(range_start, range_end)\n        yield (num1, num2, math.sqrt(abs(num1 - num2)))", "raw_solution": "import random\nimport math\n\ndef task_func(range_start=1, range_end=100, pairs_count=10, random_seed=None):\n    \"\"\"\n    Generates a sequence of tuples. Each tuple contains two random numbers and the square root of their absolute difference.\n    \"\"\"\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    for _ in range(pairs_count):\n        num1 = random.randint(range_start, range_end)\n        num2 = random.randint(range_start, range_end)\n        yield (num1, num2, math.sqrt(abs(num1 - num2)))\n\n# Test the function\npairs = task_func(1, 3, pairs_count=25, random_seed=14)"}
{"task_id": "BigCodeBench/832", "solution": "import pickle\nimport os\ndef task_func(filename, data):\n    \"\"\"\n    Serialize an object using pickle and overwrite the specified file with this serialized data.\n    Before writing, checks if the directory exists, creating it if necessary.\n    \"\"\"\n    try:\n        # Check if the directory exists, create it if necessary\n        directory = os.path.dirname(filename)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        # Serialize the data\n        serialized_data = pickle.dumps(data)\n\n        # Write the serialized data to the file\n        with open(filename, 'wb') as file:\n            file.write(serialized_data)\n\n        return True\n    except Exception:\n        return False", "raw_solution": "import pickle\nimport os\n\ndef task_func(filename, data):\n    \"\"\"\n    Serialize an object using pickle and overwrite the specified file with this serialized data.\n    Before writing, checks if the directory exists, creating it if necessary.\n    \"\"\"\n    try:\n        # Check if the directory exists, create it if necessary\n        directory = os.path.dirname(filename)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        # Serialize the data\n        serialized_data = pickle.dumps(data)\n\n        # Write the serialized data to the file\n        with open(filename, 'wb') as file:\n            file.write(serialized_data)\n\n        return True\n    except Exception:\n        return False"}
{"task_id": "BigCodeBench/833", "solution": "import random\nfrom collections import Counter\nfrom statistics import mode\ndef task_func(list_length=1000, range_start=1, range_end=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n    random_list = [random.randint(range_start, range_end) for _ in range(list_length)]\n    counter = Counter(random_list)\n    gen = ((num, freq) for num, freq in counter.items())\n    return mode(random_list), gen", "raw_solution": "import random\nfrom collections import Counter\nfrom statistics import mode\n\ndef task_func(list_length=1000, range_start=1, range_end=10, random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n    random_list = [random.randint(range_start, range_end) for _ in range(list_length)]\n    counter = Counter(random_list)\n    gen = ((num, freq) for num, freq in counter.items())\n    return mode(random_list), gen"}
{"task_id": "BigCodeBench/834", "solution": "import binascii\nimport io\nimport gzip\nfrom cryptography.fernet import Fernet\ndef task_func(compressed_hex, key):\n    try:\n        # Convert hexadecimal string to bytes\n        compressed_bytes = binascii.unhexlify(compressed_hex)\n\n        # Decrypt the bytes\n        cipher_suite = Fernet(key)\n        decrypted_bytes = cipher_suite.decrypt(compressed_bytes)\n\n        # Decompress the bytes\n        decompressed_bytes = gzip.decompress(decrypted_bytes)\n\n        # Convert bytes to string\n        decoded_string = decompressed_bytes.decode('utf-8')\n\n    except (binascii.Error, gzip.BadGzipFile, UnicodeDecodeError) as e:\n        decoded_string = f\"Error: {str(e)}\"\n\n    return decoded_string", "raw_solution": "import binascii\nimport io\nimport gzip\nfrom cryptography.fernet import Fernet\n\ndef task_func(compressed_hex, key):\n    try:\n        # Convert hexadecimal string to bytes\n        compressed_bytes = binascii.unhexlify(compressed_hex)\n\n        # Decrypt the bytes\n        cipher_suite = Fernet(key)\n        decrypted_bytes = cipher_suite.decrypt(compressed_bytes)\n\n        # Decompress the bytes\n        decompressed_bytes = gzip.decompress(decrypted_bytes)\n\n        # Convert bytes to string\n        decoded_string = decompressed_bytes.decode('utf-8')\n\n    except (binascii.Error, gzip.BadGzipFile, UnicodeDecodeError) as e:\n        decoded_string = f\"Error: {str(e)}\"\n\n    return decoded_string"}
{"task_id": "BigCodeBench/835", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(n_rows, remove_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    df = pd.DataFrame(np.random.randint(0, 100, size=(n_rows, len(columns))), columns=columns)\n    df = df.drop(df.columns[remove_cols], axis=1)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(n_rows, remove_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    df = pd.DataFrame(np.random.randint(0, 100, size=(n_rows, len(columns))), columns=columns)\n    df = df.drop(df.columns[remove_cols], axis=1)\n    return df\n\n# Test the function\ndf = task_func(3, [1, 3], columns=['test', 'rem1', 'apple', 'remove'], random_seed=12)"}
{"task_id": "BigCodeBench/836", "solution": "import os\nimport shutil\nimport csv\ndef task_func(target_value='332', csv_dir='./csv_files/', processed_dir='./processed_files/', simulate=False):\n    result = {}\n    for filename in os.listdir(csv_dir):\n        if filename.endswith('.csv'):\n            file_path = os.path.join(csv_dir, filename)\n            with open(file_path, 'r') as file:\n                reader = csv.reader(file)\n                for i, row in enumerate(reader):\n                    if row and row[0] == target_value:\n                        result[filename] = i\n                        if not simulate:\n                            shutil.move(file_path, os.path.join(processed_dir, filename))\n                        break\n    return result", "raw_solution": "import os\nimport shutil\nimport csv\n\ndef task_func(target_value='332', csv_dir='./csv_files/', processed_dir='./processed_files/', simulate=False):\n    result = {}\n    for filename in os.listdir(csv_dir):\n        if filename.endswith('.csv'):\n            file_path = os.path.join(csv_dir, filename)\n            with open(file_path, 'r') as file:\n                reader = csv.reader(file)\n                for i, row in enumerate(reader):\n                    if row and row[0] == target_value:\n                        result[filename] = i\n                        if not simulate:\n                            shutil.move(file_path, os.path.join(processed_dir, filename))\n                        break\n    return result"}
{"task_id": "BigCodeBench/837", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(n_rows, scale_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    np.random.seed(random_seed)\n    df = pd.DataFrame(np.random.randint(-100, 100, size=(n_rows, len(columns))), columns=columns)\n    scaler = StandardScaler()", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(n_rows, scale_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    np.random.seed(random_seed)\n    df = pd.DataFrame(np.random.randint(-100, 100, size=(n_rows, len(columns))), columns=columns)\n    scaler = StandardScaler()\n    df[columns[i-1] for i in scale_cols] = scaler.fit_transform(df[columns[i-1] for i in scale_cols])\n    return df\n\ndf = task_func(5, [1, 2, 3], random_seed=12)"}
{"task_id": "BigCodeBench/838", "solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nimport pandas as pd\ndef task_func(text_series):\n    # Initialize PorterStemmer\n    ps = PorterStemmer()\n\n    # Lowercase all letters\n    text_series = text_series.str.lower()\n\n    # Remove non-alphanumeric characters (except spaces)\n    text_series = text_series.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n\n    # Remove punctuation\n    text_series = text_series.str.translate(str.maketrans('', '', string.punctuation))\n\n    # Stem each word\n    text_series = text_series.apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n\n    return text_series\ntext_series = pd.Series(['Hello, world!', 'This is a test.', 'Stemming is fun.'])", "raw_solution": "import re\nimport string\nfrom nltk.stem import PorterStemmer\nimport pandas as pd\n\ndef task_func(text_series):\n    # Initialize PorterStemmer\n    ps = PorterStemmer()\n\n    # Lowercase all letters\n    text_series = text_series.str.lower()\n\n    # Remove non-alphanumeric characters (except spaces)\n    text_series = text_series.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n\n    # Remove punctuation\n    text_series = text_series.str.translate(str.maketrans('', '', string.punctuation))\n\n    # Stem each word\n    text_series = text_series.apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n\n    return text_series\n\n# Test the function\ntext_series = pd.Series(['Hello, world!', 'This is a test.', 'Stemming is fun.'])"}
{"task_id": "BigCodeBench/839", "solution": "import csv\nimport random\nimport string\nfrom typing import List, Optional\ndef task_func(file_path: str,\n              num_rows: int,\n              gender: List[str] = ['Male', 'Female', 'Non-Binary'],\n              countries: List[str] = ['USA', 'UK', 'Canada', 'Australia', 'India'],\n              seed: Optional[int] = None) -> str:\n    \"\"\"\n    Generates a CSV file with random data for the fields ['Name', 'Age', 'Gender', 'Country'].\n    The number of rows in the CSV file is determined by the 'num_rows' parameter.\n    The Ages are randomly sampled integers in the range [20, 60].\n    The names are generated by randomly choosing 5 uppercase characters from the english alphabet.\n    If num_rows <= 0 a csv containing only the headers is generated.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    with open(file_path, 'w', newline='') as csvfile:\n        fieldnames = ['Name', 'Age', 'Gender', 'Country']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n\n        if num_rows > 0:\n            for _ in range(num_rows):\n                name = ''.join(random.choices(string.ascii_uppercase, k=5))\n                age = random.randint(20, 60)\n                gend = random.choice(gender)\n                countr = random.choice(countries)\n                writer.writerow({'Name': name, 'Age': age, 'Gender': gend, 'Country': countr})\n\n    return file_path", "raw_solution": "import csv\nimport random\nimport string\nfrom typing import List, Optional\n\ndef task_func(file_path: str,\n              num_rows: int,\n              gender: List[str] = ['Male', 'Female', 'Non-Binary'],\n              countries: List[str] = ['USA', 'UK', 'Canada', 'Australia', 'India'],\n              seed: Optional[int] = None) -> str:\n    \"\"\"\n    Generates a CSV file with random data for the fields ['Name', 'Age', 'Gender', 'Country'].\n    The number of rows in the CSV file is determined by the 'num_rows' parameter.\n    The Ages are randomly sampled integers in the range [20, 60].\n    The names are generated by randomly choosing 5 uppercase characters from the english alphabet.\n    If num_rows <= 0 a csv containing only the headers is generated.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    with open(file_path, 'w', newline='') as csvfile:\n        fieldnames = ['Name', 'Age', 'Gender', 'Country']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n\n        if num_rows > 0:\n            for _ in range(num_rows):\n                name = ''.join(random.choices(string.ascii_uppercase, k=5))\n                age = random.randint(20, 60)\n                gend = random.choice(gender)\n                countr = random.choice(countries)\n                writer.writerow({'Name': name, 'Age': age, 'Gender': gend, 'Country': countr})\n\n    return file_path"}
{"task_id": "BigCodeBench/840", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(file_path, num_rows, data_dimensions=5, random_seed=None):\n    \"\"\"\n    Creates a CSV file on a given file path with random numeric data.\n    The number of rows in the CSV file is determined by the 'num_rows' parameter,\n    and the number of columns (features) is determined by the 'data_dimensions' parameter.\n    Columns are named following the convention: 'Feature_x', where x is the number of the feature column starting at 1.\n    The function returns the file path of the generated CSV file.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    data = np.random.rand(num_rows, data_dimensions)\n    df = pd.DataFrame(data, columns=[f'Feature_{i+1}' for i in range(data_dimensions)])\n    df.to_csv(file_path, index=False)\n\n    return file_path", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(file_path, num_rows, data_dimensions=5, random_seed=None):\n    \"\"\"\n    Creates a CSV file on a given file path with random numeric data.\n    The number of rows in the CSV file is determined by the 'num_rows' parameter,\n    and the number of columns (features) is determined by the 'data_dimensions' parameter.\n    Columns are named following the convention: 'Feature_x', where x is the number of the feature column starting at 1.\n    The function returns the file path of the generated CSV file.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    data = np.random.rand(num_rows, data_dimensions)\n    df = pd.DataFrame(data, columns=[f'Feature_{i+1}' for i in range(data_dimensions)])\n    df.to_csv(file_path, index=False)\n\n    return file_path"}
{"task_id": "BigCodeBench/841", "solution": "import re\nimport json\nfrom collections import defaultdict\nimport string\ndef task_func(json_string):\n    \"\"\"\n    Process a JSON string containing a \"text\" field: convert to lowercase, remove punctuation, and count word frequency.\n    \"\"\"\n    try:\n        data = json.loads(json_string)\n        if \"text\" in data:\n            text = data[\"text\"]\n            text = text.lower()\n            text = re.sub(r'[^\\w\\s]', '', text)\n            words = text.split()\n            word_counts = defaultdict(int)\n            for word in words:\n                word_counts[word] += 1\n            return word_counts\n        else:\n            return {}\n    except (json.JSONDecodeError, TypeError):\n        return {}", "raw_solution": "import re\nimport json\nfrom collections import defaultdict\nimport string\n\ndef task_func(json_string):\n    \"\"\"\n    Process a JSON string containing a \"text\" field: convert to lowercase, remove punctuation, and count word frequency.\n    \"\"\"\n    try:\n        data = json.loads(json_string)\n        if \"text\" in data:\n            text = data[\"text\"]\n            text = text.lower()\n            text = re.sub(r'[^\\w\\s]', '', text)\n            words = text.split()\n            word_counts = defaultdict(int)\n            for word in words:\n                word_counts[word] += 1\n            return word_counts\n        else:\n            return {}\n    except (json.JSONDecodeError, TypeError):\n        return {}\n\n# Test cases"}
{"task_id": "BigCodeBench/842", "solution": "import sqlite3\nimport random\nfrom faker import Faker\ndef task_func(db_path, num_entries, users=['Alice', 'Bob', 'Charlie', 'Dave', 'Eve'], countries=['USA', 'UK', 'Canada', 'Australia', 'India'], random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    fake = Faker()\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    c.execute('''\n        CREATE TABLE users\n        (id INTEGER PRIMARY KEY, name TEXT, age INTEGER, country TEXT)\n    ''')\n\n    for _ in range(num_entries):\n        name = random.choice(users)\n        age = random.randint(20, 60)\n        country = random.choice(countries)\n        c.execute(\"INSERT INTO users VALUES (?, ?, ?, ?)\", (_, name, age, country))\n\n    conn.commit()\n    conn.close()\n\n    return db_path", "raw_solution": "import sqlite3\nimport random\nfrom faker import Faker\n\ndef task_func(db_path, num_entries, users=['Alice', 'Bob', 'Charlie', 'Dave', 'Eve'], countries=['USA', 'UK', 'Canada', 'Australia', 'India'], random_seed=None):\n    if random_seed is not None:\n        random.seed(random_seed)\n\n    fake = Faker()\n\n    conn = sqlite3.connect(db_path)\n    c = conn.cursor()\n\n    c.execute('''\n        CREATE TABLE users\n        (id INTEGER PRIMARY KEY, name TEXT, age INTEGER, country TEXT)\n    ''')\n\n    for _ in range(num_entries):\n        name = random.choice(users)\n        age = random.randint(20, 60)\n        country = random.choice(countries)\n        c.execute(\"INSERT INTO users VALUES (?, ?, ?, ?)\", (_, name, age, country))\n\n    conn.commit()\n    conn.close()\n\n    return db_path"}
{"task_id": "BigCodeBench/843", "solution": "import random\nimport re\nWORD_LIST = [\"sample\", \"text\", \"contains\", \"several\", \"words\", \"including\"]\ndef task_func(n_sentences):\n    \"\"\"\n    Generates a string of random sentences using a predefined word list.\n    Each sentence is guaranteed to have one period at the end, and no period within the sentence.\n    The generated sentences are concatenated into a single string, with all letters in lowercase and all non-alphanumeric characters except spaces removed.\n    \"\"\"\n    sentences = []\n    for _ in range(n_sentences):\n        sentence = \" \".join(random.choice(WORD_LIST) for _ in range(random.randint(5, 15))) + \".\"\n        sentences.append(sentence)\n    result = \" \".join(sentences)\n    result = re.sub(r'[^a-z\\s]', '', result)\n    return result.lower()", "raw_solution": "import random\nimport re\n\n# Constants\nWORD_LIST = [\"sample\", \"text\", \"contains\", \"several\", \"words\", \"including\"]\n\ndef task_func(n_sentences):\n    \"\"\"\n    Generates a string of random sentences using a predefined word list.\n    Each sentence is guaranteed to have one period at the end, and no period within the sentence.\n    The generated sentences are concatenated into a single string, with all letters in lowercase and all non-alphanumeric characters except spaces removed.\n    \"\"\"\n    sentences = []\n    for _ in range(n_sentences):\n        sentence = \" \".join(random.choice(WORD_LIST) for _ in range(random.randint(5, 15))) + \".\"\n        sentences.append(sentence)\n    result = \" \".join(sentences)\n    result = re.sub(r'[^a-z\\s]', '', result)\n    return result.lower()\n\n# Test the function"}
{"task_id": "BigCodeBench/844", "solution": "import csv\nimport random\nfrom faker import Faker\ndef task_func(file_path, num_rows, random_seed=None):\n    \"\"\"\n    Generates a CSV file with fake personal data and returns the file path.\n    \"\"\"\n    if not isinstance(num_rows, int) or num_rows < 0:\n        raise ValueError(\"num_rows must be an integer >= 0\")\n\n    Faker.seed(random_seed)\n    fake = Faker()\n\n    data = [\n        ['Name', 'Age', 'Address', 'Email'],\n        *[\n            [\n                fake.name(),\n                str(random.randint(20, 60)),\n                fake.address().replace('\\n', ', '),\n                fake.email(),\n            ]\n            for _ in range(num_rows)\n        ],\n    ]\n\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(data)\n\n    return file_path", "raw_solution": "import csv\nimport random\nfrom faker import Faker\n\ndef task_func(file_path, num_rows, random_seed=None):\n    \"\"\"\n    Generates a CSV file with fake personal data and returns the file path.\n    \"\"\"\n    if not isinstance(num_rows, int) or num_rows < 0:\n        raise ValueError(\"num_rows must be an integer >= 0\")\n\n    Faker.seed(random_seed)\n    fake = Faker()\n\n    data = [\n        ['Name', 'Age', 'Address', 'Email'],\n        *[\n            [\n                fake.name(),\n                str(random.randint(20, 60)),\n                fake.address().replace('\\n', ', '),\n                fake.email(),\n            ]\n            for _ in range(num_rows)\n        ],\n    ]\n\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(data)\n\n    return file_path"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef clean_text(text):\n    \"\"\"\n    Clean the text by removing all non-alphanumeric characters except spaces and converting to lowercase.\n    \"\"\"\n    return ALPHANUMERIC.sub(' ', text).lower()\ndef cosine_similarity(text1, text2):\n    \"\"\"\n    Calculate the cosine similarity between two texts.\n    \"\"\"\n    vec1 = Counter(clean_text(text1).split())\n    vec2 = Counter(clean_text(text2).split())\n    intersection = set(vec1.keys()) & set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n    sum1 = sum([vec1[x] ** 2 for x in vec1.keys()])\n    sum2 = sum([vec2[x] ** 2 for x in vec2.keys()])\n    denominator = np.sqrt(sum1) * np.sqrt(sum2)\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator\ndef levenshtein_ratio(text1, text2):\n    \"\"\"\n    Calculate the Levenshtein ratio between two texts.\n    \"\"\"\n    return ratio(clean_text(text1), clean_text(text2))\ndef task_func(text1, text2):\n    \"\"\"\n    Calculate the cosine similarity and Levenshtein ratio between two texts.\n    \"\"\"\n    cosine_similarity_value = cosine_similarity(text1, text2)\n    levenshtein_ratio_value = levenshtein_ratio(text1, text2)\n    return (cosine_similarity_value, levenshtein_ratio_value)", "raw_solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef clean_text(text):\n    \"\"\"\n    Clean the text by removing all non-alphanumeric characters except spaces and converting to lowercase.\n    \"\"\"\n    return ALPHANUMERIC.sub(' ', text).lower()\n\ndef cosine_similarity(text1, text2):\n    \"\"\"\n    Calculate the cosine similarity between two texts.\n    \"\"\"\n    vec1 = Counter(clean_text(text1).split())\n    vec2 = Counter(clean_text(text2).split())\n    intersection = set(vec1.keys()) & set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n    sum1 = sum([vec1[x] ** 2 for x in vec1.keys()])\n    sum2 = sum([vec2[x] ** 2 for x in vec2.keys()])\n    denominator = np.sqrt(sum1) * np.sqrt(sum2)\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator\n\ndef levenshtein_ratio(text1, text2):\n    \"\"\"\n    Calculate the Levenshtein ratio between two texts.\n    \"\"\"\n    return ratio(clean_text(text1), clean_text(text2))\n\ndef task_func(text1, text2):\n    \"\"\"\n    Calculate the cosine similarity and Levenshtein ratio between two texts.\n    \"\"\"\n    cosine_similarity_value = cosine_similarity(text1, text2)\n    levenshtein_ratio_value = levenshtein_ratio(text1, text2)\n    return (cosine_similarity_value, levenshtein_ratio_value)"}
{"task_id": "BigCodeBench/846", "solution": "import collections\nimport pandas as pd\nclass ExampleObject:\n    def __init__(self, animal, shape):\n        self.animal = animal\n        self.shape = shape\ndef task_func(obj_list, attr):\n    \"\"\"\n    Count the frequency of each value of the given attribute from a list of objects.\n    \"\"\"\n    if not obj_list:\n        return pd.DataFrame(columns=['attribute', 'count'])\n\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n    counter = collections.Counter(attr_values)\n\n    df = pd.DataFrame(list(counter.items()), columns=['attribute', 'count'])\n    return df\nobj_list = [ExampleObject('tiger', 'Square'), ExampleObject('leopard', 'Circle'), ExampleObject('cat', 'Rectangle'), ExampleObject('elephant', 'Rectangle')]", "raw_solution": "import collections\nimport pandas as pd\n\nclass ExampleObject:\n    def __init__(self, animal, shape):\n        self.animal = animal\n        self.shape = shape\n\ndef task_func(obj_list, attr):\n    \"\"\"\n    Count the frequency of each value of the given attribute from a list of objects.\n    \"\"\"\n    if not obj_list:\n        return pd.DataFrame(columns=['attribute', 'count'])\n\n    attr_values = [getattr(obj, attr) for obj in obj_list]\n    counter = collections.Counter(attr_values)\n\n    df = pd.DataFrame(list(counter.items()), columns=['attribute', 'count'])\n    return df\n\n# Test the function\nobj_list = [ExampleObject('tiger', 'Square'), ExampleObject('leopard', 'Circle'), ExampleObject('cat', 'Rectangle'), ExampleObject('elephant', 'Rectangle')]\ncount = task_func(obj_list, 'shape')"}
{"task_id": "BigCodeBench/847", "solution": "import re\nimport os\nimport string\nimport random\ndef task_func(input_string, directory='./text_files'):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Split the input string into separate strings\n    lines = input_string.split('\\n')\n\n    file_paths = []\n    for i, line in enumerate(lines):\n        # Remove special characters\n        cleaned_line = re.sub('[%s]' % re.escape(string.punctuation), '', line)\n\n        # Generate a random filename\n        filename = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)) + '.txt'\n        file_path = os.path.join(directory, filename)\n\n        # Save the cleaned line to a text file\n        with open(file_path, 'w') as f:\n            f.write(cleaned_line)\n\n        file_paths.append(file_path)\n\n    return file_paths", "raw_solution": "import re\nimport os\nimport string\nimport random\n\ndef task_func(input_string, directory='./text_files'):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Split the input string into separate strings\n    lines = input_string.split('\\n')\n\n    file_paths = []\n    for i, line in enumerate(lines):\n        # Remove special characters\n        cleaned_line = re.sub('[%s]' % re.escape(string.punctuation), '', line)\n\n        # Generate a random filename\n        filename = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)) + '.txt'\n        file_path = os.path.join(directory, filename)\n\n        # Save the cleaned line to a text file\n        with open(file_path, 'w') as f:\n            f.write(cleaned_line)\n\n        file_paths.append(file_path)\n\n    return file_paths"}
{"task_id": "BigCodeBench/848", "solution": "import heapq\nimport random\nclass Object:\n    def __init__(self, value):\n        self.test = value\ndef task_func(obj_list, attr, top_n=5, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    values = [getattr(obj, attr) for obj in obj_list if hasattr(obj, attr)]\n    if not values:\n        return [], None\n\n    top_values = heapq.nlargest(top_n, values)\n    random_value = random.choice(values)\n\n    return top_values, random_value\nobj_list = [Object(random.randint(1, 12)) for _ in range(13)]", "raw_solution": "import heapq\nimport random\n\nclass Object:\n    def __init__(self, value):\n        self.test = value\n\ndef task_func(obj_list, attr, top_n=5, seed=None):\n    if seed is not None:\n        random.seed(seed)\n\n    values = [getattr(obj, attr) for obj in obj_list if hasattr(obj, attr)]\n    if not values:\n        return [], None\n\n    top_values = heapq.nlargest(top_n, values)\n    random_value = random.choice(values)\n\n    return top_values, random_value\n\nrandom.seed(2)\nobj_list = [Object(random.randint(1, 12)) for _ in range(13)]\ntop_values, random_value = task_func(obj_list, 'test', 2, 12)"}
{"task_id": "BigCodeBench/849", "solution": "import re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nSTOPWORDS = set(stopwords.words('english'))\ndef task_func(input_string):\n    # Split the string into individual lines\n    lines = input_string.split('\\n')\n\n    # Initialize an empty list to store words\n    words = []\n\n    # Iterate over each line\n    for line in lines:\n        # Remove punctuation and convert to lowercase\n        line = re.sub(r'[^\\w\\s]', '', line).lower()\n\n        # Split the line into words\n        line_words = line.split()\n\n        # Add the words to the list, excluding stopwords\n        words.extend([word for word in line_words if word not in STOPWORDS])\n\n    # Count the frequency of each word\n    word_freq = dict(Counter(words))\n\n    return word_freq\ninput_string = \"\"\"\nThis is a multi-line string.\nIt contains multiple sentences.\nStopwords like 'is', 'a', 'and', 'the' are removed.\nThe function should return a dictionary with word frequencies.\n\"\"\"", "raw_solution": "import re\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\nSTOPWORDS = set(stopwords.words('english'))\n\ndef task_func(input_string):\n    # Split the string into individual lines\n    lines = input_string.split('\\n')\n\n    # Initialize an empty list to store words\n    words = []\n\n    # Iterate over each line\n    for line in lines:\n        # Remove punctuation and convert to lowercase\n        line = re.sub(r'[^\\w\\s]', '', line).lower()\n\n        # Split the line into words\n        line_words = line.split()\n\n        # Add the words to the list, excluding stopwords\n        words.extend([word for word in line_words if word not in STOPWORDS])\n\n    # Count the frequency of each word\n    word_freq = dict(Counter(words))\n\n    return word_freq\n\n# Test the function\ninput_string = \"\"\"\nThis is a multi-line string.\nIt contains multiple sentences.\nStopwords like 'is', 'a', 'and', 'the' are removed.\nThe function should return a dictionary with word frequencies.\n\"\"\"\n"}
{"task_id": "BigCodeBench/850", "solution": "import pandas as pd\nimport random\ndef task_func(students, subjects, seed=None):\n    \"\"\"\n    Generates a grade report for a list of students across various subjects.\n    Each student's grades are randomly generated.\n    The report includes the average grade for each student.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    data = {'Student': []}\n    for subject in subjects:\n        data[subject] = []\n\n    for student in students:\n        data['Student'].append(student)\n        for subject in subjects:\n            data[subject].append(random.randint(0, 100))\n\n    df = pd.DataFrame(data)\n    df['Average Grade'] = df.mean(axis=1)\n\n    return df", "raw_solution": "import pandas as pd\nimport random\n\ndef task_func(students, subjects, seed=None):\n    \"\"\"\n    Generates a grade report for a list of students across various subjects.\n    Each student's grades are randomly generated.\n    The report includes the average grade for each student.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    data = {'Student': []}\n    for subject in subjects:\n        data[subject] = []\n\n    for student in students:\n        data['Student'].append(student)\n        for subject in subjects:\n            data[subject].append(random.randint(0, 100))\n\n    df = pd.DataFrame(data)\n    df['Average Grade'] = df.mean(axis=1)\n\n    return df"}
{"task_id": "BigCodeBench/851", "solution": "import textwrap\nimport re\ndef task_func(input_string, width):\n    \"\"\"\n    Divides a multi-line string into separate strings and wraps each line to a certain width.\n    \"\"\"\n    # Split the input string into lines\n    lines = input_string.split('\\n')\n\n    # Wrap each line to the specified width\n    wrapped_lines = [textwrap.fill(line, width) for line in lines]\n\n    # Join the wrapped lines back into a single string\n    output_string = '\\n'.join(wrapped_lines)\n\n    return output_string\ninput_string = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec a diam lectus. Sed sit amet ipsum mauris. Maecenas congue ligula ac quam viverra nec consectetur ante hendrerit. Donec et mollis dolor. Praesent et diam eget libero egestas mattis sit amet vitae augue. Nam tincidunt congue enim, ut porta lorem lacinia consectetur.\"\nwidth = 20", "raw_solution": "import textwrap\nimport re\n\ndef task_func(input_string, width):\n    \"\"\"\n    Divides a multi-line string into separate strings and wraps each line to a certain width.\n    \"\"\"\n    # Split the input string into lines\n    lines = input_string.split('\\n')\n\n    # Wrap each line to the specified width\n    wrapped_lines = [textwrap.fill(line, width) for line in lines]\n\n    # Join the wrapped lines back into a single string\n    output_string = '\\n'.join(wrapped_lines)\n\n    return output_string\n\n# Test the function\ninput_string = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec a diam lectus. Sed sit amet ipsum mauris. Maecenas congue ligula ac quam viverra nec consectetur ante hendrerit. Donec et mollis dolor. Praesent et diam eget libero egestas mattis sit amet vitae augue. Nam tincidunt congue enim, ut porta lorem lacinia consectetur.\"\nwidth = 20"}
{"task_id": "BigCodeBench/852", "solution": "import random\nimport string\ndef task_func(max_length, n_samples, seed=None):\n    if max_length < 1:\n        raise ValueError(\"max_length must be greater than 0\")\n    if seed is not None:\n        random.seed(seed)\n    return [''.join(random.choices(string.ascii_lowercase, k=random.randint(1, max_length))) for _ in range(n_samples)]", "raw_solution": "import random\nimport string\n\ndef task_func(max_length, n_samples, seed=None):\n    if max_length < 1:\n        raise ValueError(\"max_length must be greater than 0\")\n    if seed is not None:\n        random.seed(seed)\n    return [''.join(random.choices(string.ascii_lowercase, k=random.randint(1, max_length))) for _ in range(n_samples)]\n\n# Test the function"}
{"task_id": "BigCodeBench/853", "solution": "import os\nimport shutil\nimport string\nINVALID_CHARACTERS = string.punctuation + string.whitespace\ndef task_func(directory_path):\n    \"\"\"\n    Organizes files in a directory according to their endings.\n    Files with invalid characters in the name are moved to a new directory called \"Invalid.\"\n    \"\"\"\n    # Create the \"Invalid\" directory if it doesn't exist\n    invalid_dir = os.path.join(directory_path, \"Invalid\")\n    if not os.path.exists(invalid_dir):\n        os.makedirs(invalid_dir)\n\n    # Initialize summary dictionary\n    summary = {}\n\n    # Iterate over files in the directory\n    for filename in os.listdir(directory_path):\n        # Check if the file has invalid characters\n        if any(char in filename for char in INVALID_CHARACTERS):\n            # Move the file to the \"Invalid\" directory\n            shutil.move(os.path.join(directory_path, filename), invalid_dir)\n            # Update summary\n            if \"Invalid\" in summary:\n                summary[\"Invalid\"] += 1\n            else:\n                summary[\"Invalid\"] = 1\n        else:\n            # Organize the file according to its ending\n            ending = filename.split('.')[-1]\n            ending_dir = os.path.join(directory_path, ending)\n            if not os.path.exists(ending_dir):\n                os.makedirs(ending_dir)\n            # Move the file to the corresponding directory\n            shutil.move(os.path.join(directory_path, filename), ending_dir)\n            # Update summary\n            if ending in summary:\n                summary[ending] += 1\n            else:\n                summary[ending] = 1\n\n    return summary", "raw_solution": "import os\nimport shutil\nimport string\n\n# Constants\nINVALID_CHARACTERS = string.punctuation + string.whitespace\n\ndef task_func(directory_path):\n    \"\"\"\n    Organizes files in a directory according to their endings.\n    Files with invalid characters in the name are moved to a new directory called \"Invalid.\"\n    \"\"\"\n    # Create the \"Invalid\" directory if it doesn't exist\n    invalid_dir = os.path.join(directory_path, \"Invalid\")\n    if not os.path.exists(invalid_dir):\n        os.makedirs(invalid_dir)\n\n    # Initialize summary dictionary\n    summary = {}\n\n    # Iterate over files in the directory\n    for filename in os.listdir(directory_path):\n        # Check if the file has invalid characters\n        if any(char in filename for char in INVALID_CHARACTERS):\n            # Move the file to the \"Invalid\" directory\n            shutil.move(os.path.join(directory_path, filename), invalid_dir)\n            # Update summary\n            if \"Invalid\" in summary:\n                summary[\"Invalid\"] += 1\n            else:\n                summary[\"Invalid\"] = 1\n        else:\n            # Organize the file according to its ending\n            ending = filename.split('.')[-1]\n            ending_dir = os.path.join(directory_path, ending)\n            if not os.path.exists(ending_dir):\n                os.makedirs(ending_dir)\n            # Move the file to the corresponding directory\n            shutil.move(os.path.join(directory_path, filename), ending_dir)\n            # Update summary\n            if ending in summary:\n                summary[ending] += 1\n            else:\n                summary[ending] = 1\n\n    return summary"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input should be a list of integers\")\n    if not all(isinstance(i, int) for i in numbers):\n        raise TypeError(\"Input should be a list of integers\")\n    if any(i < 0 for i in numbers):\n        raise ValueError(\"Input should not contain negative numbers\")\n\n    fac = [math.factorial(i) for i in numbers]\n    perm = list(permutations(numbers, len(numbers)))\n\n    return fac, list(perm)", "raw_solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input should be a list of integers\")\n    if not all(isinstance(i, int) for i in numbers):\n        raise TypeError(\"Input should be a list of integers\")\n    if any(i < 0 for i in numbers):\n        raise ValueError(\"Input should not contain negative numbers\")\n\n    fac = [math.factorial(i) for i in numbers]\n    perm = list(permutations(numbers, len(numbers)))\n\n    return fac, list(perm)"}
{"task_id": "BigCodeBench/855", "solution": "import random\nimport string\nimport collections\nVALID_CHARACTERS = string.ascii_letters + string.digits\ndef task_func(n_strings, string_length):\n    # Generate n random strings\n    strings = [''.join(random.choices(VALID_CHARACTERS, k=string_length)) for _ in range(n_strings)]\n\n    # Flatten the list of strings and count the frequency of each character\n    char_counts = collections.Counter(''.join(strings))\n\n    return dict(char_counts)", "raw_solution": "import random\nimport string\nimport collections\n\n# Constants\nVALID_CHARACTERS = string.ascii_letters + string.digits\n\ndef task_func(n_strings, string_length):\n    # Generate n random strings\n    strings = [''.join(random.choices(VALID_CHARACTERS, k=string_length)) for _ in range(n_strings)]\n\n    # Flatten the list of strings and count the frequency of each character\n    char_counts = collections.Counter(''.join(strings))\n\n    return dict(char_counts)\n\n# Test the function"}
{"task_id": "BigCodeBench/856", "solution": "from functools import reduce\nfrom itertools import combinations\nimport numpy as np\ndef task_func(shape=(3, 3), low=1, high=10, seed=None):\n    if high <= low:\n        raise ValueError(\"high should be greater than low\")\n    if seed is not None:\n        np.random.seed(seed)\n    matrix = np.random.randint(low, high, shape)\n    pairs = list(combinations(matrix.flatten(), 2))\n    sum_of_products = sum(reduce(lambda x, y: x*y, pair) for pair in pairs)\n    return sum_of_products, matrix", "raw_solution": "from functools import reduce\nfrom itertools import combinations\nimport numpy as np\n\ndef task_func(shape=(3, 3), low=1, high=10, seed=None):\n    if high <= low:\n        raise ValueError(\"high should be greater than low\")\n    if seed is not None:\n        np.random.seed(seed)\n    matrix = np.random.randint(low, high, shape)\n    pairs = list(combinations(matrix.flatten(), 2))\n    sum_of_products = sum(reduce(lambda x, y: x*y, pair) for pair in pairs)\n    return sum_of_products, matrix"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfer files from one directory to another based on the specified file extensions.\n    \"\"\"\n    transferred_files = []\n    for extension in EXTENSIONS:\n        source_files = glob.glob(os.path.join(SOURCE_DIR, f'*.{extension}'))\n        for file in source_files:\n            try:\n                shutil.move(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Could not transfer file {file} due to {str(e)}\")\n    return transferred_files", "raw_solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfer files from one directory to another based on the specified file extensions.\n    \"\"\"\n    transferred_files = []\n    for extension in EXTENSIONS:\n        source_files = glob.glob(os.path.join(SOURCE_DIR, f'*.{extension}'))\n        for file in source_files:\n            try:\n                shutil.move(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                warnings.warn(f\"Could not transfer file {file} due to {str(e)}\")\n    return transferred_files"}
{"task_id": "BigCodeBench/858", "solution": "import string\nimport random\nfrom collections import Counter\ndef task_func(n, seed=None):\n    \"\"\"\n    Generate a number of random lowercase letters and count their occurrences.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    letters = [random.choice(string.ascii_lowercase) for _ in range(n)]\n    return Counter(letters)", "raw_solution": "import string\nimport random\nfrom collections import Counter\n\ndef task_func(n, seed=None):\n    \"\"\"\n    Generate a number of random lowercase letters and count their occurrences.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    letters = [random.choice(string.ascii_lowercase) for _ in range(n)]\n    return Counter(letters)\n\n# Test the function"}
{"task_id": "BigCodeBench/859", "solution": "import warnings\nimport sklearn.model_selection as model_selection\nimport sklearn.svm as svm\nimport sklearn.datasets as datasets\nimport sklearn.metrics as metrics\ndef task_func():\n    # Load iris dataset\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Split the data into a training set and a test set\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n\n    # Create a SVM classifier\n    clf = svm.SVC(gamma='scale')\n\n    # Train the classifier\n    clf.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = clf.predict(X_test)\n\n    # Calculate the accuracy\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n\n    # Warn if accuracy is less than 0.9\n    if accuracy < 0.9:\n        warnings.warn(f\"Accuracy is less than 0.9: {accuracy}\", UserWarning, stacklevel=2)\n        warning_msg = \"Accuracy is less than 0.9\"\n    else:\n        warning_msg = None\n\n    return accuracy, warning_msg", "raw_solution": "import warnings\nimport sklearn.model_selection as model_selection\nimport sklearn.svm as svm\nimport sklearn.datasets as datasets\nimport sklearn.metrics as metrics\n\ndef task_func():\n    # Load iris dataset\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Split the data into a training set and a test set\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n\n    # Create a SVM classifier\n    clf = svm.SVC(gamma='scale')\n\n    # Train the classifier\n    clf.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = clf.predict(X_test)\n\n    # Calculate the accuracy\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n\n    # Warn if accuracy is less than 0.9\n    if accuracy < 0.9:\n        warnings.warn(f\"Accuracy is less than 0.9: {accuracy}\", UserWarning, stacklevel=2)\n        warning_msg = \"Accuracy is less than 0.9\"\n    else:\n        warning_msg = None\n\n    return accuracy, warning_msg"}
{"task_id": "BigCodeBench/860", "solution": "import re\nimport random\nimport string\ndef task_func(n, pattern, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=n))\n    matches = re.findall(pattern, random_string)\n    return matches", "raw_solution": "import re\nimport random\nimport string\n\ndef task_func(n, pattern, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=n))\n    matches = re.findall(pattern, random_string)\n    return matches\n\n# Test the function"}
{"task_id": "BigCodeBench/861", "solution": "from collections import Counter\nfrom random import choice, seed\nPOSSIBLE_ITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\ndef task_func(list_of_lists):\n    baskets = []\n    for lst in list_of_lists:\n        basket = Counter()\n        for _ in range(len(lst)):\n            item = choice(POSSIBLE_ITEMS)\n            basket[item] += 1\n        baskets.append(basket)\n    return baskets\nlist_of_lists = [[], ['apple', 'banana'], ['apple', 'apple', 'apple'], ['cherry', 'date', 'elderberry', 'elderberry']]", "raw_solution": "from collections import Counter\nfrom random import choice, seed\n\n# Constants\nPOSSIBLE_ITEMS = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\ndef task_func(list_of_lists):\n    baskets = []\n    for lst in list_of_lists:\n        basket = Counter()\n        for _ in range(len(lst)):\n            item = choice(POSSIBLE_ITEMS)\n            basket[item] += 1\n        baskets.append(basket)\n    return baskets\n\n# Testing the function\nlist_of_lists = [[], ['apple', 'banana'], ['apple', 'apple', 'apple'], ['cherry', 'date', 'elderberry', 'elderberry']]\nbaskets = task_func(list_of_lists)\nfor i, basket in enumerate(baskets):\n    print(f\"Basket {i+1}: {basket}\")"}
{"task_id": "BigCodeBench/862", "solution": "import random\nimport string\nfrom collections import defaultdict\ndef task_func(n, seed=None):\n    random.seed(seed)\n    letters = list(string.ascii_lowercase)\n    result = defaultdict(list)\n    for _ in range(n):\n        letter = random.choice(letters)\n        result[letter].append(letter)\n    return result", "raw_solution": "import random\nimport string\nfrom collections import defaultdict\n\ndef task_func(n, seed=None):\n    random.seed(seed)\n    letters = list(string.ascii_lowercase)\n    result = defaultdict(list)\n    for _ in range(n):\n        letter = random.choice(letters)\n        result[letter].append(letter)\n    return result"}
{"task_id": "BigCodeBench/863", "solution": "import numpy as np\nimport math\nPOSSIBLE_NUMBERS = np.arange(1, 11)\ndef task_func(list_of_lists):\n    sums = []\n    for lst in list_of_lists:\n        if len(lst) > len(POSSIBLE_NUMBERS):\n            raise ValueError(\"Length of each list cannot exceed the length of POSSIBLE_NUMBERS\")\n        sum_of_squares = sum(POSSIBLE_NUMBERS[i] ** 2 for i in lst)\n        sums.append(sum_of_squares)\n    return sums\nlist_of_lists = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]", "raw_solution": "import numpy as np\nimport math\n\n# Constants\nPOSSIBLE_NUMBERS = np.arange(1, 11)\n\ndef task_func(list_of_lists):\n    sums = []\n    for lst in list_of_lists:\n        if len(lst) > len(POSSIBLE_NUMBERS):\n            raise ValueError(\"Length of each list cannot exceed the length of POSSIBLE_NUMBERS\")\n        sum_of_squares = sum(POSSIBLE_NUMBERS[i] ** 2 for i in lst)\n        sums.append(sum_of_squares)\n    return sums\n\n# Test the function\nlist_of_lists = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]"}
{"task_id": "BigCodeBench/864", "solution": "import pandas as pd\nimport numpy as np\nfrom typing import List, Tuple\ndef task_func(fruit_data: List[Tuple[str, int]]) -> pd.DataFrame:\n    \"\"\"\n    Calculate and return the total and average counts for each type of fruit.\n    \"\"\"\n    if not fruit_data:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(fruit_data, columns=['Fruit', 'Count'])\n    df = df.groupby('Fruit').agg({'Count': ['sum', 'mean']})\n    df.columns = ['Total Count', 'Average Count']\n    df.reset_index(inplace=True)\n\n    return df\nfruit = [('apple', 1), ('orange', 25), ('apple', 111)]", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom typing import List, Tuple\n\ndef task_func(fruit_data: List[Tuple[str, int]]) -> pd.DataFrame:\n    \"\"\"\n    Calculate and return the total and average counts for each type of fruit.\n    \"\"\"\n    if not fruit_data:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(fruit_data, columns=['Fruit', 'Count'])\n    df = df.groupby('Fruit').agg({'Count': ['sum', 'mean']})\n    df.columns = ['Total Count', 'Average Count']\n    df.reset_index(inplace=True)\n\n    return df\n\n# Test the function\nfruit = [('apple', 1), ('orange', 25), ('apple', 111)]\ndf = task_func(fruit)\ndf.sort_index(inplace=True)"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    \"\"\"\n    This function takes a list of tuples containing elements and their respective counts and weights.\n    It normalizes the counts using z-score normalization and the weights using min-max scaling.\n    Finally, it returns a pandas DataFrame with the items, normalized counts, and normalized weights.\n    \"\"\"\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n\n    # Normalize the counts using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n\n    # Normalize the weights using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    return df[['Item', 'Normalized Count', 'Normalized Weight']]\ndata = [('Item1', 10, 20), ('Item2', 20, 30), ('Item3', 30, 40)]", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    \"\"\"\n    This function takes a list of tuples containing elements and their respective counts and weights.\n    It normalizes the counts using z-score normalization and the weights using min-max scaling.\n    Finally, it returns a pandas DataFrame with the items, normalized counts, and normalized weights.\n    \"\"\"\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n\n    # Normalize the counts using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n\n    # Normalize the weights using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    return df[['Item', 'Normalized Count', 'Normalized Weight']]\n\n# Test the function\ndata = [('Item1', 10, 20), ('Item2', 20, 30), ('Item3', 30, 40)]"}
{"task_id": "BigCodeBench/866", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters=2, random_state=0):\n    # Extract the 2D coordinates from the data\n    coordinates = [point[1:] for point in data]\n    coordinates = np.array(coordinates)\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(coordinates)\n\n    # Return the cluster labels\n    return kmeans.labels_\ndata = [('T1', 1, 1), ('T2', 1, 1.1), ('T2', 1.1, 1), ('C1', 400, 400), ('C2', 401, 401), ('B1', 35, 35)]", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef task_func(data, n_clusters=2, random_state=0):\n    # Extract the 2D coordinates from the data\n    coordinates = [point[1:] for point in data]\n    coordinates = np.array(coordinates)\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n    kmeans.fit(coordinates)\n\n    # Return the cluster labels\n    return kmeans.labels_\n\n# Test the function\ndata = [('T1', 1, 1), ('T2', 1, 1.1), ('T2', 1.1, 1), ('C1', 400, 400), ('C2', 401, 401), ('B1', 35, 35)]\nlabels = task_func(data, n_clusters=3, random_state=42)"}
{"task_id": "BigCodeBench/867", "solution": "import re\nimport string\ndef task_func(text1, text2):\n    \"\"\"\n    This function takes two strings, removes any ASCII punctuation using regular expressions, and returns the cleaned strings as a tuple.\n    \"\"\"\n    # Define the set of punctuation characters to be removed\n    punctuation = set(string.punctuation)\n\n    # Use a regular expression to replace punctuation with an empty string\n    text1 = ''.join(ch for ch in text1 if ch not in punctuation)\n    text2 = ''.join(ch for ch in text2 if ch not in punctuation)\n\n    # Return the cleaned strings as a tuple\n    return (text1, text2)", "raw_solution": "import re\nimport string\n\ndef task_func(text1, text2):\n    \"\"\"\n    This function takes two strings, removes any ASCII punctuation using regular expressions, and returns the cleaned strings as a tuple.\n    \"\"\"\n    # Define the set of punctuation characters to be removed\n    punctuation = set(string.punctuation)\n\n    # Use a regular expression to replace punctuation with an empty string\n    text1 = ''.join(ch for ch in text1 if ch not in punctuation)\n    text2 = ''.join(ch for ch in text2 if ch not in punctuation)\n\n    # Return the cleaned strings as a tuple\n    return (text1, text2)\n\n# Test the function\ncleaned_text1, cleaned_text2 = task_func(\"test (with parenthesis []!!)\", \"And, other; stuff ^_`\")\n\n# Print the cleaned strings"}
{"task_id": "BigCodeBench/868", "solution": "from itertools import cycle\nfrom random import choice, seed\ndef task_func(n_colors, colors=['Red', 'Green', 'Blue', 'Yellow', 'Purple'], rng_seed=None):\n    if n_colors <= 0:\n        return []\n\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    color_pattern = []\n    color_cycle = cycle(colors)\n    for i in range(n_colors):\n        if i % 2 == 0:\n            color_pattern.append(next(color_cycle))\n        else:\n            color_pattern.append(choice(colors))\n\n    return color_pattern\ncolors = ['Brown', 'Green', 'Black']", "raw_solution": "from itertools import cycle\nfrom random import choice, seed\n\ndef task_func(n_colors, colors=['Red', 'Green', 'Blue', 'Yellow', 'Purple'], rng_seed=None):\n    if n_colors <= 0:\n        return []\n\n    if rng_seed is not None:\n        seed(rng_seed)\n\n    color_pattern = []\n    color_cycle = cycle(colors)\n    for i in range(n_colors):\n        if i % 2 == 0:\n            color_pattern.append(next(color_cycle))\n        else:\n            color_pattern.append(choice(colors))\n\n    return color_pattern\n\n# Test the function\ncolors = ['Brown', 'Green', 'Black']\ncolor_pattern = task_func(12, colors=colors, rng_seed=42)"}
{"task_id": "BigCodeBench/869", "solution": "import pandas as pd\nfrom itertools import cycle\nfrom random import randint, seed\ndef task_func(\n    n_grades,\n    students=['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    grade_range=range(1, 11),\n    rng_seed=None\n):\n    if not students:\n        raise ValueError(\"List of students is empty\")\n    if rng_seed is not None:\n        seed(rng_seed)\n    student_cycle = cycle(students)\n    grades = [randint(min(grade_range), max(grade_range)) for _ in range(n_grades)]\n    grade_report = pd.DataFrame({\n        'Student': [next(student_cycle) for _ in range(n_grades)],\n        'Grade': grades\n    })\n    grade_report.set_index('Student', inplace=True)\n    return grade_report", "raw_solution": "import pandas as pd\nfrom itertools import cycle\nfrom random import randint, seed\n\ndef task_func(\n    n_grades,\n    students=['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    grade_range=range(1, 11),\n    rng_seed=None\n):\n    if not students:\n        raise ValueError(\"List of students is empty\")\n    if rng_seed is not None:\n        seed(rng_seed)\n    student_cycle = cycle(students)\n    grades = [randint(min(grade_range), max(grade_range)) for _ in range(n_grades)]\n    grade_report = pd.DataFrame({\n        'Student': [next(student_cycle) for _ in range(n_grades)],\n        'Grade': grades\n    })\n    grade_report.set_index('Student', inplace=True)\n    return grade_report\n\n# Test the function\ngrade_report = task_func(5, rng_seed=12)"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_list=[('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize an empty list to store the mean values\n    mean_values = []\n\n    # Iterate over each tuple in the list\n    for i in range(len(data_list[0])):\n        # Initialize a list to store the numerical values at the current position\n        num_values = []\n\n        # Iterate over each tuple in the list\n        for j in range(len(data_list)):\n            # Check if the current position in the tuple is a number\n            if isinstance(data_list[j][i], (int, float)):\n                num_values.append(data_list[j][i])\n\n        # Calculate the mean of the numerical values at the current position\n        mean_value = np.mean(num_values) if num_values else np.nan\n\n        # Append the mean value to the list of mean values\n        mean_values.append(mean_value)\n\n    # Create a pandas DataFrame with the mean values and the corresponding positions as the index\n    df = pd.DataFrame(mean_values, index=['Position ' + str(i) for i in range(len(mean_values))], columns=['Mean Value'])\n\n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data_list=[('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize an empty list to store the mean values\n    mean_values = []\n\n    # Iterate over each tuple in the list\n    for i in range(len(data_list[0])):\n        # Initialize a list to store the numerical values at the current position\n        num_values = []\n\n        # Iterate over each tuple in the list\n        for j in range(len(data_list)):\n            # Check if the current position in the tuple is a number\n            if isinstance(data_list[j][i], (int, float)):\n                num_values.append(data_list[j][i])\n\n        # Calculate the mean of the numerical values at the current position\n        mean_value = np.mean(num_values) if num_values else np.nan\n\n        # Append the mean value to the list of mean values\n        mean_values.append(mean_value)\n\n    # Create a pandas DataFrame with the mean values and the corresponding positions as the index\n    df = pd.DataFrame(mean_values, index=['Position ' + str(i) for i in range(len(mean_values))], columns=['Mean Value'])\n\n    return df\n\n# Test the function\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)"}
{"task_id": "BigCodeBench/871", "solution": "import numpy as np\nimport itertools\ndef task_func(data_list, file_name):\n    means = []\n    with open(file_name, 'w') as f:\n        for i, data in enumerate(data_list, start=1):\n            numeric_data = [x for x in data[1:] if isinstance(x, (int, float))]\n            if numeric_data:\n                mean = np.mean(numeric_data)\n                means.append(mean)\n                f.write(f\"Position {i}: {mean}\\n\")\n            else:\n                means.append(np.nan)\n                f.write(f\"Position {i}: NaN\\n\")\n    return means", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(data_list, file_name):\n    means = []\n    with open(file_name, 'w') as f:\n        for i, data in enumerate(data_list, start=1):\n            numeric_data = [x for x in data[1:] if isinstance(x, (int, float))]\n            if numeric_data:\n                mean = np.mean(numeric_data)\n                means.append(mean)\n                f.write(f\"Position {i}: {mean}\\n\")\n            else:\n                means.append(np.nan)\n                f.write(f\"Position {i}: NaN\\n\")\n    return means"}
{"task_id": "BigCodeBench/872", "solution": "import numpy as np\nimport itertools\ndef task_func(data_list):\n    if not data_list:\n        return []\n\n    # Unzip the list of tuples\n    unzipped = list(itertools.zip_longest(*data_list, fillvalue=0))\n\n    # Convert all values to float, ignoring non-numeric ones\n    numeric_unzipped = [[float(j) for j in i if isinstance(j, (int, float))] for i in unzipped]\n\n    # Calculate the mean for each position\n    means = [np.mean(i) if i else np.nan for i in numeric_unzipped]\n\n    return means", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(data_list):\n    if not data_list:\n        return []\n\n    # Unzip the list of tuples\n    unzipped = list(itertools.zip_longest(*data_list, fillvalue=0))\n\n    # Convert all values to float, ignoring non-numeric ones\n    numeric_unzipped = [[float(j) for j in i if isinstance(j, (int, float))] for i in unzipped]\n\n    # Calculate the mean for each position\n    means = [np.mean(i) if i else np.nan for i in numeric_unzipped]\n\n    return means"}
{"task_id": "BigCodeBench/873", "solution": "import csv\nimport os\ndef task_func(data, file_path, headers):\n    \"\"\"\n    Writes a list of tuples to a CSV file. Each tuple in the 'data' list represents a row in the CSV file, with each element of the tuple corresponding to a cell in the row. If a tuple contains fewer elements than there are headers, the missing elements are filled with None.\n    \"\"\"\n    if file_path is None:\n        raise ValueError(\"'file_path' cannot be None\")\n\n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(headers)\n        for row in data:\n            # If a tuple contains fewer elements than there are headers, fill the missing elements with None\n            while len(row) < len(headers):\n                row += (None,)\n            writer.writerow(row)\n\n    return os.path.abspath(file_path)", "raw_solution": "import csv\nimport os\n\ndef task_func(data, file_path, headers):\n    \"\"\"\n    Writes a list of tuples to a CSV file. Each tuple in the 'data' list represents a row in the CSV file, with each element of the tuple corresponding to a cell in the row. If a tuple contains fewer elements than there are headers, the missing elements are filled with None.\n    \"\"\"\n    if file_path is None:\n        raise ValueError(\"'file_path' cannot be None\")\n\n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(headers)\n        for row in data:\n            # If a tuple contains fewer elements than there are headers, fill the missing elements with None\n            while len(row) < len(headers):\n                row += (None,)\n            writer.writerow(row)\n\n    return os.path.abspath(file_path)\n\n# Test the function"}
{"task_id": "BigCodeBench/874", "solution": "from itertools import tee, islice\nfrom math import sqrt\ndef pairwise(iterable):\n    a, b = tee(iterable, 2)\n    _ = next(b, None)\n    return zip(a, b)\ndef task_func(points):\n    if len(points) < 2:\n        return []\n    distances = []\n    for point1, point2 in pairwise(points):\n        x1, y1 = point1 if len(point1) == 2 else (point1[0], point1[0])\n        x2, y2 = point2 if len(point2) == 2 else (point2[0], point2[0])\n        dist = sqrt((x2 - x1)**2 + (y2 - y1)**2)\n        distances.append(dist)\n    return distances", "raw_solution": "from itertools import tee, islice\nfrom math import sqrt\n\ndef pairwise(iterable):\n    a, b = tee(iterable, 2)\n    _ = next(b, None)\n    return zip(a, b)\n\ndef task_func(points):\n    if len(points) < 2:\n        return []\n    distances = []\n    for point1, point2 in pairwise(points):\n        x1, y1 = point1 if len(point1) == 2 else (point1[0], point1[0])\n        x2, y2 = point2 if len(point2) == 2 else (point2[0], point2[0])\n        dist = sqrt((x2 - x1)**2 + (y2 - y1)**2)\n        distances.append(dist)\n    return distances\n\n# Test cases"}
{"task_id": "BigCodeBench/875", "solution": "import pandas as pd\nimport random\nfrom typing import List, Tuple, Optional\ndef task_func(data: List[Tuple], columns: List[str] = ['Name', 'Age', 'Occupation'], fill_missing: bool = False, num_range: Tuple[int, int] = (0, 100), seed: Optional[int] = None) -> pd.DataFrame:\n    \"\"\"\n    Creates a pandas DataFrame from a list of tuples, each representing a row.\n    Tuples of unequal lengths are allowed, and missing elements are filled with None.\n    Optionally, missing numeric values can be filled with random data.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    data_filled = []\n    for row in data:\n        row_filled = list(row)\n        for i in range(len(columns) - len(row)):\n            if fill_missing:\n                if isinstance(num_range, tuple) and len(num_range) == 2:\n                    row_filled.append(random.randint(num_range[0], num_range[1]))\n                else:\n                    row_filled.append(None)\n            else:\n                row_filled.append(None)\n        data_filled.append(tuple(row_filled))\n\n    df = pd.DataFrame(data_filled, columns=columns)\n    return df\ndata = [('Mango', 20), ('Apple', ), ('Banana', )]", "raw_solution": "import pandas as pd\nimport random\nfrom typing import List, Tuple, Optional\n\ndef task_func(data: List[Tuple], columns: List[str] = ['Name', 'Age', 'Occupation'], fill_missing: bool = False, num_range: Tuple[int, int] = (0, 100), seed: Optional[int] = None) -> pd.DataFrame:\n    \"\"\"\n    Creates a pandas DataFrame from a list of tuples, each representing a row.\n    Tuples of unequal lengths are allowed, and missing elements are filled with None.\n    Optionally, missing numeric values can be filled with random data.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    data_filled = []\n    for row in data:\n        row_filled = list(row)\n        for i in range(len(columns) - len(row)):\n            if fill_missing:\n                if isinstance(num_range, tuple) and len(num_range) == 2:\n                    row_filled.append(random.randint(num_range[0], num_range[1]))\n                else:\n                    row_filled.append(None)\n            else:\n                row_filled.append(None)\n        data_filled.append(tuple(row_filled))\n\n    df = pd.DataFrame(data_filled, columns=columns)\n    return df\n\n# Test the function\ndata = [('Mango', 20), ('Apple', ), ('Banana', )]\ndf = task_func(data, columns=['Fruit', 'Quantity'], fill_missing=False, seed=42)"}
{"task_id": "BigCodeBench/876", "solution": "import collections\nimport operator\nimport os\nimport shutil\ndef task_func(data_dict, source_directory, backup_directory):\n    # Task 1: Update the dictionary\n    data_dict['a'] = 1\n\n    # Task 2: Sort the dictionary by frequency of values\n    value_frequencies = collections.Counter(data_dict.values()).most_common()\n    value_frequencies = sorted(value_frequencies, key=operator.itemgetter(1), reverse=True)\n\n    # Task 3: Backup files from source directory to backup directory\n    backup_status = False\n    if os.path.exists(source_directory):\n        if not os.path.exists(backup_directory):\n            os.makedirs(backup_directory)\n        for file_name in os.listdir(source_directory):\n            source_file = os.path.join(source_directory, file_name)\n            backup_file = os.path.join(backup_directory, file_name)\n            shutil.copy2(source_file, backup_file)\n        backup_status = True\n\n    return data_dict, value_frequencies, backup_status\ndata_dict = {'avc': '1', 'hello': 'world', 'test': 'world', 'cat': 'meow'}", "raw_solution": "import collections\nimport operator\nimport os\nimport shutil\n\ndef task_func(data_dict, source_directory, backup_directory):\n    # Task 1: Update the dictionary\n    data_dict['a'] = 1\n\n    # Task 2: Sort the dictionary by frequency of values\n    value_frequencies = collections.Counter(data_dict.values()).most_common()\n    value_frequencies = sorted(value_frequencies, key=operator.itemgetter(1), reverse=True)\n\n    # Task 3: Backup files from source directory to backup directory\n    backup_status = False\n    if os.path.exists(source_directory):\n        if not os.path.exists(backup_directory):\n            os.makedirs(backup_directory)\n        for file_name in os.listdir(source_directory):\n            source_file = os.path.join(source_directory, file_name)\n            backup_file = os.path.join(backup_directory, file_name)\n            shutil.copy2(source_file, backup_file)\n        backup_status = True\n\n    return data_dict, value_frequencies, backup_status\n\n# Testing the function\ndata_dict = {'avc': '1', 'hello': 'world', 'test': 'world', 'cat': 'meow'}\nupdated_dict, value_frequencies, backup_status = task_func(data_dict, 'to_backup', 'backup')"}
{"task_id": "BigCodeBench/877", "solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    \"\"\"\n    Perform PCA on the provided DataFrame.\n    \"\"\"\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame.\")\n\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"Input data cannot be empty.\")\n\n    # Check if data contains non-numeric data\n    if not all(data.dtypes != 'object'):\n        raise ValueError(\"Input data should only contain numeric data.\")\n\n    # Check if n_components is not greater than the number of columns in the data\n    if n_components > data.shape[1]:\n        raise ValueError(\"n_components cannot be greater than the number of columns in the data.\")\n\n    # Scale the data\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data)\n\n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    pca_data = pca.fit_transform(data_scaled)\n\n    # Return the transformed data as a DataFrame\n    return pd.DataFrame(pca_data)", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    \"\"\"\n    Perform PCA on the provided DataFrame.\n    \"\"\"\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame.\")\n\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"Input data cannot be empty.\")\n\n    # Check if data contains non-numeric data\n    if not all(data.dtypes != 'object'):\n        raise ValueError(\"Input data should only contain numeric data.\")\n\n    # Check if n_components is not greater than the number of columns in the data\n    if n_components > data.shape[1]:\n        raise ValueError(\"n_components cannot be greater than the number of columns in the data.\")\n\n    # Scale the data\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data)\n\n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    pca_data = pca.fit_transform(data_scaled)\n\n    # Return the transformed data as a DataFrame\n    return pd.DataFrame(pca_data)"}
{"task_id": "BigCodeBench/878", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\ndef task_func(data, target, test_size=0.2, random_state=None):\n    \"\"\"\n    Trains a RandomForestRegressor model and returns the mean squared error (MSE) of the predictions and the model.\n    \"\"\"\n    # Convert the input data into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the DataFrame is empty or if the target column is not in the DataFrame\n    if df.empty or target not in df.columns:\n        raise ValueError(\"The input DataFrame is empty or the target column name is not in the DataFrame.\")\n\n    # Split the DataFrame into a train and test set\n    X = df.drop(columns=[target])\n    y = df[target]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Train a RandomForestRegressor on the train set\n    model = RandomForestRegressor(random_state=random_state)\n    model.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the mean squared error of the predictions\n    mse = mean_squared_error(y_test, y_pred)\n\n    return mse, model, df", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndef task_func(data, target, test_size=0.2, random_state=None):\n    \"\"\"\n    Trains a RandomForestRegressor model and returns the mean squared error (MSE) of the predictions and the model.\n    \"\"\"\n    # Convert the input data into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the DataFrame is empty or if the target column is not in the DataFrame\n    if df.empty or target not in df.columns:\n        raise ValueError(\"The input DataFrame is empty or the target column name is not in the DataFrame.\")\n\n    # Split the DataFrame into a train and test set\n    X = df.drop(columns=[target])\n    y = df[target]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Train a RandomForestRegressor on the train set\n    model = RandomForestRegressor(random_state=random_state)\n    model.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the mean squared error of the predictions\n    mse = mean_squared_error(y_test, y_pred)\n\n    return mse, model, df"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    \"\"\"\n    Performs a chi-square test of independence of variables in a contingency table.\n    \"\"\"\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"'data' is empty\")\n\n    # Check if col1 and col2 are in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"One or both of the columns are not in 'data'\")\n\n    # Check if col1 and col2 have multiple categories\n    if data[col1].nunique() == 1 or data[col2].nunique() == 1:\n        raise ValueError(\"One or both of the columns do not have multiple categories\")\n\n    # Check if any categories have less than 5 observations\n    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\n        raise ValueError(\"Some categories have less than 5 observations\")\n\n    # Check if col1 and col2 are categorical\n    if not pd.api.types.is_categorical_dtype(data[col1]) or not pd.api.types.is_categorical_dtype(data[col2]):\n        raise TypeError(\"One or both of the columns contain non-categorical data\")\n\n    # Construct contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n\n    # Perform chi-square test of independence\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n\n    return p_value", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    \"\"\"\n    Performs a chi-square test of independence of variables in a contingency table.\n    \"\"\"\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"'data' is empty\")\n\n    # Check if col1 and col2 are in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"One or both of the columns are not in 'data'\")\n\n    # Check if col1 and col2 have multiple categories\n    if data[col1].nunique() == 1 or data[col2].nunique() == 1:\n        raise ValueError(\"One or both of the columns do not have multiple categories\")\n\n    # Check if any categories have less than 5 observations\n    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\n        raise ValueError(\"Some categories have less than 5 observations\")\n\n    # Check if col1 and col2 are categorical\n    if not pd.api.types.is_categorical_dtype(data[col1]) or not pd.api.types.is_categorical_dtype(data[col2]):\n        raise TypeError(\"One or both of the columns contain non-categorical data\")\n\n    # Construct contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n\n    # Perform chi-square test of independence\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n\n    return p_value"}
{"task_id": "BigCodeBench/880", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\ndef task_func(data, n_clusters=3, seed=None):\n    \"\"\"\n    Performs K-Means clustering on the given DataFrame using the sklearn KMeans algorithm.\n    The function expects a DataFrame with numerical values, as KMeans cannot handle categorical data.\n    It applies standard KMeans clustering from the sklearn library to form clusters.\n    The number of clusters is configurable via the 'n_clusters' parameter, defaulting to 3.\n    The Number of times the k-means algorithm is run with different centroid seeds (n_init) is set to 10.\n    The function returns an array of cluster labels corresponding to each data point in the input as well as the fitted KMeans model.\n    \"\"\"\n    if not all(data.dtypes == 'float64'):\n        raise ValueError(\"The DataFrame contains non numeric entries.\")\n\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=seed)\n    labels = kmeans.fit_predict(data)\n\n    return labels, kmeans", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\n\ndef task_func(data, n_clusters=3, seed=None):\n    \"\"\"\n    Performs K-Means clustering on the given DataFrame using the sklearn KMeans algorithm.\n    The function expects a DataFrame with numerical values, as KMeans cannot handle categorical data.\n    It applies standard KMeans clustering from the sklearn library to form clusters.\n    The number of clusters is configurable via the 'n_clusters' parameter, defaulting to 3.\n    The Number of times the k-means algorithm is run with different centroid seeds (n_init) is set to 10.\n    The function returns an array of cluster labels corresponding to each data point in the input as well as the fitted KMeans model.\n    \"\"\"\n    if not all(data.dtypes == 'float64'):\n        raise ValueError(\"The DataFrame contains non numeric entries.\")\n\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=seed)\n    labels = kmeans.fit_predict(data)\n\n    return labels, kmeans"}
{"task_id": "BigCodeBench/881", "solution": "import pandas as pd\nimport re\nimport random\ndef task_func(csv_file, column_name='data', pattern='\\d+[xX]', sample_size=None, seed=42):\n    \"\"\"\n    Searches for matches with a specified regex pattern in a given column of a CSV file and optionally returns a random sample of these matches.\n    \"\"\"\n    # Read the CSV file\n    df = pd.read_csv(csv_file)\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' not found in the CSV file.\")\n\n    # Apply the regex pattern to the column\n    df['match'] = df[column_name].apply(lambda x: bool(re.search(pattern, str(x))))\n\n    # Filter the DataFrame based on the matches\n    df = df[df['match'] == True]\n\n    # If sample_size is not None, return a random sample\n    if sample_size is not None:\n        random.seed(seed)\n        indices = random.sample(range(len(df)), min(sample_size, len(df)))\n        df = df.iloc[indices]\n\n    # Drop the temporary 'match' column\n    df = df.drop(columns='match')\n\n    return df", "raw_solution": "import pandas as pd\nimport re\nimport random\n\ndef task_func(csv_file, column_name='data', pattern='\\d+[xX]', sample_size=None, seed=42):\n    \"\"\"\n    Searches for matches with a specified regex pattern in a given column of a CSV file and optionally returns a random sample of these matches.\n    \"\"\"\n    # Read the CSV file\n    df = pd.read_csv(csv_file)\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' not found in the CSV file.\")\n\n    # Apply the regex pattern to the column\n    df['match'] = df[column_name].apply(lambda x: bool(re.search(pattern, str(x))))\n\n    # Filter the DataFrame based on the matches\n    df = df[df['match'] == True]\n\n    # If sample_size is not None, return a random sample\n    if sample_size is not None:\n        random.seed(seed)\n        indices = random.sample(range(len(df)), min(sample_size, len(df)))\n        df = df.iloc[indices]\n\n    # Drop the temporary 'match' column\n    df = df.drop(columns='match')\n\n    return df"}
{"task_id": "BigCodeBench/882", "solution": "import sqlite3\nimport pandas as pd\nimport os\nimport re\nimport sqlalchemy\ndef task_func(db_file, table_name, column_name, pattern='\\d+[xX]'):\n    \"\"\"\n    Find all matches with a regex pattern in a list of strings in an SQL database.\n    \"\"\"\n    if not os.path.exists(db_file):\n        raise ValueError(f\"{db_file} does not exist.\")\n\n    engine = sqlalchemy.create_engine(f'sqlite:///{db_file}')\n    connection = engine.connect()\n\n    query = f\"SELECT * FROM {table_name} WHERE {column_name} REGEXP '{pattern}'\"\n    df = pd.read_sql_query(query, connection)\n\n    connection.close()\n\n    return df", "raw_solution": "import sqlite3\nimport pandas as pd\nimport os\nimport re\nimport sqlalchemy\n\ndef task_func(db_file, table_name, column_name, pattern='\\d+[xX]'):\n    \"\"\"\n    Find all matches with a regex pattern in a list of strings in an SQL database.\n    \"\"\"\n    if not os.path.exists(db_file):\n        raise ValueError(f\"{db_file} does not exist.\")\n\n    engine = sqlalchemy.create_engine(f'sqlite:///{db_file}')\n    connection = engine.connect()\n\n    query = f\"SELECT * FROM {table_name} WHERE {column_name} REGEXP '{pattern}'\"\n    df = pd.read_sql_query(query, connection)\n\n    connection.close()\n\n    return df"}
{"task_id": "BigCodeBench/883", "solution": "import pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\ndef task_func(df: pd.DataFrame, column_a: str, column_b: str, column_c: str) -> bool:\n    \"\"\"\n    Determines if a specific subset of data is stationary by filtering rows where column_b bigger than 50 and column_c equal to 900.\n    Data is considered to be stationary if the p_value returned by the Augmented Dickey-Fuller test is smaller than 0.05.\n    If column_a is empty after filtering or if its values are constant, True is returned.\n    \"\"\"\n    # Filter rows where column_b > 50 and column_c == 900\n    filtered_df = df[(df[column_b] > 50) & (df[column_c] == 900)]\n\n    # Check if column_a is empty or if its values are constant\n    if filtered_df[column_a].empty or (filtered_df[column_a].nunique() == 1):\n        return True\n\n    # Perform Augmented Dickey-Fuller test on column_a\n    result = adfuller(filtered_df[column_a])\n\n    # Check if p_value is smaller than 0.05\n    if result[1] < 0.05:\n        return True\n\n    return False", "raw_solution": "import pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\n\ndef task_func(df: pd.DataFrame, column_a: str, column_b: str, column_c: str) -> bool:\n    \"\"\"\n    Determines if a specific subset of data is stationary by filtering rows where column_b bigger than 50 and column_c equal to 900.\n    Data is considered to be stationary if the p_value returned by the Augmented Dickey-Fuller test is smaller than 0.05.\n    If column_a is empty after filtering or if its values are constant, True is returned.\n    \"\"\"\n    # Filter rows where column_b > 50 and column_c == 900\n    filtered_df = df[(df[column_b] > 50) & (df[column_c] == 900)]\n\n    # Check if column_a is empty or if its values are constant\n    if filtered_df[column_a].empty or (filtered_df[column_a].nunique() == 1):\n        return True\n\n    # Perform Augmented Dickey-Fuller test on column_a\n    result = adfuller(filtered_df[column_a])\n\n    # Check if p_value is smaller than 0.05\n    if result[1] < 0.05:\n        return True\n\n    return False"}
{"task_id": "BigCodeBench/884", "solution": "import pandas as pd\nfrom scipy.stats import chi2_contingency\ndef task_func(df, columns=['A', 'B', 'C'], larger=50, equal=900):\n    \"\"\"\n    Filters a pandas DataFrame based on the values of specific rows, and performs a chi-square independence test on the first two columns.\n    \"\"\"\n    # Check if the number of specified columns is 3\n    if len(columns) != 3:\n        raise ValueError(\"The number of specified columns must be 3.\")\n\n    # Check if the specified columns are contained in df\n    for col in columns:\n        if col not in df.columns:\n            raise ValueError(f\"The column {col} is not contained in df.\")\n\n    # Filter rows based on the criteria\n    filtered_df = df[(df[columns[1]] > larger) & (df[columns[2]] == equal)]\n\n    # Check if there's insufficient data for the test\n    if len(filtered_df) < 30:\n        raise ValueError(\"There's insufficient data for the test.\")\n\n    # Compute a contingency table of the first two columns\n    contingency_table = pd.crosstab(filtered_df[columns[0]], filtered_df[columns[1]])\n\n    # Perform a chi-square independence test\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n\n    return p_value", "raw_solution": "import pandas as pd\nfrom scipy.stats import chi2_contingency\n\ndef task_func(df, columns=['A', 'B', 'C'], larger=50, equal=900):\n    \"\"\"\n    Filters a pandas DataFrame based on the values of specific rows, and performs a chi-square independence test on the first two columns.\n    \"\"\"\n    # Check if the number of specified columns is 3\n    if len(columns) != 3:\n        raise ValueError(\"The number of specified columns must be 3.\")\n\n    # Check if the specified columns are contained in df\n    for col in columns:\n        if col not in df.columns:\n            raise ValueError(f\"The column {col} is not contained in df.\")\n\n    # Filter rows based on the criteria\n    filtered_df = df[(df[columns[1]] > larger) & (df[columns[2]] == equal)]\n\n    # Check if there's insufficient data for the test\n    if len(filtered_df) < 30:\n        raise ValueError(\"There's insufficient data for the test.\")\n\n    # Compute a contingency table of the first two columns\n    contingency_table = pd.crosstab(filtered_df[columns[0]], filtered_df[columns[1]])\n\n    # Perform a chi-square independence test\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n\n    return p_value"}
{"task_id": "BigCodeBench/885", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, col_a='A', col_b='B', col_c='C', seed=None):\n    if df.empty or df.isnull().values.any():\n        return None\n    if not all(df[col_a].apply(lambda x: pd.to_numeric(x, errors='coerce') is not None)) or \\\n       not all(df[col_b].apply(lambda x: pd.to_numeric(x, errors='coerce') is not None)) or \\\n       not all(df[col_c].apply(lambda x: pd.to_numeric(x, errors='coerce') is not None)):\n        return None\n    if col_a not in df.columns or col_b not in df.columns or col_c not in df.columns:\n        return None\n\n    df = df[df[col_b] > 50]\n    df = df[df[col_c] == 900]\n\n    if df.empty:\n        return None\n\n    X = df[col_a].values.reshape(-1, 1)\n    y = df[col_b].values.reshape(-1, 1)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    return predictions, model", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, col_a='A', col_b='B', col_c='C', seed=None):\n    if df.empty or df.isnull().values.any():\n        return None\n    if not all(df[col_a].apply(lambda x: pd.to_numeric(x, errors='coerce') is not None)) or \\\n       not all(df[col_b].apply(lambda x: pd.to_numeric(x, errors='coerce') is not None)) or \\\n       not all(df[col_c].apply(lambda x: pd.to_numeric(x, errors='coerce') is not None)):\n        return None\n    if col_a not in df.columns or col_b not in df.columns or col_c not in df.columns:\n        return None\n\n    df = df[df[col_b] > 50]\n    df = df[df[col_c] == 900]\n\n    if df.empty:\n        return None\n\n    X = df[col_a].values.reshape(-1, 1)\n    y = df[col_b].values.reshape(-1, 1)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    return predictions, model"}
{"task_id": "BigCodeBench/886", "solution": "import pandas as pd\nfrom collections import Counter\ndef task_func(data):\n    \"\"\"\n    Analyze a dictionary of student data to return a dataframe sorted by name and age in ascending order, the average score per student as a pandas Series, and the most common age as an integer.\n    \"\"\"\n    # Check if the dictionary has the required keys\n    required_keys = ['Name', 'Age', 'Score']\n    for student in data:\n        for key in required_keys:\n            if key not in student:\n                raise ValueError(f\"Missing key: {key}\")\n\n    # Convert the dictionary to a dataframe\n    df = pd.DataFrame(data)\n\n    # Sort the dataframe by 'Name' and 'Age' in ascending order\n    df = df.sort_values(by=['Name', 'Age'])\n\n    # Calculate the average score per student as a Series\n    avg_scores = df.groupby('Name')['Score'].mean()\n\n    # Calculate the most common age\n    most_common_age = Counter(df['Age']).most_common(1)\n    most_common_age = most_common_age[0][0] if most_common_age else None\n\n    return df, avg_scores, most_common_age", "raw_solution": "import pandas as pd\nfrom collections import Counter\n\ndef task_func(data):\n    \"\"\"\n    Analyze a dictionary of student data to return a dataframe sorted by name and age in ascending order, the average score per student as a pandas Series, and the most common age as an integer.\n    \"\"\"\n    # Check if the dictionary has the required keys\n    required_keys = ['Name', 'Age', 'Score']\n    for student in data:\n        for key in required_keys:\n            if key not in student:\n                raise ValueError(f\"Missing key: {key}\")\n\n    # Convert the dictionary to a dataframe\n    df = pd.DataFrame(data)\n\n    # Sort the dataframe by 'Name' and 'Age' in ascending order\n    df = df.sort_values(by=['Name', 'Age'])\n\n    # Calculate the average score per student as a Series\n    avg_scores = df.groupby('Name')['Score'].mean()\n\n    # Calculate the most common age\n    most_common_age = Counter(df['Age']).most_common(1)\n    most_common_age = most_common_age[0][0] if most_common_age else None\n\n    return df, avg_scores, most_common_age"}
{"task_id": "BigCodeBench/887", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(T1, row_num=50, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Convert elements in T1 to integers\n    T1 = [int(i) if isinstance(i, str) and i.isdigit() else i for i in T1]\n\n    # Flatten T1 and get unique values\n    T1 = list(set(itertools.chain.from_iterable(T1)))\n\n    # Create DataFrame with random numbers\n    df = pd.DataFrame(np.random.randint(0, 100, size=(row_num, len(T1))), columns=['Col_{}'.format(i+1) for i in range(len(T1))])\n\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\ndef task_func(T1, row_num=50, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Convert elements in T1 to integers\n    T1 = [int(i) if isinstance(i, str) and i.isdigit() else i for i in T1]\n\n    # Flatten T1 and get unique values\n    T1 = list(set(itertools.chain.from_iterable(T1)))\n\n    # Create DataFrame with random numbers\n    df = pd.DataFrame(np.random.randint(0, 100, size=(row_num, len(T1))), columns=['Col_{}'.format(i+1) for i in range(len(T1))])\n\n    return df\n\n# Test the function\ndf = task_func(('1', ('1', '3')), row_num=2, seed=32)"}
{"task_id": "BigCodeBench/888", "solution": "import pandas as pd\nimport os\ndef task_func(data_dir: str, csv_files: list) -> pd.DataFrame:\n    \"\"\"\n    Merges / Concatenates multiple CSV files from a specified directory into a single Pandas DataFrame.\n    If an empty list of files is passed, an empty DataFrame is returned.\n    \"\"\"\n    if not csv_files:\n        return pd.DataFrame()\n\n    dfs = []\n    for file in csv_files:\n        file_path = os.path.join(data_dir, file)\n        if os.path.isfile(file_path) and file.endswith('.csv'):\n            df = pd.read_csv(file_path)\n            dfs.append(df)\n\n    merged_df = pd.concat(dfs, ignore_index=True)\n    return merged_df", "raw_solution": "import pandas as pd\nimport os\n\ndef task_func(data_dir: str, csv_files: list) -> pd.DataFrame:\n    \"\"\"\n    Merges / Concatenates multiple CSV files from a specified directory into a single Pandas DataFrame.\n    If an empty list of files is passed, an empty DataFrame is returned.\n    \"\"\"\n    if not csv_files:\n        return pd.DataFrame()\n\n    dfs = []\n    for file in csv_files:\n        file_path = os.path.join(data_dir, file)\n        if os.path.isfile(file_path) and file.endswith('.csv'):\n            df = pd.read_csv(file_path)\n            dfs.append(df)\n\n    merged_df = pd.concat(dfs, ignore_index=True)\n    return merged_df"}
{"task_id": "BigCodeBench/889", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(data_dir: str, csv_file: str) -> pd.DataFrame:\n    \"\"\"\n    Loads a CSV file into a pandas DataFrame and replaces NaN values in numeric columns with the mean of the corresponding column.\n    If the CSV file does not exist, it raises a FileNotFoundError.\n    If the CSV file is empty, it returns an empty DataFrame.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.exists(os.path.join(data_dir, csv_file)):\n        raise FileNotFoundError(f\"The file {csv_file} does not exist in the directory {data_dir}\")\n\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(os.path.join(data_dir, csv_file))\n\n    # Replace NaN values in numeric columns with the mean of the corresponding column\n    for col in df.select_dtypes(include=[np.number]):\n        df[col].fillna(df[col].mean(), inplace=True)\n\n    return df", "raw_solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(data_dir: str, csv_file: str) -> pd.DataFrame:\n    \"\"\"\n    Loads a CSV file into a pandas DataFrame and replaces NaN values in numeric columns with the mean of the corresponding column.\n    If the CSV file does not exist, it raises a FileNotFoundError.\n    If the CSV file is empty, it returns an empty DataFrame.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.exists(os.path.join(data_dir, csv_file)):\n        raise FileNotFoundError(f\"The file {csv_file} does not exist in the directory {data_dir}\")\n\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(os.path.join(data_dir, csv_file))\n\n    # Replace NaN values in numeric columns with the mean of the corresponding column\n    for col in df.select_dtypes(include=[np.number]):\n        df[col].fillna(df[col].mean(), inplace=True)\n\n    return df"}
{"task_id": "BigCodeBench/890", "solution": "import os\nimport random\nimport pandas as pd\ndef task_func(data_dir, csv_files=['file1.csv', 'file2.csv', 'file3.csv'], seed=None):\n    \"\"\"\n    Randomly select one of the provided csv_files and select a certain number of records from the file at random.\n    The selected records are returned in a DataFrame. The name of the selected csv_file is also returned.\n    If the csv_file is empty, return an empty DataFrame.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    # Randomly select a csv_file\n    csv_file = random.choice(csv_files)\n    csv_file_path = os.path.join(data_dir, csv_file)\n\n    # Check if the csv_file exists\n    if not os.path.exists(csv_file_path):\n        return csv_file, pd.DataFrame()\n\n    # Read the csv_file\n    df = pd.read_csv(csv_file_path)\n\n    # If the csv_file is empty, return an empty DataFrame\n    if df.empty:\n        return csv_file, pd.DataFrame()\n\n    # Randomly select a number of records\n    num_records = random.randint(1, len(df))\n\n    # Select the records\n    selected_df = df.sample(n=num_records)\n\n    return csv_file, selected_df", "raw_solution": "import os\nimport random\nimport pandas as pd\n\ndef task_func(data_dir, csv_files=['file1.csv', 'file2.csv', 'file3.csv'], seed=None):\n    \"\"\"\n    Randomly select one of the provided csv_files and select a certain number of records from the file at random.\n    The selected records are returned in a DataFrame. The name of the selected csv_file is also returned.\n    If the csv_file is empty, return an empty DataFrame.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    # Randomly select a csv_file\n    csv_file = random.choice(csv_files)\n    csv_file_path = os.path.join(data_dir, csv_file)\n\n    # Check if the csv_file exists\n    if not os.path.exists(csv_file_path):\n        return csv_file, pd.DataFrame()\n\n    # Read the csv_file\n    df = pd.read_csv(csv_file_path)\n\n    # If the csv_file is empty, return an empty DataFrame\n    if df.empty:\n        return csv_file, pd.DataFrame()\n\n    # Randomly select a number of records\n    num_records = random.randint(1, len(df))\n\n    # Select the records\n    selected_df = df.sample(n=num_records)\n\n    return csv_file, selected_df"}
{"task_id": "BigCodeBench/891", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\ndef task_func(csv_file_path, attribute, test_size=0.2, random_state=42):\n    # Load the dataset\n    df = pd.read_csv(csv_file_path)\n\n    # Separate the features and the target\n    X = df.drop(columns=[attribute])\n    y = df[attribute]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Make predictions\n    predictions = model.predict(X_test)\n\n    return model, predictions", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(csv_file_path, attribute, test_size=0.2, random_state=42):\n    # Load the dataset\n    df = pd.read_csv(csv_file_path)\n\n    # Separate the features and the target\n    X = df.drop(columns=[attribute])\n    y = df[attribute]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Make predictions\n    predictions = model.predict(X_test)\n\n    return model, predictions"}
{"task_id": "BigCodeBench/892", "solution": "import re\nfrom collections import Counter\ndef task_func(strings: list) -> dict:\n    \"\"\"\n    Analyzes a given list of strings for the occurrence of a specific pattern and counts the occurrences.\n    \"\"\"\n    pattern = 'pattern'  # replace with your pattern\n    counts = Counter()\n    for string in strings:\n        matches = re.findall(pattern, string)\n        counts.update(matches)\n    return counts\nstrings = ['pattern1', 'pattern2', 'pattern1', 'pattern3', 'pattern2', 'pattern2']", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(strings: list) -> dict:\n    \"\"\"\n    Analyzes a given list of strings for the occurrence of a specific pattern and counts the occurrences.\n    \"\"\"\n    pattern = 'pattern'  # replace with your pattern\n    counts = Counter()\n    for string in strings:\n        matches = re.findall(pattern, string)\n        counts.update(matches)\n    return counts\n\n# Test the function\nstrings = ['pattern1', 'pattern2', 'pattern1', 'pattern3', 'pattern2', 'pattern2']"}
{"task_id": "BigCodeBench/893", "solution": "import re\nfrom datetime import datetime, timedelta\ndef task_func(logs: list):\n    \"\"\"\n    Analyze the given list of logs for the occurrence of errors and calculate the average time of occurrence of errors.\n    \"\"\"\n    error_times = []\n    for log in logs:\n        if 'error' in log.lower():\n            match = re.search(r'\\d{2}:\\d{2}:\\d{2}', log)\n            if match:\n                time_str = match.group(0)\n                time_obj = datetime.strptime(time_str, '%H:%M:%S')\n                error_times.append(time_obj)\n\n    if not error_times:\n        return {\"list\": [], \"time\": None}\n\n    avg_time = sum(error_times, timedelta(0)) / len(error_times)\n    avg_time = avg_time.time()\n\n    return {\"list\": [time.strftime('%H:%M:%S') for time in error_times], \"time\": avg_time.strftime('%H:%M:%S')}\nlogs = [\n    \"2022-01-01 12:00:00 [INFO] This is a normal log\",\n    \"2022-01-01 12:01:00 [ERROR] An error occurred\",\n    \"2022-01-01 12:02:00 [ERROR] Another error\",\n    \"2022-01-01 12:03:00 [INFO] Another normal log\",\n    \"2022-01-01 12:04:00 [ERROR] Yet another error\",\n]", "raw_solution": "import re\nfrom datetime import datetime, timedelta\n\ndef task_func(logs: list):\n    \"\"\"\n    Analyze the given list of logs for the occurrence of errors and calculate the average time of occurrence of errors.\n    \"\"\"\n    error_times = []\n    for log in logs:\n        if 'error' in log.lower():\n            match = re.search(r'\\d{2}:\\d{2}:\\d{2}', log)\n            if match:\n                time_str = match.group(0)\n                time_obj = datetime.strptime(time_str, '%H:%M:%S')\n                error_times.append(time_obj)\n\n    if not error_times:\n        return {\"list\": [], \"time\": None}\n\n    avg_time = sum(error_times, timedelta(0)) / len(error_times)\n    avg_time = avg_time.time()\n\n    return {\"list\": [time.strftime('%H:%M:%S') for time in error_times], \"time\": avg_time.strftime('%H:%M:%S')}\n\n# Test the function\nlogs = [\n    \"2022-01-01 12:00:00 [INFO] This is a normal log\",\n    \"2022-01-01 12:01:00 [ERROR] An error occurred\",\n    \"2022-01-01 12:02:00 [ERROR] Another error\",\n    \"2022-01-01 12:03:00 [INFO] Another normal log\",\n    \"2022-01-01 12:04:00 [ERROR] Yet another error\",\n]"}
{"task_id": "BigCodeBench/894", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nARRAY_SIZE = 10000\ndef task_func():\n    # Create a numeric array of random integers\n    array = np.random.randint(1, 101, ARRAY_SIZE)\n\n    # Calculate the mean and standard deviation\n    mean = np.mean(array)\n    std_dev = np.std(array)\n\n    # Draw a histogram of the distribution\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=100, color='blue', edgecolor='black')\n    ax.set_title('Histogram of Random Integers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    # Plot the mean as a red dashed line\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1, label='Mean')\n\n    # Plot the standard deviation as purple dashed lines\n    ax.axvline(mean - std_dev, color='purple', linestyle='dashed', linewidth=1, label='Standard Deviation')\n    ax.axvline(mean + std_dev, color='purple', linestyle='dashed', linewidth=1)\n\n    ax.legend()\n\n    return (array, mean, std_dev, ax)", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_SIZE = 10000\n\ndef task_func():\n    # Create a numeric array of random integers\n    array = np.random.randint(1, 101, ARRAY_SIZE)\n\n    # Calculate the mean and standard deviation\n    mean = np.mean(array)\n    std_dev = np.std(array)\n\n    # Draw a histogram of the distribution\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=100, color='blue', edgecolor='black')\n    ax.set_title('Histogram of Random Integers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    # Plot the mean as a red dashed line\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1, label='Mean')\n\n    # Plot the standard deviation as purple dashed lines\n    ax.axvline(mean - std_dev, color='purple', linestyle='dashed', linewidth=1, label='Standard Deviation')\n    ax.axvline(mean + std_dev, color='purple', linestyle='dashed', linewidth=1)\n\n    ax.legend()\n\n    return (array, mean, std_dev, ax)\n\n# Test the function\narray, mean, std_dev, ax = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/895", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nARRAY_SIZE = 10000\ndef task_func():\n    # Create a numeric array of random integers\n    array = np.random.randint(1, 101, ARRAY_SIZE)\n\n    # Calculate the mean and standard deviation\n    mean = np.mean(array)\n    std_dev = np.std(array)\n\n    # Draw a histogram of the distribution\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=100, color='blue', edgecolor='black')\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1, label='Mean')\n    ax.axvline(mean + std_dev, color='purple', linestyle='dashed', linewidth=1, label='Standard Deviation')\n    ax.axvline(mean - std_dev, color='purple', linestyle='dashed', linewidth=1)\n    ax.set_title('Histogram of Random Values')\n    ax.set_xlabel('Val')\n    ax.set_ylabel('Freq')\n    ax.legend()\n\n    return (array, mean, std_dev, ax)", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nARRAY_SIZE = 10000\n\ndef task_func():\n    # Create a numeric array of random integers\n    array = np.random.randint(1, 101, ARRAY_SIZE)\n\n    # Calculate the mean and standard deviation\n    mean = np.mean(array)\n    std_dev = np.std(array)\n\n    # Draw a histogram of the distribution\n    fig, ax = plt.subplots()\n    ax.hist(array, bins=100, color='blue', edgecolor='black')\n    ax.axvline(mean, color='red', linestyle='dashed', linewidth=1, label='Mean')\n    ax.axvline(mean + std_dev, color='purple', linestyle='dashed', linewidth=1, label='Standard Deviation')\n    ax.axvline(mean - std_dev, color='purple', linestyle='dashed', linewidth=1)\n    ax.set_title('Histogram of Random Values')\n    ax.set_xlabel('Val')\n    ax.set_ylabel('Freq')\n    ax.legend()\n\n    return (array, mean, std_dev, ax)\n\n# Test the function\narray, mean, std_dev, ax = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/896", "solution": "from collections import Counter\nimport random\nimport itertools\ndef task_func(length, count, seed=0):\n    random.seed(seed)\n    letters = ['a', 'b', 'c', 'd', 'e']\n    strings = [''.join(random.choices(letters, k=length)) for _ in range(count)]\n    flattened_string = ''.join(strings)\n    counter = Counter(flattened_string)\n    return counter", "raw_solution": "from collections import Counter\nimport random\nimport itertools\n\ndef task_func(length, count, seed=0):\n    random.seed(seed)\n    letters = ['a', 'b', 'c', 'd', 'e']\n    strings = [''.join(random.choices(letters, k=length)) for _ in range(count)]\n    flattened_string = ''.join(strings)\n    counter = Counter(flattened_string)\n    return counter\n\n# Test the function\ncounter = task_func(5, 1000, seed=0)"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulates a number of dice rolls, calculates the frequency of each result,\n    and returns both the frequency array and a histogram of the results.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n\n    # Calculate frequency of each result\n    frequency = np.bincount(results)\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequency)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return frequency, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulates a number of dice rolls, calculates the frequency of each result,\n    and returns both the frequency array and a histogram of the results.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n\n    # Calculate frequency of each result\n    frequency = np.bincount(results)\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequency)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return frequency, ax"}
{"task_id": "BigCodeBench/898", "solution": "from collections import Counter\nimport random\nLETTERS = ['a', 'b', 'c', 'd', 'e']\ndef task_func(count, seed=0):\n    random.seed(seed)\n    pairs = [random.choices(LETTERS, k=2) for _ in range(count)]\n    flat_pairs = [''.join(pair) for pair in pairs]\n    counter = Counter(flat_pairs)\n    return counter", "raw_solution": "from collections import Counter\nimport random\n\nLETTERS = ['a', 'b', 'c', 'd', 'e']\n\ndef task_func(count, seed=0):\n    random.seed(seed)\n    pairs = [random.choices(LETTERS, k=2) for _ in range(count)]\n    flat_pairs = [''.join(pair) for pair in pairs]\n    counter = Counter(flat_pairs)\n    return counter\n\n# Test the function"}
{"task_id": "BigCodeBench/899", "solution": "import numpy as np\nimport random\ndef task_func(length=10000, seed=0):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n    random.seed(seed)\n    steps = [random.choice([-1, 1]) for _ in range(length)]\n    walk = np.cumsum(steps)\n    return walk", "raw_solution": "import numpy as np\nimport random\n\ndef task_func(length=10000, seed=0):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n    random.seed(seed)\n    steps = [random.choice([-1, 1]) for _ in range(length)]\n    walk = np.cumsum(steps)\n    return walk"}
{"task_id": "BigCodeBench/900", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(d):\n    if not isinstance(d, list):\n        raise ValueError(\"Input should be a list of dictionaries\")\n    if not all(isinstance(i, dict) for i in d):\n        raise ValueError(\"Input should be a list of dictionaries\")\n    if not all(set(i.keys()) == {\"x\", \"y\", \"z\"} for i in d):\n        raise ValueError(\"Each dictionary should have keys 'x', 'y', and 'z'\")\n\n    df = pd.DataFrame(d)\n    stats = {\n        \"x\": {\n            \"mean\": df[\"x\"].mean(),\n            \"sum\": df[\"x\"].sum(),\n            \"max\": df[\"x\"].max(),\n            \"min\": df[\"x\"].min(),\n            \"std\": df[\"x\"].std(),\n        },\n        \"y\": {\n            \"mean\": df[\"y\"].mean(),\n            \"sum\": df[\"y\"].sum(),\n            \"max\": df[\"y\"].max(),\n            \"min\": df[\"y\"].min(),\n            \"std\": df[\"y\"].std(),\n        },\n        \"z\": {\n            \"mean\": df[\"z\"].mean(),\n            \"sum\": df[\"z\"].sum(),\n            \"max\": df[\"z\"].max(),\n            \"min\": df[\"z\"].min(),\n            \"std\": df[\"z\"].std(),\n        },\n    }\n    return stats", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(d):\n    if not isinstance(d, list):\n        raise ValueError(\"Input should be a list of dictionaries\")\n    if not all(isinstance(i, dict) for i in d):\n        raise ValueError(\"Input should be a list of dictionaries\")\n    if not all(set(i.keys()) == {\"x\", \"y\", \"z\"} for i in d):\n        raise ValueError(\"Each dictionary should have keys 'x', 'y', and 'z'\")\n\n    df = pd.DataFrame(d)\n    stats = {\n        \"x\": {\n            \"mean\": df[\"x\"].mean(),\n            \"sum\": df[\"x\"].sum(),\n            \"max\": df[\"x\"].max(),\n            \"min\": df[\"x\"].min(),\n            \"std\": df[\"x\"].std(),\n        },\n        \"y\": {\n            \"mean\": df[\"y\"].mean(),\n            \"sum\": df[\"y\"].sum(),\n            \"max\": df[\"y\"].max(),\n            \"min\": df[\"y\"].min(),\n            \"std\": df[\"y\"].std(),\n        },\n        \"z\": {\n            \"mean\": df[\"z\"].mean(),\n            \"sum\": df[\"z\"].sum(),\n            \"max\": df[\"z\"].max(),\n            \"min\": df[\"z\"].min(),\n            \"std\": df[\"z\"].std(),\n        },\n    }\n    return stats"}
{"task_id": "BigCodeBench/901", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(d):\n    if not d:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(d)\n    scaler = MinMaxScaler()\n    df[['x', 'y', 'z']] = scaler.fit_transform(df[['x', 'y', 'z']])\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Updated function to handle empty input list\ndef task_func(d):\n    if not d:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(d)\n    scaler = MinMaxScaler()\n    df[['x', 'y', 'z']] = scaler.fit_transform(df[['x', 'y', 'z']])\n    return df\n\ndata = [{'x': -1, 'y': 0, 'z': 5}, {'x': 3, 'y': -15, 'z': 0}, {'x': 0, 'y': 1, 'z': -7}]"}
{"task_id": "BigCodeBench/902", "solution": "from collections import Counter\ndef task_func(d):\n    \"\"\"\n    Count the occurrence of values with the keys \"x,\" \"y\" and \"z\" from a list of dictionaries \"d.\"\n    \"\"\"\n    x_list = []\n    y_list = []\n    z_list = []\n\n    for dic in d:\n        if 'x' in dic:\n            x_list.append(dic['x'])\n        if 'y' in dic:\n            y_list.append(dic['y'])\n        if 'z' in dic:\n            z_list.append(dic['z'])\n\n    result = {\n        'x': Counter(x_list),\n        'y': Counter(y_list),\n        'z': Counter(z_list)\n    }\n\n    return result\nd = [{'x': 1, 'y': 2, 'z': 3}, {'x': 1, 'y': 3, 'z': 2}, {'x': 2, 'y': 1, 'z': 3}, {'x': 2, 'y': 3, 'z': 1}, {'x': 3, 'y': 2, 'z': 1}]", "raw_solution": "from collections import Counter\n\ndef task_func(d):\n    \"\"\"\n    Count the occurrence of values with the keys \"x,\" \"y\" and \"z\" from a list of dictionaries \"d.\"\n    \"\"\"\n    x_list = []\n    y_list = []\n    z_list = []\n\n    for dic in d:\n        if 'x' in dic:\n            x_list.append(dic['x'])\n        if 'y' in dic:\n            y_list.append(dic['y'])\n        if 'z' in dic:\n            z_list.append(dic['z'])\n\n    result = {\n        'x': Counter(x_list),\n        'y': Counter(y_list),\n        'z': Counter(z_list)\n    }\n\n    return result\n\n# Test the function\nd = [{'x': 1, 'y': 2, 'z': 3}, {'x': 1, 'y': 3, 'z': 2}, {'x': 2, 'y': 1, 'z': 3}, {'x': 2, 'y': 3, 'z': 1}, {'x': 3, 'y': 2, 'z': 1}]"}
{"task_id": "BigCodeBench/903", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(d, target='z'):\n    df = pd.DataFrame(d)\n    X = df[['x', 'z']]\n    y = df[target]\n    model = LinearRegression()\n    model.fit(X, y)\n    return model\ndata = [{'x': 4, 'y': 20, 'z': 10}, {'x': 5, 'y': 25, 'z': 15}, {'x': 6, 'y': 5, 'z': 20}]", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(d, target='z'):\n    df = pd.DataFrame(d)\n    X = df[['x', 'z']]\n    y = df[target]\n    model = LinearRegression()\n    model.fit(X, y)\n    return model\n\n# Testing the function\ndata = [{'x': 4, 'y': 20, 'z': 10}, {'x': 5, 'y': 25, 'z': 15}, {'x': 6, 'y': 5, 'z': 20}]\nmodel = task_func(data, target='y')"}
{"task_id": "BigCodeBench/904", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(d, keys=['x', 'y', 'z']):\n    \"\"\"\n    Plot values from a list of dictionaries based on specified keys and return the plot as a Matplotlib Axes object.\n    \"\"\"\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(d)\n\n    # Create a new figure and get the Axes object\n    fig, ax = plt.subplots()\n\n    # Plot the data\n    for key in keys:\n        if key in df.columns:\n            ax.plot(df[key])\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(d, keys=['x', 'y', 'z']):\n    \"\"\"\n    Plot values from a list of dictionaries based on specified keys and return the plot as a Matplotlib Axes object.\n    \"\"\"\n    # Convert the list of dictionaries to a pandas DataFrame\n    df = pd.DataFrame(d)\n\n    # Create a new figure and get the Axes object\n    fig, ax = plt.subplots()\n\n    # Plot the data\n    for key in keys:\n        if key in df.columns:\n            ax.plot(df[key])\n\n    return ax"}
{"task_id": "BigCodeBench/905", "solution": "import os\nimport glob\nimport csv\ndef task_func(directory_path, file_extension='.csv'):\n    \"\"\"\n    Reads all files with a specified extension in a given directory and returns their data in a dictionary.\n    \"\"\"\n    data = {}\n    for file in glob.glob(os.path.join(directory_path, f'*{file_extension}')):\n        with open(file, 'r') as f:\n            reader = csv.reader(f)\n            data[os.path.splitext(os.path.basename(file))[0]] = list(reader)\n    return data", "raw_solution": "import os\nimport glob\nimport csv\n\ndef task_func(directory_path, file_extension='.csv'):\n    \"\"\"\n    Reads all files with a specified extension in a given directory and returns their data in a dictionary.\n    \"\"\"\n    data = {}\n    for file in glob.glob(os.path.join(directory_path, f'*{file_extension}')):\n        with open(file, 'r') as f:\n            reader = csv.reader(f)\n            data[os.path.splitext(os.path.basename(file))[0]] = list(reader)\n    return data\n\n# Test the function\ndata = task_func('/home/user/data', '.txt')"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from a source directory to a target directory.\n    The function identifies processed files by the '_processed' suffix in the filename.\n    \"\"\"\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(source_dir)\n\n    # Filter the list to only include files with the '_processed' suffix\n    processed_files = [file for file in files if file.endswith('_processed')]\n\n    # Create the archive file\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as archive:\n        # Add each processed file to the archive\n        for file in processed_files:\n            file_path = os.path.join(source_dir, file)\n            archive.write(file_path, arcname=file)\n\n    return archive_path", "raw_solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from a source directory to a target directory.\n    The function identifies processed files by the '_processed' suffix in the filename.\n    \"\"\"\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(source_dir)\n\n    # Filter the list to only include files with the '_processed' suffix\n    processed_files = [file for file in files if file.endswith('_processed')]\n\n    # Create the archive file\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as archive:\n        # Add each processed file to the archive\n        for file in processed_files:\n            file_path = os.path.join(source_dir, file)\n            archive.write(file_path, arcname=file)\n\n    return archive_path"}
{"task_id": "BigCodeBench/907", "solution": "import os\nimport re\ndef task_func(pattern: str, replacement: str, directory: str) -> bool:\n    \"\"\"\n    Renames all files in a directory that match a particular pattern with a given replacement string.\n    \"\"\"\n    try:\n        for filename in os.listdir(directory):\n            if re.search(pattern, filename):\n                new_filename = re.sub(pattern, replacement, filename)\n                os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n        return True\n    except Exception:\n        return False", "raw_solution": "import os\nimport re\n\ndef task_func(pattern: str, replacement: str, directory: str) -> bool:\n    \"\"\"\n    Renames all files in a directory that match a particular pattern with a given replacement string.\n    \"\"\"\n    try:\n        for filename in os.listdir(directory):\n            if re.search(pattern, filename):\n                new_filename = re.sub(pattern, replacement, filename)\n                os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n        return True\n    except Exception:\n        return False\n\n# Test the function"}
{"task_id": "BigCodeBench/908", "solution": "import os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\ndef task_func(directory: str, pattern: str) -> list:\n    \"\"\"\n    Searches a directory for CSV files matching a given regular expression pattern,\n    reads sales data from these files, and plots the sales data with month on the x-axis\n    and sales on the y-axis.\n    \"\"\"\n    # Initialize an empty list to store the axes objects\n    axes_list = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        if re.match(pattern, filename):\n            # Read the CSV file into a DataFrame\n            df = pd.read_csv(os.path.join(directory, filename))\n\n            # Create a new figure and axis\n            fig, ax = plt.subplots()\n\n            # Plot the sales data\n            ax.plot(df['Month'], df['Sales'])\n\n            # Add the axes object to the list\n            axes_list.append(ax)\n\n    # Return the list of axes objects\n    return axes_list", "raw_solution": "import os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n\ndef task_func(directory: str, pattern: str) -> list:\n    \"\"\"\n    Searches a directory for CSV files matching a given regular expression pattern,\n    reads sales data from these files, and plots the sales data with month on the x-axis\n    and sales on the y-axis.\n    \"\"\"\n    # Initialize an empty list to store the axes objects\n    axes_list = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        if re.match(pattern, filename):\n            # Read the CSV file into a DataFrame\n            df = pd.read_csv(os.path.join(directory, filename))\n\n            # Create a new figure and axis\n            fig, ax = plt.subplots()\n\n            # Plot the sales data\n            ax.plot(df['Month'], df['Sales'])\n\n            # Add the axes object to the list\n            axes_list.append(ax)\n\n    # Return the list of axes objects\n    return axes_list"}
{"task_id": "BigCodeBench/909", "solution": "import pandas as pd\nimport itertools\nfrom random import shuffle\ndef task_func(letters=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'], categories=['Category 1', 'Category 2', 'Category 3']):\n    shuffle(categories)\n    data = list(itertools.product(letters, categories))\n    df = pd.DataFrame(data, columns=['Letter', 'Category'])\n    return df", "raw_solution": "import pandas as pd\nimport itertools\nfrom random import shuffle\n\ndef task_func(letters=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'], categories=['Category 1', 'Category 2', 'Category 3']):\n    shuffle(categories)\n    data = list(itertools.product(letters, categories))\n    df = pd.DataFrame(data, columns=['Letter', 'Category'])\n    return df\n\n# Test the function\ndf = task_func()"}
{"task_id": "BigCodeBench/910", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(letters, repetitions, colors):\n    if len(letters) == 0 or len(colors) == 0:\n        raise ValueError(\"Lists 'letters' and 'colors' cannot be empty.\")\n    if len(letters) != len(repetitions) or len(repetitions) != len(colors):\n        raise ValueError(\"Lengths of the input lists do not match.\")\n\n    flat_letters = [letter for letter, rep in zip(letters, repetitions) for _ in range(rep)]\n    unique_letters, counts = np.unique(flat_letters, return_counts=True)\n\n    fig, ax = plt.subplots()\n    ax.bar(unique_letters, counts, color=colors)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Letters')\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(letters, repetitions, colors):\n    if len(letters) == 0 or len(colors) == 0:\n        raise ValueError(\"Lists 'letters' and 'colors' cannot be empty.\")\n    if len(letters) != len(repetitions) or len(repetitions) != len(colors):\n        raise ValueError(\"Lengths of the input lists do not match.\")\n\n    flat_letters = [letter for letter, rep in zip(letters, repetitions) for _ in range(rep)]\n    unique_letters, counts = np.unique(flat_letters, return_counts=True)\n\n    fig, ax = plt.subplots()\n    ax.bar(unique_letters, counts, color=colors)\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Letters')\n\n    return ax"}
{"task_id": "BigCodeBench/911", "solution": "from functools import reduce\nimport operator\nimport string\ndef task_func(letters):\n    \"\"\"\n    Calculates the product of the corresponding numbers for a list of uppercase letters.\n    \"\"\"\n    # Mapping each uppercase letter to its corresponding number\n    letter_to_num = {letter: num for num, letter in enumerate(string.ascii_uppercase, start=1)}\n\n    # Convert each letter to its corresponding number and calculate the product\n    nums = [letter_to_num[letter] for letter in letters]\n    product = reduce(operator.mul, nums)\n\n    return product", "raw_solution": "from functools import reduce\nimport operator\nimport string\n\ndef task_func(letters):\n    \"\"\"\n    Calculates the product of the corresponding numbers for a list of uppercase letters.\n    \"\"\"\n    # Mapping each uppercase letter to its corresponding number\n    letter_to_num = {letter: num for num, letter in enumerate(string.ascii_uppercase, start=1)}\n\n    # Convert each letter to its corresponding number and calculate the product\n    nums = [letter_to_num[letter] for letter in letters]\n    product = reduce(operator.mul, nums)\n\n    return product\n\n# Test the function"}
{"task_id": "BigCodeBench/912", "solution": "from collections import Counter\nimport itertools\ndef task_func(letters: list, repetitions: int) -> dict:\n    \"\"\"\n    Counts the frequency of each letter in a list after repeating it a given number of times.\n    \"\"\"\n    # Repeat the list of letters for the given number of times\n    repeated_letters = list(itertools.islice(itertools.cycle(letters), repetitions))\n    \n    # Count the frequency of each letter using Counter\n    frequency_dict = dict(Counter(repeated_letters))\n    \n    return frequency_dict\nletters = ['a', 'b', 'c', 'd']\nrepetitions = 10", "raw_solution": "from collections import Counter\nimport itertools\n\ndef task_func(letters: list, repetitions: int) -> dict:\n    \"\"\"\n    Counts the frequency of each letter in a list after repeating it a given number of times.\n    \"\"\"\n    # Repeat the list of letters for the given number of times\n    repeated_letters = list(itertools.islice(itertools.cycle(letters), repetitions))\n    \n    # Count the frequency of each letter using Counter\n    frequency_dict = dict(Counter(repeated_letters))\n    \n    return frequency_dict\n\n# Test the function\nletters = ['a', 'b', 'c', 'd']\nrepetitions = 10"}
{"task_id": "BigCodeBench/913", "solution": "from typing import List, Union\nimport numpy as np\nimport scipy.fft\nfrom collections import Counter\nfrom scipy import stats\ndef task_func(data: List[Union[int, str]], repetitions: int = 1):\n    if not data or repetitions <= 0:\n        return {'mode': np.array([]), 'count': np.array([])}\n\n    # Repeat the data\n    repeated_data = data * repetitions\n\n    # Calculate the FFT\n    fft_data = scipy.fft.fft(repeated_data)\n\n    # Calculate the mode and count\n    counter = Counter(repeated_data)\n    mode_data = stats.mode(repeated_data)\n\n    return {'mode': np.array(mode_data.mode), 'count': np.array(mode_data.count)}", "raw_solution": "from typing import List, Union\nimport numpy as np\nimport scipy.fft\nfrom collections import Counter\nfrom scipy import stats\n\ndef task_func(data: List[Union[int, str]], repetitions: int = 1):\n    if not data or repetitions <= 0:\n        return {'mode': np.array([]), 'count': np.array([])}\n\n    # Repeat the data\n    repeated_data = data * repetitions\n\n    # Calculate the FFT\n    fft_data = scipy.fft.fft(repeated_data)\n\n    # Calculate the mode and count\n    counter = Counter(repeated_data)\n    mode_data = stats.mode(repeated_data)\n\n    return {'mode': np.array(mode_data.mode), 'count': np.array(mode_data.count)}\n\n# Test the function"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    \"\"\"\n    Predicts the stock closing prices for the next 7 days using simple linear regression and plots the data.\n    \"\"\"\n    # Convert the 'Date' column to datetime format\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Create a new column 'Time' which represents the time in seconds since the start of the data\n    df['Time'] = (df.index - df.index[0]).total_seconds()\n\n    # Reshape the 'Time' column to a 2D array\n    time_array = df['Time'].values.reshape(-1, 1)\n\n    # Create a Linear Regression model\n    model = LinearRegression()\n\n    # Fit the model to the data\n    model.fit(time_array, df['Close'])\n\n    # Generate future timestamps\n    future_timestamps = pd.date_range(start=df.index[-1], periods=7, freq='D')\n    future_timestamps = (future_timestamps - df.index[0]).total_seconds()\n\n    # Predict the closing prices for the next 7 days\n    future_time_array = np.array(future_timestamps).reshape(-1, 1)\n    predictions = model.predict(future_time_array)\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df['Close'], label='Actual')\n    plt.plot(future_timestamps, predictions, label='Predicted')\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Prices Prediction')\n    plt.legend()\n    plt.grid(True)\n\n    return (predictions.tolist(), plt.gca())", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    \"\"\"\n    Predicts the stock closing prices for the next 7 days using simple linear regression and plots the data.\n    \"\"\"\n    # Convert the 'Date' column to datetime format\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n\n    # Create a new column 'Time' which represents the time in seconds since the start of the data\n    df['Time'] = (df.index - df.index[0]).total_seconds()\n\n    # Reshape the 'Time' column to a 2D array\n    time_array = df['Time'].values.reshape(-1, 1)\n\n    # Create a Linear Regression model\n    model = LinearRegression()\n\n    # Fit the model to the data\n    model.fit(time_array, df['Close'])\n\n    # Generate future timestamps\n    future_timestamps = pd.date_range(start=df.index[-1], periods=7, freq='D')\n    future_timestamps = (future_timestamps - df.index[0]).total_seconds()\n\n    # Predict the closing prices for the next 7 days\n    future_time_array = np.array(future_timestamps).reshape(-1, 1)\n    predictions = model.predict(future_time_array)\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df['Close'], label='Actual')\n    plt.plot(future_timestamps, predictions, label='Predicted')\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Prices Prediction')\n    plt.legend()\n    plt.grid(True)\n\n    return (predictions.tolist(), plt.gca())"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n    df['z_score'] = zscore(df['closing_price'])\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    plot = df.plot(kind='scatter', x='closing_price', y='z_score', color='red')\n    plt.axhline(y=z_threshold, color='r', linestyle='-')\n    plt.axhline(y=-z_threshold, color='r', linestyle='-')\n    plt.title('Outliers in Closing Prices')\n    plt.xlabel('Closing Price')\n    plt.ylabel('Z-Score')\n    return outliers, plot", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    df['z_score'] = zscore(df['closing_price'])\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    plot = df.plot(kind='scatter', x='closing_price', y='z_score', color='red')\n    plt.axhline(y=z_threshold, color='r', linestyle='-')\n    plt.axhline(y=-z_threshold, color='r', linestyle='-')\n    plt.title('Outliers in Closing Prices')\n    plt.xlabel('Closing Price')\n    plt.ylabel('Z-Score')\n    return outliers, plot"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualizes the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \"\"\"\n    # Create a new figure\n    fig, axes = plt.subplots(2, figsize=(10, 8))\n\n    # Box plot\n    sns.boxplot(data=df['Close'], orient='h', ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n    axes[0].set_xlabel('Closing Prices')\n\n    # Histogram\n    sns.histplot(data=df['Close'], kde=True, ax=axes[1])\n    axes[1].set_title('Histogram of Closing Prices')\n    axes[1].set_xlabel('Closing Prices')\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return axes[0], axes[1]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualizes the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \"\"\"\n    # Create a new figure\n    fig, axes = plt.subplots(2, figsize=(10, 8))\n\n    # Box plot\n    sns.boxplot(data=df['Close'], orient='h', ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n    axes[0].set_xlabel('Closing Prices')\n\n    # Histogram\n    sns.histplot(data=df['Close'], kde=True, ax=axes[1])\n    axes[1].set_title('Histogram of Closing Prices')\n    axes[1].set_xlabel('Closing Prices')\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return axes[0], axes[1]"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts the share closing prices for the next 7 days using the ARIMA model and plots the forecast.\n    \"\"\"\n    # Fit the model\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit(disp=0)\n\n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot the forecast\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(df['Close'])\n    ax.plot(forecast, color='red')\n    ax.set_title('Share Price Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n\n    return list(forecast), ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts the share closing prices for the next 7 days using the ARIMA model and plots the forecast.\n    \"\"\"\n    # Fit the model\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit(disp=0)\n\n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot the forecast\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(df['Close'])\n    ax.plot(forecast, color='red')\n    ax.set_title('Share Price Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n\n    return list(forecast), ax"}
{"task_id": "BigCodeBench/918", "solution": "import pandas as pd\nimport re\ndef task_func(data, mapping):\n    \"\"\"\n    Replaces all acronyms in a DataFrame with their full words according to a provided dictionary.\n    \"\"\"\n    # Create a regular expression pattern from the acronyms in the mapping\n    pattern = re.compile(\"|\".join(map(re.escape, mapping.keys())), re.IGNORECASE)\n\n    # Replace the acronyms in the DataFrame with their full words\n    data = data.applymap(lambda x: pattern.sub(lambda m: mapping[m.group(0)], str(x)))\n\n    return data\nmapping = {\n    'AI': 'Artificial Intelligence',\n    'ROCKS': 'is amazing',\n    'PYTHON': 'Python Programming',\n    'IS': 'is',\n    'THE': 'the',\n    'BEST': 'best'\n}", "raw_solution": "import pandas as pd\nimport re\n\ndef task_func(data, mapping):\n    \"\"\"\n    Replaces all acronyms in a DataFrame with their full words according to a provided dictionary.\n    \"\"\"\n    # Create a regular expression pattern from the acronyms in the mapping\n    pattern = re.compile(\"|\".join(map(re.escape, mapping.keys())), re.IGNORECASE)\n\n    # Replace the acronyms in the DataFrame with their full words\n    data = data.applymap(lambda x: pattern.sub(lambda m: mapping[m.group(0)], str(x)))\n\n    return data\n\n# Test the function\n# Create a DataFrame\ndf = pd.DataFrame({\n    'A': ['I AM AI', 'AI IS THE FUTURE'],\n    'B': ['AI ROCKS', 'PYTHON IS NOT AI'],\n    'C': ['AI IS AMAZING', 'AI IS THE BEST']\n})\n\n# Define the mapping\nmapping = {\n    'AI': 'Artificial Intelligence',\n    'ROCKS': 'is amazing',\n    'PYTHON': 'Python Programming',\n    'IS': 'is',\n    'THE': 'the',\n    'BEST': 'best'\n}\n\n# Replace the acronyms in the DataFrame\ndf = task_func(df, mapping)\n\n# Print the DataFrame"}
{"task_id": "BigCodeBench/919", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\ndef task_func(data, column):\n    \"\"\"\n    Draws and returns a bar chart that shows the distribution of categories in a specific column of a DataFrame.\n    \"\"\"\n    # Ensure the data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data must be a pandas DataFrame\")\n\n    # Ensure the column exists in the DataFrame\n    if column not in data.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame\")\n\n    # Count the occurrences of each category in the column\n    counts = data[column].value_counts()\n\n    # Ensure all categories are included in the plot\n    for category in CATEGORIES:\n        if category not in counts.index:\n            counts[category] = 0\n\n    # Sort the counts by category\n    counts = counts.sort_index()\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    counts.plot(kind='bar', ax=ax)\n\n    # Set labels and title\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Distribution of {column}')\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\nCATEGORIES = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(data, column):\n    \"\"\"\n    Draws and returns a bar chart that shows the distribution of categories in a specific column of a DataFrame.\n    \"\"\"\n    # Ensure the data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data must be a pandas DataFrame\")\n\n    # Ensure the column exists in the DataFrame\n    if column not in data.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame\")\n\n    # Count the occurrences of each category in the column\n    counts = data[column].value_counts()\n\n    # Ensure all categories are included in the plot\n    for category in CATEGORIES:\n        if category not in counts.index:\n            counts[category] = 0\n\n    # Sort the counts by category\n    counts = counts.sort_index()\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    counts.plot(kind='bar', ax=ax)\n\n    # Set labels and title\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Distribution of {column}')\n\n    return ax"}
{"task_id": "BigCodeBench/920", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Draws and returns a correlation matrix heatmap for a DataFrame containing numerical columns.\n    The title of the heatmap is set to 'Correlation Matrix'.\n    \"\"\"\n    # Calculate correlation matrix\n    corr = data.corr()\n\n    # Create a mask to hide the upper triangle of the correlation matrix\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap='RdBu_r', vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n                annot=True, fmt=\".2f\",\n                title='Correlation Matrix')\n\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    Draws and returns a correlation matrix heatmap for a DataFrame containing numerical columns.\n    The title of the heatmap is set to 'Correlation Matrix'.\n    \"\"\"\n    # Calculate correlation matrix\n    corr = data.corr()\n\n    # Create a mask to hide the upper triangle of the correlation matrix\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap='RdBu_r', vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n                annot=True, fmt=\".2f\",\n                title='Correlation Matrix')\n\n    return ax"}
{"task_id": "BigCodeBench/921", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data, columns):\n    \"\"\"\n    Normalizes specified columns of a DataFrame using min-max scaling.\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data must be a pandas DataFrame\")\n    if not isinstance(columns, list):\n        raise ValueError(\"Columns must be a list\")\n    if not all(isinstance(col, str) for col in columns):\n        raise ValueError(\"All elements in columns must be strings\")\n    if not all(col in data.columns for col in columns):\n        raise ValueError(\"All columns in columns must be in data\")\n\n    scaler = MinMaxScaler()\n    data[columns] = scaler.fit_transform(data[columns])\n    return data", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data, columns):\n    \"\"\"\n    Normalizes specified columns of a DataFrame using min-max scaling.\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data must be a pandas DataFrame\")\n    if not isinstance(columns, list):\n        raise ValueError(\"Columns must be a list\")\n    if not all(isinstance(col, str) for col in columns):\n        raise ValueError(\"All elements in columns must be strings\")\n    if not all(col in data.columns for col in columns):\n        raise ValueError(\"All columns in columns must be in data\")\n\n    scaler = MinMaxScaler()\n    data[columns] = scaler.fit_transform(data[columns])\n    return data"}
{"task_id": "BigCodeBench/922", "solution": "import pandas as pd\nfrom nltk.corpus import stopwords", "raw_solution": "import pandas as pd\nfrom nltk.corpus import stopwords\n\n# Constants\nSTOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords(data, column):\n    \"\"\"\n    Removes English stopwords from a text column in a DataFrame.\n    \"\"\"\n    # Check if the column exists in the DataFrame\n    if column not in data.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n\n    # Check if the column is of string type\n    if data[column].dtype != 'object':\n        raise ValueError(f\"Column '{column}' is not of string type.\")\n\n    # Remove stopwords\n    data[column] = data[column].apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS]))\n\n    return data"}
{"task_id": "BigCodeBench/923", "solution": "import pandas as pd\nimport random\nimport re\ndef task_func(person_names, email_domains, num_records=5):\n    if not email_domains:\n        raise ValueError(\"No email domains provided\")\n    if len(person_names) < num_records:\n        raise ValueError(\"Number of names provided is less than the number of records requested\")\n\n    names_emails = []\n    for _ in range(num_records):\n        name = random.choice(person_names)\n        domain = random.choice(email_domains)\n        email = re.sub(r'@', '[at]', f\"{name}@{domain}\")\n        names_emails.append((name, email))\n\n    df = pd.DataFrame(names_emails, columns=['Name', 'Email'])\n    return df", "raw_solution": "import pandas as pd\nimport random\nimport re\n\ndef task_func(person_names, email_domains, num_records=5):\n    if not email_domains:\n        raise ValueError(\"No email domains provided\")\n    if len(person_names) < num_records:\n        raise ValueError(\"Number of names provided is less than the number of records requested\")\n\n    names_emails = []\n    for _ in range(num_records):\n        name = random.choice(person_names)\n        domain = random.choice(email_domains)\n        email = re.sub(r'@', '[at]', f\"{name}@{domain}\")\n        names_emails.append((name, email))\n\n    df = pd.DataFrame(names_emails, columns=['Name', 'Email'])\n    return df"}
{"task_id": "BigCodeBench/924", "solution": "import pandas as pd\nimport os\nimport sys\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Loads a CSV file into a Pandas DataFrame, replaces all occurrences of the string '\\n' with the string '<br>' in the specified column, and returns the cleaned DataFrame.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(file_path):\n        print(f\"File {file_path} does not exist.\")\n        return None\n\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        print(f\"Column {column_name} does not exist in the DataFrame.\")\n        return None\n\n    # Replace all occurrences of '\\n' with '<br>'\n    df[column_name] = df[column_name].apply(lambda x: str(x).replace('\\n', '<br>'))\n\n    return df", "raw_solution": "import pandas as pd\nimport os\nimport sys\n\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Loads a CSV file into a Pandas DataFrame, replaces all occurrences of the string '\\n' with the string '<br>' in the specified column, and returns the cleaned DataFrame.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(file_path):\n        print(f\"File {file_path} does not exist.\")\n        return None\n\n    # Load the CSV file into a DataFrame\n    df = pd.read_csv(file_path)\n\n    # Check if the column exists\n    if column_name not in df.columns:\n        print(f\"Column {column_name} does not exist in the DataFrame.\")\n        return None\n\n    # Replace all occurrences of '\\n' with '<br>'\n    df[column_name] = df[column_name].apply(lambda x: str(x).replace('\\n', '<br>'))\n\n    return df"}
{"task_id": "BigCodeBench/925", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_size=1000, column_names=['A', ' ','C', 'D', 'E'], seed=0):\n    np.random.seed(seed)\n    df = pd.DataFrame(np.random.randint(1, 101, size=(data_size, len(column_names))), columns=column_names)\n    df[df < 10] = -1\n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(data_size=1000, column_names=['A', ' ','C', 'D', 'E'], seed=0):\n    np.random.seed(seed)\n    df = pd.DataFrame(np.random.randint(1, 101, size=(data_size, len(column_names))), columns=column_names)\n    df[df < 10] = -1\n    return df\n\n# Test the function\ndf = task_func()"}
{"task_id": "BigCodeBench/926", "solution": "import pandas as pd\nimport sqlite3\ndef task_func(db_path: str, table_name: str, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Loads data from an SQLite database into a Pandas DataFrame and performs a string replacement operation on a specified column.\n    Specifically, replaces all occurrences of the newline character '\\n' with the HTML line break tag '<br>'.\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_path)\n\n    # Load the data from the specified table into a DataFrame\n    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n\n    # Replace all occurrences of '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n\n    # Close the database connection\n    conn.close()\n\n    # Return the modified DataFrame\n    return df", "raw_solution": "import pandas as pd\nimport sqlite3\n\ndef task_func(db_path: str, table_name: str, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Loads data from an SQLite database into a Pandas DataFrame and performs a string replacement operation on a specified column.\n    Specifically, replaces all occurrences of the newline character '\\n' with the HTML line break tag '<br>'.\n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(db_path)\n\n    # Load the data from the specified table into a DataFrame\n    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n\n    # Replace all occurrences of '\\n' with '<br>' in the specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n\n    # Close the database connection\n    conn.close()\n\n    # Return the modified DataFrame\n    return df"}
{"task_id": "BigCodeBench/927", "solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Loads a CSV file into a Pandas DataFrame, replaces all occurrences of the string '\\n' with the string '<br>' in the specified column,\n    and encodes the specified column as a categorical variable using LabelEncoder from sklearn.\n    \"\"\"\n    # Load CSV file into DataFrame\n    df = pd.read_csv(file_path)\n\n    # Replace '\\n' with '<br>' in specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n\n    # Initialize LabelEncoder\n    le = LabelEncoder()\n\n    # Encode specified column\n    df[column_name] = le.fit_transform(df[column_name])\n\n    return df", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Loads a CSV file into a Pandas DataFrame, replaces all occurrences of the string '\\n' with the string '<br>' in the specified column,\n    and encodes the specified column as a categorical variable using LabelEncoder from sklearn.\n    \"\"\"\n    # Load CSV file into DataFrame\n    df = pd.read_csv(file_path)\n\n    # Replace '\\n' with '<br>' in specified column\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n\n    # Initialize LabelEncoder\n    le = LabelEncoder()\n\n    # Encode specified column\n    df[column_name] = le.fit_transform(df[column_name])\n\n    return df"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Create a dictionary containing all possible two-letter combinations of the lowercase English alphabets.\n    The dictionary values represent the frequency of these two-letter combinations in the given word.\n    If a combination does not appear in the word, its value will be 0.\n    \"\"\"\n    # Generate all two-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=2)]\n\n    # Create a dictionary with all combinations as keys and 0 as values\n    result = {combo: 0 for combo in combinations}\n\n    # Count the frequency of each two-letter combination in the word\n    word_counter = Counter(word[i:i+2] for i in range(len(word)))\n\n    # Update the values in the result dictionary with the counts from the word\n    for combo in combinations:\n        result[combo] = word_counter[combo]\n\n    return result", "raw_solution": "from collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Create a dictionary containing all possible two-letter combinations of the lowercase English alphabets.\n    The dictionary values represent the frequency of these two-letter combinations in the given word.\n    If a combination does not appear in the word, its value will be 0.\n    \"\"\"\n    # Generate all two-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=2)]\n\n    # Create a dictionary with all combinations as keys and 0 as values\n    result = {combo: 0 for combo in combinations}\n\n    # Count the frequency of each two-letter combination in the word\n    word_counter = Counter(word[i:i+2] for i in range(len(word)))\n\n    # Update the values in the result dictionary with the counts from the word\n    for combo in combinations:\n        result[combo] = word_counter[combo]\n\n    return result\n\n# Test the function"}
{"task_id": "BigCodeBench/929", "solution": "import numpy as np\nfrom scipy import stats\nimport math\ndef task_func(word: str) -> np.ndarray:\n    \"\"\"\n    Calculates the difference between the ASCII values of each pair of adjacent letters in the input word.\n    After calculating the difference, calculates the entropy of the differences.\n    \"\"\"\n    # Convert the word to ASCII values\n    ascii_values = [ord(c) for c in word]\n\n    # Calculate the differences\n    differences = [ascii_values[i+1] - ascii_values[i] for i in range(len(ascii_values)-1)]\n\n    # Convert the differences to numpy array\n    differences_array = np.array(differences)\n\n    # Calculate the entropy\n    entropy = stats.entropy(differences_array)\n\n    return differences_array, entropy", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport math\n\ndef task_func(word: str) -> np.ndarray:\n    \"\"\"\n    Calculates the difference between the ASCII values of each pair of adjacent letters in the input word.\n    After calculating the difference, calculates the entropy of the differences.\n    \"\"\"\n    # Convert the word to ASCII values\n    ascii_values = [ord(c) for c in word]\n\n    # Calculate the differences\n    differences = [ascii_values[i+1] - ascii_values[i] for i in range(len(ascii_values)-1)]\n\n    # Convert the differences to numpy array\n    differences_array = np.array(differences)\n\n    # Calculate the entropy\n    entropy = stats.entropy(differences_array)\n\n    return differences_array, entropy"}
{"task_id": "BigCodeBench/930", "solution": "import random\nimport string\nPOSSIBLE_LETTERS = ['a', 'b', 'c']\ndef task_func(word):\n    if not all(char in string.ascii_letters for char in word):\n        raise ValueError(\"Input contains non-letter characters\")\n\n    if len(word) < 2:\n        return [\"\"] * len(POSSIBLE_LETTERS)\n\n    pairs = []\n    for i in range(len(word) - 1):\n        pairs.append(word[i:i+2])\n\n    return pairs", "raw_solution": "import random\nimport string\n\nPOSSIBLE_LETTERS = ['a', 'b', 'c']\n\ndef task_func(word):\n    if not all(char in string.ascii_letters for char in word):\n        raise ValueError(\"Input contains non-letter characters\")\n\n    if len(word) < 2:\n        return [\"\"] * len(POSSIBLE_LETTERS)\n\n    pairs = []\n    for i in range(len(word) - 1):\n        pairs.append(word[i:i+2])\n\n    return pairs\n\n# Testing the function"}
{"task_id": "BigCodeBench/931", "solution": "from collections import defaultdict\nimport re\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Finds the occurrences of each two-letter combination in the sanitized word, where only alphabetic characters are considered.\n    \"\"\"\n    # Sanitize the word by removing non-alphabetic characters\n    sanitized_word = re.sub(r'[^a-zA-Z]', '', word)\n\n    # Initialize a defaultdict to store the two-letter combinations and their counts\n    two_letter_combinations = defaultdict(int)\n\n    # Iterate over the sanitized word and find two-letter combinations\n    for i in range(len(sanitized_word) - 1):\n        two_letter_combination = sanitized_word[i:i+2]\n        two_letter_combinations[two_letter_combination] += 1\n\n    return two_letter_combinations", "raw_solution": "from collections import defaultdict\nimport re\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Finds the occurrences of each two-letter combination in the sanitized word, where only alphabetic characters are considered.\n    \"\"\"\n    # Sanitize the word by removing non-alphabetic characters\n    sanitized_word = re.sub(r'[^a-zA-Z]', '', word)\n\n    # Initialize a defaultdict to store the two-letter combinations and their counts\n    two_letter_combinations = defaultdict(int)\n\n    # Iterate over the sanitized word and find two-letter combinations\n    for i in range(len(sanitized_word) - 1):\n        two_letter_combination = sanitized_word[i:i+2]\n        two_letter_combinations[two_letter_combination] += 1\n\n    return two_letter_combinations"}
{"task_id": "BigCodeBench/932", "solution": "from collections import Counter\nimport re\ndef task_func(word: str) -> list:\n    # Clean the word by removing non-alphabetic characters and converting to lowercase\n    cleaned_word = re.sub(r'[^a-z]', '', word.lower())\n\n    # If the cleaned word has fewer than 2 letters, return an empty list\n    if len(cleaned_word) < 2:\n        return []\n\n    # Create a list of all two-letter combinations in the cleaned word\n    two_letter_combinations = [cleaned_word[i:i+2] for i in range(len(cleaned_word)-1)]\n\n    # Use Counter to count the frequency of each two-letter combination\n    counter = Counter(two_letter_combinations)\n\n    # Find the most common two-letter combination and its frequency\n    most_common = counter.most_common(1)\n\n    return most_common", "raw_solution": "from collections import Counter\nimport re\n\ndef task_func(word: str) -> list:\n    # Clean the word by removing non-alphabetic characters and converting to lowercase\n    cleaned_word = re.sub(r'[^a-z]', '', word.lower())\n\n    # If the cleaned word has fewer than 2 letters, return an empty list\n    if len(cleaned_word) < 2:\n        return []\n\n    # Create a list of all two-letter combinations in the cleaned word\n    two_letter_combinations = [cleaned_word[i:i+2] for i in range(len(cleaned_word)-1)]\n\n    # Use Counter to count the frequency of each two-letter combination\n    counter = Counter(two_letter_combinations)\n\n    # Find the most common two-letter combination and its frequency\n    most_common = counter.most_common(1)\n\n    return most_common"}
{"task_id": "BigCodeBench/933", "solution": "import string\nimport wordninja\ndef task_func(word):\n    word = word.lower()\n    alphabet = string.ascii_lowercase\n    alphabet_dict = {letter: index+1 for index, letter in enumerate(alphabet)}\n    result = [(letter, alphabet_dict[letter]) for letter in word if letter in alphabet_dict]\n    return result", "raw_solution": "import string\nimport wordninja\n\ndef task_func(word):\n    word = word.lower()\n    alphabet = string.ascii_lowercase\n    alphabet_dict = {letter: index+1 for index, letter in enumerate(alphabet)}\n    result = [(letter, alphabet_dict[letter]) for letter in word if letter in alphabet_dict]\n    return result\n\n# Test the function"}
{"task_id": "BigCodeBench/934", "solution": "from collections import Counter\nimport hashlib\ndef task_func(word: str) -> dict:\n    pairs = [word[i:i+2] for i in range(len(word)-1)]\n    count = Counter(pairs)\n    return count", "raw_solution": "from collections import Counter\nimport hashlib\n\ndef task_func(word: str) -> dict:\n    pairs = [word[i:i+2] for i in range(len(word)-1)]\n    count = Counter(pairs)\n    return count\n\ndef test_task_func():\n    assert task_func(\"hello\") == {\"he\": 1, \"el\": 1, \"ll\": 1, \"lo\": 1}\n    assert task_func(\"mississippi\") == {\"mi\": 1, \"is\": 2, \"ss\": 2, \"si\": 1, \"ip\": 1}\n    assert task_func(\"a\") == {}\n    assert task_func(\"\") == {}\n\ntest_task_func()"}
{"task_id": "BigCodeBench/935", "solution": "import pandas as pd\nimport string\ndef task_func(word):\n    if not word.islower():\n        raise ValueError(\"Input word should be in lowercase\")\n    if not word.isalpha():\n        raise ValueError(\"Input word should not contain non-alphabetic characters\")\n\n    alphabet = string.ascii_lowercase\n    positions = {letter: index+1 for index, letter in enumerate(alphabet)}\n\n    data = [(letter, positions[letter]) for letter in word]\n    df = pd.DataFrame(data, columns=['Letter', 'Position'])\n    return df", "raw_solution": "import pandas as pd\nimport string\n\ndef task_func(word):\n    if not word.islower():\n        raise ValueError(\"Input word should be in lowercase\")\n    if not word.isalpha():\n        raise ValueError(\"Input word should not contain non-alphabetic characters\")\n\n    alphabet = string.ascii_lowercase\n    positions = {letter: index+1 for index, letter in enumerate(alphabet)}\n\n    data = [(letter, positions[letter]) for letter in word]\n    df = pd.DataFrame(data, columns=['Letter', 'Position'])\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/936", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport string\nALPHABET = list(string.ascii_lowercase)\ndef task_func(word):\n    \"\"\"\n    Draws a bar chart representing the positions of each letter in the given word within the English alphabet using numpy and matplotlib.pyplot.\n    \"\"\"\n    # Convert the word to lowercase and create a list of its characters\n    word = word.lower()\n    word_chars = list(word)\n\n    # Create a list to store the positions of each letter in the word\n    positions = []\n\n    # Iterate over each letter in the word\n    for letter in word_chars:\n        # Find the position of the letter in the alphabet and add it to the list\n        positions.append(ALPHABET.index(letter) + 1)\n\n    # Create a numpy array from the list of positions\n    positions = np.array(positions)\n\n    # Create a histogram of the positions\n    counts, bins = np.histogram(positions, bins=range(1, 27))\n\n    # Create a bar chart of the histogram\n    ax = plt.bar(bins[:-1], counts, align='center')\n\n    # Set the x-axis label\n    plt.xlabel('Position in English alphabet')\n\n    # Set the y-axis label\n    plt.ylabel('Frequency')\n\n    # Set the title of the plot\n    plt.title('Positions of each letter in the word \"{}\"'.format(word))\n\n    # Return the Axes object\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport string\n\n# Constants\nALPHABET = list(string.ascii_lowercase)\n\ndef task_func(word):\n    \"\"\"\n    Draws a bar chart representing the positions of each letter in the given word within the English alphabet using numpy and matplotlib.pyplot.\n    \"\"\"\n    # Convert the word to lowercase and create a list of its characters\n    word = word.lower()\n    word_chars = list(word)\n\n    # Create a list to store the positions of each letter in the word\n    positions = []\n\n    # Iterate over each letter in the word\n    for letter in word_chars:\n        # Find the position of the letter in the alphabet and add it to the list\n        positions.append(ALPHABET.index(letter) + 1)\n\n    # Create a numpy array from the list of positions\n    positions = np.array(positions)\n\n    # Create a histogram of the positions\n    counts, bins = np.histogram(positions, bins=range(1, 27))\n\n    # Create a bar chart of the histogram\n    ax = plt.bar(bins[:-1], counts, align='center')\n\n    # Set the x-axis label\n    plt.xlabel('Position in English alphabet')\n\n    # Set the y-axis label\n    plt.ylabel('Frequency')\n\n    # Set the title of the plot\n    plt.title('Positions of each letter in the word \"{}\"'.format(word))\n\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/937", "solution": "import re\nfrom collections import Counter\ndef task_func(input_str):\n    # Remove all non-alphanumeric characters\n    input_str = re.sub(r'\\W+', '', input_str)\n\n    # Convert to lowercase\n    input_str = input_str.lower()\n\n    # Count the frequency of each character\n    freq_dict = dict(Counter(input_str))\n\n    return freq_dict", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(input_str):\n    # Remove all non-alphanumeric characters\n    input_str = re.sub(r'\\W+', '', input_str)\n\n    # Convert to lowercase\n    input_str = input_str.lower()\n\n    # Count the frequency of each character\n    freq_dict = dict(Counter(input_str))\n\n    return freq_dict"}
{"task_id": "BigCodeBench/938", "solution": "import re\nimport pandas as pd\ndef task_func(input_df):\n    \"\"\"\n    Cleans the text in a pandas DataFrame column named 'text' by removing all special characters, punctuation marks, and spaces, then calculates the length of the cleaned text.\n    \"\"\"\n    # Check if the input is a pandas DataFrame\n    if not isinstance(input_df, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n\n    # Check if the DataFrame has a column named 'text'\n    if 'text' not in input_df.columns:\n        raise ValueError(\"Input DataFrame should have a column named 'text'\")\n\n    # Remove special characters, punctuation marks, and spaces\n    input_df['clean_text'] = input_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n    input_df['clean_text'] = input_df['clean_text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n\n    # Calculate the length of the cleaned text\n    input_df['text_length'] = input_df['clean_text'].apply(len)\n\n    return input_df", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(input_df):\n    \"\"\"\n    Cleans the text in a pandas DataFrame column named 'text' by removing all special characters, punctuation marks, and spaces, then calculates the length of the cleaned text.\n    \"\"\"\n    # Check if the input is a pandas DataFrame\n    if not isinstance(input_df, pd.DataFrame):\n        raise ValueError(\"Input should be a pandas DataFrame\")\n\n    # Check if the DataFrame has a column named 'text'\n    if 'text' not in input_df.columns:\n        raise ValueError(\"Input DataFrame should have a column named 'text'\")\n\n    # Remove special characters, punctuation marks, and spaces\n    input_df['clean_text'] = input_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n    input_df['clean_text'] = input_df['clean_text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n\n    # Calculate the length of the cleaned text\n    input_df['text_length'] = input_df['clean_text'].apply(len)\n\n    return input_df"}
{"task_id": "BigCodeBench/939", "solution": "import re\nimport os\nimport glob\ndef task_func(dir_path: str) -> list:\n    \"\"\"\n    Renames all files in the specified directory by removing all special characters, punctuation marks, and spaces, using regular expressions.\n    \"\"\"\n    # Get all files in the directory\n    files = glob.glob(os.path.join(dir_path, '*'))\n\n    new_names = []\n    for file in files:\n        # Get the base name of the file\n        base_name = os.path.basename(file)\n\n        # Remove all special characters, punctuation marks, and spaces\n        new_name = re.sub(r'[^\\w\\s]', '', base_name)\n        new_name = re.sub(r'[^A-Za-z0-9\\s]', '', new_name)\n\n        # Append the new name to the list\n        new_names.append(new_name)\n\n        # Rename the file\n        os.rename(file, os.path.join(dir_path, new_name))\n\n    return new_names", "raw_solution": "import re\nimport os\nimport glob\n\ndef task_func(dir_path: str) -> list:\n    \"\"\"\n    Renames all files in the specified directory by removing all special characters, punctuation marks, and spaces, using regular expressions.\n    \"\"\"\n    # Get all files in the directory\n    files = glob.glob(os.path.join(dir_path, '*'))\n\n    new_names = []\n    for file in files:\n        # Get the base name of the file\n        base_name = os.path.basename(file)\n\n        # Remove all special characters, punctuation marks, and spaces\n        new_name = re.sub(r'[^\\w\\s]', '', base_name)\n        new_name = re.sub(r'[^A-Za-z0-9\\s]', '', new_name)\n\n        # Append the new name to the list\n        new_names.append(new_name)\n\n        # Rename the file\n        os.rename(file, os.path.join(dir_path, new_name))\n\n    return new_names"}
{"task_id": "BigCodeBench/940", "solution": "import re\nfrom nltk import word_tokenize\nfrom collections import Counter\ndef task_func(input_str):\n    # Remove all special characters, punctuation marks and spaces from the string\n    cleaned_str = re.sub(r'[^\\w\\s]', '', input_str)\n    cleaned_str = re.sub(r'\\s+', ' ', cleaned_str)\n\n    # Tokenize the string into words\n    words = word_tokenize(cleaned_str)\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    return word_freq", "raw_solution": "import re\nfrom nltk import word_tokenize\nfrom collections import Counter\n\ndef task_func(input_str):\n    # Remove all special characters, punctuation marks and spaces from the string\n    cleaned_str = re.sub(r'[^\\w\\s]', '', input_str)\n    cleaned_str = re.sub(r'\\s+', ' ', cleaned_str)\n\n    # Tokenize the string into words\n    words = word_tokenize(cleaned_str)\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    return word_freq"}
{"task_id": "BigCodeBench/941", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\ndef task_func(start_date: str, periods: int, freq: str, random_seed: int = 0) -> Tuple[pd.DataFrame, plt.Axes]:\n    \"\"\"\n    Generates and plots a sales forecast starting from a given date, for a specified number of periods and frequency.\n    \"\"\"\n    np.random.seed(random_seed)\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    sales = np.random.randint(100, 500, size=periods)\n    df = pd.DataFrame({'Date': dates, 'Sales': sales})\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Date'], df['Sales'], marker='o')\n    ax.set_title('Sales Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n\n    return df, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\n\ndef task_func(start_date: str, periods: int, freq: str, random_seed: int = 0) -> Tuple[pd.DataFrame, plt.Axes]:\n    \"\"\"\n    Generates and plots a sales forecast starting from a given date, for a specified number of periods and frequency.\n    \"\"\"\n    np.random.seed(random_seed)\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    sales = np.random.randint(100, 500, size=periods)\n    df = pd.DataFrame({'Date': dates, 'Sales': sales})\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Date'], df['Sales'], marker='o')\n    ax.set_title('Sales Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n\n    return df, ax"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create a DataFrame with dates and categories\n    df = pd.DataFrame({'Date': dates})\n    df['Category'] = np.random.choice(categories, size=periods)\n\n    # Generate random sales data\n    df['Sales'] = np.random.randint(100, 1000, size=periods)\n\n    # Plot the sales data\n    ax = df.groupby('Category')['Sales'].sum().plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Sales Report by Category')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Sales')\n\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Create a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create a DataFrame with dates and categories\n    df = pd.DataFrame({'Date': dates})\n    df['Category'] = np.random.choice(categories, size=periods)\n\n    # Generate random sales data\n    df['Sales'] = np.random.randint(100, 1000, size=periods)\n\n    # Plot the sales data\n    ax = df.groupby('Category')['Sales'].sum().plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Sales Report by Category')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Sales')\n\n    return df, ax.figure\n\n# Test the function\ndf, fig = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/943", "solution": "import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(start_date='2016-01-01', periods=24, freq='M', model='additive'):\n    \"\"\"\n    Generate a sales time-series and decompose it into trend, seasonal, and residual components.\n    \"\"\"\n    # Generate a sales time-series\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    sales = np.random.randint(100, 500, size=periods)\n    df = pd.DataFrame({'Date': dates, 'Sales': sales})\n    df.set_index('Date', inplace=True)\n\n    # Decompose the time-series\n    decomposition = seasonal_decompose(df['Sales'], model=model)\n\n    # Return the trend, seasonal, and residual components\n    return {'trend': decomposition.trend, 'seasonal': decomposition.seasonal, 'residual': decomposition.resid}", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(start_date='2016-01-01', periods=24, freq='M', model='additive'):\n    \"\"\"\n    Generate a sales time-series and decompose it into trend, seasonal, and residual components.\n    \"\"\"\n    # Generate a sales time-series\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    sales = np.random.randint(100, 500, size=periods)\n    df = pd.DataFrame({'Date': dates, 'Sales': sales})\n    df.set_index('Date', inplace=True)\n\n    # Decompose the time-series\n    decomposition = seasonal_decompose(df['Sales'], model=model)\n\n    # Return the trend, seasonal, and residual components\n    return {'trend': decomposition.trend, 'seasonal': decomposition.seasonal, 'residual': decomposition.resid}\n\n# Test the function\nresult = task_func('2020-01-01', 24, 'M', 'multiplicative')\nassert len(result['seasonal']) == 24"}
{"task_id": "BigCodeBench/944", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', seed=0):\n    np.random.seed(seed)\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    prices = np.random.uniform(low=100, high=500, size=periods)\n    df = pd.DataFrame({'Date': date_range, 'Price': prices})\n    fig, ax = plt.subplots(figsize=(10, 6))\n    df.plot(x='Date', y='Price', ax=ax)\n    ax.set_title('Share Prices Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    return df, ax.figure", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', seed=0):\n    np.random.seed(seed)\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    prices = np.random.uniform(low=100, high=500, size=periods)\n    df = pd.DataFrame({'Date': date_range, 'Price': prices})\n    fig, ax = plt.subplots(figsize=(10, 6))\n    df.plot(x='Date', y='Price', ax=ax)\n    ax.set_title('Share Prices Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    return df, ax.figure\n\n# Test the function\ndf, fig = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    if sales_data is None:\n        sales_data = pd.date_range(start=start_date, periods=periods, freq=freq)\n        sales_data = pd.Series(np.random.randint(1, 100, len(sales_data)), index=sales_data)\n    else:\n        sales_data = pd.Series(sales_data, index=pd.date_range(start=start_date, periods=len(sales_data), freq=freq))\n\n    X = np.array(range(0, len(sales_data))).reshape(-1, 1)\n    Y = sales_data.values.reshape(-1, 1)\n\n    linear_regressor = LinearRegression()\n    linear_regressor.fit(X, Y)\n\n    future_dates = pd.date_range(start=sales_data.index[-1], periods=len(sales_data), freq=freq)\n    future_X = np.array(range(len(sales_data), len(sales_data) + len(future_dates))).reshape(-1, 1)\n\n    future_sales = linear_regressor.predict(future_X)\n\n    return future_sales.flatten()", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    if sales_data is None:\n        sales_data = pd.date_range(start=start_date, periods=periods, freq=freq)\n        sales_data = pd.Series(np.random.randint(1, 100, len(sales_data)), index=sales_data)\n    else:\n        sales_data = pd.Series(sales_data, index=pd.date_range(start=start_date, periods=len(sales_data), freq=freq))\n\n    X = np.array(range(0, len(sales_data))).reshape(-1, 1)\n    Y = sales_data.values.reshape(-1, 1)\n\n    linear_regressor = LinearRegression()\n    linear_regressor.fit(X, Y)\n\n    future_dates = pd.date_range(start=sales_data.index[-1], periods=len(sales_data), freq=freq)\n    future_X = np.array(range(len(sales_data), len(sales_data) + len(future_dates))).reshape(-1, 1)\n\n    future_sales = linear_regressor.predict(future_X)\n\n    return future_sales.flatten()"}
{"task_id": "BigCodeBench/946", "solution": "import numpy as np\nimport pandas as pd\nimport random\ndef task_func(rows=3, cols=2, min_val=0, max_val=100, seed=0):\n    \"\"\"\n    Creates a matrix of specified dimensions with random integers within a given range, and then converts it into a pandas DataFrame.\n    \"\"\"\n    np.random.seed(seed)\n    matrix = np.random.randint(min_val, max_val, size=(rows, cols))\n    df = pd.DataFrame(matrix)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nimport random\n\ndef task_func(rows=3, cols=2, min_val=0, max_val=100, seed=0):\n    \"\"\"\n    Creates a matrix of specified dimensions with random integers within a given range, and then converts it into a pandas DataFrame.\n    \"\"\"\n    np.random.seed(seed)\n    matrix = np.random.randint(min_val, max_val, size=(rows, cols))\n    df = pd.DataFrame(matrix)\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/947", "solution": "import numpy as np\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(rows=3, columns=2, start_date=datetime(2021, 1, 1), end_date=datetime(2021, 12, 31), seed=0):\n    random.seed(seed)\n    date_range = (end_date - start_date).days\n    dates = [start_date + timedelta(days=random.randint(0, date_range)) for _ in range(rows*columns)]\n    dates = list(set(dates))  # remove duplicates\n    dates.sort()\n    dates = dates[:rows*columns]  # ensure we have the correct number of dates\n    dates = np.array(dates).reshape(rows, columns)\n    return dates", "raw_solution": "import numpy as np\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(rows=3, columns=2, start_date=datetime(2021, 1, 1), end_date=datetime(2021, 12, 31), seed=0):\n    random.seed(seed)\n    date_range = (end_date - start_date).days\n    dates = [start_date + timedelta(days=random.randint(0, date_range)) for _ in range(rows*columns)]\n    dates = list(set(dates))  # remove duplicates\n    dates.sort()\n    dates = dates[:rows*columns]  # ensure we have the correct number of dates\n    dates = np.array(dates).reshape(rows, columns)\n    return dates\n\n# Test the function"}
{"task_id": "BigCodeBench/948", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(rows=3, columns=2, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaled_matrix = scaler.fit_transform(matrix)\n    return scaled_matrix", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(rows=3, columns=2, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaled_matrix = scaler.fit_transform(matrix)\n    return scaled_matrix"}
{"task_id": "BigCodeBench/949", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(rows, columns, seed=None):\n    \"\"\"\n    Generates a DataFrame with random values within a specified range.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    data = np.random.rand(rows, columns)\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(rows, columns, seed=None):\n    \"\"\"\n    Generates a DataFrame with random values within a specified range.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    data = np.random.rand(rows, columns)\n    df = pd.DataFrame(data)\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/950", "solution": "import numpy as np\nfrom scipy.linalg import svd\ndef task_func(rows=3, columns=2, seed=0):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    U, s, Vh = svd(matrix)\n    return U, s, Vh", "raw_solution": "import numpy as np\nfrom scipy.linalg import svd\n\ndef task_func(rows=3, columns=2, seed=0):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    U, s, Vh = svd(matrix)\n    return U, s, Vh\n\n# Test the function\nU, s, Vh = task_func(3, 2, 0)"}
{"task_id": "BigCodeBench/951", "solution": "import pandas as pd\nimport numpy as np\nimport random\nfrom random import randint, seed\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Toys & Games']\ndef task_func(mystrings, n_products, seed=0):\n    random.seed(seed)\n    product_names = [name.replace(' ', '_') for name in mystrings]\n    categories = [random.choice(CATEGORIES) for _ in range(n_products)]\n    prices = np.round(np.random.normal(50, 10, n_products), 2)\n\n    df = pd.DataFrame({\n        'Product Name': product_names,\n        'Category': categories,\n        'Price': prices\n    })\n\n    return df\nproduct_names = ['Laptop', 'T-Shirt', 'Oven', 'Harry Potter', 'Lego Set']", "raw_solution": "import pandas as pd\nimport numpy as np\nimport random\nfrom random import randint, seed\n\n# Constants\nCATEGORIES = ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Toys & Games']\n\ndef task_func(mystrings, n_products, seed=0):\n    random.seed(seed)\n    product_names = [name.replace(' ', '_') for name in mystrings]\n    categories = [random.choice(CATEGORIES) for _ in range(n_products)]\n    prices = np.round(np.random.normal(50, 10, n_products), 2)\n\n    df = pd.DataFrame({\n        'Product Name': product_names,\n        'Category': categories,\n        'Price': prices\n    })\n\n    return df\n\n# Test the function\nproduct_names = ['Laptop', 'T-Shirt', 'Oven', 'Harry Potter', 'Lego Set']\ndf = task_func(product_names, 5, seed=0)"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    Randomly assigns a specified number of tasks to employees with a due date of the current day.\n    \"\"\"\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    tasks = [task.replace(\" \", \"_\") for task in task_list]\n    random.shuffle(tasks)\n\n    assigned_tasks = tasks[:n_tasks]\n    random.shuffle(employees)\n\n    assigned_to = [random.choice(employees) for _ in range(n_tasks)]\n    due_date = datetime.now().date()\n\n    df = pd.DataFrame({\n        \"Task Name\": assigned_tasks,\n        \"Assigned To\": assigned_to,\n        \"Due Date\": due_date,\n    })\n\n    return df", "raw_solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    Randomly assigns a specified number of tasks to employees with a due date of the current day.\n    \"\"\"\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    tasks = [task.replace(\" \", \"_\") for task in task_list]\n    random.shuffle(tasks)\n\n    assigned_tasks = tasks[:n_tasks]\n    random.shuffle(employees)\n\n    assigned_to = [random.choice(employees) for _ in range(n_tasks)]\n    due_date = datetime.now().date()\n\n    df = pd.DataFrame({\n        \"Task Name\": assigned_tasks,\n        \"Assigned To\": assigned_to,\n        \"Due Date\": due_date,\n    })\n\n    return df"}
{"task_id": "BigCodeBench/953", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(mystrings, folder_path, seed=None):\n    \"\"\"\n    Generates random data points to plot bar charts for each in a given list of plot names, then saves them in a specified directory.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Create the directory if it does not exist\n    if not os.path.exists(folder_path):\n        try:\n            os.makedirs(folder_path)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"The provided directory path {folder_path} does not exist and cannot be created.\")\n\n    # List to store the names of the files where the plots are saved\n    file_names = []\n\n    for mystring in mystrings:\n        # Generate 10 random data points in [0, 1)\n        data = np.random.rand(10)\n\n        # Create a bar chart\n        plt.bar(range(1, 11), data)\n        plt.title(mystring)\n\n        # Save the bar chart as a .png file\n        file_name = f\"{mystring}.png\"\n        file_path = os.path.join(folder_path, file_name)\n        plt.savefig(file_path)\n\n        # Add the file name to the list\n        file_names.append(file_name)\n\n        # Close the current figure\n        plt.close()\n\n    return file_names", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(mystrings, folder_path, seed=None):\n    \"\"\"\n    Generates random data points to plot bar charts for each in a given list of plot names, then saves them in a specified directory.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Create the directory if it does not exist\n    if not os.path.exists(folder_path):\n        try:\n            os.makedirs(folder_path)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"The provided directory path {folder_path} does not exist and cannot be created.\")\n\n    # List to store the names of the files where the plots are saved\n    file_names = []\n\n    for mystring in mystrings:\n        # Generate 10 random data points in [0, 1)\n        data = np.random.rand(10)\n\n        # Create a bar chart\n        plt.bar(range(1, 11), data)\n        plt.title(mystring)\n\n        # Save the bar chart as a .png file\n        file_name = f\"{mystring}.png\"\n        file_path = os.path.join(folder_path, file_name)\n        plt.savefig(file_path)\n\n        # Add the file name to the list\n        file_names.append(file_name)\n\n        # Close the current figure\n        plt.close()\n\n    return file_names"}
{"task_id": "BigCodeBench/954", "solution": "import random\nimport re\ndef task_func(target_words, n_sentences, vocabulary):\n    if n_sentences < 0:\n        raise ValueError(\"n_sentences cannot be negative\")\n    if not vocabulary:\n        raise ValueError(\"vocabulary cannot be empty\")\n\n    result = []\n    for _ in range(n_sentences):\n        sentence = \" \".join(random.choices(vocabulary, k=10))\n        for word in target_words:\n            sentence = re.sub(r\"\\b\" + word + r\"\\b\", word.replace(\" \", \"_\"), sentence)\n        result.append(sentence.lower())\n\n    return result", "raw_solution": "import random\nimport re\n\ndef task_func(target_words, n_sentences, vocabulary):\n    if n_sentences < 0:\n        raise ValueError(\"n_sentences cannot be negative\")\n    if not vocabulary:\n        raise ValueError(\"vocabulary cannot be empty\")\n\n    result = []\n    for _ in range(n_sentences):\n        sentence = \" \".join(random.choices(vocabulary, k=10))\n        for word in target_words:\n            sentence = re.sub(r\"\\b\" + word + r\"\\b\", word.replace(\" \", \"_\"), sentence)\n        result.append(sentence.lower())\n\n    return result"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n    text = text.lower()\n    words = re.findall(r'\\b\\w+\\b', text)\n    words = [word.replace(' ', '_') for word in words]\n    counts = Counter(words)\n    mystrings.extend(counts.keys())\n    ax = plt.figure(figsize=(10,5)).gca()\n    ax.bar(counts.keys(), counts.values())\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of each unique word')\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n    text = text.lower()\n    words = re.findall(r'\\b\\w+\\b', text)\n    words = [word.replace(' ', '_') for word in words]\n    counts = Counter(words)\n    mystrings.extend(counts.keys())\n    ax = plt.figure(figsize=(10,5)).gca()\n    ax.bar(counts.keys(), counts.values())\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of each unique word')\n    return ax"}
{"task_id": "BigCodeBench/956", "solution": "import re\nimport string\nimport random\ndef task_func(text: str, seed=None) -> str:\n    \"\"\"\n    Transforms a given string by removing special characters, normalizing whitespace, and randomizing character casing.\n    \"\"\"\n    # Set the seed for the random number generator\n    if seed is not None:\n        random.seed(seed)\n\n    # Remove special characters\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Normalize whitespace\n    text = re.sub(r'\\s+', '_', text)\n    text = re.sub(r'_+', '__', text)\n    text = re.sub(r'__+', '___', text)\n\n    # Randomize casing\n    text = ''.join([char.upper() if random.random() < 0.5 else char.lower() for char in text])\n\n    return text", "raw_solution": "import re\nimport string\nimport random\n\ndef task_func(text: str, seed=None) -> str:\n    \"\"\"\n    Transforms a given string by removing special characters, normalizing whitespace, and randomizing character casing.\n    \"\"\"\n    # Set the seed for the random number generator\n    if seed is not None:\n        random.seed(seed)\n\n    # Remove special characters\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Normalize whitespace\n    text = re.sub(r'\\s+', '_', text)\n    text = re.sub(r'_+', '__', text)\n    text = re.sub(r'__+', '___', text)\n\n    # Randomize casing\n    text = ''.join([char.upper() if random.random() < 0.5 else char.lower() for char in text])\n\n    return text"}
{"task_id": "BigCodeBench/957", "solution": "import string\nimport re\ndef task_func(text: str) -> tuple:\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    words = text.split()\n    characters = [char for char in text if char != ' ']\n    unique_characters = set(characters)\n    return len(words), len(characters), len(unique_characters)\ntext = \"Hello, world! This is a test.\"", "raw_solution": "import string\nimport re\n\ndef task_func(text: str) -> tuple:\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    words = text.split()\n    characters = [char for char in text if char != ' ']\n    unique_characters = set(characters)\n    return len(words), len(characters), len(unique_characters)\n\n# Test the function\ntext = \"Hello, world! This is a test.\""}
{"task_id": "BigCodeBench/958", "solution": "import random\nimport re\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    words = re.findall(r'\\b\\w+\\b', text)\n    for i in range(len(words)):\n        if len(words[i]) > 2:\n            middle = list(words[i][1:-1])\n            random.shuffle(middle)\n            words[i] = words[i][0] + ''.join(middle) + words[i][-1]\n    return ' '.join(words)", "raw_solution": "import random\nimport re\n\ndef task_func(text, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    words = re.findall(r'\\b\\w+\\b', text)\n    for i in range(len(words)):\n        if len(words[i]) > 2:\n            middle = list(words[i][1:-1])\n            random.shuffle(middle)\n            words[i] = words[i][0] + ''.join(middle) + words[i][-1]\n    return ' '.join(words)\n\n# Testing the function"}
{"task_id": "BigCodeBench/959", "solution": "import string\nimport random\ndef task_func(text, seed=None):\n    \"\"\"\n    Transforms the input text by replacing each alphabetic character with a random letter,\n    while preserving the case and non-alphabetic characters of the original text.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    def replace_char(c):\n        if c.isalpha():\n            ascii_start = ord('a') if c.islower() else ord('A')\n            return chr(random.randint(ascii_start, ascii_start + 25))\n        return c\n\n    return ''.join(map(replace_char, text))", "raw_solution": "import string\nimport random\n\ndef task_func(text, seed=None):\n    \"\"\"\n    Transforms the input text by replacing each alphabetic character with a random letter,\n    while preserving the case and non-alphabetic characters of the original text.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    def replace_char(c):\n        if c.isalpha():\n            ascii_start = ord('a') if c.islower() else ord('A')\n            return chr(random.randint(ascii_start, ascii_start + 25))\n        return c\n\n    return ''.join(map(replace_char, text))\n\n# Testing the function"}
{"task_id": "BigCodeBench/960", "solution": "import string\nimport random\ndef task_func(text, seed=None):\n    \"\"\"\n    Generates a password that mirrors the structure of the given text by replacing alphabetic characters with random ascii lowercase letters, digits with random single-digit numbers, spaces wth either a random digit or random lowercase letter at equal probabilities, and leaving other characters unchanged.\n    \"\"\"\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    password = []\n    for char in text:\n        if char.isalpha():\n            password.append(random.choice(string.ascii_lowercase))\n        elif char.isdigit():\n            password.append(str(random.randint(0, 9)))\n        elif char.isspace():\n            password.append(str(random.randint(0, 9)) if random.random() < 0.5 else random.choice(string.ascii_lowercase))\n        else:\n            password.append(char)\n\n    return ''.join(password)", "raw_solution": "import string\nimport random\n\ndef task_func(text, seed=None):\n    \"\"\"\n    Generates a password that mirrors the structure of the given text by replacing alphabetic characters with random ascii lowercase letters, digits with random single-digit numbers, spaces wth either a random digit or random lowercase letter at equal probabilities, and leaving other characters unchanged.\n    \"\"\"\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    password = []\n    for char in text:\n        if char.isalpha():\n            password.append(random.choice(string.ascii_lowercase))\n        elif char.isdigit():\n            password.append(str(random.randint(0, 9)))\n        elif char.isspace():\n            password.append(str(random.randint(0, 9)) if random.random() < 0.5 else random.choice(string.ascii_lowercase))\n        else:\n            password.append(char)\n\n    return ''.join(password)\n\n# Testing the function"}
{"task_id": "BigCodeBench/961", "solution": "import os\nimport glob\nfrom collections import Counter\ndef task_func(directory, extensions=[\".txt\", \".docx\", \".xlsx\", \".csv\"], keep_zero=True):\n    \"\"\"\n    Traverses a given directory recursively to count files by specified extensions.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise OSError(f\"The specified directory {directory} does not exist.\")\n\n    counter = Counter()\n    for extension in extensions:\n        files = glob.glob(f\"{directory}/**/*{extension}\", recursive=True)\n        counter.update(files)\n\n    if not keep_zero:\n        counter = Counter({k: v for k, v in counter.items() if v > 0})\n\n    return counter", "raw_solution": "import os\nimport glob\nfrom collections import Counter\n\ndef task_func(directory, extensions=[\".txt\", \".docx\", \".xlsx\", \".csv\"], keep_zero=True):\n    \"\"\"\n    Traverses a given directory recursively to count files by specified extensions.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise OSError(f\"The specified directory {directory} does not exist.\")\n\n    counter = Counter()\n    for extension in extensions:\n        files = glob.glob(f\"{directory}/**/*{extension}\", recursive=True)\n        counter.update(files)\n\n    if not keep_zero:\n        counter = Counter({k: v for k, v in counter.items() if v > 0})\n\n    return counter"}
{"task_id": "BigCodeBench/962", "solution": "import os\nfrom pathlib import Path\nimport glob\nimport shutil\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Moves files with specific extensions from a source directory to a target directory, handling naming conflicts by renaming duplicates.\n    \"\"\"\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n\n    target_directory = os.path.abspath(target_directory)\n    os.makedirs(target_directory, exist_ok=True)\n\n    extensions = [\".txt\", \".docx\", \".xlsx\", \".csv\"]\n    moved_files_count = 0\n\n    for extension in extensions:\n        source_files = glob.glob(f\"{source_directory}/**/*{extension}\", recursive=True)\n\n        for source_file in source_files:\n            relative_path = os.path.relpath(source_file, start=source_directory)\n            target_file = os.path.join(target_directory, relative_path)\n\n            # Handle naming conflicts\n            if os.path.exists(target_file):\n                base, ext = os.path.splitext(target_file)\n                i = 1\n                while os.path.exists(f\"{base}-{i}{ext}\"):\n                    i += 1\n                target_file = f\"{base}-{i}{ext}\"\n\n            # Create directory if it doesn't exist\n            os.makedirs(os.path.dirname(target_file), exist_ok=True)\n\n            # Move file\n            shutil.move(source_file, target_file)\n            moved_files_count += 1\n\n    return moved_files_count", "raw_solution": "import os\nfrom pathlib import Path\nimport glob\nimport shutil\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Moves files with specific extensions from a source directory to a target directory, handling naming conflicts by renaming duplicates.\n    \"\"\"\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n\n    target_directory = os.path.abspath(target_directory)\n    os.makedirs(target_directory, exist_ok=True)\n\n    extensions = [\".txt\", \".docx\", \".xlsx\", \".csv\"]\n    moved_files_count = 0\n\n    for extension in extensions:\n        source_files = glob.glob(f\"{source_directory}/**/*{extension}\", recursive=True)\n\n        for source_file in source_files:\n            relative_path = os.path.relpath(source_file, start=source_directory)\n            target_file = os.path.join(target_directory, relative_path)\n\n            # Handle naming conflicts\n            if os.path.exists(target_file):\n                base, ext = os.path.splitext(target_file)\n                i = 1\n                while os.path.exists(f\"{base}-{i}{ext}\"):\n                    i += 1\n                target_file = f\"{base}-{i}{ext}\"\n\n            # Create directory if it doesn't exist\n            os.makedirs(os.path.dirname(target_file), exist_ok=True)\n\n            # Move file\n            shutil.move(source_file, target_file)\n            moved_files_count += 1\n\n    return moved_files_count"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory and save it as a zip file saved to a target directory.\n    \"\"\"\n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n\n    # Create target directory if it does not exist\n    if not os.path.isdir(target_directory):\n        os.makedirs(target_directory)\n\n    # Create zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through source directory\n        for folder_name, subfolders, filenames in os.walk(source_directory):\n            for filename in filenames:\n                # Check if file has a valid extension\n                if any(filename.endswith(ext) for ext in valid_extensions):\n                    # Create full file path\n                    file_path = os.path.join(folder_name, filename)\n                    # Add file to zip\n                    zipf.write(file_path, arcname=filename)\n\n    return zip_path", "raw_solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory and save it as a zip file saved to a target directory.\n    \"\"\"\n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Check if source directory exists\n    if not os.path.isdir(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n\n    # Create target directory if it does not exist\n    if not os.path.isdir(target_directory):\n        os.makedirs(target_directory)\n\n    # Create zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through source directory\n        for folder_name, subfolders, filenames in os.walk(source_directory):\n            for filename in filenames:\n                # Check if file has a valid extension\n                if any(filename.endswith(ext) for ext in valid_extensions):\n                    # Create full file path\n                    file_path = os.path.join(folder_name, filename)\n                    # Add file to zip\n                    zipf.write(file_path, arcname=filename)\n\n    return zip_path"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\nimport openpyxl\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files and saves them in a target directory.\n    Each file's text content is captured and stored in a CSV with a single 'Text' column and no row indices.\n    This function will overwrite existing files in the target directory if they have the same names as the converted files.\n    If the source directory does not exist, this function raises a FileNotFoundError.\n    Returns the number of files successfully converted to CSV.\n    \"\"\"\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    count = 0\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".txt\"):\n            with open(os.path.join(source_directory, filename), 'r') as file:\n                data = file.read()\n            df = pd.DataFrame([data], columns=['Text'])\n            df.to_csv(os.path.join(target_directory, f\"{filename.split('.')[0]}.csv\"), index=False)\n            count += 1\n\n        elif filename.endswith(\".docx\"):\n            doc = docx.Document(os.path.join(source_directory, filename))\n            data = ' '.join([p.text for p in doc.paragraphs])\n            df = pd.DataFrame([data], columns=['Text'])\n            df.to_csv(os.path.join(target_directory, f\"{filename.split('.')[0]}.csv\"), index=False)\n            count += 1\n\n        elif filename.endswith(\".xlsx\"):\n            xls = pd.read_excel(os.path.join(source_directory, filename))\n            data = xls.to_csv(index=False)\n            with open(os.path.join(target_directory, f\"{filename.split('.')[0]}.csv\"), 'w') as file:\n                file.write(data)\n            count += 1\n\n        elif filename.endswith(\".csv\"):\n            # Skip CSV files as they are already in CSV format\n            continue\n\n    return count", "raw_solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\nimport openpyxl\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files and saves them in a target directory.\n    Each file's text content is captured and stored in a CSV with a single 'Text' column and no row indices.\n    This function will overwrite existing files in the target directory if they have the same names as the converted files.\n    If the source directory does not exist, this function raises a FileNotFoundError.\n    Returns the number of files successfully converted to CSV.\n    \"\"\"\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory {source_directory} does not exist.\")\n\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    count = 0\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".txt\"):\n            with open(os.path.join(source_directory, filename), 'r') as file:\n                data = file.read()\n            df = pd.DataFrame([data], columns=['Text'])\n            df.to_csv(os.path.join(target_directory, f\"{filename.split('.')[0]}.csv\"), index=False)\n            count += 1\n\n        elif filename.endswith(\".docx\"):\n            doc = docx.Document(os.path.join(source_directory, filename))\n            data = ' '.join([p.text for p in doc.paragraphs])\n            df = pd.DataFrame([data], columns=['Text'])\n            df.to_csv(os.path.join(target_directory, f\"{filename.split('.')[0]}.csv\"), index=False)\n            count += 1\n\n        elif filename.endswith(\".xlsx\"):\n            xls = pd.read_excel(os.path.join(source_directory, filename))\n            data = xls.to_csv(index=False)\n            with open(os.path.join(target_directory, f\"{filename.split('.')[0]}.csv\"), 'w') as file:\n                file.write(data)\n            count += 1\n\n        elif filename.endswith(\".csv\"):\n            # Skip CSV files as they are already in CSV format\n            continue\n\n    return count"}
{"task_id": "BigCodeBench/965", "solution": "import os\nimport re\nimport shutil\ndef task_func(source_directory: str, target_directory: str, pattern: str = r\"\\d{4}\") -> int:\n    \"\"\"\n    Moves files matching a specific regex pattern from a source directory to a target directory.\n    If source_directory does not exist or is not a directory, this function returns 0.\n    If target_directory does not exist, this function will create it.\n    \"\"\"\n    if not os.path.isdir(source_directory):\n        return 0\n\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    count = 0\n    for filename in os.listdir(source_directory):\n        if re.search(pattern, filename):\n            shutil.move(os.path.join(source_directory, filename), target_directory)\n            count += 1\n\n    return count", "raw_solution": "import os\nimport re\nimport shutil\n\ndef task_func(source_directory: str, target_directory: str, pattern: str = r\"\\d{4}\") -> int:\n    \"\"\"\n    Moves files matching a specific regex pattern from a source directory to a target directory.\n    If source_directory does not exist or is not a directory, this function returns 0.\n    If target_directory does not exist, this function will create it.\n    \"\"\"\n    if not os.path.isdir(source_directory):\n        return 0\n\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    count = 0\n    for filename in os.listdir(source_directory):\n        if re.search(pattern, filename):\n            shutil.move(os.path.join(source_directory, filename), target_directory)\n            count += 1\n\n    return count"}
{"task_id": "BigCodeBench/966", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if not df.select_dtypes(include=[pd.np.number]).empty:\n        raise ValueError(\"DataFrame contains non-numeric data\")\n\n    cumulative_sum_df = df.cumsum(axis=0)\n\n    fig, ax = plt.subplots()\n    cumulative_sum_df.plot(kind='bar', ax=ax)\n    ax.set_title('Cumulative Sum per Column')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    plt.legend(loc='upper left')\n\n    return cumulative_sum_df, fig", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if not df.select_dtypes(include=[pd.np.number]).empty:\n        raise ValueError(\"DataFrame contains non-numeric data\")\n\n    cumulative_sum_df = df.cumsum(axis=0)\n\n    fig, ax = plt.subplots()\n    cumulative_sum_df.plot(kind='bar', ax=ax)\n    ax.set_title('Cumulative Sum per Column')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Cumulative Sum')\n    plt.legend(loc='upper left')\n\n    return cumulative_sum_df, fig"}
{"task_id": "BigCodeBench/967", "solution": "import numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    \"\"\"\n    Calculates and plots both a given function and its cumulative integral over a specified range, using a linearly spaced range of x-values.\n    The function should output with:\n        matplotlib.axes.Axes: The Axes object containing the plots of the function and its integral.\n    \"\"\"\n    # Generate x values\n    x = np.linspace(*x_range, num_points)\n\n    # Calculate y values for the function\n    y = [func(i) for i in x]\n\n    # Calculate the cumulative integral\n    integral, error = integrate.cumtrapz(y, x, initial=0)\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot the function\n    ax.plot(x, y, label=f'Function: {func.__name__}')\n\n    # Plot the cumulative integral\n    ax.plot(x[:-1], integral, label=f'Integral: {func.__name__}')\n\n    # Add legend and labels\n    ax.legend()\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    return ax", "raw_solution": "import numpy as np\nfrom scipy import integrate\nimport matplotlib.pyplot as plt\n\ndef task_func(func, x_range=(-2, 2), num_points=1000):\n    \"\"\"\n    Calculates and plots both a given function and its cumulative integral over a specified range, using a linearly spaced range of x-values.\n    The function should output with:\n        matplotlib.axes.Axes: The Axes object containing the plots of the function and its integral.\n    \"\"\"\n    # Generate x values\n    x = np.linspace(*x_range, num_points)\n\n    # Calculate y values for the function\n    y = [func(i) for i in x]\n\n    # Calculate the cumulative integral\n    integral, error = integrate.cumtrapz(y, x, initial=0)\n\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Plot the function\n    ax.plot(x, y, label=f'Function: {func.__name__}')\n\n    # Plot the cumulative integral\n    ax.plot(x[:-1], integral, label=f'Integral: {func.__name__}')\n\n    # Add legend and labels\n    ax.legend()\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n\n    return ax"}
{"task_id": "BigCodeBench/968", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(data):\n    \"\"\"\n    Creates and return a heatmap of the cumulative sum of each column in a dictionary.\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n    if data.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Select numeric columns\n    numeric_cols = data.select_dtypes(include=[np.number]).columns\n    if not numeric_cols.any():\n        raise ValueError(\"No numeric columns are present\")\n\n    # Compute cumulative sum of each column\n    cumulative_sum_data = data[numeric_cols].cumsum()\n\n    # Create heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(cumulative_sum_data, cmap='coolwarm', annot=True, fmt=\".0f\")\n\n    return heatmap", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(data):\n    \"\"\"\n    Creates and return a heatmap of the cumulative sum of each column in a dictionary.\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data should be a pandas DataFrame\")\n    if data.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Select numeric columns\n    numeric_cols = data.select_dtypes(include=[np.number]).columns\n    if not numeric_cols.any():\n        raise ValueError(\"No numeric columns are present\")\n\n    # Compute cumulative sum of each column\n    cumulative_sum_data = data[numeric_cols].cumsum()\n\n    # Create heatmap\n    plt.figure(figsize=(10, 8))\n    heatmap = sns.heatmap(cumulative_sum_data, cmap='coolwarm', annot=True, fmt=\".0f\")\n\n    return heatmap"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nfrom typing import Union\ndef task_func(df: pd.DataFrame) -> Union[pd.DataFrame, Exception]:\n    \"\"\"\n    Computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame cannot be empty\")\n    if df.isna().any().any():\n        raise ValueError(\"DataFrame cannot contain NaN values\")\n    if not df.select_dtypes(include=[np.number]).empty:\n        raise TypeError(\"DataFrame must contain only numeric data types\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    df_cumulative = df_normalized.cumsum()\n\n    return df_cumulative", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nfrom typing import Union\n\ndef task_func(df: pd.DataFrame) -> Union[pd.DataFrame, Exception]:\n    \"\"\"\n    Computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame cannot be empty\")\n    if df.isna().any().any():\n        raise ValueError(\"DataFrame cannot contain NaN values\")\n    if not df.select_dtypes(include=[np.number]).empty:\n        raise TypeError(\"DataFrame must contain only numeric data types\")\n\n    scaler = MinMaxScaler()\n    df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    df_cumulative = df_normalized.cumsum()\n\n    return df_cumulative"}
{"task_id": "BigCodeBench/970", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data: np.ndarray) -> plt.Axes:\n    \"\"\"\n    Plots the cumulative probability distribution of a given NumPy array of numbers.\n    \"\"\"\n    if not isinstance(data, np.ndarray):\n        raise TypeError(\"Input data should be a NumPy array.\")\n    if np.issubdtype(data.dtype, np.number):\n        raise ValueError(\"Input data should not contain negative numbers or NaNs.\")\n    if np.isnan(data).any():\n        raise ValueError(\"Input data should not contain negative numbers or NaNs.\")\n    if (data < 0).any():\n        raise ValueError(\"Input data should not contain negative numbers or NaNs.\")\n\n    sorted_data = np.sort(data)\n    n = len(sorted_data)\n    yvals = np.arange(1, n+1) / n\n\n    plt.plot(sorted_data, yvals, marker='o', linestyle='-')\n    plt.title(\"Cumulative Probability Plot\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Cumulative Probability\")\n    return plt.gca()", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data: np.ndarray) -> plt.Axes:\n    \"\"\"\n    Plots the cumulative probability distribution of a given NumPy array of numbers.\n    \"\"\"\n    if not isinstance(data, np.ndarray):\n        raise TypeError(\"Input data should be a NumPy array.\")\n    if np.issubdtype(data.dtype, np.number):\n        raise ValueError(\"Input data should not contain negative numbers or NaNs.\")\n    if np.isnan(data).any():\n        raise ValueError(\"Input data should not contain negative numbers or NaNs.\")\n    if (data < 0).any():\n        raise ValueError(\"Input data should not contain negative numbers or NaNs.\")\n\n    sorted_data = np.sort(data)\n    n = len(sorted_data)\n    yvals = np.arange(1, n+1) / n\n\n    plt.plot(sorted_data, yvals, marker='o', linestyle='-')\n    plt.title(\"Cumulative Probability Plot\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Cumulative Probability\")\n    return plt.gca()"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n    \"\"\"\n    if not os.path.isdir(directory_path):\n        raise ValueError(\"Provided directory does not exist.\")\n\n    result = []\n    for file_name in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, file_name)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path)\n            creation_time = os.path.getctime(file_path)\n            modification_time = os.path.getmtime(file_path)\n            creation_time_iso = datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat()\n            modification_time_iso = datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat()\n            result.append((file_name, file_size, creation_time_iso, modification_time_iso))\n\n    return result", "raw_solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n    \"\"\"\n    if not os.path.isdir(directory_path):\n        raise ValueError(\"Provided directory does not exist.\")\n\n    result = []\n    for file_name in os.listdir(directory_path):\n        file_path = os.path.join(directory_path, file_name)\n        if os.path.isfile(file_path):\n            file_size = os.path.getsize(file_path)\n            creation_time = os.path.getctime(file_path)\n            modification_time = os.path.getmtime(file_path)\n            creation_time_iso = datetime.fromtimestamp(creation_time, tz=timezone.utc).isoformat()\n            modification_time_iso = datetime.fromtimestamp(modification_time, tz=timezone.utc).isoformat()\n            result.append((file_name, file_size, creation_time_iso, modification_time_iso))\n\n    return result"}
{"task_id": "BigCodeBench/972", "solution": "import pathlib\nimport os\nimport re\ndef task_func(path: str, delimiter: str = os.path.sep) -> list:\n    \"\"\"\n    Validates that a given file path does not contain invalid characters for file paths then splits it into path components using a specified delimiter.\n    \"\"\"\n    # Replace backslashes with forward slashes\n    path = path.replace('\\\\', '/')\n\n    # Define invalid characters\n    invalid_chars = ['<', '>', ':', '\"', '|', '?', '*']\n\n    # Check for invalid characters in the path\n    for char in invalid_chars:\n        if char in path:\n            raise ValueError(f\"Invalid character '{char}' found in path.\")\n\n    # Split the path into components using the specified delimiter\n    components = path.split(delimiter)\n\n    return components", "raw_solution": "import pathlib\nimport os\nimport re\n\ndef task_func(path: str, delimiter: str = os.path.sep) -> list:\n    \"\"\"\n    Validates that a given file path does not contain invalid characters for file paths then splits it into path components using a specified delimiter.\n    \"\"\"\n    # Replace backslashes with forward slashes\n    path = path.replace('\\\\', '/')\n\n    # Define invalid characters\n    invalid_chars = ['<', '>', ':', '\"', '|', '?', '*']\n\n    # Check for invalid characters in the path\n    for char in invalid_chars:\n        if char in path:\n            raise ValueError(f\"Invalid character '{char}' found in path.\")\n\n    # Split the path into components using the specified delimiter\n    components = path.split(delimiter)\n\n    return components"}
{"task_id": "BigCodeBench/973", "solution": "import os\nimport shutil\ndef task_func(path, delimiter=\"/\"):\n    \"\"\"\n    Splits a given file path by a specific delimiter and computes disk usage for each directory component.\n    \"\"\"\n    if not isinstance(path, str):\n        raise ValueError(\"'path' must be a string\")\n    if not path:\n        raise ValueError(\"'path' cannot be empty\")\n    if delimiter not in path:\n        raise ValueError(\"'path' must contain the delimiter\")\n    if not os.path.exists(path):\n        raise FileNotFoundError(\"'path' does not exist in the filesystem\")\n\n    components = path.split(delimiter)\n    result = []\n\n    for component in components:\n        if not component:\n            continue\n        usage = shutil.disk_usage(component)\n        result.append((component, {'total': usage.total, 'used': usage.used, 'free': usage.free}))\n\n    return result", "raw_solution": "import os\nimport shutil\n\ndef task_func(path, delimiter=\"/\"):\n    \"\"\"\n    Splits a given file path by a specific delimiter and computes disk usage for each directory component.\n    \"\"\"\n    if not isinstance(path, str):\n        raise ValueError(\"'path' must be a string\")\n    if not path:\n        raise ValueError(\"'path' cannot be empty\")\n    if delimiter not in path:\n        raise ValueError(\"'path' must contain the delimiter\")\n    if not os.path.exists(path):\n        raise FileNotFoundError(\"'path' does not exist in the filesystem\")\n\n    components = path.split(delimiter)\n    result = []\n\n    for component in components:\n        if not component:\n            continue\n        usage = shutil.disk_usage(component)\n        result.append((component, {'total': usage.total, 'used': usage.used, 'free': usage.free}))\n\n    return result"}
{"task_id": "BigCodeBench/974", "solution": "import shutil\nimport pathlib\nfrom typing import Tuple, List\ndef task_func(source_path: str, destination_path: str) -> Tuple[str, List[str]]:\n    \"\"\"\n    Lists files in the specified source directory without descending into subdirectories and copies them to a destination directory.\n    \"\"\"\n    source_path = pathlib.Path(source_path)\n    destination_path = pathlib.Path(destination_path)\n\n    if not source_path.exists():\n        raise ValueError(f\"Source path {source_path} does not exist\")\n    if not source_path.is_dir():\n        raise ValueError(f\"Source path {source_path} is not a directory\")\n\n    copied_files = []\n    for file in source_path.iterdir():\n        if file.is_file():\n            shutil.copy2(file, destination_path)\n            copied_files.append(file.name)\n\n    return source_path.name, copied_files", "raw_solution": "import shutil\nimport pathlib\nfrom typing import Tuple, List\n\ndef task_func(source_path: str, destination_path: str) -> Tuple[str, List[str]]:\n    \"\"\"\n    Lists files in the specified source directory without descending into subdirectories and copies them to a destination directory.\n    \"\"\"\n    source_path = pathlib.Path(source_path)\n    destination_path = pathlib.Path(destination_path)\n\n    if not source_path.exists():\n        raise ValueError(f\"Source path {source_path} does not exist\")\n    if not source_path.is_dir():\n        raise ValueError(f\"Source path {source_path} is not a directory\")\n\n    copied_files = []\n    for file in source_path.iterdir():\n        if file.is_file():\n            shutil.copy2(file, destination_path)\n            copied_files.append(file.name)\n\n    return source_path.name, copied_files"}
{"task_id": "BigCodeBench/975", "solution": "import numpy as np\nimport pandas as pd\ndef task_func(rows, columns=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=0) -> pd.DataFrame:\n    np.random.seed(seed)\n    data = np.random.rand(rows, len(columns))\n    df = pd.DataFrame(data, columns=columns)\n    df = df.reindex(np.random.permutation(df.columns))\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\n\ndef task_func(rows, columns=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=0) -> pd.DataFrame:\n    np.random.seed(seed)\n    data = np.random.rand(rows, len(columns))\n    df = pd.DataFrame(data, columns=columns)\n    df = df.reindex(np.random.permutation(df.columns))\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/976", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(records: np.ndarray, random_seed: int = 0) -> pd.DataFrame:\n    \"\"\"\n    Randomly shuffle the given array's features, normalize its values, then convert to a DataFrame with shuffled feature names.\n    \"\"\"\n    if records.ndim != 2:\n        raise ValueError(\"records must be a 2D array\")\n\n    np.random.seed(random_seed)\n    np.random.shuffle(records)\n\n    scaler = StandardScaler()\n    records = scaler.fit_transform(records)\n\n    feature_names = [f\"f{i+1}\" for i in range(records.shape[1])]\n    np.random.shuffle(feature_names)\n\n    df = pd.DataFrame(records, columns=feature_names)\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(records: np.ndarray, random_seed: int = 0) -> pd.DataFrame:\n    \"\"\"\n    Randomly shuffle the given array's features, normalize its values, then convert to a DataFrame with shuffled feature names.\n    \"\"\"\n    if records.ndim != 2:\n        raise ValueError(\"records must be a 2D array\")\n\n    np.random.seed(random_seed)\n    np.random.shuffle(records)\n\n    scaler = StandardScaler()\n    records = scaler.fit_transform(records)\n\n    feature_names = [f\"f{i+1}\" for i in range(records.shape[1])]\n    np.random.shuffle(feature_names)\n\n    df = pd.DataFrame(records, columns=feature_names)\n\n    return df"}
{"task_id": "BigCodeBench/977", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(array, features=None, seed=None):\n    \"\"\"\n    Shuffles the columns of a given 2D numpy array and visualizes it as a heatmap.\n    \"\"\"\n    if array.ndim != 2:\n        raise ValueError(\"'array' should be a 2-dimensional numpy array.\")\n    if array.size == 0:\n        raise ValueError(\"'array' should not be empty.\")\n    if features is not None and len(features) != array.shape[1]:\n        raise ValueError(\"'features' should have the same number of elements as the number of columns in 'array'.\")\n\n    if seed is not None:\n        np.random.seed(seed)\n    shuffled_array = np.random.permutation(array.T).T\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    if features is None:\n        features = [str(i+1) for i in range(array.shape[1])]\n    sns.heatmap(shuffled_array, annot=True, fmt=\".2f\", cmap='viridis', xticklabels=features, yticklabels=True, ax=ax)\n    plt.xlabel(\"Features\")\n    plt.ylabel(\"Samples\")\n    plt.title(\"Heatmap of Shuffled Array\")\n    plt.show()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(array, features=None, seed=None):\n    \"\"\"\n    Shuffles the columns of a given 2D numpy array and visualizes it as a heatmap.\n    \"\"\"\n    if array.ndim != 2:\n        raise ValueError(\"'array' should be a 2-dimensional numpy array.\")\n    if array.size == 0:\n        raise ValueError(\"'array' should not be empty.\")\n    if features is not None and len(features) != array.shape[1]:\n        raise ValueError(\"'features' should have the same number of elements as the number of columns in 'array'.\")\n\n    if seed is not None:\n        np.random.seed(seed)\n    shuffled_array = np.random.permutation(array.T).T\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    if features is None:\n        features = [str(i+1) for i in range(array.shape[1])]\n    sns.heatmap(shuffled_array, annot=True, fmt=\".2f\", cmap='viridis', xticklabels=features, yticklabels=True, ax=ax)\n    plt.xlabel(\"Features\")\n    plt.ylabel(\"Samples\")\n    plt.title(\"Heatmap of Shuffled Array\")\n    plt.show()\n\n    return ax"}
{"task_id": "BigCodeBench/978", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\ndef task_func(array, seed=None):\n    \"\"\"\n    Shuffles the columns of a numpy array randomly, performs Principal Component Analysis (PCA) to reduce the dimensionality to 2 principal components, and returns these components as a pandas DataFrame.\n    \"\"\"\n    if array.ndim != 2:\n        raise ValueError(\"Input array must be 2D\")\n\n    if array.size == 0:\n        return pd.DataFrame()\n\n    np.random.seed(seed)\n    np.random.shuffle(array.T)\n\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(array)\n\n    df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\n\ndef task_func(array, seed=None):\n    \"\"\"\n    Shuffles the columns of a numpy array randomly, performs Principal Component Analysis (PCA) to reduce the dimensionality to 2 principal components, and returns these components as a pandas DataFrame.\n    \"\"\"\n    if array.ndim != 2:\n        raise ValueError(\"Input array must be 2D\")\n\n    if array.size == 0:\n        return pd.DataFrame()\n\n    np.random.seed(seed)\n    np.random.shuffle(array.T)\n\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(array)\n\n    df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n    return df"}
{"task_id": "BigCodeBench/979", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\ndef task_func(\n    feature_array,\n    target_array,\n    feature_names=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"],\n    target_name=\"target\",\n    seed=None,\n):\n    \"\"\"\n    Shuffles the columns of a given numpy array and trains a Random Forest Classifier on the shuffled data.\n    \"\"\"\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Create a pandas DataFrame from the feature and target arrays\n    df = pd.DataFrame(data=np.column_stack((feature_array, target_array)),\n                      columns=feature_names + [target_name])\n\n    # Shuffle the columns of the DataFrame\n    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n    # Split the DataFrame into features and target\n    X = df[feature_names]\n    y = df[target_name]\n\n    # Train a Random Forest Classifier on the shuffled data\n    clf = RandomForestClassifier(random_state=seed)\n    clf.fit(X, y)\n\n    return clf", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(\n    feature_array,\n    target_array,\n    feature_names=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"],\n    target_name=\"target\",\n    seed=None,\n):\n    \"\"\"\n    Shuffles the columns of a given numpy array and trains a Random Forest Classifier on the shuffled data.\n    \"\"\"\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Create a pandas DataFrame from the feature and target arrays\n    df = pd.DataFrame(data=np.column_stack((feature_array, target_array)),\n                      columns=feature_names + [target_name])\n\n    # Shuffle the columns of the DataFrame\n    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n    # Split the DataFrame into features and target\n    X = df[feature_names]\n    y = df[target_name]\n\n    # Train a Random Forest Classifier on the shuffled data\n    clf = RandomForestClassifier(random_state=seed)\n    clf.fit(X, y)\n\n    return clf"}
{"task_id": "BigCodeBench/980", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\ndef task_func(df):\n    \"\"\"\n    Plots the correlation matrix from numeric columns in a DataFrame and returns a DataFrame where the numeric columns are standardized to have mean 0 and variance 1.\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if not numeric_cols.any():\n        raise ValueError(\"No numeric columns are present.\")\n\n    # Standardize the numeric columns\n    scaler = StandardScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    # Create the correlation matrix\n    corr_matrix = df[numeric_cols].corr()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix')\n    plt.show()\n\n    return df", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\ndef task_func(df):\n    \"\"\"\n    Plots the correlation matrix from numeric columns in a DataFrame and returns a DataFrame where the numeric columns are standardized to have mean 0 and variance 1.\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if not numeric_cols.any():\n        raise ValueError(\"No numeric columns are present.\")\n\n    # Standardize the numeric columns\n    scaler = StandardScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n    # Create the correlation matrix\n    corr_matrix = df[numeric_cols].corr()\n\n    # Create a heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix')\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/981", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(start_date, end_date, num_series, seed=None):\n    \"\"\"\n    Generates a DataFrame with multiple random integer time series (each ranging from 0 to 100) from a start date to an end date, then returns the generated time series on a line plot.\n    \"\"\"\n    # Validate inputs\n    if not isinstance(start_date, datetime):\n        raise TypeError(\"start_date must be a datetime object\")\n    if not isinstance(end_date, datetime):\n        raise TypeError(\"end_date must be a datetime object\")\n    if not isinstance(num_series, int) or num_series < 1:\n        raise ValueError(\"num_series must be a positive integer\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than end_date\")\n\n    # Set random seed if provided\n    if seed is not None:\n        random.seed(seed)\n\n    # Generate time series\n    date_range = pd.date_range(start=start_date, end=end_date)\n    data = {f'series_{i+1}': [random.randint(0, 100) for _ in range(len(date_range))] for i in range(num_series)}\n    df = pd.DataFrame(data, index=date_range)\n\n    # Plot time series\n    plt.figure(figsize=(10, 6))\n    for col in df.columns:\n        plt.plot(df.index, df[col], label=col)\n    plt.title(\"Random Time Series\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Value\")\n    plt.legend()\n    plt.show()\n\n    return df", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport random\n\ndef task_func(start_date, end_date, num_series, seed=None):\n    \"\"\"\n    Generates a DataFrame with multiple random integer time series (each ranging from 0 to 100) from a start date to an end date, then returns the generated time series on a line plot.\n    \"\"\"\n    # Validate inputs\n    if not isinstance(start_date, datetime):\n        raise TypeError(\"start_date must be a datetime object\")\n    if not isinstance(end_date, datetime):\n        raise TypeError(\"end_date must be a datetime object\")\n    if not isinstance(num_series, int) or num_series < 1:\n        raise ValueError(\"num_series must be a positive integer\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than end_date\")\n\n    # Set random seed if provided\n    if seed is not None:\n        random.seed(seed)\n\n    # Generate time series\n    date_range = pd.date_range(start=start_date, end=end_date)\n    data = {f'series_{i+1}': [random.randint(0, 100) for _ in range(len(date_range))] for i in range(num_series)}\n    df = pd.DataFrame(data, index=date_range)\n\n    # Plot time series\n    plt.figure(figsize=(10, 6))\n    for col in df.columns:\n        plt.plot(df.index, df[col], label=col)\n    plt.title(\"Random Time Series\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Value\")\n    plt.legend()\n    plt.show()\n\n    return df"}
{"task_id": "BigCodeBench/982", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport pandas as pd\ndef task_func(df, column, bins=30, density=True, alpha=0.6, color=\"g\", seed=None):\n    \"\"\"\n    Plots a histogram for a specified column of a pandas DataFrame and overlays it with a fitted normal distribution curve.\n    \"\"\"\n    # Set seed for reproducibility\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Extract the data from the specified column\n    data = df[column].dropna()\n\n    # Create the histogram\n    ax = data.hist(bins=bins, density=density, alpha=alpha, color=color, label='Histogram')\n\n    # Fit a normal distribution to the data\n    mean, std = norm.fit(data)\n\n    # Create the x values for the normal distribution\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n\n    # Create the normal distribution\n    p = norm.pdf(x, mean, std)\n\n    # Overlay the normal distribution on the histogram\n    ax.plot(x, p, 'k', linewidth=2, label='Normal distribution')\n\n    # Add legend\n    ax.legend()\n\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport pandas as pd\n\ndef task_func(df, column, bins=30, density=True, alpha=0.6, color=\"g\", seed=None):\n    \"\"\"\n    Plots a histogram for a specified column of a pandas DataFrame and overlays it with a fitted normal distribution curve.\n    \"\"\"\n    # Set seed for reproducibility\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Extract the data from the specified column\n    data = df[column].dropna()\n\n    # Create the histogram\n    ax = data.hist(bins=bins, density=density, alpha=alpha, color=color, label='Histogram')\n\n    # Fit a normal distribution to the data\n    mean, std = norm.fit(data)\n\n    # Create the x values for the normal distribution\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n\n    # Create the normal distribution\n    p = norm.pdf(x, mean, std)\n\n    # Overlay the normal distribution on the histogram\n    ax.plot(x, p, 'k', linewidth=2, label='Normal distribution')\n\n    # Add legend\n    ax.legend()\n\n    return ax"}
{"task_id": "BigCodeBench/983", "solution": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    if not np.issubdtype(df.dtypes, np.number).all():\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n\n    covariance_df = df.cov()\n    pair_plot = sns.pairplot(df)\n\n    return covariance_df, pair_plot", "raw_solution": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    if not np.issubdtype(df.dtypes, np.number).all():\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n\n    covariance_df = df.cov()\n    pair_plot = sns.pairplot(df)\n\n    return covariance_df, pair_plot"}
{"task_id": "BigCodeBench/984", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df, x_column, y_column):\n    \"\"\"\n    Draws a scatter plot for the specified columns from a pandas DataFrame and fits a linear regression model to the data.\n    After plotting the scatterplot, this function overlays the predicted regression line on top in red on the same Axes.\n    The function returns the Axes object containing the scatter plot and the linear regression line.\n    \"\"\"\n    # Create a scatter plot\n    plt.scatter(df[x_column], df[y_column])\n\n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(df[[x_column]], df[y_column])\n\n    # Plot the regression line\n    plt.plot(df[x_column], model.predict(df[[x_column]]), color='red')\n\n    return plt.gca()\ndf = pd.DataFrame({\n    'x': [1, 2, 3, 4, 5],\n    'y': [2, 3, 5, 7, 11]\n})", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df, x_column, y_column):\n    \"\"\"\n    Draws a scatter plot for the specified columns from a pandas DataFrame and fits a linear regression model to the data.\n    After plotting the scatterplot, this function overlays the predicted regression line on top in red on the same Axes.\n    The function returns the Axes object containing the scatter plot and the linear regression line.\n    \"\"\"\n    # Create a scatter plot\n    plt.scatter(df[x_column], df[y_column])\n\n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(df[[x_column]], df[y_column])\n\n    # Plot the regression line\n    plt.plot(df[x_column], model.predict(df[[x_column]]), color='red')\n\n    return plt.gca()\n\n# Test the function\ndf = pd.DataFrame({\n    'x': [1, 2, 3, 4, 5],\n    'y': [2, 3, 5, 7, 11]\n})\nax = task_func(df, 'x', 'y')\nassert isinstance(ax, plt.Axes)\nplt.show()"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on provided JSON data.\n    \"\"\"\n    # Check if the JSON data is malformed\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n\n    # Check if the JSON data is empty\n    if not data:\n        raise ValueError(\"The JSON data is empty.\")\n\n    # Create a list to store the population data\n    population_data = []\n\n    # Iterate over the data\n    for country, population in data.items():\n        # Check if the country name is a string\n        if not isinstance(country, str):\n            raise ValueError(\"Non-string country names are not allowed.\")\n\n        # Check if the population is a number and non-negative\n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Non-numeric or negative populations are not allowed.\")\n\n        # Round down the population to the nearest integer\n        population = math.floor(population)\n\n        # Append the data to the list\n        population_data.append({\"Country\": country, \"Population\": population})\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(population_data)\n\n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Create the file path\n    file_path = os.path.join(output_dir, file_name)\n\n    # Write the DataFrame to a CSV file\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"The file cannot be written to the specified directory.\")\n\n    return file_path, df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on provided JSON data.\n    \"\"\"\n    # Check if the JSON data is malformed\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n\n    # Check if the JSON data is empty\n    if not data:\n        raise ValueError(\"The JSON data is empty.\")\n\n    # Create a list to store the population data\n    population_data = []\n\n    # Iterate over the data\n    for country, population in data.items():\n        # Check if the country name is a string\n        if not isinstance(country, str):\n            raise ValueError(\"Non-string country names are not allowed.\")\n\n        # Check if the population is a number and non-negative\n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Non-numeric or negative populations are not allowed.\")\n\n        # Round down the population to the nearest integer\n        population = math.floor(population)\n\n        # Append the data to the list\n        population_data.append({\"Country\": country, \"Population\": population})\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(population_data)\n\n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Create the file path\n    file_path = os.path.join(output_dir, file_name)\n\n    # Write the DataFrame to a CSV file\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"The file cannot be written to the specified directory.\")\n\n    return file_path, df"}
{"task_id": "BigCodeBench/986", "solution": "import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom typing import List, Union\ndef task_func(json_data: str, key_path: List[str]) -> Union[plt.Figure, None]:\n    \"\"\"\n    Extracts and visualizes numerical data from a JSON structure based on a specified path of keys.\n    Raises KeyError if a specified key is not found.\n    Raises ValueError if no numeric data is found, or the data string is empty or corrupted.\n    Returns a matplotlib figure showing a boxplot of the data values.\n    \"\"\"\n    data = json.loads(json_data)\n    for key in key_path:\n        try:\n            data = data[key]\n        except KeyError:\n            raise KeyError(f\"Key '{key}' not found in JSON data.\")\n\n    if not isinstance(data, (list, np.ndarray, pd.Series)):\n        raise ValueError(\"No numeric data found.\")\n\n    data = np.array(data)\n    if np.issubdtype(data.dtype, np.number):\n        fig, ax = plt.subplots()\n        sns.boxplot(data=data, ax=ax)\n        ax.set_title(f\"Boxplot of data values at path {key_path}\")\n        return fig\n    else:\n        raise ValueError(\"Numeric data is corrupted or empty.\")", "raw_solution": "import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom typing import List, Union\n\ndef task_func(json_data: str, key_path: List[str]) -> Union[plt.Figure, None]:\n    \"\"\"\n    Extracts and visualizes numerical data from a JSON structure based on a specified path of keys.\n    Raises KeyError if a specified key is not found.\n    Raises ValueError if no numeric data is found, or the data string is empty or corrupted.\n    Returns a matplotlib figure showing a boxplot of the data values.\n    \"\"\"\n    data = json.loads(json_data)\n    for key in key_path:\n        try:\n            data = data[key]\n        except KeyError:\n            raise KeyError(f\"Key '{key}' not found in JSON data.\")\n\n    if not isinstance(data, (list, np.ndarray, pd.Series)):\n        raise ValueError(\"No numeric data found.\")\n\n    data = np.array(data)\n    if np.issubdtype(data.dtype, np.number):\n        fig, ax = plt.subplots()\n        sns.boxplot(data=data, ax=ax)\n        ax.set_title(f\"Boxplot of data values at path {key_path}\")\n        return fig\n    else:\n        raise ValueError(\"Numeric data is corrupted or empty.\")"}
{"task_id": "BigCodeBench/987", "solution": "import json\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(json_data: str, data_key: str):\n    \"\"\"\n    Processes a JSON string to extract numerical data, Min-Max normalize them, and generate a line plot.\n    \"\"\"\n    # Load JSON data\n    data = json.loads(json_data)\n\n    # Extract numerical data\n    try:\n        num_data = [float(data[data_key][i]) for i in range(len(data[data_key]))]\n    except KeyError:\n        raise KeyError(f\"Key path '{data_key}' not found in the given data.\")\n\n    # Convert to pandas Series\n    original_data = pd.Series(num_data, dtype='float64')\n\n    # Check if data is empty\n    if original_data.empty:\n        return original_data, None, None\n\n    # Min-Max normalize data\n    scaler = MinMaxScaler()\n    normalized_data = pd.Series(scaler.fit_transform(original_data.values.reshape(-1, 1)).reshape(-1), dtype='float64')\n\n    # Generate line plot\n    fig, ax = plt.subplots()\n    ax.plot(original_data, label='Original Data')\n    ax.plot(normalized_data, label='Normalized Data')\n    ax.set_title('Comparison of Original and Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return original_data, normalized_data, ax.figure", "raw_solution": "import json\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(json_data: str, data_key: str):\n    \"\"\"\n    Processes a JSON string to extract numerical data, Min-Max normalize them, and generate a line plot.\n    \"\"\"\n    # Load JSON data\n    data = json.loads(json_data)\n\n    # Extract numerical data\n    try:\n        num_data = [float(data[data_key][i]) for i in range(len(data[data_key]))]\n    except KeyError:\n        raise KeyError(f\"Key path '{data_key}' not found in the given data.\")\n\n    # Convert to pandas Series\n    original_data = pd.Series(num_data, dtype='float64')\n\n    # Check if data is empty\n    if original_data.empty:\n        return original_data, None, None\n\n    # Min-Max normalize data\n    scaler = MinMaxScaler()\n    normalized_data = pd.Series(scaler.fit_transform(original_data.values.reshape(-1, 1)).reshape(-1), dtype='float64')\n\n    # Generate line plot\n    fig, ax = plt.subplots()\n    ax.plot(original_data, label='Original Data')\n    ax.plot(normalized_data, label='Normalized Data')\n    ax.set_title('Comparison of Original and Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n\n    return original_data, normalized_data, ax.figure"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n    \"\"\"\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(\"Specified directory does not exist or is not a directory.\")\n\n    result = {}\n    for item in os.listdir(dir_path):\n        item_path = os.path.join(dir_path, item)\n        for predicate in predicates:\n            if re.search(predicate, item):\n                result[item] = True\n                break\n        else:\n            result[item] = False\n\n    return result", "raw_solution": "import os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n    \"\"\"\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(\"Specified directory does not exist or is not a directory.\")\n\n    result = {}\n    for item in os.listdir(dir_path):\n        item_path = os.path.join(dir_path, item)\n        for predicate in predicates:\n            if re.search(predicate, item):\n                result[item] = True\n                break\n        else:\n            result[item] = False\n\n    return result"}
{"task_id": "BigCodeBench/989", "solution": "import random\nimport string\nfrom typing import List, Tuple, Dict\ndef task_func(length: int, predicates: List[str] = None, seed: int = None) -> Tuple[str, Dict[str, int]]:\n    \"\"\"\n    Generates a random string of specified length and evaluates it for specific characteristics.\n    \"\"\"\n    if length < 0:\n        raise ValueError(\"Length must be non-negative\")\n\n    if predicates is None:\n        predicates = []\n\n    if seed is not None:\n        random.seed(seed)\n\n    valid_predicates = ['digits', 'letters', 'punctuation']\n    for predicate in predicates:\n        if predicate not in valid_predicates:\n            raise KeyError(f\"Unrecognized predicate: {predicate}\")\n\n    charset = ''.join([string.digits, string.ascii_letters, string.punctuation])\n    text = ''.join(random.choices(charset, k=length))\n\n    counts = {predicate: sum(c in charset[i] for c in text) for i, predicate in enumerate(valid_predicates)}\n\n    return text, counts", "raw_solution": "import random\nimport string\nfrom typing import List, Tuple, Dict\n\ndef task_func(length: int, predicates: List[str] = None, seed: int = None) -> Tuple[str, Dict[str, int]]:\n    \"\"\"\n    Generates a random string of specified length and evaluates it for specific characteristics.\n    \"\"\"\n    if length < 0:\n        raise ValueError(\"Length must be non-negative\")\n\n    if predicates is None:\n        predicates = []\n\n    if seed is not None:\n        random.seed(seed)\n\n    valid_predicates = ['digits', 'letters', 'punctuation']\n    for predicate in predicates:\n        if predicate not in valid_predicates:\n            raise KeyError(f\"Unrecognized predicate: {predicate}\")\n\n    charset = ''.join([string.digits, string.ascii_letters, string.punctuation])\n    text = ''.join(random.choices(charset, k=length))\n\n    counts = {predicate: sum(c in charset[i] for c in text) for i, predicate in enumerate(valid_predicates)}\n\n    return text, counts"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    # Convert hexadecimal to bytes\n    bytes_obj = binascii.unhexlify(hex_string)\n\n    # Convert bytes to UTF-8 string\n    utf8_str = bytes_obj.decode('utf-8')\n\n    # Initialize dictionary to store results\n    result = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_obj).decode('utf-8'),\n        'utf-8': utf8_str,\n        'utf-16': utf8_str,\n        'utf-32': utf8_str,\n        'ASCII': 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_str),\n        'ROT13': codecs.encode(utf8_str, 'rot_13')\n    }\n\n    # Check if UTF-16, UTF-32, ASCII are possible\n    for encoding in ['utf-16', 'utf-32', 'ASCII']:\n        try:\n            result[encoding] = utf8_str.encode(encoding).decode(encoding)\n        except UnicodeEncodeError:\n            result[encoding] = 'Not representable in ' + encoding.upper()\n\n    return result", "raw_solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    # Convert hexadecimal to bytes\n    bytes_obj = binascii.unhexlify(hex_string)\n\n    # Convert bytes to UTF-8 string\n    utf8_str = bytes_obj.decode('utf-8')\n\n    # Initialize dictionary to store results\n    result = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_obj).decode('utf-8'),\n        'utf-8': utf8_str,\n        'utf-16': utf8_str,\n        'utf-32': utf8_str,\n        'ASCII': 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_str),\n        'ROT13': codecs.encode(utf8_str, 'rot_13')\n    }\n\n    # Check if UTF-16, UTF-32, ASCII are possible\n    for encoding in ['utf-16', 'utf-32', 'ASCII']:\n        try:\n            result[encoding] = utf8_str.encode(encoding).decode(encoding)\n        except UnicodeEncodeError:\n            result[encoding] = 'Not representable in ' + encoding.upper()\n\n    return result\n\n# Test the function"}
{"task_id": "BigCodeBench/991", "solution": "import binascii\nimport string\nimport random\ndef task_func(length):\n    # Generate a random hexadecimal string of a given length\n    hex_string = ''.join(random.choices(string.hexdigits, k=length))\n\n    # Attempt to decode the hexadecimal string in ASCII\n    try:\n        ascii_string = binascii.unhexlify(hex_string).decode('ascii')\n    except binascii.Error:\n        ascii_string = \"Decoding failed. The resulting ASCII string may contain non-printable characters or be shorter than the input length.\"\n\n    return ascii_string", "raw_solution": "import binascii\nimport string\nimport random\n\ndef task_func(length):\n    # Generate a random hexadecimal string of a given length\n    hex_string = ''.join(random.choices(string.hexdigits, k=length))\n\n    # Attempt to decode the hexadecimal string in ASCII\n    try:\n        ascii_string = binascii.unhexlify(hex_string).decode('ascii')\n    except binascii.Error:\n        ascii_string = \"Decoding failed. The resulting ASCII string may contain non-printable characters or be shorter than the input length.\"\n\n    return ascii_string"}
{"task_id": "BigCodeBench/992", "solution": "import sys\nimport sqlite3\nPATH_TO_APPEND = \"path/to/whatever\"\nDATABASE = \"path/to/database.db\"\ndef task_func(path_to_append=PATH_TO_APPEND, database=DATABASE):\n    # Append path to sys.path\n    sys.path.append(path_to_append)\n\n    # Connect to SQLite database\n    conn = sqlite3.connect(database)\n    c = conn.cursor()\n\n    # Create table if it doesn't exist\n    c.execute('''\n        CREATE TABLE IF NOT EXISTS paths (path TEXT UNIQUE)\n    ''')\n\n    # Insert path into database\n    c.execute('''\n        INSERT OR IGNORE INTO paths VALUES (?)\n    ''', (path_to_append,))\n\n    # Commit changes and close connection\n    conn.commit()\n    conn.close()\n\n    # Return the path that was appended to sys.path and inserted into the database\n    return path_to_append", "raw_solution": "import sys\nimport sqlite3\n\n# Constants\nPATH_TO_APPEND = \"path/to/whatever\"\nDATABASE = \"path/to/database.db\"\n\ndef task_func(path_to_append=PATH_TO_APPEND, database=DATABASE):\n    # Append path to sys.path\n    sys.path.append(path_to_append)\n\n    # Connect to SQLite database\n    conn = sqlite3.connect(database)\n    c = conn.cursor()\n\n    # Create table if it doesn't exist\n    c.execute('''\n        CREATE TABLE IF NOT EXISTS paths (path TEXT UNIQUE)\n    ''')\n\n    # Insert path into database\n    c.execute('''\n        INSERT OR IGNORE INTO paths VALUES (?)\n    ''', (path_to_append,))\n\n    # Commit changes and close connection\n    conn.commit()\n    conn.close()\n\n    # Return the path that was appended to sys.path and inserted into the database\n    return path_to_append"}
{"task_id": "BigCodeBench/993", "solution": "import re\nfrom scipy.stats import gaussian_kde\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    \"\"\"\n    This function takes a text input, calculates the lengths of the words, and visualizes the distribution of word lengths using a histogram and a KDE curve (if applicable) on a matplotlib subplot.\n    \"\"\"\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Calculate the lengths of the words\n    lengths = [len(word) for word in words]\n\n    # Create a subplot\n    fig, ax = plt.subplots()\n\n    # Plot the histogram of word lengths\n    ax.hist(lengths, bins='auto', alpha=0.7, rwidth=0.85, color='#003f5c', density=True)\n\n    # Calculate the kernel density estimate of the word lengths\n    kde = gaussian_kde(lengths)\n\n    # Define the x-axis range\n    x_range = range(min(lengths), max(lengths))\n\n    # Evaluate the KDE at the x-axis range\n    kde_values = kde(x_range)\n\n    # Plot the KDE curve\n    ax.plot(x_range, kde_values, color='#ffa600')\n\n    # Set the x-axis label\n    ax.set_xlabel('Word Length')\n\n    # Set the y-axis label\n    ax.set_ylabel('Frequency')\n\n    # Set the title\n    ax.set_title('Distribution of Word Lengths')\n\n    return ax", "raw_solution": "import re\nfrom scipy.stats import gaussian_kde\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    \"\"\"\n    This function takes a text input, calculates the lengths of the words, and visualizes the distribution of word lengths using a histogram and a KDE curve (if applicable) on a matplotlib subplot.\n    \"\"\"\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Calculate the lengths of the words\n    lengths = [len(word) for word in words]\n\n    # Create a subplot\n    fig, ax = plt.subplots()\n\n    # Plot the histogram of word lengths\n    ax.hist(lengths, bins='auto', alpha=0.7, rwidth=0.85, color='#003f5c', density=True)\n\n    # Calculate the kernel density estimate of the word lengths\n    kde = gaussian_kde(lengths)\n\n    # Define the x-axis range\n    x_range = range(min(lengths), max(lengths))\n\n    # Evaluate the KDE at the x-axis range\n    kde_values = kde(x_range)\n\n    # Plot the KDE curve\n    ax.plot(x_range, kde_values, color='#ffa600')\n\n    # Set the x-axis label\n    ax.set_xlabel('Word Length')\n\n    # Set the y-axis label\n    ax.set_ylabel('Frequency')\n\n    # Set the title\n    ax.set_title('Distribution of Word Lengths')\n\n    return ax"}
{"task_id": "BigCodeBench/994", "solution": "import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom requests.exceptions import RequestException\ndef task_func(url: str, csv_file_path: str) -> list:\n    \"\"\"\n    Extracts title, date, and author information from a webpage and writes the data to a CSV file.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except RequestException as e:\n        raise RuntimeError(f\"Error fetching URL: {e}\")\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    data = []\n    for container in soup.find_all('div', class_='container'):\n        title = container.find('h1').text if container.find('h1') else 'No Title'\n        date = container.find('span', class_='date').text if container.find('span', class_='date') else 'No Date'\n        author = container.find('span', class_='author').text if container.find('span', class_='author') else 'No Author'\n        data.append((title, date, author))\n\n    df = pd.DataFrame(data, columns=['Title', 'Date', 'Author'])\n    df.to_csv(csv_file_path, index=False)\n\n    return data", "raw_solution": "import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom requests.exceptions import RequestException\n\ndef task_func(url: str, csv_file_path: str) -> list:\n    \"\"\"\n    Extracts title, date, and author information from a webpage and writes the data to a CSV file.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except RequestException as e:\n        raise RuntimeError(f\"Error fetching URL: {e}\")\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    data = []\n    for container in soup.find_all('div', class_='container'):\n        title = container.find('h1').text if container.find('h1') else 'No Title'\n        date = container.find('span', class_='date').text if container.find('span', class_='date') else 'No Date'\n        author = container.find('span', class_='author').text if container.find('span', class_='author') else 'No Author'\n        data.append((title, date, author))\n\n    df = pd.DataFrame(data, columns=['Title', 'Date', 'Author'])\n    df.to_csv(csv_file_path, index=False)\n\n    return data"}
{"task_id": "BigCodeBench/995", "solution": "import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(file_path: str, plot_path: str) -> (float, float, str):\n    \"\"\"\n    Processes a CSV file at the given path by reading its contents, cleaning the data,\n    performing statistical analysis, and generating a plot, which is saved to the specified path.\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Read CSV file\n    df = pd.read_csv(file_path)\n\n    # Clean data\n    df = df.replace('[^0-9]', '', regex=True)\n    df = df.apply(pd.to_numeric, errors='coerce')\n    df = df.dropna()\n\n    # Perform statistical analysis\n    mean_val = df.mean().values[0]\n    median_val = df.median().values[0]\n\n    # Generate plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df.values)\n    plt.title(\"Data Visualization\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Value\")\n\n    # Save plot\n    plt.savefig(plot_path)\n\n    return mean_val, median_val, plot_path", "raw_solution": "import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(file_path: str, plot_path: str) -> (float, float, str):\n    \"\"\"\n    Processes a CSV file at the given path by reading its contents, cleaning the data,\n    performing statistical analysis, and generating a plot, which is saved to the specified path.\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n\n    # Read CSV file\n    df = pd.read_csv(file_path)\n\n    # Clean data\n    df = df.replace('[^0-9]', '', regex=True)\n    df = df.apply(pd.to_numeric, errors='coerce')\n    df = df.dropna()\n\n    # Perform statistical analysis\n    mean_val = df.mean().values[0]\n    median_val = df.median().values[0]\n\n    # Generate plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df.values)\n    plt.title(\"Data Visualization\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Value\")\n\n    # Save plot\n    plt.savefig(plot_path)\n\n    return mean_val, median_val, plot_path"}
{"task_id": "BigCodeBench/996", "solution": "import requests\nimport json\nfrom bs4 import BeautifulSoup\ndef task_func(url: str, file_name: str = \"Output.txt\") -> str:\n    \"\"\"\n    Scrapes the 'title' from a specified web page, saves it in JSON format to a given file, and appends to the file if it exists.\n    If the web page does not have a title, 'None' is saved as the title value in the JSON data.\n    Data is appended to the specified file in JSON format, with each title on a new line.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Parse the HTML content of the page with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find the title tag\n    title_tag = soup.find('title')\n\n    # If the title tag exists, get the text; otherwise, set it to 'None'\n    title = title_tag.text if title_tag else 'None'\n\n    # Create a dictionary with the title\n    data = {'title': title}\n\n    # Open the file in append mode\n    with open(file_name, 'a') as file:\n        # Write the data to the file in JSON format\n        json.dump(data, file)\n        # Add a newline character for readability\n        file.write('\\n')\n\n    # Return the file path\n    return file_name", "raw_solution": "import requests\nimport json\nfrom bs4 import BeautifulSoup\n\ndef task_func(url: str, file_name: str = \"Output.txt\") -> str:\n    \"\"\"\n    Scrapes the 'title' from a specified web page, saves it in JSON format to a given file, and appends to the file if it exists.\n    If the web page does not have a title, 'None' is saved as the title value in the JSON data.\n    Data is appended to the specified file in JSON format, with each title on a new line.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Parse the HTML content of the page with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find the title tag\n    title_tag = soup.find('title')\n\n    # If the title tag exists, get the text; otherwise, set it to 'None'\n    title = title_tag.text if title_tag else 'None'\n\n    # Create a dictionary with the title\n    data = {'title': title}\n\n    # Open the file in append mode\n    with open(file_name, 'a') as file:\n        # Write the data to the file in JSON format\n        json.dump(data, file)\n        # Add a newline character for readability\n        file.write('\\n')\n\n    # Return the file path\n    return file_name"}
{"task_id": "BigCodeBench/997", "solution": "import urllib.request\nimport os\nimport zipfile\nTARGET_DIR = \"downloaded_files\"\nTARGET_ZIP_FILE = \"downloaded_files.zip\"\ndef task_func(url):\n    \"\"\"\n    Downloads a zip file from the given URL and extracts it to the TARGET_DIR.\n    If the TARGET_DIR does not exist, it is created.\n    The downloaded zip file is deleted after extraction.\n    \"\"\"\n    # Create target directory if it does not exist\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n\n    # Download zip file\n    urllib.request.urlretrieve(url, TARGET_ZIP_FILE)\n\n    # Extract zip file\n    with zipfile.ZipFile(TARGET_ZIP_FILE, 'r') as zip_ref:\n        zip_ref.extractall(TARGET_DIR)\n\n    # Delete downloaded zip file\n    os.remove(TARGET_ZIP_FILE)\n\n    return TARGET_DIR", "raw_solution": "import urllib.request\nimport os\nimport zipfile\n\n# Constants\nTARGET_DIR = \"downloaded_files\"\nTARGET_ZIP_FILE = \"downloaded_files.zip\"\n\ndef task_func(url):\n    \"\"\"\n    Downloads a zip file from the given URL and extracts it to the TARGET_DIR.\n    If the TARGET_DIR does not exist, it is created.\n    The downloaded zip file is deleted after extraction.\n    \"\"\"\n    # Create target directory if it does not exist\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n\n    # Download zip file\n    urllib.request.urlretrieve(url, TARGET_ZIP_FILE)\n\n    # Extract zip file\n    with zipfile.ZipFile(TARGET_ZIP_FILE, 'r') as zip_ref:\n        zip_ref.extractall(TARGET_DIR)\n\n    # Delete downloaded zip file\n    os.remove(TARGET_ZIP_FILE)\n\n    return TARGET_DIR"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Failed to download file: {e}\")\n        return False\n\n    # Calculate the MD5 checksum of the downloaded file\n    md5_hash = hashlib.md5()\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(byte_block)\n    calculated_md5_checksum = md5_hash.hexdigest()\n\n    # Check if the calculated checksum matches the expected checksum\n    if calculated_md5_checksum != EXPECTED_MD5_CHECKSUM:\n        print(\"Checksum does not match. Deleting the file.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the contents of the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Failed to extract file: {e}\")\n        return False\n\n    return True", "raw_solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Failed to download file: {e}\")\n        return False\n\n    # Calculate the MD5 checksum of the downloaded file\n    md5_hash = hashlib.md5()\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(byte_block)\n    calculated_md5_checksum = md5_hash.hexdigest()\n\n    # Check if the calculated checksum matches the expected checksum\n    if calculated_md5_checksum != EXPECTED_MD5_CHECKSUM:\n        print(\"Checksum does not match. Deleting the file.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract the contents of the tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Failed to extract file: {e}\")\n        return False\n\n    return True"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\nfrom typing import Dict\ndef task_func(url: str, column_name: str, csv_file_path: str) -> Dict[str, int]:\n    \"\"\"\n    Downloads a CSV file from a given URL, saves it to a specified path, and counts the occurrences of each value in a particular column.\n    \"\"\"\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except urllib.error.URLError:\n        print(f\"Failed to download file from URL: {url}\")\n        return\n\n    if not os.path.exists(csv_file_path):\n        print(f\"File does not exist at path: {csv_file_path}\")\n        return\n\n    try:\n        with open(csv_file_path, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            column_values = [row[column_name] for row in csv_reader if column_name in row]\n    except KeyError:\n        print(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n        os.remove(csv_file_path)\n        raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n\n    count_dict = collections.Counter(column_values)\n    os.remove(csv_file_path)\n    return count_dict", "raw_solution": "import urllib.request\nimport os\nimport csv\nimport collections\nfrom typing import Dict\n\ndef task_func(url: str, column_name: str, csv_file_path: str) -> Dict[str, int]:\n    \"\"\"\n    Downloads a CSV file from a given URL, saves it to a specified path, and counts the occurrences of each value in a particular column.\n    \"\"\"\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except urllib.error.URLError:\n        print(f\"Failed to download file from URL: {url}\")\n        return\n\n    if not os.path.exists(csv_file_path):\n        print(f\"File does not exist at path: {csv_file_path}\")\n        return\n\n    try:\n        with open(csv_file_path, 'r') as csv_file:\n            csv_reader = csv.DictReader(csv_file)\n            column_values = [row[column_name] for row in csv_reader if column_name in row]\n    except KeyError:\n        print(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n        os.remove(csv_file_path)\n        raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n\n    count_dict = collections.Counter(column_values)\n    os.remove(csv_file_path)\n    return count_dict"}
{"task_id": "BigCodeBench/1000", "solution": "import urllib.request\nimport os\nimport json\nimport pandas as pd\nTARGET_JSON_FILE = \"downloaded_file.json\"\ndef task_func(url):\n    # Retrieve JSON file from URL\n    urllib.request.urlretrieve(url, TARGET_JSON_FILE)\n\n    # Read JSON file\n    with open(TARGET_JSON_FILE, 'r') as f:\n        data = json.load(f)\n\n    # Convert JSON data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Delete temporary JSON file\n    os.remove(TARGET_JSON_FILE)\n\n    return df", "raw_solution": "import urllib.request\nimport os\nimport json\nimport pandas as pd\n\n# Constants\nTARGET_JSON_FILE = \"downloaded_file.json\"\n\ndef task_func(url):\n    # Retrieve JSON file from URL\n    urllib.request.urlretrieve(url, TARGET_JSON_FILE)\n\n    # Read JSON file\n    with open(TARGET_JSON_FILE, 'r') as f:\n        data = json.load(f)\n\n    # Convert JSON data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Delete temporary JSON file\n    os.remove(TARGET_JSON_FILE)\n\n    return df"}
{"task_id": "BigCodeBench/1001", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(csv_file_path: str) -> 'matplotlib.axes.Axes':\n    \"\"\"\n    This function reads data from a CSV file, normalizes a specific column named 'column1', and then plots the normalized data.\n    \"\"\"\n    # Read data from CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Normalize 'column1'\n    df['column1'] = (df['column1'] - df['column1'].min()) / (df['column1'].max() - df['column1'].min())\n\n    # Create plot\n    fig, ax = plt.subplots()\n    ax.hist(df['column1'], bins=30, color='blue', edgecolor='black')\n\n    # Set title\n    title = 'Plot Title    : Normalized Column 1'\n    ax.set_title(title)\n\n    # Set x-label\n    x_label = 'Index         : Normalized Value'\n    ax.set_xlabel(x_label)\n\n    # Set y-label\n    y_label = 'Frequency     : Normalized Value'\n    ax.set_ylabel(y_label)\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file_path: str) -> 'matplotlib.axes.Axes':\n    \"\"\"\n    This function reads data from a CSV file, normalizes a specific column named 'column1', and then plots the normalized data.\n    \"\"\"\n    # Read data from CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Normalize 'column1'\n    df['column1'] = (df['column1'] - df['column1'].min()) / (df['column1'].max() - df['column1'].min())\n\n    # Create plot\n    fig, ax = plt.subplots()\n    ax.hist(df['column1'], bins=30, color='blue', edgecolor='black')\n\n    # Set title\n    title = 'Plot Title    : Normalized Column 1'\n    ax.set_title(title)\n\n    # Set x-label\n    x_label = 'Index         : Normalized Value'\n    ax.set_xlabel(x_label)\n\n    # Set y-label\n    y_label = 'Frequency     : Normalized Value'\n    ax.set_ylabel(y_label)\n\n    return ax"}
{"task_id": "BigCodeBench/1002", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data, column_name=\"target_column\"):\n    \"\"\"\n    Converts a given JSON data into a Pandas DataFrame and plots a histogram of a specified column.\n    \"\"\"\n    # Convert JSON data to DataFrame\n    df = pd.read_json(data)\n\n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column '{column_name}' does not exist in the DataFrame.\")\n\n    # Handle non-numeric columns by converting them to categorical type and then to numeric codes\n    df[column_name] = pd.Categorical(df[column_name]).codes\n\n    # Plot histogram\n    plt.figure(figsize=(10, 6))\n    plt.hist(df[column_name], bins='auto', color='#007BFF', ec='black')\n    plt.title(f'Histogram of {column_name}')\n    plt.xlabel(column_name)\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return df, plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data, column_name=\"target_column\"):\n    \"\"\"\n    Converts a given JSON data into a Pandas DataFrame and plots a histogram of a specified column.\n    \"\"\"\n    # Convert JSON data to DataFrame\n    df = pd.read_json(data)\n\n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column '{column_name}' does not exist in the DataFrame.\")\n\n    # Handle non-numeric columns by converting them to categorical type and then to numeric codes\n    df[column_name] = pd.Categorical(df[column_name]).codes\n\n    # Plot histogram\n    plt.figure(figsize=(10, 6))\n    plt.hist(df[column_name], bins='auto', color='#007BFF', ec='black')\n    plt.title(f'Histogram of {column_name}')\n    plt.xlabel(column_name)\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    return df, plt.gca()"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\nimport io\ndef task_func(url):\n    try:\n        response = urllib.request.urlopen(url)\n        content = response.read()\n        parser = etree.XMLParser(recover=True)\n        tree = etree.fromstring(content, parser=parser)\n    except (urllib.error.URLError, etree.XMLSyntaxError) as e:\n        raise ValueError(f\"Error fetching or parsing XML: {str(e)}\")\n\n    items = tree.xpath('//item')\n\n    if not items:\n        raise ValueError(\"XML structure does not match expected format\")\n\n    data = []\n    for item in items:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        data.append(item_data)\n\n    df = pd.DataFrame(data)\n    return df", "raw_solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\nimport io\n\ndef task_func(url):\n    try:\n        response = urllib.request.urlopen(url)\n        content = response.read()\n        parser = etree.XMLParser(recover=True)\n        tree = etree.fromstring(content, parser=parser)\n    except (urllib.error.URLError, etree.XMLSyntaxError) as e:\n        raise ValueError(f\"Error fetching or parsing XML: {str(e)}\")\n\n    items = tree.xpath('//item')\n\n    if not items:\n        raise ValueError(\"XML structure does not match expected format\")\n\n    data = []\n    for item in items:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        data.append(item_data)\n\n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    \"\"\"\n    Downloads a text file from a specified URL, processes the text to count the frequency of each word,\n    and then plots a bar chart showing the ten most frequently occurring words.\n    \"\"\"\n    # Download the file\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n\n    # Process the text to count the frequency of each word\n    words = re.findall(r'\\b\\w+\\b', text)\n    counter = Counter(words)\n\n    # Plot a bar chart showing the ten most frequently occurring words\n    most_common_words = counter.most_common(10)\n    words, frequencies = zip(*most_common_words)\n    ax = plt.bar(words, frequencies)\n\n    return counter, ax", "raw_solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    \"\"\"\n    Downloads a text file from a specified URL, processes the text to count the frequency of each word,\n    and then plots a bar chart showing the ten most frequently occurring words.\n    \"\"\"\n    # Download the file\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n\n    # Process the text to count the frequency of each word\n    words = re.findall(r'\\b\\w+\\b', text)\n    counter = Counter(words)\n\n    # Plot a bar chart showing the ten most frequently occurring words\n    most_common_words = counter.most_common(10)\n    words, frequencies = zip(*most_common_words)\n    ax = plt.bar(words, frequencies)\n\n    return counter, ax"}
{"task_id": "BigCodeBench/1005", "solution": "import urllib.request\nimport zipfile\nimport os\nimport urllib.error\ndef task_func(\n    url: str,\n    save_path: str = \"downloaded_file.zip\",\n    extract_path: str = \"extracted_files\",\n) -> str:\n    try:\n        urllib.request.urlretrieve(url, save_path)\n    except urllib.error.URLError as e:\n        return f\"URL Error: {str(e.reason)}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n    try:\n        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_path)\n    except zipfile.BadZipFile as e:\n        os.remove(save_path)\n        return f\"BadZipFile Error: {str(e)}\"\n    except Exception as e:\n        os.remove(save_path)\n        return f\"Error: {str(e)}\"\n\n    os.remove(save_path)\n    return extract_path", "raw_solution": "import urllib.request\nimport zipfile\nimport os\nimport urllib.error\n\ndef task_func(\n    url: str,\n    save_path: str = \"downloaded_file.zip\",\n    extract_path: str = \"extracted_files\",\n) -> str:\n    try:\n        urllib.request.urlretrieve(url, save_path)\n    except urllib.error.URLError as e:\n        return f\"URL Error: {str(e.reason)}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n    try:\n        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_path)\n    except zipfile.BadZipFile as e:\n        os.remove(save_path)\n        return f\"BadZipFile Error: {str(e)}\"\n    except Exception as e:\n        os.remove(save_path)\n        return f\"Error: {str(e)}\"\n\n    os.remove(save_path)\n    return extract_path"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n\n        if response.headers.get('content-type') != 'application/zip':\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        file_name = url.split(\"/\")[-1]\n        file_path = os.path.join(download_path, file_name)\n\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n        try:\n            with ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.testzip()\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        return file_path\n\n    except requests.exceptions.RequestException as e:\n        return \"Error: Unable to download the file from the provided URL.\"\n\n    except Exception as e:\n        return f\"Error: {str(e)}\"", "raw_solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n\n        if response.headers.get('content-type') != 'application/zip':\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        file_name = url.split(\"/\")[-1]\n        file_path = os.path.join(download_path, file_name)\n\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n        try:\n            with ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.testzip()\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        return file_path\n\n    except requests.exceptions.RequestException as e:\n        return \"Error: Unable to download the file from the provided URL.\"\n\n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1007", "solution": "import requests\nimport pandas as pd\nfrom requests.exceptions import RequestException, HTTPError\nfrom typing import Union\ndef task_func(url: str) -> Union[pd.DataFrame, None]:\n    \"\"\"\n    Fetches JSON data from a specified URL and converts it into a Pandas DataFrame.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        # If the response was successful, no Exception will be raised\n        response.raise_for_status()\n    except HTTPError as http_err:\n        print(f'HTTP error occurred: {http_err}')\n        return None\n    except Exception as err:\n        print(f'Other error occurred: {err}')  # Python 3.6\n        return None\n    else:\n        try:\n            json_data = response.json()\n            if not isinstance(json_data, list):\n                raise ValueError(\"Fetched data is not a list.\")\n            for item in json_data:\n                if not isinstance(item, dict):\n                    raise ValueError(\"List items are not dictionaries.\")\n            df = pd.DataFrame(json_data)\n            return df\n        except ValueError as val_err:\n            print(f'Invalid JSON format: {val_err}')\n            return None\n        except Exception as err:\n            print(f'Other error occurred: {err}')\n            return None", "raw_solution": "import requests\nimport pandas as pd\nfrom requests.exceptions import RequestException, HTTPError\nfrom typing import Union\n\ndef task_func(url: str) -> Union[pd.DataFrame, None]:\n    \"\"\"\n    Fetches JSON data from a specified URL and converts it into a Pandas DataFrame.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        # If the response was successful, no Exception will be raised\n        response.raise_for_status()\n    except HTTPError as http_err:\n        print(f'HTTP error occurred: {http_err}')\n        return None\n    except Exception as err:\n        print(f'Other error occurred: {err}')  # Python 3.6\n        return None\n    else:\n        try:\n            json_data = response.json()\n            if not isinstance(json_data, list):\n                raise ValueError(\"Fetched data is not a list.\")\n            for item in json_data:\n                if not isinstance(item, dict):\n                    raise ValueError(\"List items are not dictionaries.\")\n            df = pd.DataFrame(json_data)\n            return df\n        except ValueError as val_err:\n            print(f'Invalid JSON format: {val_err}')\n            return None\n        except Exception as err:\n            print(f'Other error occurred: {err}')\n            return None"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    \"\"\"\n    Extracts and converts data from a specified HTML table based on the given 'table_id' on a webpage into a Pandas DataFrame.\n    If the table is present but contains no data rows (i.e., no <tr> tags), the function returns an empty DataFrame.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        print (\"HTTP Error:\",errh)\n        return\n    except requests.exceptions.ConnectionError as errc:\n        print (\"Error Connecting:\",errc)\n        return\n    except requests.exceptions.Timeout as errt:\n        print (\"Timeout Error:\",errt)\n        return\n    except requests.exceptions.RequestException as err:\n        print (\"Something went wrong\",err)\n        return\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if table is None:\n        raise ValueError('Table with the specified ID not found.')\n\n    csv = pd.read_html(str(table))[0].to_csv(index=False)\n    df = pd.read_csv(StringIO(csv))\n\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(url, table_id):\n    \"\"\"\n    Extracts and converts data from a specified HTML table based on the given 'table_id' on a webpage into a Pandas DataFrame.\n    If the table is present but contains no data rows (i.e., no <tr> tags), the function returns an empty DataFrame.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as errh:\n        print (\"HTTP Error:\",errh)\n        return\n    except requests.exceptions.ConnectionError as errc:\n        print (\"Error Connecting:\",errc)\n        return\n    except requests.exceptions.Timeout as errt:\n        print (\"Timeout Error:\",errt)\n        return\n    except requests.exceptions.RequestException as err:\n        print (\"Something went wrong\",err)\n        return\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if table is None:\n        raise ValueError('Table with the specified ID not found.')\n\n    csv = pd.read_html(str(table))[0].to_csv(index=False)\n    df = pd.read_csv(StringIO(csv))\n\n    return df"}
{"task_id": "BigCodeBench/1009", "solution": "import xml.etree.ElementTree as ET\nimport csv\ndef task_func(xml_content, output_csv_path):\n    try:\n        root = ET.fromstring(xml_content)\n    except ET.ParseError as e:\n        raise Exception(f\"XML parsing error: {str(e)}\")\n\n    try:\n        with open(output_csv_path, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            for elem in root.iter():\n                writer.writerow([elem.tag, elem.text])\n    except IOError as e:\n        raise Exception(f\"IO error: {str(e)}\")", "raw_solution": "import xml.etree.ElementTree as ET\nimport csv\n\ndef task_func(xml_content, output_csv_path):\n    try:\n        root = ET.fromstring(xml_content)\n    except ET.ParseError as e:\n        raise Exception(f\"XML parsing error: {str(e)}\")\n\n    try:\n        with open(output_csv_path, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            for elem in root.iter():\n                writer.writerow([elem.tag, elem.text])\n    except IOError as e:\n        raise Exception(f\"IO error: {str(e)}\")"}
{"task_id": "BigCodeBench/1010", "solution": "import requests\nfrom PIL import Image\nimport io\ndef task_func(url):\n    \"\"\"\n    Fetches an image from a given URL and returns it as a PIL Image object.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Invalid URL or cannot be reached within the timeout period: {e}\")\n\n    if not 200 <= response.status_code <= 299:\n        raise ValueError(f\"Unsuccessful HTTP status code: {response.status_code}\")\n\n    try:\n        image = Image.open(io.BytesIO(response.content))\n    except IOError:\n        raise ValueError(\"Invalid image format\")\n\n    return image", "raw_solution": "import requests\nfrom PIL import Image\nimport io\n\ndef task_func(url):\n    \"\"\"\n    Fetches an image from a given URL and returns it as a PIL Image object.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Invalid URL or cannot be reached within the timeout period: {e}\")\n\n    if not 200 <= response.status_code <= 299:\n        raise ValueError(f\"Unsuccessful HTTP status code: {response.status_code}\")\n\n    try:\n        image = Image.open(io.BytesIO(response.content))\n    except IOError:\n        raise ValueError(\"Invalid image format\")\n\n    return image"}
{"task_id": "BigCodeBench/1011", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    # Read data from CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Ensure that the columns exist in the DataFrame\n    assert col1_name in df.columns, f\"Column {col1_name} not found in the DataFrame.\"\n    assert col2_name in df.columns, f\"Column {col2_name} not found in the DataFrame.\"\n\n    # Group by col1_name and calculate mean of col2_name\n    grouped_df = df.groupby(col1_name)[col2_name].mean().reset_index()\n\n    # Create bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(grouped_df[col1_name], grouped_df[col2_name], color='blue')\n\n    # Set title and axis labels\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(col1_name)\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file_path, col1_name=\"column1\", col2_name=\"column2\"):\n    # Read data from CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Ensure that the columns exist in the DataFrame\n    assert col1_name in df.columns, f\"Column {col1_name} not found in the DataFrame.\"\n    assert col2_name in df.columns, f\"Column {col2_name} not found in the DataFrame.\"\n\n    # Group by col1_name and calculate mean of col2_name\n    grouped_df = df.groupby(col1_name)[col2_name].mean().reset_index()\n\n    # Create bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(grouped_df[col1_name], grouped_df[col2_name], color='blue')\n\n    # Set title and axis labels\n    ax.set_title(f\"Mean of {col2_name} Grouped by {col1_name}\")\n    ax.set_xlabel(col1_name)\n    ax.set_ylabel(f\"Mean of {col2_name}\")\n\n    return ax"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nimport os\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    \"\"\"\n    Downloads and extracts a zip file from a specified URL.\n    \"\"\"\n    # Create download directory if it doesn't exist\n    if not DOWNLOAD_DIR.exists():\n        DOWNLOAD_DIR.mkdir()\n\n    # Create unzipped files directory if it doesn't exist\n    if not ZIP_DIR.exists():\n        ZIP_DIR.mkdir()\n\n    # Download file\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception occurred. Details: {}\".format(str(e)), [])\n\n    # Save file\n    download_path = DOWNLOAD_DIR / filename\n    try:\n        with open(download_path, 'wb') as f:\n            f.write(response.content)\n    except Exception as e:\n        return (\"Error: File-related exception occurred. Details: {}\".format(str(e)), [])\n\n    # Extract files\n    try:\n        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n    except Exception as e:\n        return (\"Error: File-related exception occurred. Details: {}\".format(str(e)), [])\n\n    # Get list of filenames in unzipped directory\n    try:\n        filenames = os.listdir(ZIP_DIR)\n    except Exception as e:\n        return (\"Error: File-related exception occurred. Details: {}\".format(str(e)), [])\n\n    return (\"Success\", filenames)", "raw_solution": "import requests\nfrom pathlib import Path\nimport zipfile\nimport os\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    \"\"\"\n    Downloads and extracts a zip file from a specified URL.\n    \"\"\"\n    # Create download directory if it doesn't exist\n    if not DOWNLOAD_DIR.exists():\n        DOWNLOAD_DIR.mkdir()\n\n    # Create unzipped files directory if it doesn't exist\n    if not ZIP_DIR.exists():\n        ZIP_DIR.mkdir()\n\n    # Download file\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception occurred. Details: {}\".format(str(e)), [])\n\n    # Save file\n    download_path = DOWNLOAD_DIR / filename\n    try:\n        with open(download_path, 'wb') as f:\n            f.write(response.content)\n    except Exception as e:\n        return (\"Error: File-related exception occurred. Details: {}\".format(str(e)), [])\n\n    # Extract files\n    try:\n        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n    except Exception as e:\n        return (\"Error: File-related exception occurred. Details: {}\".format(str(e)), [])\n\n    # Get list of filenames in unzipped directory\n    try:\n        filenames = os.listdir(ZIP_DIR)\n    except Exception as e:\n        return (\"Error: File-related exception occurred. Details: {}\".format(str(e)), [])\n\n    return (\"Success\", filenames)"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Parse the content of the response with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all the <a> tags in the HTML\n    links = soup.find_all('a')\n\n    # Initialize a set to store unique absolute links\n    unique_links = set()\n\n    # Loop over the links\n    for link in links:\n        # Get the href attribute of the link\n        href = link.get('href')\n\n        # If the href attribute exists and is not None, join it with the base URL\n        if href and href != '#':\n            absolute_url = urljoin(base_url, href)\n\n            # Add the absolute URL to the set of unique links\n            unique_links.add(absolute_url)\n\n    # Open the CSV file in write mode\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write each unique link to a new row in the CSV file\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique links\n    return len(unique_links)", "raw_solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Parse the content of the response with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all the <a> tags in the HTML\n    links = soup.find_all('a')\n\n    # Initialize a set to store unique absolute links\n    unique_links = set()\n\n    # Loop over the links\n    for link in links:\n        # Get the href attribute of the link\n        href = link.get('href')\n\n        # If the href attribute exists and is not None, join it with the base URL\n        if href and href != '#':\n            absolute_url = urljoin(base_url, href)\n\n            # Add the absolute URL to the set of unique links\n            unique_links.add(absolute_url)\n\n    # Open the CSV file in write mode\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write each unique link to a new row in the CSV file\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique links\n    return len(unique_links)"}
{"task_id": "BigCodeBench/1014", "solution": "import requests\nimport pandas as pd\nfrom requests.exceptions import HTTPError\nimport matplotlib.pyplot as plt\ndef task_func(api_url):\n    if not isinstance(api_url, str):\n        raise TypeError(\"api_url should be a string\")\n\n    try:\n        response = requests.get(api_url)\n        response.raise_for_status()\n    except HTTPError as e:\n        raise HTTPError(f\"Failed to fetch data from API: {e}\")\n\n    data = response.json()\n\n    if not data:\n        return None, None\n\n    df = pd.DataFrame(data)\n\n    fig, ax = plt.subplots()\n    df.plot(ax=ax)\n\n    return df, ax.figure", "raw_solution": "import requests\nimport pandas as pd\nfrom requests.exceptions import HTTPError\nimport matplotlib.pyplot as plt\n\ndef task_func(api_url):\n    if not isinstance(api_url, str):\n        raise TypeError(\"api_url should be a string\")\n\n    try:\n        response = requests.get(api_url)\n        response.raise_for_status()\n    except HTTPError as e:\n        raise HTTPError(f\"Failed to fetch data from API: {e}\")\n\n    data = response.json()\n\n    if not data:\n        return None, None\n\n    df = pd.DataFrame(data)\n\n    fig, ax = plt.subplots()\n    df.plot(ax=ax)\n\n    return df, ax.figure"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\nfrom typing import Union\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> Union[int, None]:\n    \"\"\"\n    This function parses HTML table data from a specified URL or local file and stores it into an SQLite database.\n    It handles different scenarios for fetching, processing, and storing data.\n    \"\"\"\n    try:\n        # Fetch HTML content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error: {e}\")\n        return\n\n    # Parse HTML content\n    try:\n        parsed_content = html.fromstring(response.content)\n        table_element = parsed_content.xpath('//table')\n        if not table_element:\n            print(\"No table found in the HTML content.\")\n            return 0\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return\n\n    # Convert HTML table to pandas DataFrame\n    try:\n        table_data = pd.read_html(webpage_url)[0]\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return\n\n    # Store DataFrame into SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n        table_data.to_sql(\"my_table\", conn, if_exists='replace', index=False)\n        conn.close()\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return\n\n    return table_data.shape[0]", "raw_solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\nfrom typing import Union\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> Union[int, None]:\n    \"\"\"\n    This function parses HTML table data from a specified URL or local file and stores it into an SQLite database.\n    It handles different scenarios for fetching, processing, and storing data.\n    \"\"\"\n    try:\n        # Fetch HTML content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error: {e}\")\n        return\n\n    # Parse HTML content\n    try:\n        parsed_content = html.fromstring(response.content)\n        table_element = parsed_content.xpath('//table')\n        if not table_element:\n            print(\"No table found in the HTML content.\")\n            return 0\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return\n\n    # Convert HTML table to pandas DataFrame\n    try:\n        table_data = pd.read_html(webpage_url)[0]\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return\n\n    # Store DataFrame into SQLite database\n    try:\n        conn = sqlite3.connect(database_name)\n        table_data.to_sql(\"my_table\", conn, if_exists='replace', index=False)\n        conn.close()\n    except sqlite3.DatabaseError as e:\n        print(f\"Error: {e}\")\n        return\n\n    return table_data.shape[0]"}
{"task_id": "BigCodeBench/1016", "solution": "import requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\ndef task_func(url: str) -> \"matplotlib.axes._axes.Axes\":\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Error downloading image: {str(e)}\")\n\n    try:\n        image = Image.open(BytesIO(response.content))\n    except IOError as e:\n        raise IOError(f\"Error processing image: {str(e)}\")\n\n    image = image.convert('L')\n    image_array = np.array(image)\n\n    plt.hist(image_array.ravel(), bins=256, range=(0,256), fc='k', ec='k')\n    plt.title('Grayscale Histogram')\n    plt.xlabel('Grayscale Value')\n    plt.ylabel('Frequency')\n\n    return plt.gca()", "raw_solution": "import requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\ndef task_func(url: str) -> \"matplotlib.axes._axes.Axes\":\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Error downloading image: {str(e)}\")\n\n    try:\n        image = Image.open(BytesIO(response.content))\n    except IOError as e:\n        raise IOError(f\"Error processing image: {str(e)}\")\n\n    image = image.convert('L')\n    image_array = np.array(image)\n\n    plt.hist(image_array.ravel(), bins=256, range=(0,256), fc='k', ec='k')\n    plt.title('Grayscale Histogram')\n    plt.xlabel('Grayscale Value')\n    plt.ylabel('Frequency')\n\n    return plt.gca()"}
{"task_id": "BigCodeBench/1017", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\ndef task_func(csv_file_path, target_column=\"target\", test_size=0.2, n_estimators=100):\n    \"\"\"\n    Processes a CSV file to train a Random Forest classifier and generates a formatted classification report.\n    \"\"\"\n    # Load the CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except Exception as e:\n        return f\"Error: {e}\"\n\n    # Check if the target column exists\n    if target_column not in df.columns:\n        raise ValueError(f\"The specified target_column '{target_column}' is not found in the CSV file.\")\n\n    # Split the data into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n\n    # Train the Random Forest classifier\n    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n    clf.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = clf.predict(X_test)\n\n    # Generate and return the classification report\n    report = classification_report(y_test, y_pred)\n    return report", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\ndef task_func(csv_file_path, target_column=\"target\", test_size=0.2, n_estimators=100):\n    \"\"\"\n    Processes a CSV file to train a Random Forest classifier and generates a formatted classification report.\n    \"\"\"\n    # Load the CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except Exception as e:\n        return f\"Error: {e}\"\n\n    # Check if the target column exists\n    if target_column not in df.columns:\n        raise ValueError(f\"The specified target_column '{target_column}' is not found in the CSV file.\")\n\n    # Split the data into features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n\n    # Train the Random Forest classifier\n    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n    clf.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = clf.predict(X_test)\n\n    # Generate and return the classification report\n    report = classification_report(y_test, y_pred)\n    return report"}
{"task_id": "BigCodeBench/1018", "solution": "from bs4 import BeautifulSoup\nimport requests\nURL = \"http://example.com\"\ndef task_func(url=URL, from_encoding=\"cp1251\", use_lxml=False):\n    \"\"\"\n    Fetches a web page from a given URL, decodes its content from a specified encoding, and returns the parsed HTML using BeautifulSoup.\n    If specified, 'lxml' is used as the parser for improved performance.\n    In case of any failure (like network issues, invalid URL, or decoding errors), the function returns None.\n    \"\"\"\n    # Return None if URL is empty or None\n    if not url:\n        return None\n\n    try:\n        # Fetch the webpage\n        response = requests.get(url)\n\n        # Check if the request was successful\n        if response.status_code != 200:\n            return None\n\n        # Decode the content using the specified encoding\n        content = response.content.decode(from_encoding)\n\n        # Parse the HTML using BeautifulSoup\n        parser = 'lxml' if use_lxml else 'html.parser'\n        soup = BeautifulSoup(content, parser)\n\n        return soup\n\n    except (requests.exceptions.RequestException, UnicodeDecodeError):\n        # Handle network issues, invalid URL, or decoding errors\n        return None", "raw_solution": "from bs4 import BeautifulSoup\nimport requests\n\n# Constants\nURL = \"http://example.com\"\n\ndef task_func(url=URL, from_encoding=\"cp1251\", use_lxml=False):\n    \"\"\"\n    Fetches a web page from a given URL, decodes its content from a specified encoding, and returns the parsed HTML using BeautifulSoup.\n    If specified, 'lxml' is used as the parser for improved performance.\n    In case of any failure (like network issues, invalid URL, or decoding errors), the function returns None.\n    \"\"\"\n    # Return None if URL is empty or None\n    if not url:\n        return None\n\n    try:\n        # Fetch the webpage\n        response = requests.get(url)\n\n        # Check if the request was successful\n        if response.status_code != 200:\n            return None\n\n        # Decode the content using the specified encoding\n        content = response.content.decode(from_encoding)\n\n        # Parse the HTML using BeautifulSoup\n        parser = 'lxml' if use_lxml else 'html.parser'\n        soup = BeautifulSoup(content, parser)\n\n        return soup\n\n    except (requests.exceptions.RequestException, UnicodeDecodeError):\n        # Handle network issues, invalid URL, or decoding errors\n        return None"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nfrom PIL import Image\nfrom PIL.ExifTags import TAGS\nIMAGE_PATH = \"image.png\"\ndef get_image_comment(image_path):\n    image = Image.open(image_path)\n    exif_data = image._getexif()\n    if exif_data is not None:\n        for tag, value in exif_data.items():\n            tag_name = TAGS.get(tag, tag)\n            if tag_name == 'Artist':\n                return value\n    return None\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        ocr_text = pytesseract.image_to_string(Image.open(filename), lang='eng')\n        decoded_text = codecs.decode(ocr_text, from_encoding)\n        encoded_text = codecs.encode(decoded_text, to_encoding)\n        return encoded_text.decode()\n    except (ValueError, TypeError, LookupError) as e:\n        try:\n            comment = get_image_comment(filename)\n            decoded_comment = codecs.decode(comment, from_encoding)\n            encoded_comment = codecs.encode(decoded_comment, to_encoding)\n            return encoded_comment.decode()\n        except (ValueError, TypeError, LookupError):\n            return ''", "raw_solution": "from PIL import Image\nimport codecs\nimport pytesseract\nfrom PIL import Image\nfrom PIL.ExifTags import TAGS\n\nIMAGE_PATH = \"image.png\"\n\ndef get_image_comment(image_path):\n    image = Image.open(image_path)\n    exif_data = image._getexif()\n    if exif_data is not None:\n        for tag, value in exif_data.items():\n            tag_name = TAGS.get(tag, tag)\n            if tag_name == 'Artist':\n                return value\n    return None\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        ocr_text = pytesseract.image_to_string(Image.open(filename), lang='eng')\n        decoded_text = codecs.decode(ocr_text, from_encoding)\n        encoded_text = codecs.encode(decoded_text, to_encoding)\n        return encoded_text.decode()\n    except (ValueError, TypeError, LookupError) as e:\n        try:\n            comment = get_image_comment(filename)\n            decoded_comment = codecs.decode(comment, from_encoding)\n            encoded_comment = codecs.encode(decoded_comment, to_encoding)\n            return encoded_comment.decode()\n        except (ValueError, TypeError, LookupError):\n            return ''"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        content = response.content\n\n        if not content:\n            return {}\n\n        if not from_encoding:\n            from_encoding = chardet.detect(content)['encoding']\n\n        if not from_encoding:\n            raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        decoded_content = content.decode(from_encoding)\n        reencoded_content = decoded_content.encode(to_encoding)\n\n        return json.loads(reencoded_content)\n\n    except Exception as e:\n        raise e", "raw_solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        content = response.content\n\n        if not content:\n            return {}\n\n        if not from_encoding:\n            from_encoding = chardet.detect(content)['encoding']\n\n        if not from_encoding:\n            raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        decoded_content = content.decode(from_encoding)\n        reencoded_content = decoded_content.encode(to_encoding)\n\n        return json.loads(reencoded_content)\n\n    except Exception as e:\n        raise e"}
{"task_id": "BigCodeBench/1021", "solution": "import hashlib\nimport binascii\ndef task_func(input_string, verify_hash=None):\n    if not isinstance(input_string, str):\n        raise TypeError(\"input_string must be a string\")\n    if verify_hash is not None and not isinstance(verify_hash, str):\n        raise TypeError(\"verify_hash must be a string or None\")\n\n    hash_object = hashlib.sha256(input_string.encode())\n    hex_dig = hash_object.hexdigest()\n\n    if verify_hash is not None:\n        return hex_dig, hex_dig == verify_hash\n    else:\n        return hex_dig, None", "raw_solution": "import hashlib\nimport binascii\n\ndef task_func(input_string, verify_hash=None):\n    if not isinstance(input_string, str):\n        raise TypeError(\"input_string must be a string\")\n    if verify_hash is not None and not isinstance(verify_hash, str):\n        raise TypeError(\"verify_hash must be a string or None\")\n\n    hash_object = hashlib.sha256(input_string.encode())\n    hex_dig = hash_object.hexdigest()\n\n    if verify_hash is not None:\n        return hex_dig, hex_dig == verify_hash\n    else:\n        return hex_dig, None\n\n# Testing the function"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads a CSV file and processes its date-related data.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"File {csv_file_path} not found.\")\n\n    # Read CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n\n    # Check if column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"Column {column_name} not found in CSV file.\")\n\n    # Convert date values to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n\n    # Filter rows based on the current date\n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n\n    # Sort the resulting data\n    df = df.sort_values(by=column_name)\n\n    return df", "raw_solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads a CSV file and processes its date-related data.\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"File {csv_file_path} not found.\")\n\n    # Read CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n\n    # Check if column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"Column {column_name} not found in CSV file.\")\n\n    # Convert date values to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n\n    # Filter rows based on the current date\n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n\n    # Sort the resulting data\n    df = df.sort_values(by=column_name)\n\n    return df"}
{"task_id": "BigCodeBench/1023", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(dataframe):\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input should be a pandas DataFrame\")\n    if dataframe.empty:\n        raise ValueError(\"Input DataFrame is empty\")\n    if dataframe.shape[1] < 2:\n        raise ValueError(\"DataFrame should have at least two columns\")\n    if not all(dataframe.dtypes == 'float64'):\n        raise TypeError(\"All columns in the DataFrame should be numeric\")\n\n    corr_matrix = dataframe.corr().abs()\n    # Find the pair of columns with the highest absolute correlation\n    unstacked_corr = corr_matrix.unstack()\n    sorted_pairs = unstacked_corr.sort_values(ascending=False, kind='quicksort')\n    highest_corr_pair = sorted_pairs[1]\n\n    fig, ax = plt.subplots()\n    ax.scatter(dataframe[highest_corr_pair.name[0]], dataframe[highest_corr_pair.name[1]])\n    ax.set_xlabel(highest_corr_pair.name[0])\n    ax.set_ylabel(highest_corr_pair.name[1])\n    ax.set_title(f\"Scatter plot of {highest_corr_pair.name[0]} and {highest_corr_pair.name[1]}\")\n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(dataframe):\n    if not isinstance(dataframe, pd.DataFrame):\n        raise TypeError(\"Input should be a pandas DataFrame\")\n    if dataframe.empty:\n        raise ValueError(\"Input DataFrame is empty\")\n    if dataframe.shape[1] < 2:\n        raise ValueError(\"DataFrame should have at least two columns\")\n    if not all(dataframe.dtypes == 'float64'):\n        raise TypeError(\"All columns in the DataFrame should be numeric\")\n\n    corr_matrix = dataframe.corr().abs()\n    # Find the pair of columns with the highest absolute correlation\n    unstacked_corr = corr_matrix.unstack()\n    sorted_pairs = unstacked_corr.sort_values(ascending=False, kind='quicksort')\n    highest_corr_pair = sorted_pairs[1]\n\n    fig, ax = plt.subplots()\n    ax.scatter(dataframe[highest_corr_pair.name[0]], dataframe[highest_corr_pair.name[1]])\n    ax.set_xlabel(highest_corr_pair.name[0])\n    ax.set_ylabel(highest_corr_pair.name[1])\n    ax.set_title(f\"Scatter plot of {highest_corr_pair.name[0]} and {highest_corr_pair.name[1]}\")\n    return ax"}
{"task_id": "BigCodeBench/1024", "solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nPLOT_TITLE = \"Value Distribution\"\ndef task_func(data_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(data_dict)\n\n    # Remove None values\n    df = df.dropna()\n\n    # If the DataFrame is empty or all values are the same, return None\n    if df.empty or df.nunique().max() == 1:\n        return df, None\n\n    # Calculate the number of bins dynamically\n    num_bins = max(2, min(11, int(df.shape[0] / 2)))\n\n    # Generate a histogram using seaborn\n    plot = sns.histplot(df, bins=num_bins, kde=False)\n    plot.set_title(PLOT_TITLE)\n\n    return df, plot.figure", "raw_solution": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nPLOT_TITLE = \"Value Distribution\"\n\ndef task_func(data_dict):\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(data_dict)\n\n    # Remove None values\n    df = df.dropna()\n\n    # If the DataFrame is empty or all values are the same, return None\n    if df.empty or df.nunique().max() == 1:\n        return df, None\n\n    # Calculate the number of bins dynamically\n    num_bins = max(2, min(11, int(df.shape[0] / 2)))\n\n    # Generate a histogram using seaborn\n    plot = sns.histplot(df, bins=num_bins, kde=False)\n    plot.set_title(PLOT_TITLE)\n\n    return df, plot.figure"}
{"task_id": "BigCodeBench/1025", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nPLOT_TITLE = \"Scaled Values\"\ndef task_func(data_dict):\n    \"\"\"\n    Scales the values in a given dictionary using MinMaxScaler and plots the scaled data.\n    \"\"\"\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Plot the scaled data\n    ax = df_scaled.plot(kind='line', title=PLOT_TITLE)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    return df_scaled, ax.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Constants\nPLOT_TITLE = \"Scaled Values\"\n\ndef task_func(data_dict):\n    \"\"\"\n    Scales the values in a given dictionary using MinMaxScaler and plots the scaled data.\n    \"\"\"\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the data\n    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Plot the scaled data\n    ax = df_scaled.plot(kind='line', title=PLOT_TITLE)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    return df_scaled, ax.figure"}
{"task_id": "BigCodeBench/1026", "solution": "import numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\ndef task_func(group1, group2, alpha=0.05, min_samples=2, var_threshold=1e-8):\n    \"\"\"\n    Performs a two-sample t-test on numerical data from two groups to determine if there is a significant difference in their means.\n    \"\"\"\n    # Remove NaN values\n    group1 = group1[~np.isnan(group1)]\n    group2 = group2[~np.isnan(group2)]\n\n    # Check for sufficient non-NaN data points and adequate variance in each group\n    if len(group1) < min_samples or len(group2) < min_samples:\n        raise ValueError(\"Each group must have at least two non-NaN values.\")\n    if np.var(group1) < var_threshold or np.var(group2) < var_threshold:\n        raise ValueError(\"The variance in each group must be above a threshold.\")\n\n    # Perform t-test\n    t_stat, p_val = ttest_ind(group1, group2, nan_policy='omit')\n\n    # Generate descriptive statistics\n    group1_stats = {'mean': np.mean(group1), 'std': np.std(group1)}\n    group2_stats = {'mean': np.mean(group2), 'std': np.std(group2)}\n\n    # Create boxplot and histogram\n    fig, (ax_boxplot, ax_histogram) = plt.subplots(2, sharex=True)\n    ax_boxplot.boxplot([group1, group2], labels=['Group 1', 'Group 2'])\n    ax_boxplot.set_title('Boxplot of Group Means')\n    ax_histogram.hist(group1, alpha=0.5, label='Group 1')\n    ax_histogram.hist(group2, alpha=0.5, label='Group 2')\n    ax_histogram.set_title('Histogram of Group Means')\n    ax_histogram.legend()\n\n    # Return results\n    return {\n        'significant': p_val < alpha,\n        'group1_stats': group1_stats,\n        'group2_stats': group2_stats,\n        'ax_boxplot': ax_boxplot,\n        'ax_histogram': ax_histogram,\n    }", "raw_solution": "import numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\n\ndef task_func(group1, group2, alpha=0.05, min_samples=2, var_threshold=1e-8):\n    \"\"\"\n    Performs a two-sample t-test on numerical data from two groups to determine if there is a significant difference in their means.\n    \"\"\"\n    # Remove NaN values\n    group1 = group1[~np.isnan(group1)]\n    group2 = group2[~np.isnan(group2)]\n\n    # Check for sufficient non-NaN data points and adequate variance in each group\n    if len(group1) < min_samples or len(group2) < min_samples:\n        raise ValueError(\"Each group must have at least two non-NaN values.\")\n    if np.var(group1) < var_threshold or np.var(group2) < var_threshold:\n        raise ValueError(\"The variance in each group must be above a threshold.\")\n\n    # Perform t-test\n    t_stat, p_val = ttest_ind(group1, group2, nan_policy='omit')\n\n    # Generate descriptive statistics\n    group1_stats = {'mean': np.mean(group1), 'std': np.std(group1)}\n    group2_stats = {'mean': np.mean(group2), 'std': np.std(group2)}\n\n    # Create boxplot and histogram\n    fig, (ax_boxplot, ax_histogram) = plt.subplots(2, sharex=True)\n    ax_boxplot.boxplot([group1, group2], labels=['Group 1', 'Group 2'])\n    ax_boxplot.set_title('Boxplot of Group Means')\n    ax_histogram.hist(group1, alpha=0.5, label='Group 1')\n    ax_histogram.hist(group2, alpha=0.5, label='Group 2')\n    ax_histogram.set_title('Histogram of Group Means')\n    ax_histogram.legend()\n\n    # Return results\n    return {\n        'significant': p_val < alpha,\n        'group1_stats': group1_stats,\n        'group2_stats': group2_stats,\n        'ax_boxplot': ax_boxplot,\n        'ax_histogram': ax_histogram,\n    }"}
{"task_id": "BigCodeBench/1027", "solution": "import binascii\nimport urllib.parse\ndef task_func(url):\n    \"\"\"\n    Decodes a hexadecimal string from the 'q' query parameter of a URL.\n    \"\"\"\n    # Extract the 'q' query parameter from the URL\n    query_params = urllib.parse.urlparse(url).query\n    q_param = urllib.parse.parse_qs(query_params).get('q', [None])[0]\n\n    # If 'q' parameter does not exist, return None\n    if q_param is None:\n        return None\n\n    # Try to decode the hexadecimal string\n    try:\n        decoded_bytes = binascii.unhexlify(q_param)\n    except binascii.Error:\n        return None\n\n    # Try to decode the bytes into a UTF-8 string\n    try:\n        decoded_str = decoded_bytes.decode('utf-8')\n    except UnicodeDecodeError:\n        return None\n\n    return decoded_str", "raw_solution": "import binascii\nimport urllib.parse\n\ndef task_func(url):\n    \"\"\"\n    Decodes a hexadecimal string from the 'q' query parameter of a URL.\n    \"\"\"\n    # Extract the 'q' query parameter from the URL\n    query_params = urllib.parse.urlparse(url).query\n    q_param = urllib.parse.parse_qs(query_params).get('q', [None])[0]\n\n    # If 'q' parameter does not exist, return None\n    if q_param is None:\n        return None\n\n    # Try to decode the hexadecimal string\n    try:\n        decoded_bytes = binascii.unhexlify(q_param)\n    except binascii.Error:\n        return None\n\n    # Try to decode the bytes into a UTF-8 string\n    try:\n        decoded_str = decoded_bytes.decode('utf-8')\n    except UnicodeDecodeError:\n        return None\n\n    return decoded_str"}
{"task_id": "BigCodeBench/1028", "solution": "import subprocess\nimport time\nimport json\nimport platform\nimport psutil\nfrom datetime import datetime\nLOGFILE_PATH = \"logfile.log\"\ndef task_func(interval, duration):\n    if interval <= 0 or duration <= 0:\n        raise ValueError(\"Interval and duration must be greater than 0\")\n\n    start_time = time.time()\n    cpu_data = []\n\n    while time.time() - start_time <= duration:\n        cpu_percent = psutil.cpu_percent(interval=interval)\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        cpu_data.append({\"timestamp\": timestamp, \"cpu_percent\": cpu_percent})\n\n    try:\n        with open(LOGFILE_PATH, \"w\") as logfile:\n            json.dump(cpu_data, logfile)\n    except IOError:\n        return None\n\n    return LOGFILE_PATH", "raw_solution": "import subprocess\nimport time\nimport json\nimport platform\nimport psutil\nfrom datetime import datetime\n\nLOGFILE_PATH = \"logfile.log\"\n\ndef task_func(interval, duration):\n    if interval <= 0 or duration <= 0:\n        raise ValueError(\"Interval and duration must be greater than 0\")\n\n    start_time = time.time()\n    cpu_data = []\n\n    while time.time() - start_time <= duration:\n        cpu_percent = psutil.cpu_percent(interval=interval)\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        cpu_data.append({\"timestamp\": timestamp, \"cpu_percent\": cpu_percent})\n\n    try:\n        with open(LOGFILE_PATH, \"w\") as logfile:\n            json.dump(cpu_data, logfile)\n    except IOError:\n        return None\n\n    return LOGFILE_PATH"}
{"task_id": "BigCodeBench/1029", "solution": "import pandas as pd\nimport numpy as np\nimport string\nimport random\ndef task_func(rows=100, columns=3):\n    \"\"\"\n    Creates a pandas DataFrame with random alphabets in each cell.\n    The DataFrame will have a specified number of rows and columns.\n    Each column is named with a string from the list ['a', 'b', 'c', ...] depending on the number of columns specified.\n    \"\"\"\n    # Generate a list of random alphabets\n    alphabets = [random.choice(string.ascii_lowercase) for _ in range(rows*columns)]\n    \n    # Reshape the list into a 2D array\n    data = np.array(alphabets).reshape(rows, columns)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=[string.ascii_lowercase[i] for i in range(columns)])\n    \n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport string\nimport random\n\ndef task_func(rows=100, columns=3):\n    \"\"\"\n    Creates a pandas DataFrame with random alphabets in each cell.\n    The DataFrame will have a specified number of rows and columns.\n    Each column is named with a string from the list ['a', 'b', 'c', ...] depending on the number of columns specified.\n    \"\"\"\n    # Generate a list of random alphabets\n    alphabets = [random.choice(string.ascii_lowercase) for _ in range(rows*columns)]\n    \n    # Reshape the list into a 2D array\n    data = np.array(alphabets).reshape(rows, columns)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=[string.ascii_lowercase[i] for i in range(columns)])\n    \n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/1030", "solution": "import itertools\nimport string\nimport pandas as pd\ndef task_func():\n    \"\"\"\n    Generate all possible combinations (with replacement) of three letters from the alphabet and save them in a pandas DataFrame.\n    \"\"\"\n    # Get all the letters from the alphabet\n    letters = string.ascii_lowercase\n\n    # Generate all possible combinations of three letters\n    combinations = list(itertools.product(letters, repeat=3))\n\n    # Convert the list of tuples to a list of strings\n    combinations = [''.join(comb) for comb in combinations]\n\n    # Create a pandas DataFrame from the list of combinations\n    df = pd.DataFrame(combinations, columns=['Combination'])\n\n    return df", "raw_solution": "import itertools\nimport string\nimport pandas as pd\n\ndef task_func():\n    \"\"\"\n    Generate all possible combinations (with replacement) of three letters from the alphabet and save them in a pandas DataFrame.\n    \"\"\"\n    # Get all the letters from the alphabet\n    letters = string.ascii_lowercase\n\n    # Generate all possible combinations of three letters\n    combinations = list(itertools.product(letters, repeat=3))\n\n    # Convert the list of tuples to a list of strings\n    combinations = [''.join(comb) for comb in combinations]\n\n    # Create a pandas DataFrame from the list of combinations\n    df = pd.DataFrame(combinations, columns=['Combination'])\n\n    return df"}
{"task_id": "BigCodeBench/1031", "solution": "import random\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(n_rows=1000):\n    if n_rows <= 0:\n        raise ValueError(\"n_rows should be greater than 0\")\n\n    # Generate random 3-letter strings\n    strings = [''.join(random.choices(string.ascii_lowercase, k=3)) for _ in range(n_rows)]\n\n    # Count the frequency of each string\n    counts = pd.Series(strings).value_counts()\n\n    # Get the top 30 unique strings\n    top_30_strings = counts.head(30).index.tolist()\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(counts[top_30_strings], bins=range(counts[top_30_strings].max()+2), edgecolor='black')\n    ax.set_xticks(range(counts[top_30_strings].max()+1))\n    ax.set_xlabel('Frequency')\n    ax.set_ylabel('Count')\n    ax.set_title('Frequency of the top 30 unique random 3-letter strings')\n    ax.legend(top_30_strings)\n\n    return ax", "raw_solution": "import random\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(n_rows=1000):\n    if n_rows <= 0:\n        raise ValueError(\"n_rows should be greater than 0\")\n\n    # Generate random 3-letter strings\n    strings = [''.join(random.choices(string.ascii_lowercase, k=3)) for _ in range(n_rows)]\n\n    # Count the frequency of each string\n    counts = pd.Series(strings).value_counts()\n\n    # Get the top 30 unique strings\n    top_30_strings = counts.head(30).index.tolist()\n\n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(counts[top_30_strings], bins=range(counts[top_30_strings].max()+2), edgecolor='black')\n    ax.set_xticks(range(counts[top_30_strings].max()+1))\n    ax.set_xlabel('Frequency')\n    ax.set_ylabel('Count')\n    ax.set_title('Frequency of the top 30 unique random 3-letter strings')\n    ax.legend(top_30_strings)\n\n    return ax"}
{"task_id": "BigCodeBench/1032", "solution": "import matplotlib.pyplot as plt\nimport random\nimport string\nimport pandas as pd\nimport seaborn as sns\nLETTERS = list(string.ascii_lowercase)\ndef task_func(rows=1000, string_length=3):\n    if rows == 0:\n        print(\"No data to generate heatmap.\")\n        return None\n\n    # Generate random strings\n    random_strings = [''.join(random.choices(LETTERS, k=string_length)) for _ in range(rows)]\n\n    # Create DataFrame\n    df = pd.DataFrame(random_strings, columns=['String'])\n\n    # One-hot encode each string\n    df = pd.get_dummies(df['String'])\n\n    # Calculate frequency of each letter\n    letter_freq = df.sum().sort_values(ascending=False)\n\n    # Create correlation matrix\n    corr_matrix = df.corr()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Heatmap of Correlation Matrix')\n    plt.show()\n\n    return letter_freq", "raw_solution": "import matplotlib.pyplot as plt\nimport random\nimport string\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nLETTERS = list(string.ascii_lowercase)\n\ndef task_func(rows=1000, string_length=3):\n    if rows == 0:\n        print(\"No data to generate heatmap.\")\n        return None\n\n    # Generate random strings\n    random_strings = [''.join(random.choices(LETTERS, k=string_length)) for _ in range(rows)]\n\n    # Create DataFrame\n    df = pd.DataFrame(random_strings, columns=['String'])\n\n    # One-hot encode each string\n    df = pd.get_dummies(df['String'])\n\n    # Calculate frequency of each letter\n    letter_freq = df.sum().sort_values(ascending=False)\n\n    # Create correlation matrix\n    corr_matrix = df.corr()\n\n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Heatmap of Correlation Matrix')\n    plt.show()\n\n    return letter_freq"}
{"task_id": "BigCodeBench/1033", "solution": "import itertools\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func():\n    # Generate all possible 3-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=3)]\n\n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Combinations'])\n\n    # Count the frequency of the first letters\n    counts = df['Combinations'].str[0].value_counts()\n\n    # Plot a histogram of the counts\n    fig, ax = plt.subplots()\n    counts.plot(kind='bar', ax=ax)\n    ax.set_title('Frequency of First Letters in 3-Letter Combinations')\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Frequency')\n\n    return (combinations, df, ax)", "raw_solution": "import itertools\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    # Generate all possible 3-letter combinations\n    combinations = [''.join(x) for x in itertools.product(string.ascii_lowercase, repeat=3)]\n\n    # Create a DataFrame from the combinations\n    df = pd.DataFrame(combinations, columns=['Combinations'])\n\n    # Count the frequency of the first letters\n    counts = df['Combinations'].str[0].value_counts()\n\n    # Plot a histogram of the counts\n    fig, ax = plt.subplots()\n    counts.plot(kind='bar', ax=ax)\n    ax.set_title('Frequency of First Letters in 3-Letter Combinations')\n    ax.set_xlabel('Letter')\n    ax.set_ylabel('Frequency')\n\n    return (combinations, df, ax)\n\n# Test the function\ncombinations, df, ax = task_func()"}
{"task_id": "BigCodeBench/1034", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance\nfrom typing import Tuple\nCATEGORIES = [\"Electronics\", \"Clothing\", \"Home Decor\", \"Automotive\", \"Books\"]\ndef task_func(s1: pd.Series, s2: pd.Series) -> Tuple[plt.Axes, float]:\n    \"\"\"\n    Compares and visualizes the sales data of two stores for predefined categories.\n    The function generates a bar plot for categories where both stores have sales exceeding a specified threshold.\n    The Euclidean distance between the two series is also computed.\n    \"\"\"\n    assert s1.index.tolist() == CATEGORIES, \"s1 should have sales data for all categories\"\n    assert s2.index.tolist() == CATEGORIES, \"s2 should have sales data for all categories\"\n\n    # Filter categories where both stores have sales exceeding the threshold of 200\n    filtered_categories = (s1 > 200) & (s2 > 200)\n    if not filtered_categories.any():\n        return None, 0.0\n\n    # Create a bar plot for the filtered categories\n    fig, ax = plt.subplots()\n    ax.bar(s1[filtered_categories].index, s1[filtered_categories], label='Store 1')\n    ax.bar(s2[filtered_categories].index, s2[filtered_categories], label='Store 2')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Sales')\n    ax.legend()\n\n    # Compute Euclidean distance between the two series\n    euclidean_distance = distance.euclidean(s1[filtered_categories], s2[filtered_categories])\n\n    return ax, euclidean_distance", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance\nfrom typing import Tuple\n\nCATEGORIES = [\"Electronics\", \"Clothing\", \"Home Decor\", \"Automotive\", \"Books\"]\n\ndef task_func(s1: pd.Series, s2: pd.Series) -> Tuple[plt.Axes, float]:\n    \"\"\"\n    Compares and visualizes the sales data of two stores for predefined categories.\n    The function generates a bar plot for categories where both stores have sales exceeding a specified threshold.\n    The Euclidean distance between the two series is also computed.\n    \"\"\"\n    assert s1.index.tolist() == CATEGORIES, \"s1 should have sales data for all categories\"\n    assert s2.index.tolist() == CATEGORIES, \"s2 should have sales data for all categories\"\n\n    # Filter categories where both stores have sales exceeding the threshold of 200\n    filtered_categories = (s1 > 200) & (s2 > 200)\n    if not filtered_categories.any():\n        return None, 0.0\n\n    # Create a bar plot for the filtered categories\n    fig, ax = plt.subplots()\n    ax.bar(s1[filtered_categories].index, s1[filtered_categories], label='Store 1')\n    ax.bar(s2[filtered_categories].index, s2[filtered_categories], label='Store 2')\n    ax.set_xlabel('Category')\n    ax.set_ylabel('Sales')\n    ax.legend()\n\n    # Compute Euclidean distance between the two series\n    euclidean_distance = distance.euclidean(s1[filtered_categories], s2[filtered_categories])\n\n    return ax, euclidean_distance"}
{"task_id": "BigCodeBench/1035", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(feature: pd.Series, target: pd.Series) -> (np.ndarray, plt.Axes):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(feature.values.reshape(-1, 1), target.values, test_size=0.2, random_state=42)\n\n    # Standardize the features\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n\n    # Train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the target for the test set\n    y_pred = model.predict(X_test)\n\n    # Create the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Plot the confusion matrix\n    fig, ax = plt.subplots(figsize=(6, 6))\n    sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r', ax=ax)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.title('Confusion matrix')\n\n    return cm, ax", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(feature: pd.Series, target: pd.Series) -> (np.ndarray, plt.Axes):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(feature.values.reshape(-1, 1), target.values, test_size=0.2, random_state=42)\n\n    # Standardize the features\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n\n    # Train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Predict the target for the test set\n    y_pred = model.predict(X_test)\n\n    # Create the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Plot the confusion matrix\n    fig, ax = plt.subplots(figsize=(6, 6))\n    sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r', ax=ax)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.title('Confusion matrix')\n\n    return cm, ax"}
{"task_id": "BigCodeBench/1036", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(s1, s2):\n    \"\"\"\n    Visualizes two Series using a swarm plot with a highlight on their intersecting data points.\n    \"\"\"\n    # Create a DataFrame from the two Series\n    df = pd.DataFrame({'s1': s1, 's2': s2})\n\n    # Create a swarm plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.swarmplot(x='s1', y='s2', data=df)\n\n    # Highlight intersecting data points\n    intersecting_data = df[df['s1'].isin(df['s2'])]\n    for i, row in intersecting_data.iterrows():\n        ax.plot(row['s1'], row['s2'], 'ro')\n\n    # Count the number of unique intersecting data points\n    intersection_count = len(intersecting_data['s1'].unique())\n\n    return ax, intersection_count\ns1 = pd.Series([1, 2, 3, 4, 5])\ns2 = pd.Series([4, 5, 6, 7, 8])", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(s1, s2):\n    \"\"\"\n    Visualizes two Series using a swarm plot with a highlight on their intersecting data points.\n    \"\"\"\n    # Create a DataFrame from the two Series\n    df = pd.DataFrame({'s1': s1, 's2': s2})\n\n    # Create a swarm plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.swarmplot(x='s1', y='s2', data=df)\n\n    # Highlight intersecting data points\n    intersecting_data = df[df['s1'].isin(df['s2'])]\n    for i, row in intersecting_data.iterrows():\n        ax.plot(row['s1'], row['s2'], 'ro')\n\n    # Count the number of unique intersecting data points\n    intersection_count = len(intersecting_data['s1'].unique())\n\n    return ax, intersection_count\n\n# Test the function\ns1 = pd.Series([1, 2, 3, 4, 5])\ns2 = pd.Series([4, 5, 6, 7, 8])\nax, intersection_count = task_func(s1, s2)"}
{"task_id": "BigCodeBench/1037", "solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(s1, s2, n_clusters=3):\n    \"\"\"\n    Performs K-Means clustering on data points from two pandas Series and visualizes the clusters.\n    \"\"\"\n    if not isinstance(s1, pd.Series) or not isinstance(s2, pd.Series):\n        raise ValueError(\"s1 and s2 must be pandas Series\")\n    if len(s1) != len(s2):\n        raise ValueError(\"s1 and s2 must have the same length\")\n\n    # Combine the two series into a dataframe\n    df = pd.DataFrame({'s1': s1, 's2': s2})\n\n    # Perform K-Means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(df)\n\n    # Create a scatter plot\n    plt.figure(figsize=(8, 6))\n    plt.scatter(df['s1'], df['s2'], c=labels, cmap='viridis')\n    plt.title(\"K-Means Clustering\")\n    plt.xlabel(s1.name)\n    plt.ylabel(s2.name)\n    plt.legend(title='Clusters')\n\n    return labels, plt.gca()", "raw_solution": "import pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(s1, s2, n_clusters=3):\n    \"\"\"\n    Performs K-Means clustering on data points from two pandas Series and visualizes the clusters.\n    \"\"\"\n    if not isinstance(s1, pd.Series) or not isinstance(s2, pd.Series):\n        raise ValueError(\"s1 and s2 must be pandas Series\")\n    if len(s1) != len(s2):\n        raise ValueError(\"s1 and s2 must have the same length\")\n\n    # Combine the two series into a dataframe\n    df = pd.DataFrame({'s1': s1, 's2': s2})\n\n    # Perform K-Means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(df)\n\n    # Create a scatter plot\n    plt.figure(figsize=(8, 6))\n    plt.scatter(df['s1'], df['s2'], c=labels, cmap='viridis')\n    plt.title(\"K-Means Clustering\")\n    plt.xlabel(s1.name)\n    plt.ylabel(s2.name)\n    plt.legend(title='Clusters')\n\n    return labels, plt.gca()"}
{"task_id": "BigCodeBench/1038", "solution": "from datetime import datetime\nimport json\ndef task_func(client_socket):\n    \"\"\"\n    Responds to a client's request by sending a JSON-formatted message containing the current server time and a greeting.\n    \"\"\"\n    # Get the current server time\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Create a JSON message\n    message = {\n        \"time\": current_time,\n        \"greeting\": \"Hello, client!\"\n    }\n\n    # Convert the message to JSON format\n    json_message = json.dumps(message)\n\n    # Send the JSON message to the client\n    client_socket.send(json_message.encode())\n\n    # Close the client socket\n    client_socket.close()\n\n    # Return None\n    return None", "raw_solution": "from datetime import datetime\nimport json\n\ndef task_func(client_socket):\n    \"\"\"\n    Responds to a client's request by sending a JSON-formatted message containing the current server time and a greeting.\n    \"\"\"\n    # Get the current server time\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Create a JSON message\n    message = {\n        \"time\": current_time,\n        \"greeting\": \"Hello, client!\"\n    }\n\n    # Convert the message to JSON format\n    json_message = json.dumps(message)\n\n    # Send the JSON message to the client\n    client_socket.send(json_message.encode())\n\n    # Close the client socket\n    client_socket.close()\n\n    # Return None\n    return None"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    context.load_cert_chain(cert_file, key_file)\n\n    secure_sock = context.wrap_socket(client_socket, server_side=True)\n\n    try:\n        secure_sock.do_handshake()\n    except Exception as e:\n        return f\"Handshake error: {str(e)}\"\n\n    try:\n        file_path = secure_sock.recv(buffer_size).decode().strip()\n    except Exception as e:\n        return f\"Receiving file path error: {str(e)}\"\n\n    if not os.path.exists(file_path):\n        return 'File not found'\n\n    try:\n        with open(file_path, 'rb') as file:\n            file_data = file.read()\n    except Exception as e:\n        return f\"File opening error: {str(e)}\"\n\n    try:\n        hash_obj = hashlib.sha256()\n        hash_obj.update(file_data)\n        return hash_obj.hexdigest()\n    except Exception as e:\n        return f\"Hashing error: {str(e)}\"", "raw_solution": "import ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    context.load_cert_chain(cert_file, key_file)\n\n    secure_sock = context.wrap_socket(client_socket, server_side=True)\n\n    try:\n        secure_sock.do_handshake()\n    except Exception as e:\n        return f\"Handshake error: {str(e)}\"\n\n    try:\n        file_path = secure_sock.recv(buffer_size).decode().strip()\n    except Exception as e:\n        return f\"Receiving file path error: {str(e)}\"\n\n    if not os.path.exists(file_path):\n        return 'File not found'\n\n    try:\n        with open(file_path, 'rb') as file:\n            file_data = file.read()\n    except Exception as e:\n        return f\"File opening error: {str(e)}\"\n\n    try:\n        hash_obj = hashlib.sha256()\n        hash_obj.update(file_data)\n        return hash_obj.hexdigest()\n    except Exception as e:\n        return f\"Hashing error: {str(e)}\""}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # List for socket descriptors\n    sockets_list = [server_socket]\n\n    # List for connected clients - socket as a key, user header and name as data\n    clients = {}\n\n    start_time = datetime.now()\n    run_duration_sec = timedelta(seconds=run_duration)\n\n    print(f\"Server started at {start_time}\")\n\n    while datetime.now() - start_time < run_duration_sec:\n        read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list)\n\n        for notified_socket in read_sockets:\n            if notified_socket == server_socket:\n                client_socket, client_address = server_socket.accept()\n                user = {\"header\": b\"\", \"data\": b\"\"}\n                clients[client_socket] = user\n                sockets_list.append(client_socket)\n                print(f\"Accepted new connection from {client_address[0]}:{client_address[1]}\")\n            else:\n                data = notified_socket.recv(buffer_size)\n                if data:\n                    user = clients[notified_socket]\n                    user[\"data\"] += data\n                    if user[\"header\"] == b\"\":\n                        if len(user[\"data\"]) < 2:\n                            continue\n                        else:\n                            msg_length = int.from_bytes(user[\"data\"][:2], \"big\")\n                            user[\"header\"] = user[\"data\"][2 : 2 + msg_length]\n                            user[\"data\"] = user[\"data\"][2 + msg_length :]\n                    message = user[\"header\"] + b\" \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\").encode()\n                    notified_socket.send(message)\n                else:\n                    notified_socket.close()\n                    sockets_list.remove(notified_socket)\n                    del clients[notified_socket]\n\n    server_socket.close()\n    print(f\"Server stopped at {datetime.now()} after {run_duration} seconds\")\n    return f\"Server started at {start_time} and stopped at {datetime.now()} after {run_duration} seconds\"", "raw_solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # List for socket descriptors\n    sockets_list = [server_socket]\n\n    # List for connected clients - socket as a key, user header and name as data\n    clients = {}\n\n    start_time = datetime.now()\n    run_duration_sec = timedelta(seconds=run_duration)\n\n    print(f\"Server started at {start_time}\")\n\n    while datetime.now() - start_time < run_duration_sec:\n        read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list)\n\n        for notified_socket in read_sockets:\n            if notified_socket == server_socket:\n                client_socket, client_address = server_socket.accept()\n                user = {\"header\": b\"\", \"data\": b\"\"}\n                clients[client_socket] = user\n                sockets_list.append(client_socket)\n                print(f\"Accepted new connection from {client_address[0]}:{client_address[1]}\")\n            else:\n                data = notified_socket.recv(buffer_size)\n                if data:\n                    user = clients[notified_socket]\n                    user[\"data\"] += data\n                    if user[\"header\"] == b\"\":\n                        if len(user[\"data\"]) < 2:\n                            continue\n                        else:\n                            msg_length = int.from_bytes(user[\"data\"][:2], \"big\")\n                            user[\"header\"] = user[\"data\"][2 : 2 + msg_length]\n                            user[\"data\"] = user[\"data\"][2 + msg_length :]\n                    message = user[\"header\"] + b\" \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\").encode()\n                    notified_socket.send(message)\n                else:\n                    notified_socket.close()\n                    sockets_list.remove(notified_socket)\n                    del clients[notified_socket]\n\n    server_socket.close()\n    print(f\"Server stopped at {datetime.now()} after {run_duration} seconds\")\n    return f\"Server started at {start_time} and stopped at {datetime.now()} after {run_duration} seconds\"\n\n# Test the function"}
{"task_id": "BigCodeBench/1041", "solution": "import re\nimport os\ndef task_func(request):\n    \"\"\"\n    Handles an HTTP GET request to retrieve a static file from the server.\n    \"\"\"\n    # Check if the request is a valid HTTP GET request\n    if request.startswith('GET /') and request.endswith(' HTTP/1.1'):\n        # Extract the filename from the request\n        filename = request[4:].split(' ')[0]\n\n        # Check if the file exists in the server's directory\n        if os.path.isfile(filename):\n            # Read the file content\n            with open(filename, 'r') as file:\n                file_content = file.read()\n\n            # Create the HTTP response\n            response = f'HTTP/1.1 200 OK\\nContent-Length: {len(file_content)}\\n\\n{file_content}'\n        else:\n            # Create an HTTP response for a file not found error\n            response = 'HTTP/1.1 404 Not Found\\nContent-Length: 0\\n\\n'\n    else:\n        # Create an HTTP response for an invalid request error\n        response = 'HTTP/1.1 400 Bad Request\\nContent-Length: 0\\n\\n'\n\n    return response", "raw_solution": "import re\nimport os\n\ndef task_func(request):\n    \"\"\"\n    Handles an HTTP GET request to retrieve a static file from the server.\n    \"\"\"\n    # Check if the request is a valid HTTP GET request\n    if request.startswith('GET /') and request.endswith(' HTTP/1.1'):\n        # Extract the filename from the request\n        filename = request[4:].split(' ')[0]\n\n        # Check if the file exists in the server's directory\n        if os.path.isfile(filename):\n            # Read the file content\n            with open(filename, 'r') as file:\n                file_content = file.read()\n\n            # Create the HTTP response\n            response = f'HTTP/1.1 200 OK\\nContent-Length: {len(file_content)}\\n\\n{file_content}'\n        else:\n            # Create an HTTP response for a file not found error\n            response = 'HTTP/1.1 404 Not Found\\nContent-Length: 0\\n\\n'\n    else:\n        # Create an HTTP response for an invalid request error\n        response = 'HTTP/1.1 400 Bad Request\\nContent-Length: 0\\n\\n'\n\n    return response"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    \"\"\"\n    Receive a message from a client socket and send it as an email via an SMTP server.\n    \"\"\"\n    # Receive message from client\n    data = client_socket.recv(1024).decode()\n\n    # Prompt for sender's email, recipient's email, and password\n    sender_email = input(\"Enter your email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    password = getpass.getpass(\"Enter your password: \")\n\n    # Create an email message\n    msg = EmailMessage()\n    msg.set_content(data)\n    msg['Subject'] = 'Message from client'\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n\n    # Send email\n    try:\n        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n        server.starttls()\n        server.login(sender_email, password)\n        server.send_message(msg)\n        server.quit()\n    except Exception as e:\n        print(f\"Failed to send email: {str(e)}\")\n        return\n\n    print(\"Email sent successfully.\")", "raw_solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    \"\"\"\n    Receive a message from a client socket and send it as an email via an SMTP server.\n    \"\"\"\n    # Receive message from client\n    data = client_socket.recv(1024).decode()\n\n    # Prompt for sender's email, recipient's email, and password\n    sender_email = input(\"Enter your email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    password = getpass.getpass(\"Enter your password: \")\n\n    # Create an email message\n    msg = EmailMessage()\n    msg.set_content(data)\n    msg['Subject'] = 'Message from client'\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n\n    # Send email\n    try:\n        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n        server.starttls()\n        server.login(sender_email, password)\n        server.send_message(msg)\n        server.quit()\n    except Exception as e:\n        print(f\"Failed to send email: {str(e)}\")\n        return\n\n    print(\"Email sent successfully.\")"}
{"task_id": "BigCodeBench/1043", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nCATEGORIES = [\"A\", \"B\", \"C\", \"D\", \"E\"]\ndef task_func(data_list):\n    \"\"\"\n    Processes a list of category labels to create a histogram that visualizes their distribution.\n    \"\"\"\n    if not data_list:\n        raise ValueError(\"The data list is empty.\")\n\n    # Create a pandas Series from the data_list\n    series = pd.Series(data_list)\n\n    # Count the occurrences of each category\n    category_counts = series.value_counts()\n\n    # Get all categories, including extra categories\n    all_categories = CATEGORIES + category_counts.index.tolist()\n\n    # Reindex the category_counts to include all categories\n    category_counts = category_counts.reindex(all_categories, fill_value=0)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(all_categories, category_counts, width=0.8, align=\"center\")\n\n    # Check if the distribution is uniform\n    uniform = category_counts.std() == 0\n    if not uniform:\n        print(\"The distribution of predefined categories is not uniform.\")\n\n    return ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCATEGORIES = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n\ndef task_func(data_list):\n    \"\"\"\n    Processes a list of category labels to create a histogram that visualizes their distribution.\n    \"\"\"\n    if not data_list:\n        raise ValueError(\"The data list is empty.\")\n\n    # Create a pandas Series from the data_list\n    series = pd.Series(data_list)\n\n    # Count the occurrences of each category\n    category_counts = series.value_counts()\n\n    # Get all categories, including extra categories\n    all_categories = CATEGORIES + category_counts.index.tolist()\n\n    # Reindex the category_counts to include all categories\n    category_counts = category_counts.reindex(all_categories, fill_value=0)\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(all_categories, category_counts, width=0.8, align=\"center\")\n\n    # Check if the distribution is uniform\n    uniform = category_counts.std() == 0\n    if not uniform:\n        print(\"The distribution of predefined categories is not uniform.\")\n\n    return ax"}
{"task_id": "BigCodeBench/1044", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nROOMS = [\"Room1\", \"Room2\", \"Room3\", \"Room4\", \"Room5\"]\ndef task_func(date_str, booking_data):\n    \"\"\"\n    Generates a status report of room bookings for a specified date and displays a bar plot representing the booking statuses of various rooms.\n    \"\"\"\n    # Validate date_str\n    try:\n        date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    except ValueError:\n        raise ValueError(\"date_str must be in 'yyyy-mm-dd' format\")\n\n    # Validate date_str is not a past date\n    if date < datetime.now():\n        raise ValueError(\"date_str cannot be a past date\")\n\n    # Compile booking status report\n    report = pd.DataFrame(columns=[\"Room\", \"Status\"])\n    for room in ROOMS:\n        status = \"Available\" if date not in booking_data[room] else \"Booked\"\n        report = report.append({\"Room\": room, \"Status\": status}, ignore_index=True)\n\n    # Visualize data\n    fig, ax = plt.subplots()\n    report.groupby(\"Status\").count().plot(kind=\"bar\", ax=ax)\n    ax.set_title(f\"Booking statuses for {date_str}\")\n    ax.set_xlabel(\"Status\")\n    ax.set_ylabel(\"Count\")\n\n    return report, ax.figure", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Constants\nROOMS = [\"Room1\", \"Room2\", \"Room3\", \"Room4\", \"Room5\"]\n\ndef task_func(date_str, booking_data):\n    \"\"\"\n    Generates a status report of room bookings for a specified date and displays a bar plot representing the booking statuses of various rooms.\n    \"\"\"\n    # Validate date_str\n    try:\n        date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    except ValueError:\n        raise ValueError(\"date_str must be in 'yyyy-mm-dd' format\")\n\n    # Validate date_str is not a past date\n    if date < datetime.now():\n        raise ValueError(\"date_str cannot be a past date\")\n\n    # Compile booking status report\n    report = pd.DataFrame(columns=[\"Room\", \"Status\"])\n    for room in ROOMS:\n        status = \"Available\" if date not in booking_data[room] else \"Booked\"\n        report = report.append({\"Room\": room, \"Status\": status}, ignore_index=True)\n\n    # Visualize data\n    fig, ax = plt.subplots()\n    report.groupby(\"Status\").count().plot(kind=\"bar\", ax=ax)\n    ax.set_title(f\"Booking statuses for {date_str}\")\n    ax.set_xlabel(\"Status\")\n    ax.set_ylabel(\"Count\")\n\n    return report, ax.figure"}
{"task_id": "BigCodeBench/1045", "solution": "from datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\nLEAP_SECONDS = np.array(\n    [\n        1972,\n        1973,\n        1974,\n        1975,\n        1976,\n        1977,\n        1978,\n        1979,\n        1980,\n        1981,\n        1982,\n        1983,\n        1985,\n        1988,\n        1990,\n        1993,\n        1994,\n        1997,\n        1999,\n        2006,\n        2009,\n        2012,\n        2015,\n        2016,\n        2020,\n    ]\n)\ndef task_func(date_str):\n    \"\"\"\n    Calculates the total number of seconds elapsed from a given date until the current time,\n    including any leap seconds that occurred in this period.\n    \"\"\"\n    given_date = parse(date_str)\n    current_date = datetime.now()\n    total_seconds = (current_date - given_date).total_seconds()\n    leap_seconds_added = len(LEAP_SECONDS[LEAP_SECONDS >= given_date.year])\n    total_seconds += leap_seconds_added\n    return int(total_seconds)", "raw_solution": "from datetime import datetime\nimport numpy as np\nfrom dateutil.parser import parse\n\nLEAP_SECONDS = np.array(\n    [\n        1972,\n        1973,\n        1974,\n        1975,\n        1976,\n        1977,\n        1978,\n        1979,\n        1980,\n        1981,\n        1982,\n        1983,\n        1985,\n        1988,\n        1990,\n        1993,\n        1994,\n        1997,\n        1999,\n        2006,\n        2009,\n        2012,\n        2015,\n        2016,\n        2020,\n    ]\n)\n\ndef task_func(date_str):\n    \"\"\"\n    Calculates the total number of seconds elapsed from a given date until the current time,\n    including any leap seconds that occurred in this period.\n    \"\"\"\n    given_date = parse(date_str)\n    current_date = datetime.now()\n    total_seconds = (current_date - given_date).total_seconds()\n    leap_seconds_added = len(LEAP_SECONDS[LEAP_SECONDS >= given_date.year])\n    total_seconds += leap_seconds_added\n    return int(total_seconds)"}
{"task_id": "BigCodeBench/1046", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\ndef task_func(date_str):\n    \"\"\"\n    Generate a Pandas DataFrame containing a series of dates for a predefined list of employees.\n    \"\"\"\n    date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n    date_list = [(employee, date_obj + timedelta(days=i)) for employee in EMPLOYEES for i in range(10)]\n    df = pd.DataFrame(date_list, columns=[\"Employee\", \"Date\"])\n    df[\"Date\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n    return df", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\n\n# Constants\nEMPLOYEES = [\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\n\ndef task_func(date_str):\n    \"\"\"\n    Generate a Pandas DataFrame containing a series of dates for a predefined list of employees.\n    \"\"\"\n    date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n    date_list = [(employee, date_obj + timedelta(days=i)) for employee in EMPLOYEES for i in range(10)]\n    df = pd.DataFrame(date_list, columns=[\"Employee\", \"Date\"])\n    df[\"Date\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/1047", "solution": "from datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\ndef task_func(date_str):\n    \"\"\"\n    Generates a list of random integers, where the count of integers equals the day of the month in the provided date,\n    then generates a line plot of these integers and returns the Axes object of the plot.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n\n    # Generate a list of random integers\n    random_list = [random.randint(0, 100) for _ in range(date.day)]\n\n    # Create a line plot of the random integers\n    ax = plt.plot(random_list)\n\n    return ax", "raw_solution": "from datetime import datetime\nimport random\nimport matplotlib.pyplot as plt\n\ndef task_func(date_str):\n    \"\"\"\n    Generates a list of random integers, where the count of integers equals the day of the month in the provided date,\n    then generates a line plot of these integers and returns the Axes object of the plot.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = datetime.strptime(date_str, '%Y-%m-%d')\n\n    # Generate a list of random integers\n    random_list = [random.randint(0, 100) for _ in range(date.day)]\n\n    # Create a line plot of the random integers\n    ax = plt.plot(random_list)\n\n    return ax"}
{"task_id": "BigCodeBench/1048", "solution": "from datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(date_str):\n    \"\"\"\n    Plots a sine wave whose frequency is determined by the day of the month from the given date.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n\n    # Extract the day of the month\n    day_of_month = date.day\n\n    # Generate x values from 0 to 2*pi (the range of a sine wave)\n    x = np.linspace(0, 2 * np.pi, 1000)\n\n    # Generate the sine wave with frequency determined by the day of the month\n    y = np.sin(2 * np.pi * day_of_month * x)\n\n    # Create a new figure and plot the sine wave\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n\n    # Set the title and labels\n    ax.set_title(f\"Sine wave with frequency determined by the day of the month ({day_of_month})\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"sin(2\u03c0x)\")\n\n    return ax", "raw_solution": "from datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(date_str):\n    \"\"\"\n    Plots a sine wave whose frequency is determined by the day of the month from the given date.\n    \"\"\"\n    # Parse the date string into a datetime object\n    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n\n    # Extract the day of the month\n    day_of_month = date.day\n\n    # Generate x values from 0 to 2*pi (the range of a sine wave)\n    x = np.linspace(0, 2 * np.pi, 1000)\n\n    # Generate the sine wave with frequency determined by the day of the month\n    y = np.sin(2 * np.pi * day_of_month * x)\n\n    # Create a new figure and plot the sine wave\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n\n    # Set the title and labels\n    ax.set_title(f\"Sine wave with frequency determined by the day of the month ({day_of_month})\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"sin(2\u03c0x)\")\n\n    return ax"}
{"task_id": "BigCodeBench/1049", "solution": "import re\nimport pandas as pd\ndef task_func(input_string: str) -> pd.DataFrame:\n    \"\"\"\n    Process a multi-line string by replacing tabs with spaces and converting it into a pandas DataFrame.\n    Each non-empty line of the input string is transformed into a separate row in the DataFrame.\n    \"\"\"\n    # Split the input string into lines\n    lines = input_string.split('\\n')\n\n    # Filter out empty lines and replace tabs with single spaces in the remaining lines\n    lines = [re.sub(r'\\t', ' ', line) for line in lines if line.strip()]\n\n    # Create a DataFrame from the lines\n    df = pd.DataFrame(lines, columns=['Text'])\n\n    return df", "raw_solution": "import re\nimport pandas as pd\n\ndef task_func(input_string: str) -> pd.DataFrame:\n    \"\"\"\n    Process a multi-line string by replacing tabs with spaces and converting it into a pandas DataFrame.\n    Each non-empty line of the input string is transformed into a separate row in the DataFrame.\n    \"\"\"\n    # Split the input string into lines\n    lines = input_string.split('\\n')\n\n    # Filter out empty lines and replace tabs with single spaces in the remaining lines\n    lines = [re.sub(r'\\t', ' ', line) for line in lines if line.strip()]\n\n    # Create a DataFrame from the lines\n    df = pd.DataFrame(lines, columns=['Text'])\n\n    return df"}
{"task_id": "BigCodeBench/1050", "solution": "import os\nimport hashlib\nDIRECTORY = \"./hashed_files\"\ndef task_func(input_string):\n    \"\"\"\n    Hash each non-empty line of a multi-line string using SHA256 and save the hashes to files.\n    The filename is the first 10 characters of the hash, with a '.txt' extension.\n    If the DIRECTORY does not exist, it is created. Empty lines in the input string are ignored.\n    \"\"\"\n    # Create directory if it doesn't exist\n    if not os.path.exists(DIRECTORY):\n        os.makedirs(DIRECTORY)\n\n    # Split the input string into lines\n    lines = input_string.split('\\n')\n\n    # List to store the file paths\n    file_paths = []\n\n    # Iterate over each line\n    for line in lines:\n        # Ignore empty lines\n        if line.strip():\n            # Hash the line\n            hashed_line = hashlib.sha256(line.encode()).hexdigest()\n\n            # Create the file path\n            file_path = os.path.join(DIRECTORY, hashed_line[:10] + '.txt')\n\n            # Save the hashed line to the file\n            with open(file_path, 'w') as file:\n                file.write(hashed_line)\n\n            # Add the file path to the list\n            file_paths.append(file_path)\n\n    # Return the list of file paths\n    return file_paths", "raw_solution": "import os\nimport hashlib\n\n# Constants\nDIRECTORY = \"./hashed_files\"\n\ndef task_func(input_string):\n    \"\"\"\n    Hash each non-empty line of a multi-line string using SHA256 and save the hashes to files.\n    The filename is the first 10 characters of the hash, with a '.txt' extension.\n    If the DIRECTORY does not exist, it is created. Empty lines in the input string are ignored.\n    \"\"\"\n    # Create directory if it doesn't exist\n    if not os.path.exists(DIRECTORY):\n        os.makedirs(DIRECTORY)\n\n    # Split the input string into lines\n    lines = input_string.split('\\n')\n\n    # List to store the file paths\n    file_paths = []\n\n    # Iterate over each line\n    for line in lines:\n        # Ignore empty lines\n        if line.strip():\n            # Hash the line\n            hashed_line = hashlib.sha256(line.encode()).hexdigest()\n\n            # Create the file path\n            file_path = os.path.join(DIRECTORY, hashed_line[:10] + '.txt')\n\n            # Save the hashed line to the file\n            with open(file_path, 'w') as file:\n                file.write(hashed_line)\n\n            # Add the file path to the list\n            file_paths.append(file_path)\n\n    # Return the list of file paths\n    return file_paths"}
{"task_id": "BigCodeBench/1051", "solution": "import collections\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    if not data_dict:\n        return None, \"The distribution is uniform.\"\n\n    counts = list(data_dict.values())\n    avg_count = np.mean(counts)\n    is_uniform = all(abs(count - avg_count) <= 1e-5 for count in counts)\n\n    fig, ax = plt.subplots()\n    ax.hist(list(data_dict.keys()), bins=min(10, len(set(counts))), weights=counts)\n    ax.set_xticks(list(data_dict.keys()))\n    ax.set_xticklabels(list(data_dict.keys()))\n    plt.show()\n\n    return ax, \"The distribution is uniform.\" if is_uniform else \"The distribution is not uniform.\"", "raw_solution": "import collections\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict):\n    if not data_dict:\n        return None, \"The distribution is uniform.\"\n\n    counts = list(data_dict.values())\n    avg_count = np.mean(counts)\n    is_uniform = all(abs(count - avg_count) <= 1e-5 for count in counts)\n\n    fig, ax = plt.subplots()\n    ax.hist(list(data_dict.keys()), bins=min(10, len(set(counts))), weights=counts)\n    ax.set_xticks(list(data_dict.keys()))\n    ax.set_xticklabels(list(data_dict.keys()))\n    plt.show()\n\n    return ax, \"The distribution is uniform.\" if is_uniform else \"The distribution is not uniform.\""}
{"task_id": "BigCodeBench/1052", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    This function processes a text dataset from a CSV file, performs text vectorization while excluding specific stopwords,\n    and creates a histogram of the ten most common words. The function is robust to different input scenarios, such as empty\n    data or data containing only stopwords.\n    \"\"\"\n    # Load the CSV file\n    try:\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        print(f\"Error loading CSV file: {e}\")\n        return None\n\n    # Check if the dataframe is empty\n    if df.empty:\n        print(\"The input file is empty or contains only stop words.\")\n        return None\n\n    # Perform text vectorization while excluding specific stopwords\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    X = vectorizer.fit_transform(df['text'])\n\n    # Get the word counts\n    word_counts = vectorizer.get_feature_names_out()\n    word_counts = pd.Series(X.toarray().sum(axis=0), index=word_counts)\n\n    # Get the 10 most common words\n    top_10_words = word_counts.nlargest(10)\n\n    # Create a histogram of the 10 most common words\n    fig, ax = plt.subplots()\n    top_10_words.plot(kind='bar', ax=ax)\n    ax.set_title('10 Most Common Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n\n    # Save the plot if a save_path is provided\n    if save_path is not None:\n        try:\n            plt.savefig(save_path)\n        except Exception as e:\n            print(f\"Error saving plot: {e}\")\n            return None\n\n    return ax", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    This function processes a text dataset from a CSV file, performs text vectorization while excluding specific stopwords,\n    and creates a histogram of the ten most common words. The function is robust to different input scenarios, such as empty\n    data or data containing only stopwords.\n    \"\"\"\n    # Load the CSV file\n    try:\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        print(f\"Error loading CSV file: {e}\")\n        return None\n\n    # Check if the dataframe is empty\n    if df.empty:\n        print(\"The input file is empty or contains only stop words.\")\n        return None\n\n    # Perform text vectorization while excluding specific stopwords\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    X = vectorizer.fit_transform(df['text'])\n\n    # Get the word counts\n    word_counts = vectorizer.get_feature_names_out()\n    word_counts = pd.Series(X.toarray().sum(axis=0), index=word_counts)\n\n    # Get the 10 most common words\n    top_10_words = word_counts.nlargest(10)\n\n    # Create a histogram of the 10 most common words\n    fig, ax = plt.subplots()\n    top_10_words.plot(kind='bar', ax=ax)\n    ax.set_title('10 Most Common Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n\n    # Save the plot if a save_path is provided\n    if save_path is not None:\n        try:\n            plt.savefig(save_path)\n        except Exception as e:\n            print(f\"Error saving plot: {e}\")\n            return None\n\n    return ax"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File {file_path} not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    # Check if the CSV file has a header\n    if 'Text' in df.columns:\n        text_data = df['Text']\n    else:\n        text_data = df[df.columns[0]]\n\n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n\n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n\n    # Get the word counts\n    word_counts = vectorizer.get_feature_names_out()\n    count_values = X.toarray().sum(axis=0)\n\n    # Create a DataFrame for word counts\n    df_word_counts = pd.DataFrame(list(zip(word_counts, count_values)), columns=['Word', 'Count'])\n\n    # Get the top 10 most common words\n    top_10_words = df_word_counts.nlargest(10, 'Count')\n\n    # Plot the histogram\n    plt.figure(figsize=(10, 5))\n    plt.bar(top_10_words['Word'], top_10_words['Count'])\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the top 10 most common words')\n    plt.xticks(rotation=90)\n\n    if save_path is not None:\n        # Save the plot to a file\n        plt.savefig(save_path)\n        return None\n    else:\n        # Display the plot\n        plt.show()\n        return plt.gca()", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File {file_path} not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n    # Check if the CSV file has a header\n    if 'Text' in df.columns:\n        text_data = df['Text']\n    else:\n        text_data = df[df.columns[0]]\n\n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n\n    # Fit and transform the text data\n    X = vectorizer.fit_transform(text_data)\n\n    # Get the word counts\n    word_counts = vectorizer.get_feature_names_out()\n    count_values = X.toarray().sum(axis=0)\n\n    # Create a DataFrame for word counts\n    df_word_counts = pd.DataFrame(list(zip(word_counts, count_values)), columns=['Word', 'Count'])\n\n    # Get the top 10 most common words\n    top_10_words = df_word_counts.nlargest(10, 'Count')\n\n    # Plot the histogram\n    plt.figure(figsize=(10, 5))\n    plt.bar(top_10_words['Word'], top_10_words['Count'])\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the top 10 most common words')\n    plt.xticks(rotation=90)\n\n    if save_path is not None:\n        # Save the plot to a file\n        plt.savefig(save_path)\n        return None\n    else:\n        # Display the plot\n        plt.show()\n        return plt.gca()"}
{"task_id": "BigCodeBench/1054", "solution": "import csv\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    try:\n        with open(file_path, 'r') as file:\n            reader = csv.reader(file)\n            population = [float(row[0]) for row in reader if row]\n    except (FileNotFoundError, IOError) as e:\n        return f\"Error: {str(e)}\"\n    except Exception as e:\n        return f\"Unexpected error: {str(e)}\"\n\n    sample = np.random.choice(population, size=30, replace=False)\n    sample_mean = np.mean(sample)\n    sample_stddev = np.std(sample, ddof=1)\n\n    fig, ax = plt.subplots()\n    ax.hist(sample, bins='auto', density=True, alpha=0.7, color='g', label='Sample Histogram')\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, sample_mean, sample_stddev)\n    ax.plot(x, p, 'k', linewidth=2, label='Normal Distribution')\n    ax.legend()\n    plt.title('Sample Mean: {:.2f}, Sample Standard Deviation: {:.2f}'.format(sample_mean, sample_stddev))\n    plt.show()\n\n    return (sample_mean, sample_stddev, ax)", "raw_solution": "import csv\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    try:\n        with open(file_path, 'r') as file:\n            reader = csv.reader(file)\n            population = [float(row[0]) for row in reader if row]\n    except (FileNotFoundError, IOError) as e:\n        return f\"Error: {str(e)}\"\n    except Exception as e:\n        return f\"Unexpected error: {str(e)}\"\n\n    sample = np.random.choice(population, size=30, replace=False)\n    sample_mean = np.mean(sample)\n    sample_stddev = np.std(sample, ddof=1)\n\n    fig, ax = plt.subplots()\n    ax.hist(sample, bins='auto', density=True, alpha=0.7, color='g', label='Sample Histogram')\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, sample_mean, sample_stddev)\n    ax.plot(x, p, 'k', linewidth=2, label='Normal Distribution')\n    ax.legend()\n    plt.title('Sample Mean: {:.2f}, Sample Standard Deviation: {:.2f}'.format(sample_mean, sample_stddev))\n    plt.show()\n\n    return (sample_mean, sample_stddev, ax)"}
{"task_id": "BigCodeBench/1055", "solution": "import pandas as pd\nimport itertools\nimport random\ndef task_func(colors, states):\n    # Generate all possible combinations of colors and states\n    combinations = list(itertools.product(colors, states))\n    \n    # Shuffle the combinations\n    random.shuffle(combinations)\n    \n    # Determine the number of columns\n    num_cols = min(len(colors), len(states))\n    \n    # Split the combinations into num_cols chunks\n    chunks = [combinations[i::num_cols] for i in range(num_cols)]\n    \n    # Create a DataFrame from the chunks\n    df = pd.DataFrame(chunks).T\n    \n    # Format the DataFrame\n    df.columns = [f\"Color:State {i+1}\" for i in range(num_cols)]\n    df = df.fillna(\"\")\n    \n    return df", "raw_solution": "import pandas as pd\nimport itertools\nimport random\n\ndef task_func(colors, states):\n    # Generate all possible combinations of colors and states\n    combinations = list(itertools.product(colors, states))\n    \n    # Shuffle the combinations\n    random.shuffle(combinations)\n    \n    # Determine the number of columns\n    num_cols = min(len(colors), len(states))\n    \n    # Split the combinations into num_cols chunks\n    chunks = [combinations[i::num_cols] for i in range(num_cols)]\n    \n    # Create a DataFrame from the chunks\n    df = pd.DataFrame(chunks).T\n    \n    # Format the DataFrame\n    df.columns = [f\"Color:State {i+1}\" for i in range(num_cols)]\n    df = df.fillna(\"\")\n    \n    return df"}
{"task_id": "BigCodeBench/1056", "solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\nNUMBERS = list(range(1, 27))\ndef task_func(n_pairs=26):\n    if n_pairs < 1 or n_pairs > 26:\n        raise ValueError(\"'n_pairs' must be between 1 and 26, inclusive.\")\n\n    # Generate letter-number pairs\n    pairs = [f\"{LETTERS[i]}:{NUMBERS[i]}\" for i in range(n_pairs)]\n    random.shuffle(pairs)\n\n    # Generate random counts\n    counts = [random.randint(1, 9) for _ in range(n_pairs)]\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    bars = ax.bar(pairs, counts)\n\n    # Label bars with their corresponding pair\n    ax.bar_label(bars, labels=pairs)\n\n    # Set chart title and labels\n    ax.set_title(\"Random Letter:Number Pairs Chart\")\n    ax.set_xlabel(\"Letter:Number Pairs\")\n    ax.set_ylabel(\"Counts\")\n\n    plt.show()\n\n    return bars", "raw_solution": "import numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\n# Constants\nLETTERS = list(\"abcdefghijklmnopqrstuvwxyz\")\nNUMBERS = list(range(1, 27))\n\ndef task_func(n_pairs=26):\n    if n_pairs < 1 or n_pairs > 26:\n        raise ValueError(\"'n_pairs' must be between 1 and 26, inclusive.\")\n\n    # Generate letter-number pairs\n    pairs = [f\"{LETTERS[i]}:{NUMBERS[i]}\" for i in range(n_pairs)]\n    random.shuffle(pairs)\n\n    # Generate random counts\n    counts = [random.randint(1, 9) for _ in range(n_pairs)]\n\n    # Create bar chart\n    fig, ax = plt.subplots()\n    bars = ax.bar(pairs, counts)\n\n    # Label bars with their corresponding pair\n    ax.bar_label(bars, labels=pairs)\n\n    # Set chart title and labels\n    ax.set_title(\"Random Letter:Number Pairs Chart\")\n    ax.set_xlabel(\"Letter:Number Pairs\")\n    ax.set_ylabel(\"Counts\")\n\n    plt.show()\n\n    return bars"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    if animals is None:\n        animals = ['Dog', 'Cat', 'Bird']\n    if foods is None:\n        foods = ['Bread', 'Milk', 'Fish']\n    if not animals and not foods:\n        return pd.DataFrame()\n\n    combinations = list(itertools.product(animals, foods))\n    np.random.shuffle(combinations)\n    df = pd.DataFrame(combinations, columns=foods)\n    df.index = animals\n    df = df.astype(str) + ':' + df\n    return df", "raw_solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    if animals is None:\n        animals = ['Dog', 'Cat', 'Bird']\n    if foods is None:\n        foods = ['Bread', 'Milk', 'Fish']\n    if not animals and not foods:\n        return pd.DataFrame()\n\n    combinations = list(itertools.product(animals, foods))\n    np.random.shuffle(combinations)\n    df = pd.DataFrame(combinations, columns=foods)\n    df.index = animals\n    df = df.astype(str) + ':' + df\n    return df\n\n# Testing the function"}
{"task_id": "BigCodeBench/1058", "solution": "import itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nSHAPES = [\n    \"Circle\",\n    \"Square\",\n    \"Triangle\",\n    \"Rectangle\",\n    \"Pentagon\",\n    \"Hexagon\",\n    \"Heptagon\",\n    \"Octagon\",\n    \"Nonagon\",\n    \"Decagon\",\n]\nCOLORS = [\n    \"Red\",\n    \"Blue\",\n    \"Green\",\n    \"Yellow\",\n    \"Black\",\n    \"White\",\n    \"Purple\",\n    \"Orange\",\n    \"Pink\",\n    \"Brown\",\n]\ndef task_func(num_pairs=10):\n    # Generate shape-color pairs\n    pairs = list(itertools.product(SHAPES, COLORS))\n    # Select the first 'num_pairs' pairs\n    pairs = pairs[:num_pairs]\n    # Create a DataFrame from the pairs\n    df = pd.DataFrame(pairs, columns=[\"Shape\", \"Color\"])\n    # Generate the countplot\n    ax = sns.countplot(x=\"Shape\", hue=\"Color\", data=df)\n    # Return the Axes object\n    return ax", "raw_solution": "import itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants\nSHAPES = [\n    \"Circle\",\n    \"Square\",\n    \"Triangle\",\n    \"Rectangle\",\n    \"Pentagon\",\n    \"Hexagon\",\n    \"Heptagon\",\n    \"Octagon\",\n    \"Nonagon\",\n    \"Decagon\",\n]\nCOLORS = [\n    \"Red\",\n    \"Blue\",\n    \"Green\",\n    \"Yellow\",\n    \"Black\",\n    \"White\",\n    \"Purple\",\n    \"Orange\",\n    \"Pink\",\n    \"Brown\",\n]\n\ndef task_func(num_pairs=10):\n    # Generate shape-color pairs\n    pairs = list(itertools.product(SHAPES, COLORS))\n    # Select the first 'num_pairs' pairs\n    pairs = pairs[:num_pairs]\n    # Create a DataFrame from the pairs\n    df = pd.DataFrame(pairs, columns=[\"Shape\", \"Color\"])\n    # Generate the countplot\n    ax = sns.countplot(x=\"Shape\", hue=\"Color\", data=df)\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/1059", "solution": "import numpy as np\nimport random\nimport itertools\nimport pandas as pd\nPLANETS = [\n    \"Mercury\",\n    \"Venus\",\n    \"Earth\",\n    \"Mars\",\n    \"Jupiter\",\n    \"Saturn\",\n    \"Uranus\",\n    \"Neptune\",\n]\nELEMENTS = [\n    \"Hydrogen\",\n    \"Helium\",\n    \"Oxygen\",\n    \"Carbon\",\n    \"Nitrogen\",\n    \"Magnesium\",\n    \"Silicon\",\n    \"Iron\",\n    \"Nickel\",\n]\ndef task_func():\n    \"\"\"\n    Generate a DataFrame where each row contains random planet-element pairs.\n    Each pair is formatted as 'Planet:Element'. The number of rows is determined\n    by the number of planets, and each row will contain as many planet-element pairs\n    as there are elements.\n    \"\"\"\n    # Create a list of planet-element pairs\n    pairs = list(itertools.product(PLANETS, ELEMENTS))\n\n    # Shuffle the list to ensure randomness\n    random.shuffle(pairs)\n\n    # Create a DataFrame from the list of pairs\n    df = pd.DataFrame(pairs, columns=['Planet:Element'])\n\n    return df", "raw_solution": "import numpy as np\nimport random\nimport itertools\nimport pandas as pd\n\n# Constants\nPLANETS = [\n    \"Mercury\",\n    \"Venus\",\n    \"Earth\",\n    \"Mars\",\n    \"Jupiter\",\n    \"Saturn\",\n    \"Uranus\",\n    \"Neptune\",\n]\nELEMENTS = [\n    \"Hydrogen\",\n    \"Helium\",\n    \"Oxygen\",\n    \"Carbon\",\n    \"Nitrogen\",\n    \"Magnesium\",\n    \"Silicon\",\n    \"Iron\",\n    \"Nickel\",\n]\n\ndef task_func():\n    \"\"\"\n    Generate a DataFrame where each row contains random planet-element pairs.\n    Each pair is formatted as 'Planet:Element'. The number of rows is determined\n    by the number of planets, and each row will contain as many planet-element pairs\n    as there are elements.\n    \"\"\"\n    # Create a list of planet-element pairs\n    pairs = list(itertools.product(PLANETS, ELEMENTS))\n\n    # Shuffle the list to ensure randomness\n    random.shuffle(pairs)\n\n    # Create a DataFrame from the list of pairs\n    df = pd.DataFrame(pairs, columns=['Planet:Element'])\n\n    return df\n\n# Test the function"}
{"task_id": "BigCodeBench/1060", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame, column_name: str) -> (str, plt.Axes):\n    if df.empty or column_name not in df.columns or df[column_name].isnull().all():\n        plt.figure()\n        plt.title(f\"Distribution of values in {column_name} (No Data)\")\n        plt.xlabel(\"Values\")\n        plt.ylabel(\"Frequency\")\n        plt.hist([], alpha=0.7, edgecolor='black')\n        plt.tight_layout()\n        return \"The DataFrame is empty or the specified column has no data.\", plt.gca()\n\n    unique_values = df[column_name].nunique()\n    total_values = df[column_name].count()\n    uniform_distribution = unique_values / total_values\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(df[column_name], bins=unique_values, alpha=0.7, edgecolor='black')\n    plt.xlabel(\"Values\")\n    plt.ylabel(\"Frequency\")\n    plt.title(f\"Distribution of values in {column_name}\")\n    plt.tight_layout()\n\n    if uniform_distribution == 1:\n        return \"The distribution of values is uniform.\", plt.gca()\n    else:\n        return \"The distribution of values is not uniform.\", plt.gca()", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame, column_name: str) -> (str, plt.Axes):\n    if df.empty or column_name not in df.columns or df[column_name].isnull().all():\n        plt.figure()\n        plt.title(f\"Distribution of values in {column_name} (No Data)\")\n        plt.xlabel(\"Values\")\n        plt.ylabel(\"Frequency\")\n        plt.hist([], alpha=0.7, edgecolor='black')\n        plt.tight_layout()\n        return \"The DataFrame is empty or the specified column has no data.\", plt.gca()\n\n    unique_values = df[column_name].nunique()\n    total_values = df[column_name].count()\n    uniform_distribution = unique_values / total_values\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(df[column_name], bins=unique_values, alpha=0.7, edgecolor='black')\n    plt.xlabel(\"Values\")\n    plt.ylabel(\"Frequency\")\n    plt.title(f\"Distribution of values in {column_name}\")\n    plt.tight_layout()\n\n    if uniform_distribution == 1:\n        return \"The distribution of values is uniform.\", plt.gca()\n    else:\n        return \"The distribution of values is not uniform.\", plt.gca()"}
{"task_id": "BigCodeBench/1061", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\ndef task_func(arr: np.ndarray) -> (plt.Axes, np.ndarray):\n    # Calculate the sum of elements in each row of the array\n    row_sums = np.sum(arr, axis=1)\n\n    # Normalize the row sums\n    mean = np.mean(row_sums)\n    std_dev = np.std(row_sums)\n    if std_dev == 0:\n        normalized_data = np.zeros_like(row_sums)\n    else:\n        normalized_data = (row_sums - mean) / std_dev\n\n    # Plot the histogram of the normalized data\n    fig, ax = plt.subplots()\n    ax.hist(normalized_data, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the PDF of a standard normal distribution\n    x = np.linspace(-4, 4, 1000)\n    pdf = norm.pdf(x)\n    ax.plot(x, pdf, color='r', linewidth=2)\n\n    # Set the title of the plot\n    ax.set_title(\"Histogram of Normalized Data with Standard Normal PDF\")\n\n    return ax, normalized_data", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef task_func(arr: np.ndarray) -> (plt.Axes, np.ndarray):\n    # Calculate the sum of elements in each row of the array\n    row_sums = np.sum(arr, axis=1)\n\n    # Normalize the row sums\n    mean = np.mean(row_sums)\n    std_dev = np.std(row_sums)\n    if std_dev == 0:\n        normalized_data = np.zeros_like(row_sums)\n    else:\n        normalized_data = (row_sums - mean) / std_dev\n\n    # Plot the histogram of the normalized data\n    fig, ax = plt.subplots()\n    ax.hist(normalized_data, bins=30, density=True, alpha=0.6, color='g')\n\n    # Plot the PDF of a standard normal distribution\n    x = np.linspace(-4, 4, 1000)\n    pdf = norm.pdf(x)\n    ax.plot(x, pdf, color='r', linewidth=2)\n\n    # Set the title of the plot\n    ax.set_title(\"Histogram of Normalized Data with Standard Normal PDF\")\n\n    return ax, normalized_data"}
{"task_id": "BigCodeBench/1062", "solution": "import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndef task_func(arr):\n    \"\"\"\n    Calculates the sum of each row in a 2D numpy array and plots these sums as a time series.\n    \"\"\"\n    # Create a date range starting from January 1, 2020\n    dates = pd.date_range(start='01-01-2020', periods=arr.shape[1])\n\n    if arr.size == 0:  # If the array is empty\n        fig, ax = plt.subplots()\n        ax.set_title('Time Series of Row Sums')\n        return ax\n\n    # Calculate the sum of each row\n    row_sums = arr.sum(axis=1)\n\n    # Create a DataFrame with the sums and dates\n    df = pd.DataFrame({'Date': dates, 'Sum': row_sums})\n\n    # Plot the sums as a time series\n    fig, ax = plt.subplots()\n    df.plot(x='Date', y='Sum', ax=ax)\n    ax.set_title('Time Series of Row Sums')\n\n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef task_func(arr):\n    \"\"\"\n    Calculates the sum of each row in a 2D numpy array and plots these sums as a time series.\n    \"\"\"\n    # Create a date range starting from January 1, 2020\n    dates = pd.date_range(start='01-01-2020', periods=arr.shape[1])\n\n    if arr.size == 0:  # If the array is empty\n        fig, ax = plt.subplots()\n        ax.set_title('Time Series of Row Sums')\n        return ax\n\n    # Calculate the sum of each row\n    row_sums = arr.sum(axis=1)\n\n    # Create a DataFrame with the sums and dates\n    df = pd.DataFrame({'Date': dates, 'Sum': row_sums})\n\n    # Plot the sums as a time series\n    fig, ax = plt.subplots()\n    df.plot(x='Date', y='Sum', ax=ax)\n    ax.set_title('Time Series of Row Sums')\n\n    return ax"}
{"task_id": "BigCodeBench/1063", "solution": "from matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\ndef task_func(arr):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the sum of rows of a 2D numpy array and plots the explained variance ratio.\n    \"\"\"\n    # Perform PCA\n    pca = PCA(n_components=1)\n    pca.fit(arr)\n\n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n    ax.set_title(\"Explained Variance Ratio of Principal Components\")\n    ax.set_xlabel(\"Principal Component\")\n    ax.set_ylabel(\"Explained Variance Ratio\")\n\n    return ax\narr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])", "raw_solution": "from matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\ndef task_func(arr):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the sum of rows of a 2D numpy array and plots the explained variance ratio.\n    \"\"\"\n    # Perform PCA\n    pca = PCA(n_components=1)\n    pca.fit(arr)\n\n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n    ax.set_title(\"Explained Variance Ratio of Principal Components\")\n    ax.set_xlabel(\"Principal Component\")\n    ax.set_ylabel(\"Explained Variance Ratio\")\n\n    return ax\n\n# Test the function\narr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nax = task_func(arr)\nplt.show()"}
{"task_id": "BigCodeBench/1064", "solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(arr):\n    \"\"\"\n    Plots a heatmap of a given 2D numerical array and prints the sum of each row.\n    The heatmap's color range is set based on the minimum and maximum values in the array.\n    \"\"\"\n    # Calculate the sum of each row\n    row_sums = np.sum(arr, axis=1)\n    print(f\"Sum of each row: {row_sums}\")\n\n    # Create a heatmap\n    min_val = np.min(arr)\n    max_val = np.max(arr)\n    ax = sns.heatmap(arr, vmin=min_val, vmax=max_val)\n\n    return ax\narr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])", "raw_solution": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(arr):\n    \"\"\"\n    Plots a heatmap of a given 2D numerical array and prints the sum of each row.\n    The heatmap's color range is set based on the minimum and maximum values in the array.\n    \"\"\"\n    # Calculate the sum of each row\n    row_sums = np.sum(arr, axis=1)\n    print(f\"Sum of each row: {row_sums}\")\n\n    # Create a heatmap\n    min_val = np.min(arr)\n    max_val = np.max(arr)\n    ax = sns.heatmap(arr, vmin=min_val, vmax=max_val)\n\n    return ax\n\n# Test the function\narr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nax = task_func(arr)\nplt.show()"}
{"task_id": "BigCodeBench/1065", "solution": "from scipy import fftpack\nfrom matplotlib import pyplot as plt\nimport numpy as np\ndef task_func(arr):\n    \"\"\"\n    Performs a Fast Fourier Transform (FFT) on the sum of each row in a 2D array and plots the absolute values of the FFT coefficients.\n    \"\"\"\n    # Perform FFT on each row\n    fft_rows = [fftpack.fft(row) for row in arr]\n\n    # Compute absolute values of FFT coefficients\n    abs_fft_rows = [np.abs(fft_row) for fft_row in fft_rows]\n\n    # Plot the absolute values of the FFT coefficients\n    fig, ax = plt.subplots()\n    ax.imshow(abs_fft_rows, cmap='hot', interpolation='nearest')\n    ax.set_title(\"Absolute values of FFT coefficients\")\n    ax.set_xlabel(\"FFT coefficient index\")\n    ax.set_ylabel(\"Row index\")\n\n    return ax", "raw_solution": "from scipy import fftpack\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef task_func(arr):\n    \"\"\"\n    Performs a Fast Fourier Transform (FFT) on the sum of each row in a 2D array and plots the absolute values of the FFT coefficients.\n    \"\"\"\n    # Perform FFT on each row\n    fft_rows = [fftpack.fft(row) for row in arr]\n\n    # Compute absolute values of FFT coefficients\n    abs_fft_rows = [np.abs(fft_row) for fft_row in fft_rows]\n\n    # Plot the absolute values of the FFT coefficients\n    fig, ax = plt.subplots()\n    ax.imshow(abs_fft_rows, cmap='hot', interpolation='nearest')\n    ax.set_title(\"Absolute values of FFT coefficients\")\n    ax.set_xlabel(\"FFT coefficient index\")\n    ax.set_ylabel(\"Row index\")\n\n    return ax"}
{"task_id": "BigCodeBench/1066", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nNUM_SAMPLES = 100\nNUM_OUTLIERS = 5\ndef task_func(num_samples=NUM_SAMPLES, num_outliers=NUM_OUTLIERS):\n    # Generate normally distributed data\n    data = np.random.normal(loc=0.0, scale=1.0, size=num_samples)\n\n    # Generate artificially introduced outliers\n    outliers = np.random.uniform(low=-3.0, high=3.0, size=num_outliers)\n    data = np.append(data, outliers)\n\n    # Detect outliers using IQR method\n    q1, q3 = np.percentile(data[:num_samples], [25, 75])\n    iqr = q3 - q1\n    lower_bound = q1 - (iqr * 1.5)\n    upper_bound = q3 + (iqr * 1.5)\n    outliers_detected = data[(data < lower_bound) | (data > upper_bound)]\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=20, color='skyblue', edgecolor='black')\n    ax.axvline(x=lower_bound, color='red', linestyle='dashed', linewidth=1, label='Lower bound')\n    ax.axvline(x=upper_bound, color='red', linestyle='dashed', linewidth=1, label='Upper bound')\n    ax.legend()\n\n    return data, outliers_detected, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nNUM_SAMPLES = 100\nNUM_OUTLIERS = 5\n\ndef task_func(num_samples=NUM_SAMPLES, num_outliers=NUM_OUTLIERS):\n    # Generate normally distributed data\n    data = np.random.normal(loc=0.0, scale=1.0, size=num_samples)\n\n    # Generate artificially introduced outliers\n    outliers = np.random.uniform(low=-3.0, high=3.0, size=num_outliers)\n    data = np.append(data, outliers)\n\n    # Detect outliers using IQR method\n    q1, q3 = np.percentile(data[:num_samples], [25, 75])\n    iqr = q3 - q1\n    lower_bound = q1 - (iqr * 1.5)\n    upper_bound = q3 + (iqr * 1.5)\n    outliers_detected = data[(data < lower_bound) | (data > upper_bound)]\n\n    # Plot histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=20, color='skyblue', edgecolor='black')\n    ax.axvline(x=lower_bound, color='red', linestyle='dashed', linewidth=1, label='Lower bound')\n    ax.axvline(x=upper_bound, color='red', linestyle='dashed', linewidth=1, label='Upper bound')\n    ax.legend()\n\n    return data, outliers_detected, ax\n\n# Test the function\ndata, outliers_detected, ax = task_func()"}
{"task_id": "BigCodeBench/1067", "solution": "import requests\nimport logging\nfrom requests.exceptions import HTTPError, RequestException\ndef task_func(repo_url: str) -> dict:\n    \"\"\"\n    Fetches and returns information about a GitHub repository using its API URL.\n    \"\"\"\n    try:\n        response = requests.get(repo_url)\n        # If the response was successful, no Exception will be raised\n        response.raise_for_status()\n    except HTTPError as http_err:\n        logging.error(f'HTTP error occurred: {http_err}')\n        raise\n    except RequestException as req_err:\n        logging.error(f'Request error occurred: {req_err}')\n        raise\n    else:\n        repo_info = response.json()\n        open_issues_count = repo_info.get('open_issues_count', 0)\n        if open_issues_count > 100:\n            logging.warning(f'The repository has more than 100 open issues: {open_issues_count}')\n        return repo_info", "raw_solution": "import requests\nimport logging\nfrom requests.exceptions import HTTPError, RequestException\n\ndef task_func(repo_url: str) -> dict:\n    \"\"\"\n    Fetches and returns information about a GitHub repository using its API URL.\n    \"\"\"\n    try:\n        response = requests.get(repo_url)\n        # If the response was successful, no Exception will be raised\n        response.raise_for_status()\n    except HTTPError as http_err:\n        logging.error(f'HTTP error occurred: {http_err}')\n        raise\n    except RequestException as req_err:\n        logging.error(f'Request error occurred: {req_err}')\n        raise\n    else:\n        repo_info = response.json()\n        open_issues_count = repo_info.get('open_issues_count', 0)\n        if open_issues_count > 100:\n            logging.warning(f'The repository has more than 100 open issues: {open_issues_count}')\n        return repo_info"}
{"task_id": "BigCodeBench/1068", "solution": "import warnings\nimport sqlite3\nimport pandas as pd\nfrom pandas import DataFrame\ndef task_func(db_path: str, query: str, warn_large_dataset: bool = True) -> DataFrame:\n    \"\"\"\n    Fetches data from an SQLite database using the provided database path and SQL query.\n    If the data contains more than 10000 rows, a warning is issued.\n    If any error occurs during database connection, SQL query execution, or data fetching, an exception is raised with a detailed error message.\n    \"\"\"\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(db_path)\n    except Exception as e:\n        raise Exception(f\"Error connecting to the database: {str(e)}\")\n\n    try:\n        # Execute the SQL query\n        cursor = conn.execute(query)\n    except Exception as e:\n        raise Exception(f\"Error executing the query: {str(e)}\")\n\n    try:\n        # Fetch the data\n        data = cursor.fetchall()\n    except Exception as e:\n        raise Exception(f\"Error fetching data from the database: {str(e)}\")\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    # If the data contains more than 10000 rows, issue a warning\n    if warn_large_dataset and len(df) > 10000:\n        warnings.warn(\"The data contains more than 10000 rows.\")\n\n    return df", "raw_solution": "import warnings\nimport sqlite3\nimport pandas as pd\nfrom pandas import DataFrame\n\ndef task_func(db_path: str, query: str, warn_large_dataset: bool = True) -> DataFrame:\n    \"\"\"\n    Fetches data from an SQLite database using the provided database path and SQL query.\n    If the data contains more than 10000 rows, a warning is issued.\n    If any error occurs during database connection, SQL query execution, or data fetching, an exception is raised with a detailed error message.\n    \"\"\"\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(db_path)\n    except Exception as e:\n        raise Exception(f\"Error connecting to the database: {str(e)}\")\n\n    try:\n        # Execute the SQL query\n        cursor = conn.execute(query)\n    except Exception as e:\n        raise Exception(f\"Error executing the query: {str(e)}\")\n\n    try:\n        # Fetch the data\n        data = cursor.fetchall()\n    except Exception as e:\n        raise Exception(f\"Error fetching data from the database: {str(e)}\")\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    # If the data contains more than 10000 rows, issue a warning\n    if warn_large_dataset and len(df) > 10000:\n        warnings.warn(\"The data contains more than 10000 rows.\")\n\n    return df"}
{"task_id": "BigCodeBench/1069", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(data_dict):\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Initialize an empty list to store the Axes objects\n    axes = []\n\n    # Iterate over each column in the DataFrame\n    for col in df.columns:\n        # Create a histogram for the column\n        ax = df[col].hist(bins=10, alpha=0.5, legend=True)\n        axes.append(ax)\n\n        # Check if the distribution is uniform\n        if not df[col].nunique() == df[col].size:\n            print(f\"The distribution of '{col}' is not uniform.\")\n\n    # Return the list of Axes objects\n    return axes", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict):\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(data_dict)\n\n    # Initialize an empty list to store the Axes objects\n    axes = []\n\n    # Iterate over each column in the DataFrame\n    for col in df.columns:\n        # Create a histogram for the column\n        ax = df[col].hist(bins=10, alpha=0.5, legend=True)\n        axes.append(ax)\n\n        # Check if the distribution is uniform\n        if not df[col].nunique() == df[col].size:\n            print(f\"The distribution of '{col}' is not uniform.\")\n\n    # Return the list of Axes objects\n    return axes"}
{"task_id": "BigCodeBench/1070", "solution": "import pandas as pd\nfrom random import shuffle\nPOSSIBLE_VALUES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]\ndef task_func(list_of_lists):\n    \"\"\"\n    Generates a list of pandas DataFrames, each created from a sublist in 'list_of_lists'.\n    Each DataFrame has columns named as per the elements of the sublist, and each column is filled with\n    randomly shuffled values from 'POSSIBLE_VALUES'.\n    \"\"\"\n    # Initialize an empty list to store the DataFrames\n    dataframes = []\n\n    # Iterate over each sublist in 'list_of_lists'\n    for sublist in list_of_lists:\n        # Shuffle 'POSSIBLE_VALUES'\n        shuffle(POSSIBLE_VALUES)\n\n        # Create a DataFrame from the sublist and 'POSSIBLE_VALUES'\n        df = pd.DataFrame(POSSIBLE_VALUES, columns=sublist)\n\n        # Append the DataFrame to 'dataframes'\n        dataframes.append(df)\n\n    # Return the list of DataFrames\n    return dataframes", "raw_solution": "import pandas as pd\nfrom random import shuffle\n\n# Constants\nPOSSIBLE_VALUES = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]\n\ndef task_func(list_of_lists):\n    \"\"\"\n    Generates a list of pandas DataFrames, each created from a sublist in 'list_of_lists'.\n    Each DataFrame has columns named as per the elements of the sublist, and each column is filled with\n    randomly shuffled values from 'POSSIBLE_VALUES'.\n    \"\"\"\n    # Initialize an empty list to store the DataFrames\n    dataframes = []\n\n    # Iterate over each sublist in 'list_of_lists'\n    for sublist in list_of_lists:\n        # Shuffle 'POSSIBLE_VALUES'\n        shuffle(POSSIBLE_VALUES)\n\n        # Create a DataFrame from the sublist and 'POSSIBLE_VALUES'\n        df = pd.DataFrame(POSSIBLE_VALUES, columns=sublist)\n\n        # Append the DataFrame to 'dataframes'\n        dataframes.append(df)\n\n    # Return the list of DataFrames\n    return dataframes"}
{"task_id": "BigCodeBench/1071", "solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\nimport numpy as np\nfrom random import shuffle\nCOLORS = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\ndef task_func(list_of_lists):\n    \"\"\"\n    Plots a series of lines for each list in `list_of_lists`. Each line is plotted with shuffled y-values and sequential x-values starting from 1.\n    The function shuffles the y-values of each inner list before plotting. Each line is plotted with a different color from a predetermined set of colors.\n    The function cycles through these colors for each inner list.\n    \"\"\"\n    # Create a cycle iterator for colors\n    color_cycle = cycle(COLORS)\n\n    # Create a figure and axes\n    fig, ax = plt.subplots()\n\n    # Iterate over each list in list_of_lists\n    for lst in list_of_lists:\n        # If the list is not empty\n        if lst:\n            # Shuffle the y-values of the list\n            shuffle(lst)\n\n            # Get the next color from the color cycle\n            color = next(color_cycle)\n\n            # Plot the list as a line with the next color\n            ax.plot(np.arange(1, len(lst) + 1), lst, color=color)\n\n    # Return the figure and axes\n    return fig, ax", "raw_solution": "import matplotlib.pyplot as plt\nfrom itertools import cycle\nimport numpy as np\nfrom random import shuffle\n\nCOLORS = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\n\ndef task_func(list_of_lists):\n    \"\"\"\n    Plots a series of lines for each list in `list_of_lists`. Each line is plotted with shuffled y-values and sequential x-values starting from 1.\n    The function shuffles the y-values of each inner list before plotting. Each line is plotted with a different color from a predetermined set of colors.\n    The function cycles through these colors for each inner list.\n    \"\"\"\n    # Create a cycle iterator for colors\n    color_cycle = cycle(COLORS)\n\n    # Create a figure and axes\n    fig, ax = plt.subplots()\n\n    # Iterate over each list in list_of_lists\n    for lst in list_of_lists:\n        # If the list is not empty\n        if lst:\n            # Shuffle the y-values of the list\n            shuffle(lst)\n\n            # Get the next color from the color cycle\n            color = next(color_cycle)\n\n            # Plot the list as a line with the next color\n            ax.plot(np.arange(1, len(lst) + 1), lst, color=color)\n\n    # Return the figure and axes\n    return fig, ax"}
{"task_id": "BigCodeBench/1072", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(list_of_lists):\n    np.random.seed(0)\n    series_list = []\n    for sublist in list_of_lists:\n        n = len(sublist)\n        values = np.arange(1, n+1)\n        np.random.shuffle(values)\n        series = pd.Series(values, index=sublist)\n        series_list.append(series)\n    return series_list\nlist_of_lists = [['a', 'b', 'c'], ['x', 'y', 'z', 'w'], ['p', 'q', 'r', 's', 't']]", "raw_solution": "import pandas as pd\nimport numpy as np\n\ndef task_func(list_of_lists):\n    np.random.seed(0)\n    series_list = []\n    for sublist in list_of_lists:\n        n = len(sublist)\n        values = np.arange(1, n+1)\n        np.random.shuffle(values)\n        series = pd.Series(values, index=sublist)\n        series_list.append(series)\n    return series_list\n\n# Test the function\nlist_of_lists = [['a', 'b', 'c'], ['x', 'y', 'z', 'w'], ['p', 'q', 'r', 's', 't']]\nseries_list = task_func(list_of_lists)\nfor series in series_list:\n    print(series)"}
{"task_id": "BigCodeBench/1073", "solution": "import time\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\ndef task_func(time_strings, time_format=\"%d/%m/%Y %H:%M:%S.%f\"):\n    \"\"\"\n    Parses a list of time strings and plots a histogram of the seconds component.\n    \"\"\"\n    try:\n        # Parse the time strings\n        parsed_times = [datetime.strptime(t, time_format) for t in time_strings]\n\n        # Extract the seconds component\n        seconds_list = [t.second for t in parsed_times]\n\n        # Plot the histogram\n        fig, ax = plt.subplots()\n        ax.hist(seconds_list, bins=60, edgecolor='black', color='skyblue')\n        ax.set_xlabel('Seconds')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Histogram of Seconds Component')\n        plt.show()\n\n        return ax\n    except ValueError:\n        # If a time string cannot be parsed, raise a ValueError\n        print(\"ValueError: One or more time strings could not be parsed.\")\n        return None", "raw_solution": "import time\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\ndef task_func(time_strings, time_format=\"%d/%m/%Y %H:%M:%S.%f\"):\n    \"\"\"\n    Parses a list of time strings and plots a histogram of the seconds component.\n    \"\"\"\n    try:\n        # Parse the time strings\n        parsed_times = [datetime.strptime(t, time_format) for t in time_strings]\n\n        # Extract the seconds component\n        seconds_list = [t.second for t in parsed_times]\n\n        # Plot the histogram\n        fig, ax = plt.subplots()\n        ax.hist(seconds_list, bins=60, edgecolor='black', color='skyblue')\n        ax.set_xlabel('Seconds')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Histogram of Seconds Component')\n        plt.show()\n\n        return ax\n    except ValueError:\n        # If a time string cannot be parsed, raise a ValueError\n        print(\"ValueError: One or more time strings could not be parsed.\")\n        return None"}
{"task_id": "BigCodeBench/1074", "solution": "from datetime import datetime\nimport pytz\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_string, from_tz, to_tz):\n    \"\"\"\n    Converts a time string from one timezone to another, considering daylight saving time.\n    \"\"\"\n    # Parse the time string into a datetime object\n    dt = datetime.strptime(time_string, TIME_FORMAT)\n\n    # Localize the datetime object to the source timezone\n    dt = dt.replace(tzinfo=pytz.timezone(from_tz))\n\n    # Convert to the target timezone\n    dt = dt.astimezone(pytz.timezone(to_tz))\n\n    # Format the datetime object back into a string\n    time_string = dt.strftime(TIME_FORMAT)\n\n    return time_string", "raw_solution": "from datetime import datetime\nimport pytz\n\n# Constants\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\n\ndef task_func(time_string, from_tz, to_tz):\n    \"\"\"\n    Converts a time string from one timezone to another, considering daylight saving time.\n    \"\"\"\n    # Parse the time string into a datetime object\n    dt = datetime.strptime(time_string, TIME_FORMAT)\n\n    # Localize the datetime object to the source timezone\n    dt = dt.replace(tzinfo=pytz.timezone(from_tz))\n\n    # Convert to the target timezone\n    dt = dt.astimezone(pytz.timezone(to_tz))\n\n    # Format the datetime object back into a string\n    time_string = dt.strftime(TIME_FORMAT)\n\n    return time_string\n\n# Test the function"}
{"task_id": "BigCodeBench/1075", "solution": "import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_strings):\n    \"\"\"\n    Computes the differences in seconds with integer values between consecutive datetime strings and plots these differences as a bar chart.\n    \"\"\"\n    # Convert the datetime strings to datetime objects\n    time_objs = [datetime.datetime.strptime(time, TIME_FORMAT) for time in time_strings]\n\n    # Compute the differences in seconds\n    diffs = [(time_objs[i+1] - time_objs[i]).total_seconds() for i in range(len(time_objs)-1)]\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(len(diffs)), diffs)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Time Difference (s)')\n    ax.set_title('Time Difference Between Consecutive Datetime Strings')\n\n    return ax", "raw_solution": "import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\n\ndef task_func(time_strings):\n    \"\"\"\n    Computes the differences in seconds with integer values between consecutive datetime strings and plots these differences as a bar chart.\n    \"\"\"\n    # Convert the datetime strings to datetime objects\n    time_objs = [datetime.datetime.strptime(time, TIME_FORMAT) for time in time_strings]\n\n    # Compute the differences in seconds\n    diffs = [(time_objs[i+1] - time_objs[i]).total_seconds() for i in range(len(time_objs)-1)]\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(range(len(diffs)), diffs)\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Time Difference (s)')\n    ax.set_title('Time Difference Between Consecutive Datetime Strings')\n\n    return ax"}
{"task_id": "BigCodeBench/1076", "solution": "from datetime import datetime\nimport pandas as pd\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\ndef task_func(time_strings, target_tz):\n    \"\"\"\n    Converts a list of time strings from UTC to a specified timezone and returns a DataFrame.\n    \"\"\"\n    # Initialize empty lists to store original and converted times\n    original_times = []\n    converted_times = []\n\n    # Iterate over each time string in the input list\n    for time_string in time_strings:\n        # Parse the time string into a datetime object\n        utc_time = datetime.strptime(time_string, TIME_FORMAT)\n\n        # Convert the UTC time to the target timezone\n        target_time = utc_time.astimezone(ZoneInfo(target_tz))\n\n        # Append the original and converted times to their respective lists\n        original_times.append(utc_time)\n        converted_times.append(target_time)\n\n    # Create a DataFrame from the original and converted times\n    df = pd.DataFrame({\n        'Original Time': original_times,\n        'Converted Time': converted_times\n    })\n\n    return df", "raw_solution": "from datetime import datetime\nimport pandas as pd\n\n# For Python versions lower than 3.9, use 'pytz' instead of 'zoneinfo'\ntry:\n    from zoneinfo import ZoneInfo\nexcept ImportError:\n    from pytz import timezone as ZoneInfo\n\nTIME_FORMAT = \"%d/%m/%y %H:%M:%S.%f\"\n\ndef task_func(time_strings, target_tz):\n    \"\"\"\n    Converts a list of time strings from UTC to a specified timezone and returns a DataFrame.\n    \"\"\"\n    # Initialize empty lists to store original and converted times\n    original_times = []\n    converted_times = []\n\n    # Iterate over each time string in the input list\n    for time_string in time_strings:\n        # Parse the time string into a datetime object\n        utc_time = datetime.strptime(time_string, TIME_FORMAT)\n\n        # Convert the UTC time to the target timezone\n        target_time = utc_time.astimezone(ZoneInfo(target_tz))\n\n        # Append the original and converted times to their respective lists\n        original_times.append(utc_time)\n        converted_times.append(target_time)\n\n    # Create a DataFrame from the original and converted times\n    df = pd.DataFrame({\n        'Original Time': original_times,\n        'Converted Time': converted_times\n    })\n\n    return df"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert all timestamps to the specified timezone\n    converted_times = []\n    for time_string in time_strings:\n        dt = datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S')\n        converted_times.append(pytz.timezone(timezone).localize(dt))\n\n    # Calculate the time difference in seconds between each consecutive pair of timestamps\n    time_diffs = []\n    for i in range(1, len(converted_times)):\n        diff = converted_times[i] - converted_times[i-1]\n        time_diffs.append(abs(diff.total_seconds()))\n\n    # Return the average time difference\n    return np.mean(time_diffs)", "raw_solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert all timestamps to the specified timezone\n    converted_times = []\n    for time_string in time_strings:\n        dt = datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S')\n        converted_times.append(pytz.timezone(timezone).localize(dt))\n\n    # Calculate the time difference in seconds between each consecutive pair of timestamps\n    time_diffs = []\n    for i in range(1, len(converted_times)):\n        diff = converted_times[i] - converted_times[i-1]\n        time_diffs.append(abs(diff.total_seconds()))\n\n    # Return the average time difference\n    return np.mean(time_diffs)"}
{"task_id": "BigCodeBench/1078", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(arr):\n    unique, counts = np.unique(arr, return_counts=True)\n    bin_edges = np.arange(len(unique) + 1) - 0.5\n    ax = plt.hist(arr, bins=bin_edges, edgecolor='black')\n    plt.xticks(unique)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Array Values')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n    uniform_distribution = np.all(counts == counts[0])\n    return uniform_distribution, ax\narr = np.array([1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4])", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(arr):\n    unique, counts = np.unique(arr, return_counts=True)\n    bin_edges = np.arange(len(unique) + 1) - 0.5\n    ax = plt.hist(arr, bins=bin_edges, edgecolor='black')\n    plt.xticks(unique)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Array Values')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n    uniform_distribution = np.all(counts == counts[0])\n    return uniform_distribution, ax\n\n# Test the function\narr = np.array([1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4])"}
{"task_id": "BigCodeBench/1079", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert string prices to float\n    data['Price'] = data['Price_String'].apply(lambda x: float(x.replace(',', '')))\n\n    # Calculate statistics\n    stats = {\n        'mean': data['Price'].mean(),\n        'median': data['Price'].median(),\n        'std_dev': data['Price'].std()\n    }\n\n    # Generate histogram\n    fig, ax = plt.subplots()\n    ax.hist(data['Price'], bins='auto', color='blue', alpha=0.7, rwidth=0.85)\n    ax.set_title('Histogram of Product Prices')\n    ax.set_xlabel('Price')\n    ax.set_ylabel('Frequency')\n\n    return stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert string prices to float\n    data['Price'] = data['Price_String'].apply(lambda x: float(x.replace(',', '')))\n\n    # Calculate statistics\n    stats = {\n        'mean': data['Price'].mean(),\n        'median': data['Price'].median(),\n        'std_dev': data['Price'].std()\n    }\n\n    # Generate histogram\n    fig, ax = plt.subplots()\n    ax.hist(data['Price'], bins='auto', color='blue', alpha=0.7, rwidth=0.85)\n    ax.set_title('Histogram of Product Prices')\n    ax.set_xlabel('Price')\n    ax.set_ylabel('Frequency')\n\n    return stats, ax"}
{"task_id": "BigCodeBench/1080", "solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nDATA = {\n    \"Area_String\": [\"1,000\", \"2,000\", \"3,000\", \"4,000\", \"5,000\"],\n    \"Price\": [100, 200, 300, 400, 500],\n}\ndef task_func(area_string, data=DATA):\n    # Convert the area_string to integer\n    data['Area'] = [int(area.replace(',', '')) for area in data['Area_String']]\n\n    # Create a dataframe from the data\n    df = pd.DataFrame(data)\n\n    # Reshape the data\n    X = df['Area'].values.reshape(-1, 1)\n    y = df['Price'].values.reshape(-1, 1)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X, y)\n\n    # Predict the price for the given area\n    area_value = int(area_string.replace(',', ''))\n    prediction = model.predict([[area_value]])\n\n    return float(prediction[0])", "raw_solution": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nDATA = {\n    \"Area_String\": [\"1,000\", \"2,000\", \"3,000\", \"4,000\", \"5,000\"],\n    \"Price\": [100, 200, 300, 400, 500],\n}\n\ndef task_func(area_string, data=DATA):\n    # Convert the area_string to integer\n    data['Area'] = [int(area.replace(',', '')) for area in data['Area_String']]\n\n    # Create a dataframe from the data\n    df = pd.DataFrame(data)\n\n    # Reshape the data\n    X = df['Area'].values.reshape(-1, 1)\n    y = df['Price'].values.reshape(-1, 1)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X, y)\n\n    # Predict the price for the given area\n    area_value = int(area_string.replace(',', ''))\n    prediction = model.predict([[area_value]])\n\n    return float(prediction[0])\n\n# Test the function"}
{"task_id": "BigCodeBench/1081", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(data=None):\n    if data is None:\n        data = {\n            'Weight_String': ['70', '75', '80', '85', '90'],\n            'Height': [170, 175, 180, 185, 190]\n        }\n\n    df = pd.DataFrame(data)\n\n    # Validate that all weights are strings\n    for weight in df['Weight_String']:\n        if not isinstance(weight, str):\n            raise ValueError(\"All weights must be strings\")\n\n    # Convert weights to floats\n    df['Weight'] = df['Weight_String'].astype(float)\n\n    # Plot weight vs height\n    ax = sns.scatterplot(x='Weight', y='Height', data=df)\n    ax.set_title('Weight vs Height')\n\n    return ax", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data=None):\n    if data is None:\n        data = {\n            'Weight_String': ['70', '75', '80', '85', '90'],\n            'Height': [170, 175, 180, 185, 190]\n        }\n\n    df = pd.DataFrame(data)\n\n    # Validate that all weights are strings\n    for weight in df['Weight_String']:\n        if not isinstance(weight, str):\n            raise ValueError(\"All weights must be strings\")\n\n    # Convert weights to floats\n    df['Weight'] = df['Weight_String'].astype(float)\n\n    # Plot weight vs height\n    ax = sns.scatterplot(x='Weight', y='Height', data=df)\n    ax.set_title('Weight vs Height')\n\n    return ax\n\n# Test the function\nax = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/1082", "solution": "import pandas as pd\nfrom scipy.stats import pearsonr\nimport numpy as np\ndef task_func(data):\n    \"\"\"\n    Calculates the Pearson correlation coefficient between numerical scores and categorical grades.\n    \"\"\"\n    # Check if data is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input data should be a pandas DataFrame\")\n\n    # Check if data has at least 2 rows\n    if len(data) < 2:\n        return np.nan\n\n    # Check if columns are named 'score' and 'grade'\n    if 'score' not in data.columns or 'grade' not in data.columns:\n        raise ValueError(\"Input data should have columns named 'score' and 'grade'\")\n\n    # Convert scores from string format to floats\n    data['score'] = pd.to_numeric(data['score'], errors='coerce')\n\n    # Encode categorical grades into numerical values based on their rank order\n    data['grade_rank'] = data['grade'].rank(method='min')\n\n    # Compute the Pearson correlation coefficient between the numerical scores and the encoded grades\n    correlation, _ = pearsonr(data['score'], data['grade_rank'])\n\n    return correlation", "raw_solution": "import pandas as pd\nfrom scipy.stats import pearsonr\nimport numpy as np\n\ndef task_func(data):\n    \"\"\"\n    Calculates the Pearson correlation coefficient between numerical scores and categorical grades.\n    \"\"\"\n    # Check if data is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input data should be a pandas DataFrame\")\n\n    # Check if data has at least 2 rows\n    if len(data) < 2:\n        return np.nan\n\n    # Check if columns are named 'score' and 'grade'\n    if 'score' not in data.columns or 'grade' not in data.columns:\n        raise ValueError(\"Input data should have columns named 'score' and 'grade'\")\n\n    # Convert scores from string format to floats\n    data['score'] = pd.to_numeric(data['score'], errors='coerce')\n\n    # Encode categorical grades into numerical values based on their rank order\n    data['grade_rank'] = data['grade'].rank(method='min')\n\n    # Compute the Pearson correlation coefficient between the numerical scores and the encoded grades\n    correlation, _ = pearsonr(data['score'], data['grade_rank'])\n\n    return correlation"}
{"task_id": "BigCodeBench/1083", "solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Processes a dataset containing salary information and experience, then plots normalized salary against experience.\n    \"\"\"\n    # 1. Input Validation\n    if not all(key in data for key in ('Salary_String', 'Experience')):\n        raise ValueError(\"Input data dictionary must contain 'Salary_String' and 'Experience' keys.\")\n\n    # 2. DataFrame Conversion\n    df = pd.DataFrame(data)\n\n    # 3. Empty Data Handling\n    if df.empty:\n        return plt.gca()\n\n    # 4. Salary Conversion\n    try:\n        df['Salary'] = df['Salary_String'].apply(lambda x: float(x.replace(',', '')))\n    except ValueError as e:\n        raise ValueError(\"Failed to convert salary strings to floats: {}\".format(e))\n\n    # 5. Salary Normalization\n    scaler = MinMaxScaler()\n    df[['Salary']] = scaler.fit_transform(df[['Salary']])\n\n    # 6. Data Plotting\n    ax = df.plot(kind='scatter', x='Experience', y='Salary', figsize=(8, 6))\n    ax.set_xlabel('Experience')\n    ax.set_ylabel('Normalized Salary')\n    ax.set_title('Normalized Salary against Experience')\n\n    return ax", "raw_solution": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    Processes a dataset containing salary information and experience, then plots normalized salary against experience.\n    \"\"\"\n    # 1. Input Validation\n    if not all(key in data for key in ('Salary_String', 'Experience')):\n        raise ValueError(\"Input data dictionary must contain 'Salary_String' and 'Experience' keys.\")\n\n    # 2. DataFrame Conversion\n    df = pd.DataFrame(data)\n\n    # 3. Empty Data Handling\n    if df.empty:\n        return plt.gca()\n\n    # 4. Salary Conversion\n    try:\n        df['Salary'] = df['Salary_String'].apply(lambda x: float(x.replace(',', '')))\n    except ValueError as e:\n        raise ValueError(\"Failed to convert salary strings to floats: {}\".format(e))\n\n    # 5. Salary Normalization\n    scaler = MinMaxScaler()\n    df[['Salary']] = scaler.fit_transform(df[['Salary']])\n\n    # 6. Data Plotting\n    ax = df.plot(kind='scatter', x='Experience', y='Salary', figsize=(8, 6))\n    ax.set_xlabel('Experience')\n    ax.set_ylabel('Normalized Salary')\n    ax.set_title('Normalized Salary against Experience')\n\n    return ax"}
{"task_id": "BigCodeBench/1084", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom typing import List, Tuple\ndef task_func(data_file_path: str) -> Tuple[pd.Series, pd.Series, List[plt.Axes], pd.DataFrame]:\n    \"\"\"\n    Analyzes numerical data from a CSV file.\n    \"\"\"\n    # Read the CSV file\n    df = pd.read_csv(data_file_path)\n\n    # Convert string representations of numbers with commas into floating point numbers\n    df = df.apply(pd.to_numeric, errors='ignore')\n\n    # Calculate the mean and standard deviation for each numerical column\n    means = df.mean()\n    std_devs = df.std()\n\n    # Generate a histogram plot for each numerical column\n    axes = []\n    for col in df.select_dtypes(include=[pd.np.number]).columns:\n        ax = df[col].hist(bins=10, figsize=(10, 5))\n        axes.append(ax)\n\n    # Perform an ANOVA test to check the statistical significance of differences between means of numerical columns\n    anova_results = pd.DataFrame()\n    if len(df.select_dtypes(include=[pd.np.number]).columns) > 1:\n        for col1 in df.select_dgroups_dtypes(include=[pd.np.number]).columns:\n            for col2 in df.select_dtypes(include=[pd.np.number]).columns:\n                if col1 != col2:\n                    f_val, p_val = stats.f_oneway(df[col1], df[col2])\n                    anova_results = anova_results.append(pd.Series({\"F-value\": f_val, \"P-value\": p_val}, name=f\"{col1} vs {col2}\"))\n\n    return means, std_devs, axes, anova_results", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom typing import List, Tuple\n\ndef task_func(data_file_path: str) -> Tuple[pd.Series, pd.Series, List[plt.Axes], pd.DataFrame]:\n    \"\"\"\n    Analyzes numerical data from a CSV file.\n    \"\"\"\n    # Read the CSV file\n    df = pd.read_csv(data_file_path)\n\n    # Convert string representations of numbers with commas into floating point numbers\n    df = df.apply(pd.to_numeric, errors='ignore')\n\n    # Calculate the mean and standard deviation for each numerical column\n    means = df.mean()\n    std_devs = df.std()\n\n    # Generate a histogram plot for each numerical column\n    axes = []\n    for col in df.select_dtypes(include=[pd.np.number]).columns:\n        ax = df[col].hist(bins=10, figsize=(10, 5))\n        axes.append(ax)\n\n    # Perform an ANOVA test to check the statistical significance of differences between means of numerical columns\n    anova_results = pd.DataFrame()\n    if len(df.select_dtypes(include=[pd.np.number]).columns) > 1:\n        for col1 in df.select_dgroups_dtypes(include=[pd.np.number]).columns:\n            for col2 in df.select_dtypes(include=[pd.np.number]).columns:\n                if col1 != col2:\n                    f_val, p_val = stats.f_oneway(df[col1], df[col2])\n                    anova_results = anova_results.append(pd.Series({\"F-value\": f_val, \"P-value\": p_val}, name=f\"{col1} vs {col2}\"))\n\n    return means, std_devs, axes, anova_results"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Split into words\n    words = text.split()\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Get the 10 most common words\n    top_10_words = word_counts.most_common(10)\n\n    # Create a bar chart\n    words, counts = zip(*top_10_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 most common words')\n\n    return top_10_words, ax", "raw_solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Split into words\n    words = text.split()\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Get the 10 most common words\n    top_10_words = word_counts.most_common(10)\n\n    # Create a bar chart\n    words, counts = zip(*top_10_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 most common words')\n\n    return top_10_words, ax"}
{"task_id": "BigCodeBench/1086", "solution": "import string\nimport random\nimport pandas as pd\nimport numpy as np\nNUM_SAMPLES = 1000\ndef task_func():\n    \"\"\"\n    Generates a DataFrame with two columns: a string field and a float field.\n    The string field contains randomly generated strings of 10 ASCII letters.\n    The float field contains randomly generated numbers between 0 and 10000,\n    formatted with two decimal places and a comma as the thousands separator.\n    \"\"\"\n    # Generate random strings\n    strings = [''.join(random.choices(string.ascii_letters, k=10)) for _ in range(NUM_SAMPLES)]\n\n    # Generate random floats\n    floats = np.random.uniform(0, 10000, NUM_SAMPLES).round(2)\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'String Field': strings,\n        'Float Field': floats\n    })\n\n    # Format 'Float Field' as string with comma as thousands separator\n    df['Float Field'] = df['Float Field'].apply(lambda x: \"{:,}\".format(x))\n\n    return df", "raw_solution": "import string\nimport random\nimport pandas as pd\nimport numpy as np\n\n# Constants\nNUM_SAMPLES = 1000  # Number of samples\n\ndef task_func():\n    \"\"\"\n    Generates a DataFrame with two columns: a string field and a float field.\n    The string field contains randomly generated strings of 10 ASCII letters.\n    The float field contains randomly generated numbers between 0 and 10000,\n    formatted with two decimal places and a comma as the thousands separator.\n    \"\"\"\n    # Generate random strings\n    strings = [''.join(random.choices(string.ascii_letters, k=10)) for _ in range(NUM_SAMPLES)]\n\n    # Generate random floats\n    floats = np.random.uniform(0, 10000, NUM_SAMPLES).round(2)\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'String Field': strings,\n        'Float Field': floats\n    })\n\n    # Format 'Float Field' as string with comma as thousands separator\n    df['Float Field'] = df['Float Field'].apply(lambda x: \"{:,}\".format(x))\n\n    return df\n\n# Test the function\ndf = task_func()"}
{"task_id": "BigCodeBench/1087", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport os\ndef task_func(mean=123456.908, std_dev=1.2, save_plots=False):\n    # Generate a random sample from a normal distribution\n    sample = np.random.normal(mean, std_dev, 1000)\n\n    # Calculate skewness and kurtosis\n    skewness = stats.skew(sample)\n    kurtosis = stats.kurtosis(sample)\n\n    # Create a histogram and a QQ plot\n    if save_plots:\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.hist(sample, bins=30, density=True)\n        plt.title('Histogram')\n        plt.xlabel('Value')\n        plt.ylabel('Frequency')\n\n        plt.subplot(1, 2, 2)\n        stats.probplot(sample, plot=plt)\n        plt.title('QQ Plot')\n        plt.tight_layout()\n\n        # Save the plots\n        hist_path = os.path.join(os.getcwd(), 'histogram.png')\n        qq_path = os.path.join(os.getcwd(), 'qq_plot.png')\n        plt.savefig(hist_path)\n        plt.savefig(qq_path)\n        plot_paths = [hist_path, qq_path]\n    else:\n        plot_paths = []\n\n    return skewness, kurtosis, plot_paths", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport os\n\ndef task_func(mean=123456.908, std_dev=1.2, save_plots=False):\n    # Generate a random sample from a normal distribution\n    sample = np.random.normal(mean, std_dev, 1000)\n\n    # Calculate skewness and kurtosis\n    skewness = stats.skew(sample)\n    kurtosis = stats.kurtosis(sample)\n\n    # Create a histogram and a QQ plot\n    if save_plots:\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.hist(sample, bins=30, density=True)\n        plt.title('Histogram')\n        plt.xlabel('Value')\n        plt.ylabel('Frequency')\n\n        plt.subplot(1, 2, 2)\n        stats.probplot(sample, plot=plt)\n        plt.title('QQ Plot')\n        plt.tight_layout()\n\n        # Save the plots\n        hist_path = os.path.join(os.getcwd(), 'histogram.png')\n        qq_path = os.path.join(os.getcwd(), 'qq_plot.png')\n        plt.savefig(hist_path)\n        plt.savefig(qq_path)\n        plot_paths = [hist_path, qq_path]\n    else:\n        plot_paths = []\n\n    return skewness, kurtosis, plot_paths"}
{"task_id": "BigCodeBench/1088", "solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data=None):\n    \"\"\"\n    Pre-process a dataset by converting it to a Pandas DataFrame, replacing values less than 0.5 with zeros,\n    and standardizing the data using StandardScaler.\n    \"\"\"\n    if data is None:\n        return None\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Replace values less than 0.5 with zeros\n    df[df < 0.5] = 0\n\n    # Standardize the data\n    scaler = StandardScaler()\n    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data=None):\n    \"\"\"\n    Pre-process a dataset by converting it to a Pandas DataFrame, replacing values less than 0.5 with zeros,\n    and standardizing the data using StandardScaler.\n    \"\"\"\n    if data is None:\n        return None\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Replace values less than 0.5 with zeros\n    df[df < 0.5] = 0\n\n    # Standardize the data\n    scaler = StandardScaler()\n    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    return df"}
{"task_id": "BigCodeBench/1089", "solution": "import numpy as np\nfrom collections import Counter\ndef task_func(list_of_tuples):\n    \"\"\"\n    Computes the sum of numeric values and counts the occurrences of categories in a list of tuples.\n    Each tuple in the input list contains a numeric value and a category. This function calculates the sum of all the numeric values and also counts how many times each category appears in the list.\n    The function should output with:\n        tuple: A 2-element tuple where the first element is the sum of the numeric values, and the\n        second element is a dictionary with categories as keys and their counts as values.\n    \"\"\"\n    sum_of_values = 0\n    categories = []\n    for value, category in list_of_tuples:\n        sum_of_values += value\n        categories.append(category)\n    count_of_categories = dict(Counter(categories))\n    return (sum_of_values, count_of_categories)", "raw_solution": "import numpy as np\nfrom collections import Counter\n\ndef task_func(list_of_tuples):\n    \"\"\"\n    Computes the sum of numeric values and counts the occurrences of categories in a list of tuples.\n    Each tuple in the input list contains a numeric value and a category. This function calculates the sum of all the numeric values and also counts how many times each category appears in the list.\n    The function should output with:\n        tuple: A 2-element tuple where the first element is the sum of the numeric values, and the\n        second element is a dictionary with categories as keys and their counts as values.\n    \"\"\"\n    sum_of_values = 0\n    categories = []\n    for value, category in list_of_tuples:\n        sum_of_values += value\n        categories.append(category)\n    count_of_categories = dict(Counter(categories))\n    return (sum_of_values, count_of_categories)\n\n# Test the function\ntuples = [(1, 'a'), (2, 'b'), (3, 'a'), (4, 'b'), (5, 'a'), (6, 'b'), (7, 'a'), (8, 'b'), (9, 'a'), (10, 'b')]"}
{"task_id": "BigCodeBench/1090", "solution": "import ast\nimport json\nfrom collections import Counter\ndef task_func(file_pointer):\n    \"\"\"\n    Reads from a given file pointer to a JSON file, evaluates strings that represent dictionaries to actual dictionaries,\n    and counts the frequency of each key across all dictionary entries in the JSON data.\n    \"\"\"\n    # Initialize a Counter object\n    key_counter = Counter()\n\n    # Read the JSON data from the file\n    json_data = json.load(file_pointer)\n\n    # Iterate over each entry in the JSON data\n    for entry in json_data:\n        # If the entry is a string, evaluate it as a dictionary\n        if isinstance(entry, str):\n            entry = ast.literal_eval(entry)\n\n        # If the entry is a dictionary, update the Counter with its keys\n        if isinstance(entry, dict):\n            key_counter.update(entry.keys())\n\n    return key_counter", "raw_solution": "import ast\nimport json\nfrom collections import Counter\n\ndef task_func(file_pointer):\n    \"\"\"\n    Reads from a given file pointer to a JSON file, evaluates strings that represent dictionaries to actual dictionaries,\n    and counts the frequency of each key across all dictionary entries in the JSON data.\n    \"\"\"\n    # Initialize a Counter object\n    key_counter = Counter()\n\n    # Read the JSON data from the file\n    json_data = json.load(file_pointer)\n\n    # Iterate over each entry in the JSON data\n    for entry in json_data:\n        # If the entry is a string, evaluate it as a dictionary\n        if isinstance(entry, str):\n            entry = ast.literal_eval(entry)\n\n        # If the entry is a dictionary, update the Counter with its keys\n        if isinstance(entry, dict):\n            key_counter.update(entry.keys())\n\n    return key_counter"}
{"task_id": "BigCodeBench/1091", "solution": "import ast\nimport os\nimport glob\nDIRECTORY = 'data'\ndef task_func(directory):\n    \"\"\"\n    Converts all Unicode string representations of dictionaries in all text files in the specified directory to Python dictionaries.\n    \"\"\"\n    # Initialize an empty list to store the dictionaries\n    dict_list = []\n\n    # Get all text files in the directory\n    text_files = glob.glob(os.path.join(directory, '*.txt'))\n\n    # Iterate over each text file\n    for text_file in text_files:\n        # Open the text file\n        with open(text_file, 'r', encoding='utf-8') as file:\n            # Read the content of the text file\n            content = file.read()\n\n            # Try to convert the Unicode string representation to a Python dictionary\n            try:\n                dict_obj = ast.literal_eval(content)\n\n                # Check if the object is a dictionary\n                if not isinstance(dict_obj, dict):\n                    raise ValueError(f\"Invalid dictionary representation in file: {text_file}\")\n\n                # Append the dictionary to the list\n                dict_list.append(dict_obj)\n            except (SyntaxError, ValueError) as e:\n                # Raise an exception if the Unicode string representation is not a valid dictionary\n                raise ValueError(f\"Invalid dictionary representation in file: {text_file}\") from e\n\n    # Return the list of dictionaries\n    return dict_list", "raw_solution": "import ast\nimport os\nimport glob\n\n# Constants\nDIRECTORY = 'data'\n\ndef task_func(directory):\n    \"\"\"\n    Converts all Unicode string representations of dictionaries in all text files in the specified directory to Python dictionaries.\n    \"\"\"\n    # Initialize an empty list to store the dictionaries\n    dict_list = []\n\n    # Get all text files in the directory\n    text_files = glob.glob(os.path.join(directory, '*.txt'))\n\n    # Iterate over each text file\n    for text_file in text_files:\n        # Open the text file\n        with open(text_file, 'r', encoding='utf-8') as file:\n            # Read the content of the text file\n            content = file.read()\n\n            # Try to convert the Unicode string representation to a Python dictionary\n            try:\n                dict_obj = ast.literal_eval(content)\n\n                # Check if the object is a dictionary\n                if not isinstance(dict_obj, dict):\n                    raise ValueError(f\"Invalid dictionary representation in file: {text_file}\")\n\n                # Append the dictionary to the list\n                dict_list.append(dict_obj)\n            except (SyntaxError, ValueError) as e:\n                # Raise an exception if the Unicode string representation is not a valid dictionary\n                raise ValueError(f\"Invalid dictionary representation in file: {text_file}\") from e\n\n    # Return the list of dictionaries\n    return dict_list\n\n# Test the function\ndict_list = task_func(DIRECTORY)\nfor i, dict_obj in enumerate(dict_list, 1):\n    print(f\"Dictionary {i}: {dict_obj}\")"}
{"task_id": "BigCodeBench/1092", "solution": "import ast\nimport requests\nfrom bs4 import BeautifulSoup\ndef task_func(url):\n    \"\"\"\n    Fetches the content of a webpage specified by its URL, parses it to find <script> tags,\n    and attempts to evaluate any string within these tags as a Python dictionary.\n    If an error occurs during the request or if no dictionaries are found/evaluable, an empty list is returned.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except (requests.RequestException, ValueError):\n        return []\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    scripts = soup.find_all('script')\n\n    results = []\n    for script in scripts:\n        script_text = script.string\n        if script_text is None:\n            continue\n        try:\n            # Try to parse the script text as a Python dictionary\n            result = ast.literal_eval(script_text)\n            if isinstance(result, dict):\n                results.append(result)\n        except (SyntaxError, ValueError):\n            continue\n\n    return results", "raw_solution": "import ast\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef task_func(url):\n    \"\"\"\n    Fetches the content of a webpage specified by its URL, parses it to find <script> tags,\n    and attempts to evaluate any string within these tags as a Python dictionary.\n    If an error occurs during the request or if no dictionaries are found/evaluable, an empty list is returned.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except (requests.RequestException, ValueError):\n        return []\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    scripts = soup.find_all('script')\n\n    results = []\n    for script in scripts:\n        script_text = script.string\n        if script_text is None:\n            continue\n        try:\n            # Try to parse the script text as a Python dictionary\n            result = ast.literal_eval(script_text)\n            if isinstance(result, dict):\n                results.append(result)\n        except (SyntaxError, ValueError):\n            continue\n\n    return results"}
{"task_id": "BigCodeBench/1093", "solution": "import ast\nimport re", "raw_solution": "import ast\nimport re\n\ndef f_1008(text_file: str) -> list:\n    \"\"\"\n    Extracts all string representations of dictionaries from a text file using regular expressions and converts them to Python dictionaries.\n    \"\"\"\n    with open(text_file, 'r') as file:\n        content = file.read()\n\n    # Use regular expression to find all string representations of dictionaries\n    dict_strs = re.findall(r\"{[\\s\\S]*?}\", content)\n\n    # Convert string representations to Python dictionaries\n    dicts = [ast.literal_eval(s) for s in dict_strs]\n\n    return dicts"}
{"task_id": "BigCodeBench/1094", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\nimport re\ndef task_func(text):\n    \"\"\"\n    Identifies and counts words in a given text that start with the \"$\" symbol.\n    It returns the five most frequent dollar-prefixed words along with their counts.\n    Words solely consisting of \"$\" symbols without any following alphanumeric characters are ignored in the frequency count.\n    \"\"\"\n    # Tokenize the text using nltk's RegexpTokenizer\n    tokenizer = RegexpTokenizer(r'\\b\\$\\w+\\b')\n    tokens = tokenizer.tokenize(text)\n\n    # Filter out words solely consisting of \"$\" symbols without any following alphanumeric characters\n    tokens = [token[1:] for token in tokens if re.match(r'\\$\\w+', token)]\n\n    # Count the frequency of each token\n    counter = Counter(tokens)\n\n    # Return the five most common tokens along with their counts, ordered by most to least common\n    return counter.most_common(5)", "raw_solution": "from nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\nimport re\n\ndef task_func(text):\n    \"\"\"\n    Identifies and counts words in a given text that start with the \"$\" symbol.\n    It returns the five most frequent dollar-prefixed words along with their counts.\n    Words solely consisting of \"$\" symbols without any following alphanumeric characters are ignored in the frequency count.\n    \"\"\"\n    # Tokenize the text using nltk's RegexpTokenizer\n    tokenizer = RegexpTokenizer(r'\\b\\$\\w+\\b')\n    tokens = tokenizer.tokenize(text)\n\n    # Filter out words solely consisting of \"$\" symbols without any following alphanumeric characters\n    tokens = [token[1:] for token in tokens if re.match(r'\\$\\w+', token)]\n\n    # Count the frequency of each token\n    counter = Counter(tokens)\n\n    # Return the five most common tokens along with their counts, ordered by most to least common\n    return counter.most_common(5)\n\n# Test the function\ntext = \"$money $money $MONEY $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONEY $money $MONE"}
{"task_id": "BigCodeBench/1095", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport os\ndef task_func(text, output_filename):\n    \"\"\"\n    Extracts words from the input text that begin with the '$' character and saves them to a specified file, excluding any words that are solely composed of punctuation characters.\n    \"\"\"\n    # Tokenize the text\n    tokenizer = RegexpTokenizer(r'\\b\\$\\w+\\b')\n    words = tokenizer.tokenize(text)\n\n    # Exclude words that are solely composed of punctuation characters\n    words = [word for word in words if not all(char in punctuation for char in word)]\n\n    # Save the words to the output file\n    with open(output_filename, 'w') as file:\n        for word in words:\n            file.write(word + '\\n')\n\n    # Return the absolute path to the output file\n    return os.path.abspath(output_filename)", "raw_solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport os\n\ndef task_func(text, output_filename):\n    \"\"\"\n    Extracts words from the input text that begin with the '$' character and saves them to a specified file, excluding any words that are solely composed of punctuation characters.\n    \"\"\"\n    # Tokenize the text\n    tokenizer = RegexpTokenizer(r'\\b\\$\\w+\\b')\n    words = tokenizer.tokenize(text)\n\n    # Exclude words that are solely composed of punctuation characters\n    words = [word for word in words if not all(char in punctuation for char in word)]\n\n    # Save the words to the output file\n    with open(output_filename, 'w') as file:\n        for word in words:\n            file.write(word + '\\n')\n\n    # Return the absolute path to the output file\n    return os.path.abspath(output_filename)"}
{"task_id": "BigCodeBench/1096", "solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport csv\nimport os\nPUNCTUATION = set(punctuation)\ndef task_func(text, filename):\n    \"\"\"\n    Saves all words in a text beginning with the \"$\" character in a CSV file, excluding any words that are solely composed of punctuation characters.\n    \"\"\"\n    tokenizer = RegexpTokenizer(r'\\$\\w+')\n    words = tokenizer.tokenize(text)\n    words = [word for word in words if not set(word).issubset(PUNCTUATION)]\n\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"Word\"])\n        writer.writerows([[word] for word in words])\n\n    return os.path.abspath(filename)", "raw_solution": "from nltk.tokenize import RegexpTokenizer\nfrom string import punctuation\nimport csv\nimport os\n\nPUNCTUATION = set(punctuation)\n\ndef task_func(text, filename):\n    \"\"\"\n    Saves all words in a text beginning with the \"$\" character in a CSV file, excluding any words that are solely composed of punctuation characters.\n    \"\"\"\n    tokenizer = RegexpTokenizer(r'\\$\\w+')\n    words = tokenizer.tokenize(text)\n    words = [word for word in words if not set(word).issubset(PUNCTUATION)]\n\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"Word\"])\n        writer.writerows([[word] for word in words])\n\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/1097", "solution": "import re\nfrom string import punctuation\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\ndef task_func(text):\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Remove punctuation\n    text = \"\".join([char for char in text if char not in punctuation])\n\n    # Tokenize text\n    words = text.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in PREDEFINED_STOPWORDS]\n\n    # Join words back into a string\n    text = \" \".join(words)\n\n    return text", "raw_solution": "import re\nfrom string import punctuation\n\n# Predefined list of common stopwords\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\n\ndef task_func(text):\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Remove punctuation\n    text = \"\".join([char for char in text if char not in punctuation])\n\n    # Tokenize text\n    words = text.split()\n\n    # Remove stopwords\n    words = [word for word in words if word not in PREDEFINED_STOPWORDS]\n\n    # Join words back into a string\n    text = \" \".join(words)\n\n    return text"}
{"task_id": "BigCodeBench/1098", "solution": "import re\nfrom collections import Counter\ndef task_func(text, top_n):\n    # Remove URLs from the text\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = url_pattern.sub('', text)\n\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Count the words\n    word_counts = Counter(words)\n\n    # Return the N most common words\n    return word_counts.most_common(top_n)", "raw_solution": "import re\nfrom collections import Counter\n\ndef task_func(text, top_n):\n    # Remove URLs from the text\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = url_pattern.sub('', text)\n\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n\n    # Count the words\n    word_counts = Counter(words)\n\n    # Return the N most common words\n    return word_counts.most_common(top_n)"}
{"task_id": "BigCodeBench/1099", "solution": "import re\nfrom collections import Counter\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\ndef task_func(text):\n    # Remove URLs from the text\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = url_pattern.sub('', text)\n\n    # Tokenize the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Filter out stopwords\n    word_counts = {word: count for word, count in word_counts.items() if word.lower() not in PREDEFINED_STOPWORDS}\n\n    # Convert the Counter object to a list of tuples\n    result = [(word, count) for word, count in word_counts.items()]\n\n    return result", "raw_solution": "import re\nfrom collections import Counter\n\n# Predefined list of common stopwords\nPREDEFINED_STOPWORDS = {\n    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \n    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \n    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \n    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \n    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \n    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"more\"\n}\n\ndef task_func(text):\n    # Remove URLs from the text\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = url_pattern.sub('', text)\n\n    # Tokenize the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_counts = Counter(words)\n\n    # Filter out stopwords\n    word_counts = {word: count for word, count in word_counts.items() if word.lower() not in PREDEFINED_STOPWORDS}\n\n    # Convert the Counter object to a list of tuples\n    result = [(word, count) for word, count in word_counts.items()]\n\n    return result"}
{"task_id": "BigCodeBench/1100", "solution": "import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef task_func(texts):\n    # Remove URLs from the texts\n    texts = [re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) for text in texts]\n\n    # Initialize TfidfVectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the texts\n    tfidf_matrix = vectorizer.fit_transform(texts)\n\n    # Get the feature names\n    features = vectorizer.get_feature_names_out()\n\n    # Convert the sparse matrix to a dense matrix and round the values to 8 decimal places\n    tfidf_matrix = tfidf_matrix.toarray().round(8)\n\n    # Convert the dense matrix to a list of tuples\n    tfidf_list = [tuple(row) for row in tfidf_matrix]\n\n    return tfidf_list, list(features)", "raw_solution": "import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef task_func(texts):\n    # Remove URLs from the texts\n    texts = [re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) for text in texts]\n\n    # Initialize TfidfVectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the texts\n    tfidf_matrix = vectorizer.fit_transform(texts)\n\n    # Get the feature names\n    features = vectorizer.get_feature_names_out()\n\n    # Convert the sparse matrix to a dense matrix and round the values to 8 decimal places\n    tfidf_matrix = tfidf_matrix.toarray().round(8)\n\n    # Convert the dense matrix to a list of tuples\n    tfidf_list = [tuple(row) for row in tfidf_matrix]\n\n    return tfidf_list, list(features)"}
{"task_id": "BigCodeBench/1101", "solution": "import subprocess\nimport os\nimport glob\nimport time\ndef task_func(test_dir):\n    \"\"\"\n    This function runs all Python scripts in a specific directory and returns their execution times.\n    \"\"\"\n    # Get all Python files in the directory\n    python_files = glob.glob(os.path.join(test_dir, \"*.py\"))\n\n    # Initialize an empty dictionary to store the execution times\n    execution_times = {}\n\n    # Iterate over each Python file\n    for file in python_files:\n        # Get the base name of the file (without the extension)\n        base_name = os.path.basename(file)\n\n        # Start the timer\n        start_time = time.time()\n\n        # Run the Python file using subprocess\n        subprocess.run([\"python\", file])\n\n        # Stop the timer\n        end_time = time.time()\n\n        # Calculate the execution time and add it to the dictionary\n        execution_times[base_name] = end_time - start_time\n\n    # Return the dictionary of execution times\n    return execution_times", "raw_solution": "import subprocess\nimport os\nimport glob\nimport time\n\ndef task_func(test_dir):\n    \"\"\"\n    This function runs all Python scripts in a specific directory and returns their execution times.\n    \"\"\"\n    # Get all Python files in the directory\n    python_files = glob.glob(os.path.join(test_dir, \"*.py\"))\n\n    # Initialize an empty dictionary to store the execution times\n    execution_times = {}\n\n    # Iterate over each Python file\n    for file in python_files:\n        # Get the base name of the file (without the extension)\n        base_name = os.path.basename(file)\n\n        # Start the timer\n        start_time = time.time()\n\n        # Run the Python file using subprocess\n        subprocess.run([\"python\", file])\n\n        # Stop the timer\n        end_time = time.time()\n\n        # Calculate the execution time and add it to the dictionary\n        execution_times[base_name] = end_time - start_time\n\n    # Return the dictionary of execution times\n    return execution_times"}
{"task_id": "BigCodeBench/1102", "solution": "import subprocess\nimport shlex\nfrom datetime import datetime\ndef task_func(script_path: str) -> dict:\n    \"\"\"\n    Runs an R script and returns the start time, end time, decoded stdout, and decoded stderr as a dictionary.\n    \"\"\"\n    start_time = datetime.now()\n    with subprocess.Popen(shlex.split(f\"Rscript {script_path}\"), stdout=subprocess.PIPE, stderr=subprocess.PIPE) as process:\n        stdout, stderr = process.communicate()\n    end_time = datetime.now()\n\n    return {\n        \"start_time\": start_time,\n        \"end_time\": end_time,\n        \"stdout\": stdout.decode(),\n        \"stderr\": stderr.decode(),\n    }", "raw_solution": "import subprocess\nimport shlex\nfrom datetime import datetime\n\ndef task_func(script_path: str) -> dict:\n    \"\"\"\n    Runs an R script and returns the start time, end time, decoded stdout, and decoded stderr as a dictionary.\n    \"\"\"\n    start_time = datetime.now()\n    with subprocess.Popen(shlex.split(f\"Rscript {script_path}\"), stdout=subprocess.PIPE, stderr=subprocess.PIPE) as process:\n        stdout, stderr = process.communicate()\n    end_time = datetime.now()\n\n    return {\n        \"start_time\": start_time,\n        \"end_time\": end_time,\n        \"stdout\": stdout.decode(),\n        \"stderr\": stderr.decode(),\n    }"}
{"task_id": "BigCodeBench/1103", "solution": "import subprocess\nimport shutil\nimport os\ndef task_func(script_path: str, temp_dir: str) -> str:\n    \"\"\"\n    Executes a given Python code in a temporary directory.\n    \"\"\"\n    # Create a temporary directory\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n\n    # Copy the script to the temporary directory\n    shutil.copy(script_path, temp_dir)\n\n    # Change the working directory to the temporary directory\n    os.chdir(temp_dir)\n\n    # Execute the script\n    try:\n        subprocess.check_output(['python', os.path.basename(script_path)], stderr=subprocess.STDOUT)\n        return \"Script executed successfully!\"\n    except subprocess.CalledProcessError:\n        return \"Script execution failed!\"\n    finally:\n        # Clean up the temporary directory\n        shutil.rmtree(temp_dir)", "raw_solution": "import subprocess\nimport shutil\nimport os\n\ndef task_func(script_path: str, temp_dir: str) -> str:\n    \"\"\"\n    Executes a given Python code in a temporary directory.\n    \"\"\"\n    # Create a temporary directory\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir)\n\n    # Copy the script to the temporary directory\n    shutil.copy(script_path, temp_dir)\n\n    # Change the working directory to the temporary directory\n    os.chdir(temp_dir)\n\n    # Execute the script\n    try:\n        subprocess.check_output(['python', os.path.basename(script_path)], stderr=subprocess.STDOUT)\n        return \"Script executed successfully!\"\n    except subprocess.CalledProcessError:\n        return \"Script execution failed!\"\n    finally:\n        # Clean up the temporary directory\n        shutil.rmtree(temp_dir)"}
{"task_id": "BigCodeBench/1104", "solution": "import subprocess\nimport os\nimport threading\ndef task_func(script_path: str, timeout: int = 60) -> str:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The file {script_path} does not exist.\")\n\n    def target():\n        subprocess.run([\"python\", script_path])\n\n    thread = threading.Thread(target=target)\n    thread.start()\n    thread.join(timeout)\n\n    if thread.is_alive():\n        # The script execution exceeded the timeout\n        thread.daemon = True # Allow main thread to exit even if this thread is still running\n        return \"Terminating process due to timeout.\"\n    else:\n        return \"Script executed successfully.\"", "raw_solution": "import subprocess\nimport os\nimport threading\n\ndef task_func(script_path: str, timeout: int = 60) -> str:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The file {script_path} does not exist.\")\n\n    def target():\n        subprocess.run([\"python\", script_path])\n\n    thread = threading.Thread(target=target)\n    thread.start()\n    thread.join(timeout)\n\n    if thread.is_alive():\n        # The script execution exceeded the timeout\n        thread.daemon = True # Allow main thread to exit even if this thread is still running\n        return \"Terminating process due to timeout.\"\n    else:\n        return \"Script executed successfully.\""}
{"task_id": "BigCodeBench/1105", "solution": "import subprocess\nimport os\nimport time\nimport glob\ndef task_func(r_script_path: str, output_path: str, duration: int) -> (bool, str):\n    \"\"\"\n    This function executes an R script and verifies if the output file is generated within a given duration.\n    \"\"\"\n    start_time = time.time()\n    while True:\n        if time.time() - start_time > duration:\n            return False, 'File not generated within the specified duration.'\n        if os.path.exists(output_path):\n            return True, 'File generated successfully within the specified duration.'\n        subprocess.run([\"Rscript\", r_script_path])\n        time.sleep(1)\nr_script_path = \"/path/to/your/r_script.R\"\noutput_path = \"/path/to/your/output.txt\"\nduration = 10", "raw_solution": "import subprocess\nimport os\nimport time\nimport glob\n\ndef task_func(r_script_path: str, output_path: str, duration: int) -> (bool, str):\n    \"\"\"\n    This function executes an R script and verifies if the output file is generated within a given duration.\n    \"\"\"\n    start_time = time.time()\n    while True:\n        if time.time() - start_time > duration:\n            return False, 'File not generated within the specified duration.'\n        if os.path.exists(output_path):\n            return True, 'File generated successfully within the specified duration.'\n        subprocess.run([\"Rscript\", r_script_path])\n        time.sleep(1)\n\n# Test the function\nr_script_path = \"/path/to/your/r_script.R\"\noutput_path = \"/path/to/your/output.txt\"\nduration = 10  # in seconds"}
{"task_id": "BigCodeBench/1106", "solution": "from datetime import datetime\nimport os\nfrom pathlib import Path\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(file_path):\n    \"\"\"\n    Determines the creation time of a file and converts it to a formatted string.\n    \"\"\"\n    # Get the creation time of the file\n    creation_time = os.path.getctime(file_path)\n\n    # Convert the creation time to a datetime object\n    creation_time_dt = datetime.fromtimestamp(creation_time)\n\n    # Format the datetime object to the desired string format\n    creation_time_str = creation_time_dt.strftime(DATE_FORMAT)\n\n    return creation_time_str\nfile_path = Path(__file__).resolve()", "raw_solution": "from datetime import datetime\nimport os\nfrom pathlib import Path\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(file_path):\n    \"\"\"\n    Determines the creation time of a file and converts it to a formatted string.\n    \"\"\"\n    # Get the creation time of the file\n    creation_time = os.path.getctime(file_path)\n\n    # Convert the creation time to a datetime object\n    creation_time_dt = datetime.fromtimestamp(creation_time)\n\n    # Format the datetime object to the desired string format\n    creation_time_str = creation_time_dt.strftime(DATE_FORMAT)\n\n    return creation_time_str\n\n# Test the function\nfile_path = Path(__file__).resolve()"}
{"task_id": "BigCodeBench/1107", "solution": "from datetime import datetime\nimport pytz\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\ndef task_func(unix_timestamp, target_timezone):\n    \"\"\"\n    Converts a Unix timestamp to a formatted date and time string in a specified timezone.\n    \"\"\"\n    # Convert Unix timestamp to datetime object\n    dt = datetime.utcfromtimestamp(unix_timestamp)\n\n    # Create a timezone object for the target timezone\n    target_tz = pytz.timezone(target_timezone)\n\n    # Localize the datetime object to the target timezone\n    dt = dt.replace(tzinfo=pytz.utc).astimezone(target_tz)\n\n    # Format the datetime object as a string\n    str_time = dt.strftime(DATE_FORMAT)\n\n    return str_time", "raw_solution": "from datetime import datetime\nimport pytz\n\n# Constants\nDATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n\ndef task_func(unix_timestamp, target_timezone):\n    \"\"\"\n    Converts a Unix timestamp to a formatted date and time string in a specified timezone.\n    \"\"\"\n    # Convert Unix timestamp to datetime object\n    dt = datetime.utcfromtimestamp(unix_timestamp)\n\n    # Create a timezone object for the target timezone\n    target_tz = pytz.timezone(target_timezone)\n\n    # Localize the datetime object to the target timezone\n    dt = dt.replace(tzinfo=pytz.utc).astimezone(target_tz)\n\n    # Format the datetime object as a string\n    str_time = dt.strftime(DATE_FORMAT)\n\n    return str_time\n\n# Test the function"}
{"task_id": "BigCodeBench/1108", "solution": "from collections import Counter\nimport re\ndef task_func(result):\n    \"\"\"\n    This function takes a list of dictionaries as input and returns a dictionary with the most common values associated with the 'url' key.\n    \"\"\"\n    # Extract the 'url' values from the dictionaries\n    urls = [d['url'] for d in result if 'url' in d]\n\n    # Count the occurrences of each 'url' value\n    counter = Counter(urls)\n\n    # Find the most common 'url' value\n    most_common_url, most_common_count = counter.most_common(1)[0]\n\n    # Return a dictionary with the most common 'url' value and its count\n    return {most_common_url: most_common_count}", "raw_solution": "from collections import Counter\nimport re\n\ndef task_func(result):\n    \"\"\"\n    This function takes a list of dictionaries as input and returns a dictionary with the most common values associated with the 'url' key.\n    \"\"\"\n    # Extract the 'url' values from the dictionaries\n    urls = [d['url'] for d in result if 'url' in d]\n\n    # Count the occurrences of each 'url' value\n    counter = Counter(urls)\n\n    # Find the most common 'url' value\n    most_common_url, most_common_count = counter.most_common(1)[0]\n\n    # Return a dictionary with the most common 'url' value and its count\n    return {most_common_url: most_common_count}"}
{"task_id": "BigCodeBench/1109", "solution": "import os\nfrom nltk import word_tokenize\ndef task_func(file_path='File.txt'):\n    \"\"\"\n    Tokenizes a text file using the NLTK library. This function reads each line from the file, breaks it into words or punctuation, and stores the tokens in a list.\n    \"\"\"\n    if not os.path.exists(file_path):\n        return \"File not found!\"\n\n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n\n    tokens = []\n    for line in lines:\n        tokens.extend(word_tokenize(line))\n\n    return tokens", "raw_solution": "import os\nfrom nltk import word_tokenize\n\ndef task_func(file_path='File.txt'):\n    \"\"\"\n    Tokenizes a text file using the NLTK library. This function reads each line from the file, breaks it into words or punctuation, and stores the tokens in a list.\n    \"\"\"\n    if not os.path.exists(file_path):\n        return \"File not found!\"\n\n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n\n    tokens = []\n    for line in lines:\n        tokens.extend(word_tokenize(line))\n\n    return tokens"}
{"task_id": "BigCodeBench/1110", "solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\ndef task_func(word_dict):\n    \"\"\"\n    Given a dictionary of words as keys and letters as values, count the frequency of each letter in the words.\n    \"\"\"\n    # Combine all the words into one string\n    all_words = ''.join(word_dict.keys())\n\n    # Count the frequency of each letter\n    letter_counts = Counter(all_words)\n\n    # Create a dictionary with letters as keys and their frequencies as values\n    letter_dict = {letter: count for letter, count in letter_counts.items()}\n\n    return letter_dict\nword_dict = {\n    'hello': 'a',\n    'world': 'b',\n    'python': 'c',\n    'programming': 'd'\n}", "raw_solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\n\ndef task_func(word_dict):\n    \"\"\"\n    Given a dictionary of words as keys and letters as values, count the frequency of each letter in the words.\n    \"\"\"\n    # Combine all the words into one string\n    all_words = ''.join(word_dict.keys())\n\n    # Count the frequency of each letter\n    letter_counts = Counter(all_words)\n\n    # Create a dictionary with letters as keys and their frequencies as values\n    letter_dict = {letter: count for letter, count in letter_counts.items()}\n\n    return letter_dict\n\n# Test the function\nword_dict = {\n    'hello': 'a',\n    'world': 'b',\n    'python': 'c',\n    'programming': 'd'\n}"}
{"task_id": "BigCodeBench/1111", "solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\nANIMAL = ['cat', 'camel', 'cow', 'dog', 'elephant', 'fox', 'giraffe', 'hippo', 'iguana', 'jaguar']\ndef task_func(animal_dict):\n    # Remove keys not in ANIMAL\n    animal_dict = {key: value for key, value in animal_dict.items() if key in ANIMAL}\n\n    # Flatten the values of the dictionary into a single string\n    all_letters = ''.join(itertools.chain.from_iterable(animal_dict.values()))\n\n    # Count the frequency of each letter\n    letter_counts = Counter(all_letters)\n\n    # Sort the dictionary by value in descending order\n    sorted_counts = dict(sorted(letter_counts.items(), key=itemgetter(1), reverse=True))\n\n    return sorted_counts\nanimal_dict = {\n    'cat': 'abc',\n    'camel': 'def',\n    'cow': 'ghi',\n    'dog': 'jkl',\n    'elephant': 'mno',\n    'fox': 'pqr',\n    'giraffe': 'stu',\n    'hippo': 'vwx',\n    'iguana': 'yza',\n    'jaguar': 'bcd',\n    'koala': 'efg',\n    'lion': 'ghi',\n    'monkey': 'jkl'\n}", "raw_solution": "from collections import Counter\nfrom operator import itemgetter\nimport itertools\n\n#CONSTANT\nANIMAL = ['cat', 'camel', 'cow', 'dog', 'elephant', 'fox', 'giraffe', 'hippo', 'iguana', 'jaguar']\n\ndef task_func(animal_dict):\n    # Remove keys not in ANIMAL\n    animal_dict = {key: value for key, value in animal_dict.items() if key in ANIMAL}\n\n    # Flatten the values of the dictionary into a single string\n    all_letters = ''.join(itertools.chain.from_iterable(animal_dict.values()))\n\n    # Count the frequency of each letter\n    letter_counts = Counter(all_letters)\n\n    # Sort the dictionary by value in descending order\n    sorted_counts = dict(sorted(letter_counts.items(), key=itemgetter(1), reverse=True))\n\n    return sorted_counts\n\n# Test the function\nanimal_dict = {\n    'cat': 'abc',\n    'camel': 'def',\n    'cow': 'ghi',\n    'dog': 'jkl',\n    'elephant': 'mno',\n    'fox': 'pqr',\n    'giraffe': 'stu',\n    'hippo': 'vwx',\n    'iguana': 'yza',\n    'jaguar': 'bcd',\n    'koala': 'efg',\n    'lion': 'ghi',\n    'monkey': 'jkl'\n}"}
{"task_id": "BigCodeBench/1112", "solution": "import csv\nimport random\nfrom datetime import datetime, timedelta\nDATA = ['Temperature', 'Humidity', 'Pressure']\nRANGE = {\n    'Temperature': (-50, 50),\n    'Humidity': (0, 100),\n    'Pressure': (980, 1040)\n}\ndef generate_random_data(start_time, end_time):\n    data = []\n    current_time = start_time\n    while current_time <= end_time:\n        row = {\n            'Time': current_time.strftime('%Y-%m-%d %H:%M:%S'),\n            'Temperature': random.uniform(*RANGE['Temperature']),\n            'Humidity': random.uniform(*RANGE['Humidity']),\n            'Pressure': random.uniform(*RANGE['Pressure'])\n        }\n        data.append(row)\n        current_time += timedelta(hours=1)\n    return data\ndef task_func(file_name=\"data.csv\"):\n    now = datetime.now()\n    data = generate_random_data(now, now + timedelta(hours=24))\n\n    with open(file_name, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=['Time'] + DATA)\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    return file_name", "raw_solution": "import csv\nimport random\nfrom datetime import datetime, timedelta\n\n# Constants\nDATA = ['Temperature', 'Humidity', 'Pressure']\nRANGE = {\n    'Temperature': (-50, 50),\n    'Humidity': (0, 100),\n    'Pressure': (980, 1040)\n}\n\ndef generate_random_data(start_time, end_time):\n    data = []\n    current_time = start_time\n    while current_time <= end_time:\n        row = {\n            'Time': current_time.strftime('%Y-%m-%d %H:%M:%S'),\n            'Temperature': random.uniform(*RANGE['Temperature']),\n            'Humidity': random.uniform(*RANGE['Humidity']),\n            'Pressure': random.uniform(*RANGE['Pressure'])\n        }\n        data.append(row)\n        current_time += timedelta(hours=1)\n    return data\n\ndef task_func(file_name=\"data.csv\"):\n    now = datetime.now()\n    data = generate_random_data(now, now + timedelta(hours=24))\n\n    with open(file_name, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=['Time'] + DATA)\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    return file_name"}
{"task_id": "BigCodeBench/1113", "solution": "import csv\nimport collections\nfrom typing import Dict\ndef task_func(csv_file: str, emp_prefix: str = 'EMP$$') -> Dict[str, int]:\n    \"\"\"\n    Count the number of records for each employee in a CSV file.\n    \"\"\"\n    # Initialize a dictionary to store the counts\n    counts = collections.defaultdict(int)\n\n    # Open the CSV file\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file)\n\n        # Iterate over each row in the CSV file\n        for row in reader:\n            # Iterate over each cell in the row\n            for cell in row:\n                # If the cell starts with the employee prefix, increment the count for that employee\n                if cell.startswith(emp_prefix):\n                    counts[cell] += 1\n\n    return counts", "raw_solution": "import csv\nimport collections\nfrom typing import Dict\n\ndef task_func(csv_file: str, emp_prefix: str = 'EMP$$') -> Dict[str, int]:\n    \"\"\"\n    Count the number of records for each employee in a CSV file.\n    \"\"\"\n    # Initialize a dictionary to store the counts\n    counts = collections.defaultdict(int)\n\n    # Open the CSV file\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file)\n\n        # Iterate over each row in the CSV file\n        for row in reader:\n            # Iterate over each cell in the row\n            for cell in row:\n                # If the cell starts with the employee prefix, increment the count for that employee\n                if cell.startswith(emp_prefix):\n                    counts[cell] += 1\n\n    return counts"}
{"task_id": "BigCodeBench/1114", "solution": "from collections import defaultdict\nfrom random import randint\ndef task_func(dict1):\n    \"\"\"\n    Create a dictionary of employee data for departments starting with 'EMP$$'. The keys are department codes and the values are lists of the salaries of employees in that department.\n    \"\"\"\n    # Create a defaultdict to store the salaries of employees in each department\n    dept_salaries = defaultdict(list)\n\n    # Iterate over the dictionary\n    for dept, salaries in dict1.items():\n        # Check if the department code starts with 'EMP$$'\n        if dept.startswith('EMP$$'):\n            # Add the salaries to the corresponding department\n            dept_salaries[dept].extend(salaries)\n\n    return dept_salaries", "raw_solution": "from collections import defaultdict\nfrom random import randint\n\ndef task_func(dict1):\n    \"\"\"\n    Create a dictionary of employee data for departments starting with 'EMP$$'. The keys are department codes and the values are lists of the salaries of employees in that department.\n    \"\"\"\n    # Create a defaultdict to store the salaries of employees in each department\n    dept_salaries = defaultdict(list)\n\n    # Iterate over the dictionary\n    for dept, salaries in dict1.items():\n        # Check if the department code starts with 'EMP$$'\n        if dept.startswith('EMP$$'):\n            # Add the salaries to the corresponding department\n            dept_salaries[dept].extend(salaries)\n\n    return dept_salaries\n\n# Test the function"}
{"task_id": "BigCodeBench/1115", "solution": "import random\nfrom string import ascii_uppercase\ndef task_func(dict1):\n    \"\"\"\n    Assigns a unique ID to each employee based on their department code, consisting of the department code,\n    followed by a random string of 5 letters.\n    \"\"\"\n    unique_ids = []\n    for department, employees in dict1.items():\n        for employee in employees:\n            random_string = ''.join(random.choice(ascii_uppercase) for _ in range(5))\n            unique_id = department + random_string\n            unique_ids.append(unique_id)\n    return unique_ids\ndict1 = {\n    'HR': ['John', 'Emily'],\n    'IT': ['Sam', 'Alex'],\n    'Sales': ['Tom', 'Jerry']\n}", "raw_solution": "import random\nfrom string import ascii_uppercase\n\ndef task_func(dict1):\n    \"\"\"\n    Assigns a unique ID to each employee based on their department code, consisting of the department code,\n    followed by a random string of 5 letters.\n    \"\"\"\n    unique_ids = []\n    for department, employees in dict1.items():\n        for employee in employees:\n            random_string = ''.join(random.choice(ascii_uppercase) for _ in range(5))\n            unique_id = department + random_string\n            unique_ids.append(unique_id)\n    return unique_ids\n\n# Test the function\ndict1 = {\n    'HR': ['John', 'Emily'],\n    'IT': ['Sam', 'Alex'],\n    'Sales': ['Tom', 'Jerry']\n}"}
{"task_id": "BigCodeBench/1116", "solution": "import random\nimport statistics\nfrom collections import Counter\ndef task_func(dict1):\n    \"\"\"\n    Calculates the mean, median, and mode(s) of the ages of employees in the department \"EMP$$\".\n    \"\"\"\n    # Extract ages of employees in \"EMP$$\"\n    ages = [age for name, age in dict1.items() if name.startswith(\"EMP$$\")]\n\n    # Calculate mean\n    mean_age = statistics.mean(ages)\n\n    # Calculate median\n    median_age = statistics.median(ages)\n\n    # Calculate mode(s)\n    counter = Counter(ages)\n    max_count = max(counter.values())\n    modes = [age for age, count in counter.items() if count == max_count]\n\n    return (mean_age, median_age, modes)", "raw_solution": "import random\nimport statistics\nfrom collections import Counter\n\n# Constants\nAGE_RANGE = (22, 60)\n\ndef task_func(dict1):\n    \"\"\"\n    Calculates the mean, median, and mode(s) of the ages of employees in the department \"EMP$$\".\n    \"\"\"\n    # Extract ages of employees in \"EMP$$\"\n    ages = [age for name, age in dict1.items() if name.startswith(\"EMP$$\")]\n\n    # Calculate mean\n    mean_age = statistics.mean(ages)\n\n    # Calculate median\n    median_age = statistics.median(ages)\n\n    # Calculate mode(s)\n    counter = Counter(ages)\n    max_count = max(counter.values())\n    modes = [age for age, count in counter.items() if count == max_count]\n\n    return (mean_age, median_age, modes)\n\n# Test the function"}
{"task_id": "BigCodeBench/1117", "solution": "import collections\nimport random\nimport json\nPREFICES = ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']\nLEVELS = ['Junior', 'Mid', 'Senior']\ndef task_func(department_data):\n    \"\"\"\n    Generate a JSON object from employee data based on given department codes and their employee counts.\n    \"\"\"\n    # Create a dictionary to store the department data\n    department_dict = collections.defaultdict(list)\n\n    # Generate random employee levels for each department\n    for department in PREFICES:\n        for _ in range(department_data[department]):\n            department_dict[department].append(random.choice(LEVELS))\n\n    # Convert the dictionary to a JSON object\n    json_object = json.dumps(department_dict, indent=4)\n\n    return json_object", "raw_solution": "import collections\nimport random\nimport json\n\n# Constants\nPREFICES = ['EMP$$', 'MAN$$', 'DEV$$', 'HR$$']\nLEVELS = ['Junior', 'Mid', 'Senior']\n\ndef task_func(department_data):\n    \"\"\"\n    Generate a JSON object from employee data based on given department codes and their employee counts.\n    \"\"\"\n    # Create a dictionary to store the department data\n    department_dict = collections.defaultdict(list)\n\n    # Generate random employee levels for each department\n    for department in PREFICES:\n        for _ in range(department_data[department]):\n            department_dict[department].append(random.choice(LEVELS))\n\n    # Convert the dictionary to a JSON object\n    json_object = json.dumps(department_dict, indent=4)\n\n    return json_object\n\n# Test the function"}
{"task_id": "BigCodeBench/1118", "solution": "import json\nimport csv\nimport requests\nfrom io import StringIO\nCSV_URL = 'https://example.com/data.csv'\nJSON_FILE = 'data.json'\ndef task_func(csv_url=CSV_URL, json_file_path=JSON_FILE):\n    \"\"\"\n    Downloads a CSV file from a specified URL, converts it to JSON format, and saves it to a specified file path.\n    \"\"\"\n    # Download CSV file\n    response = requests.get(csv_url)\n    response.raise_for_status()\n\n    # Convert CSV to JSON\n    csv_data = response.text\n    csv_reader = csv.DictReader(StringIO(csv_data))\n    json_data = [row for row in csv_reader]\n\n    # Save JSON to file\n    with open(json_file_path, 'w') as json_file:\n        json.dump(json_data, json_file)\n\n    return json_file_path", "raw_solution": "import json\nimport csv\nimport requests\nfrom io import StringIO\n\n# Constants\nCSV_URL = 'https://example.com/data.csv'\nJSON_FILE = 'data.json'\n\ndef task_func(csv_url=CSV_URL, json_file_path=JSON_FILE):\n    \"\"\"\n    Downloads a CSV file from a specified URL, converts it to JSON format, and saves it to a specified file path.\n    \"\"\"\n    # Download CSV file\n    response = requests.get(csv_url)\n    response.raise_for_status()\n\n    # Convert CSV to JSON\n    csv_data = response.text\n    csv_reader = csv.DictReader(StringIO(csv_data))\n    json_data = [row for row in csv_reader]\n\n    # Save JSON to file\n    with open(json_file_path, 'w') as json_file:\n        json.dump(json_data, json_file)\n\n    return json_file_path"}
{"task_id": "BigCodeBench/1119", "solution": "import codecs\nimport random\nimport string\nimport hashlib\ndef task_func(password_length=10, salt=\"salty\"):\n    characters = string.ascii_letters + string.digits + string.punctuation\n    password = ''.join(random.choice(characters) for _ in range(password_length))\n    password_with_salt = password + salt\n    hashed_password = hashlib.sha256(password_with_salt.encode()).hexdigest()\n    return hashed_password", "raw_solution": "import codecs\nimport random\nimport string\nimport hashlib\n\ndef task_func(password_length=10, salt=\"salty\"):\n    characters = string.ascii_letters + string.digits + string.punctuation\n    password = ''.join(random.choice(characters) for _ in range(password_length))\n    password_with_salt = password + salt\n    hashed_password = hashlib.sha256(password_with_salt.encode()).hexdigest()\n    return hashed_password\n\n# Test the function"}
{"task_id": "BigCodeBench/1120", "solution": "import re\nimport urllib.parse\nimport requests\nimport json\ndef task_func(myString, API_KEY):\n    # Extract URLs from the string\n    urls = re.findall('(?:(?:https?|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', myString)\n\n    # Extract domain from URLs\n    domains = [urllib.parse.urlparse(url).netloc for url in urls]\n\n    # Initialize an empty dictionary to store the geolocation data\n    geolocation_data = {}\n\n    # Use the IP API to get the geolocation data for each domain\n    for domain in domains:\n        try:\n            # Construct the API request URL\n            url = f\"https://ipapi.co/{domain}/json/\"\n\n            # Send the API request\n            response = requests.get(url, headers={\"Authorization\": f\"Bearer {API_KEY}\"})\n\n            # Check if the request was successful\n            if response.status_code == 200:\n                # Parse the response as JSON\n                data = response.json()\n\n                # Store the geolocation data in the dictionary\n                geolocation_data[domain] = data\n            else:\n                # If the request was not successful, store None in the dictionary\n                geolocation_data[domain] = None\n        except:\n            # If an error occurs, store None in the dictionary\n            geolocation_data[domain] = None\n\n    return geolocation_data", "raw_solution": "import re\nimport urllib.parse\nimport requests\nimport json\n\ndef task_func(myString, API_KEY):\n    # Extract URLs from the string\n    urls = re.findall('(?:(?:https?|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', myString)\n\n    # Extract domain from URLs\n    domains = [urllib.parse.urlparse(url).netloc for url in urls]\n\n    # Initialize an empty dictionary to store the geolocation data\n    geolocation_data = {}\n\n    # Use the IP API to get the geolocation data for each domain\n    for domain in domains:\n        try:\n            # Construct the API request URL\n            url = f\"https://ipapi.co/{domain}/json/\"\n\n            # Send the API request\n            response = requests.get(url, headers={\"Authorization\": f\"Bearer {API_KEY}\"})\n\n            # Check if the request was successful\n            if response.status_code == 200:\n                # Parse the response as JSON\n                data = response.json()\n\n                # Store the geolocation data in the dictionary\n                geolocation_data[domain] = data\n            else:\n                # If the request was not successful, store None in the dictionary\n                geolocation_data[domain] = None\n        except:\n            # If an error occurs, store None in the dictionary\n            geolocation_data[domain] = None\n\n    return geolocation_data"}
{"task_id": "BigCodeBench/1121", "solution": "import re\nimport urllib.parse\nimport requests\nimport json\ndef task_func(myString, API_KEY):\n    # Extract URLs from the string\n    urls = re.findall('(?:(?:https?|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', myString)\n\n    # Extract domain from URLs\n    domains = [urllib.parse.urlparse(url).netloc for url in urls]\n\n    # Initialize an empty dictionary to store the geolocation data\n    geolocation_data = {}\n\n    # Use the IP API to get the geolocation data for each domain\n    for domain in domains:\n        try:\n            # Construct the API request URL\n            url = f\"https://ipapi.co/{domain}/json/\"\n\n            # Send the API request\n            response = requests.get(url, headers={\"Authorization\": f\"Bearer {API_KEY}\"})\n\n            # Check if the request was successful\n            if response.status_code == 200:\n                # Parse the response as JSON\n                data = response.json()\n\n                # Store the geolocation data in the dictionary\n                geolocation_data[domain] = data\n            else:\n                # If the request was not successful, store None in the dictionary\n                geolocation_data[domain] = None\n        except:\n            # If an error occurs, store None in the dictionary\n            geolocation_data[domain] = None\n\n    return geolocation_data", "raw_solution": "import re\nimport urllib.parse\nimport requests\nimport json\n\ndef task_func(myString, API_KEY):\n    # Extract URLs from the string\n    urls = re.findall('(?:(?:https?|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', myString)\n\n    # Extract domain from URLs\n    domains = [urllib.parse.urlparse(url).netloc for url in urls]\n\n    # Initialize an empty dictionary to store the geolocation data\n    geolocation_data = {}\n\n    # Use the IP API to get the geolocation data for each domain\n    for domain in domains:\n        try:\n            # Construct the API request URL\n            url = f\"https://ipapi.co/{domain}/json/\"\n\n            # Send the API request\n            response = requests.get(url, headers={\"Authorization\": f\"Bearer {API_KEY}\"})\n\n            # Check if the request was successful\n            if response.status_code == 200:\n                # Parse the response as JSON\n                data = response.json()\n\n                # Store the geolocation data in the dictionary\n                geolocation_data[domain] = data\n            else:\n                # If the request was not successful, store None in the dictionary\n                geolocation_data[domain] = None\n        except:\n            # If an error occurs, store None in the dictionary\n            geolocation_data[domain] = None\n\n    return geolocation_data"}
{"task_id": "BigCodeBench/1122", "solution": "import re\nimport socket\nimport urllib.parse\ndef task_func(myString):\n    # Extract URLs from the string\n    urls = re.findall('(?:(?:https?|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', myString)\n\n    # Initialize an empty dictionary to store domain-IP pairs\n    domain_ip_pairs = {}\n\n    # Iterate over each URL\n    for url in urls:\n        # Extract the domain from the URL\n        domain = urllib.parse.urlparse(url).netloc\n\n        # Try to resolve the domain to an IP address\n        try:\n            ip = socket.gethostbyname(domain)\n        except socket.gaierror:\n            # If the domain cannot be resolved, set the IP address to None\n            ip = None\n\n        # Add the domain-IP pair to the dictionary\n        domain_ip_pairs[domain] = ip\n\n    # Return the dictionary of domain-IP pairs\n    return domain_ip_pairs", "raw_solution": "import re\nimport socket\nimport urllib.parse\n\ndef task_func(myString):\n    # Extract URLs from the string\n    urls = re.findall('(?:(?:https?|ftp):\\/\\/)[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', myString)\n\n    # Initialize an empty dictionary to store domain-IP pairs\n    domain_ip_pairs = {}\n\n    # Iterate over each URL\n    for url in urls:\n        # Extract the domain from the URL\n        domain = urllib.parse.urlparse(url).netloc\n\n        # Try to resolve the domain to an IP address\n        try:\n            ip = socket.gethostbyname(domain)\n        except socket.gaierror:\n            # If the domain cannot be resolved, set the IP address to None\n            ip = None\n\n        # Add the domain-IP pair to the dictionary\n        domain_ip_pairs[domain] = ip\n\n    # Return the dictionary of domain-IP pairs\n    return domain_ip_pairs"}
{"task_id": "BigCodeBench/1123", "solution": "import re\nimport urllib.parse\nimport ssl\nimport socket\nfrom datetime import datetime\ndef task_func(myString):\n    # Extract all URLs from the string\n    urls = re.findall('https://[^\\s]*', myString)\n\n    # Dictionary to store domain and SSL certificate expiry dates\n    ssl_info = {}\n\n    for url in urls:\n        try:\n            # Parse the URL to get the domain\n            domain = urllib.parse.urlparse(url).netloc\n\n            # Get the SSL certificate\n            context = ssl.create_default_context()\n            with socket.create_connection((domain, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=domain) as ssock:\n                    cert = ssock.getpeercert()\n\n            # Extract the expiry date from the certificate\n            expiry_date = datetime.strptime(cert['notAfter'], r'%b %d %H:%M:%S %Y %Z')\n\n            # Convert the expiry date to UTC format\n            expiry_date = expiry_date.astimezone(datetime.utc)\n\n            # Store the domain and expiry date in the dictionary\n            ssl_info[domain] = expiry_date.strftime(r'%Y-%m-%d %H:%M:%S')\n\n        except Exception:\n            # Ignore any URLs that cannot be processed due to SSL errors\n            pass\n\n    return ssl_info", "raw_solution": "import re\nimport urllib.parse\nimport ssl\nimport socket\nfrom datetime import datetime\n\ndef task_func(myString):\n    # Extract all URLs from the string\n    urls = re.findall('https://[^\\s]*', myString)\n\n    # Dictionary to store domain and SSL certificate expiry dates\n    ssl_info = {}\n\n    for url in urls:\n        try:\n            # Parse the URL to get the domain\n            domain = urllib.parse.urlparse(url).netloc\n\n            # Get the SSL certificate\n            context = ssl.create_default_context()\n            with socket.create_connection((domain, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=domain) as ssock:\n                    cert = ssock.getpeercert()\n\n            # Extract the expiry date from the certificate\n            expiry_date = datetime.strptime(cert['notAfter'], r'%b %d %H:%M:%S %Y %Z')\n\n            # Convert the expiry date to UTC format\n            expiry_date = expiry_date.astimezone(datetime.utc)\n\n            # Store the domain and expiry date in the dictionary\n            ssl_info[domain] = expiry_date.strftime(r'%Y-%m-%d %H:%M:%S')\n\n        except Exception:\n            # Ignore any URLs that cannot be processed due to SSL errors\n            pass\n\n    return ssl_info"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Extract URL from string\n    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    if not url:\n        return \"No valid URL found in the provided string.\"\n    url = url[0]\n\n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n    except requests.exceptions.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # Parse the content and extract the title\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.find('title')\n    if not title:\n        return \"No title tag found in the webpage.\"\n    return title.text", "raw_solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Extract URL from string\n    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n    if not url:\n        return \"No valid URL found in the provided string.\"\n    url = url[0]\n\n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n    except requests.exceptions.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n\n    # Parse the content and extract the title\n    soup = BeautifulSoup(response.text, 'html.parser')\n    title = soup.find('title')\n    if not title:\n        return \"No title tag found in the webpage.\"\n    return title.text"}
{"task_id": "BigCodeBench/1125", "solution": "import re\nimport json\nimport requests\ndef task_func(myString, token):\n    \"\"\"\n    Extracts a URL from a string and sends it to a REST API via a POST request.\n    The URL is included in the JSON payload, and an authorization token is used in the headers for API access.\n    If multiple URLs are in myString, then use the first one.\n    \"\"\"\n    # Extract URLs from the string\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n\n    if urls:\n        # Use the first URL\n        url = urls[0]\n\n        # Prepare the JSON payload\n        payload = json.dumps({\"url\": url})\n\n        # Prepare the headers\n        headers = {\n            \"Authorization\": f\"Bearer {token}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        # Send the POST request\n        response = requests.post(url, data=payload, headers=headers)\n\n        # Return the response as a dictionary\n        return response.json()\n    else:\n        return {\"error\": \"No URL found in the string\"}", "raw_solution": "import re\nimport json\nimport requests\n\ndef task_func(myString, token):\n    \"\"\"\n    Extracts a URL from a string and sends it to a REST API via a POST request.\n    The URL is included in the JSON payload, and an authorization token is used in the headers for API access.\n    If multiple URLs are in myString, then use the first one.\n    \"\"\"\n    # Extract URLs from the string\n    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', myString)\n\n    if urls:\n        # Use the first URL\n        url = urls[0]\n\n        # Prepare the JSON payload\n        payload = json.dumps({\"url\": url})\n\n        # Prepare the headers\n        headers = {\n            \"Authorization\": f\"Bearer {token}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        # Send the POST request\n        response = requests.post(url, data=payload, headers=headers)\n\n        # Return the response as a dictionary\n        return response.json()\n    else:\n        return {\"error\": \"No URL found in the string\"}"}
{"task_id": "BigCodeBench/1126", "solution": "import re\nimport hashlib\ndef task_func(input_str):\n    # Remove all special characters, punctuation marks, and spaces from the input string\n    cleaned_str = re.sub(r'[^A-Za-z0-9]', '', input_str)\n\n    # Hash the cleaned string with SHA256\n    hashed_str = hashlib.sha256(cleaned_str.encode()).hexdigest()\n\n    return hashed_str", "raw_solution": "import re\nimport hashlib\n\ndef task_func(input_str):\n    # Remove all special characters, punctuation marks, and spaces from the input string\n    cleaned_str = re.sub(r'[^A-Za-z0-9]', '', input_str)\n\n    # Hash the cleaned string with SHA256\n    hashed_str = hashlib.sha256(cleaned_str.encode()).hexdigest()\n\n    return hashed_str"}
{"task_id": "BigCodeBench/1127", "solution": "import os\nimport re\nimport hashlib\ndef task_func(path, delimiter):\n    \"\"\"\n    Splits a file path by a specified delimiter, retaining the delimiter in the output, and computes the hash of each component if it is a file.\n    \"\"\"\n    # Split the path by the delimiter\n    components = re.split(delimiter, path)\n\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over each component\n    for component in components:\n        # If the component is a file, compute its hash\n        if os.path.isfile(component):\n            with open(component, 'rb') as file:\n                file_hash = hashlib.sha256()\n                chunk = file.read(8192)\n                while chunk:\n                    file_hash.update(chunk)\n                    chunk = file.read(8192)\n                hash_value = file_hash.hexdigest()\n        else:\n            hash_value = None\n\n        # Append the component and its hash to the results list\n        results.append((component, hash_value))\n\n    return results", "raw_solution": "import os\nimport re\nimport hashlib\n\ndef task_func(path, delimiter):\n    \"\"\"\n    Splits a file path by a specified delimiter, retaining the delimiter in the output, and computes the hash of each component if it is a file.\n    \"\"\"\n    # Split the path by the delimiter\n    components = re.split(delimiter, path)\n\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over each component\n    for component in components:\n        # If the component is a file, compute its hash\n        if os.path.isfile(component):\n            with open(component, 'rb') as file:\n                file_hash = hashlib.sha256()\n                chunk = file.read(8192)\n                while chunk:\n                    file_hash.update(chunk)\n                    chunk = file.read(8192)\n                hash_value = file_hash.hexdigest()\n        else:\n            hash_value = None\n\n        # Append the component and its hash to the results list\n        results.append((component, hash_value))\n\n    return results"}
{"task_id": "BigCodeBench/1128", "solution": "import json\nimport os\nimport hashlib\nimport base64\nimport time\nfrom typing import Optional\ndef task_func(file_path: str, unknown_key: str) -> Optional[str]:\n    \"\"\"\n    Reads a JSON file, extracts a value specified by an 'unknown_key' within a nested structure,\n    hashes this value using SHA256, and writes the base64-encoded hash to a new file with a timestamp in its name.\n    \"\"\"\n    # Read JSON file\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    # Extract value from nested structure\n    value = data.get('A', {}).get(unknown_key, {}).get('maindata', [{}])[0].get('Info', '')\n\n    # Hash value\n    hash_obj = hashlib.sha256(value.encode())\n    hash_str = hash_obj.hexdigest()\n\n    # Base64-encode hash\n    base64_hash = base64.b64encode(hash_str.encode()).decode()\n\n    # Create new file with timestamp in its name\n    timestamp = str(int(time.time()))\n    new_file_path = os.path.join(os.path.dirname(file_path), f'{timestamp}.json')\n    with open(new_file_path, 'w') as file:\n        file.write(base64_hash)\n\n    return new_file_path", "raw_solution": "import json\nimport os\nimport hashlib\nimport base64\nimport time\nfrom typing import Optional\n\ndef task_func(file_path: str, unknown_key: str) -> Optional[str]:\n    \"\"\"\n    Reads a JSON file, extracts a value specified by an 'unknown_key' within a nested structure,\n    hashes this value using SHA256, and writes the base64-encoded hash to a new file with a timestamp in its name.\n    \"\"\"\n    # Read JSON file\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    # Extract value from nested structure\n    value = data.get('A', {}).get(unknown_key, {}).get('maindata', [{}])[0].get('Info', '')\n\n    # Hash value\n    hash_obj = hashlib.sha256(value.encode())\n    hash_str = hash_obj.hexdigest()\n\n    # Base64-encode hash\n    base64_hash = base64.b64encode(hash_str.encode()).decode()\n\n    # Create new file with timestamp in its name\n    timestamp = str(int(time.time()))\n    new_file_path = os.path.join(os.path.dirname(file_path), f'{timestamp}.json')\n    with open(new_file_path, 'w') as file:\n        file.write(base64_hash)\n\n    return new_file_path"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with a specified key, downloads the file from the URL, \n    and saves it with a timestamped filename. The filename format is '{unknown_key}_{timestamp}.txt', \n    where 'timestamp' is formatted as '%Y%m%d%H%M%S%f' to include the date and time down to microseconds. \n    The file is saved in the specified directory or in the current working directory by default.\n    \"\"\"\n    data = json.loads(json_data)\n    url = data[unknown_key]\n\n    if save_dir is None:\n        save_dir = os.getcwd()\n\n    response = requests.get(url)\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    filepath = os.path.join(save_dir, filename)\n\n    with open(filepath, 'wb') as f:\n        f.write(response.content)\n\n    return os.path.abspath(filepath)", "raw_solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with a specified key, downloads the file from the URL, \n    and saves it with a timestamped filename. The filename format is '{unknown_key}_{timestamp}.txt', \n    where 'timestamp' is formatted as '%Y%m%d%H%M%S%f' to include the date and time down to microseconds. \n    The file is saved in the specified directory or in the current working directory by default.\n    \"\"\"\n    data = json.loads(json_data)\n    url = data[unknown_key]\n\n    if save_dir is None:\n        save_dir = os.getcwd()\n\n    response = requests.get(url)\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    filepath = os.path.join(save_dir, filename)\n\n    with open(filepath, 'wb') as f:\n        f.write(response.content)\n\n    return os.path.abspath(filepath)"}
{"task_id": "BigCodeBench/1130", "solution": "import os\nimport hashlib\nimport json\nfrom pathlib import Path\ndef task_func(directory: str) -> str:\n    \"\"\"\n    Create SHA256 hashes for all files in the specified directory, including files in subdirectories,\n    and save these hashes in a JSON file named 'hashes.json' in the given directory.\n    \"\"\"\n    hashes = {}\n    for foldername, subfolders, filenames in os.walk(directory):\n        for filename in filenames:\n            file_path = os.path.join(foldername, filename)\n            with open(file_path, 'rb') as f:\n                file_hash = hashlib.sha256(f.read()).hexdigest()\n            hashes[file_path] = file_hash\n\n    json_file_path = os.path.join(directory, 'hashes.json')\n    with open(json_file_path, 'w') as f:\n        json.dump(hashes, f)\n\n    return json_file_path", "raw_solution": "import os\nimport hashlib\nimport json\nfrom pathlib import Path\n\ndef task_func(directory: str) -> str:\n    \"\"\"\n    Create SHA256 hashes for all files in the specified directory, including files in subdirectories,\n    and save these hashes in a JSON file named 'hashes.json' in the given directory.\n    \"\"\"\n    hashes = {}\n    for foldername, subfolders, filenames in os.walk(directory):\n        for filename in filenames:\n            file_path = os.path.join(foldername, filename)\n            with open(file_path, 'rb') as f:\n                file_hash = hashlib.sha256(f.read()).hexdigest()\n            hashes[file_path] = file_hash\n\n    json_file_path = os.path.join(directory, 'hashes.json')\n    with open(json_file_path, 'w') as f:\n        json.dump(hashes, f)\n\n    return json_file_path"}
{"task_id": "BigCodeBench/1131", "solution": "import hashlib\nimport binascii\nimport sqlite3\ndef task_func(salt, cursor):\n    if not isinstance(salt, str):\n        raise TypeError(\"Salt must be a string\")\n\n    # Create a new hashlib object\n    hash_obj = hashlib.sha256()\n\n    # Update the hash object with the salt\n    hash_obj.update(salt.encode('utf-8'))\n\n    # Get the hexadecimal representation of the salted hash\n    salted_hash = hash_obj.hexdigest()\n\n    # Prepare the SQL query\n    query = \"UPDATE users SET password = ? WHERE password = ?\"\n\n    # Execute the query\n    cursor.execute(query, (salted_hash, salt))\n\n    # Get the number of rows affected\n    num_rows_affected = cursor.rowcount\n\n    return num_rows_affected", "raw_solution": "import hashlib\nimport binascii\nimport sqlite3\n\ndef task_func(salt, cursor):\n    if not isinstance(salt, str):\n        raise TypeError(\"Salt must be a string\")\n\n    # Create a new hashlib object\n    hash_obj = hashlib.sha256()\n\n    # Update the hash object with the salt\n    hash_obj.update(salt.encode('utf-8'))\n\n    # Get the hexadecimal representation of the salted hash\n    salted_hash = hash_obj.hexdigest()\n\n    # Prepare the SQL query\n    query = \"UPDATE users SET password = ? WHERE password = ?\"\n\n    # Execute the query\n    cursor.execute(query, (salted_hash, salt))\n\n    # Get the number of rows affected\n    num_rows_affected = cursor.rowcount\n\n    return num_rows_affected"}
{"task_id": "BigCodeBench/1132", "solution": "import os\nimport hashlib\nimport base64\ndef task_func(password, PREFIX=\"ME\", SALT_LENGTH=16):\n    if SALT_LENGTH < 0:\n        raise ValueError(\"SALT_LENGTH cannot be negative\")\n    salt = os.urandom(SALT_LENGTH)\n    salted_password = password + PREFIX + salt.hex()\n    hashed_password = hashlib.sha256(salted_password.encode())\n    encoded_password = base64.b64encode(hashed_password.digest())\n    return encoded_password.decode()", "raw_solution": "import os\nimport hashlib\nimport base64\n\ndef task_func(password, PREFIX=\"ME\", SALT_LENGTH=16):\n    if SALT_LENGTH < 0:\n        raise ValueError(\"SALT_LENGTH cannot be negative\")\n    salt = os.urandom(SALT_LENGTH)\n    salted_password = password + PREFIX + salt.hex()\n    hashed_password = hashlib.sha256(salted_password.encode())\n    encoded_password = base64.b64encode(hashed_password.digest())\n    return encoded_password.decode()\n\n# Testing the function"}
{"task_id": "BigCodeBench/1133", "solution": "import json\nimport requests\ndef task_func(API_URL, endpoint, PREFIX):\n    \"\"\"\n    Retrieves JSON data from a specified API endpoint and writes it to a file.\n    The filename is generated by concatenating a provided prefix with the endpoint name, followed by '.json'.\n    \"\"\"\n    try:\n        # Fetch data from API\n        response = requests.get(f\"{API_URL}/{endpoint}\")\n        response.raise_for_status()\n        data = response.json()\n\n        # Generate filename\n        filename = f\"{PREFIX}_{endpoint}.json\"\n\n        # Write data to file\n        with open(filename, 'w') as file:\n            json.dump(data, file)\n\n        return filename\n\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error fetching data from API: {str(e)}\")\n    except IOError as e:\n        raise RuntimeError(f\"Error writing to file: {str(e)}\")", "raw_solution": "import json\nimport requests\n\ndef task_func(API_URL, endpoint, PREFIX):\n    \"\"\"\n    Retrieves JSON data from a specified API endpoint and writes it to a file.\n    The filename is generated by concatenating a provided prefix with the endpoint name, followed by '.json'.\n    \"\"\"\n    try:\n        # Fetch data from API\n        response = requests.get(f\"{API_URL}/{endpoint}\")\n        response.raise_for_status()\n        data = response.json()\n\n        # Generate filename\n        filename = f\"{PREFIX}_{endpoint}.json\"\n\n        # Write data to file\n        with open(filename, 'w') as file:\n            json.dump(data, file)\n\n        return filename\n\n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Error fetching data from API: {str(e)}\")\n    except IOError as e:\n        raise RuntimeError(f\"Error writing to file: {str(e)}\")"}
{"task_id": "BigCodeBench/1134", "solution": "import os\nimport hashlib\ndef task_func(source_dir, target_dir, prefix='#Hash: '):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory {source_dir} does not exist.\")\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    file_paths = []\n    for filename in os.listdir(source_dir):\n        file_path = os.path.join(source_dir, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'r') as file:\n                content = file.read()\n                md5_hash = hashlib.md5(content.encode()).hexdigest()\n                modified_content = f\"{prefix}{md5_hash}\\n{content}\"\n\n                target_file_path = os.path.join(target_dir, filename)\n                with open(target_file_path, 'w') as target_file:\n                    target_file.write(modified_content)\n\n                file_paths.append(target_file_path)\n\n    return file_paths", "raw_solution": "import os\nimport hashlib\n\ndef task_func(source_dir, target_dir, prefix='#Hash: '):\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory {source_dir} does not exist.\")\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    file_paths = []\n    for filename in os.listdir(source_dir):\n        file_path = os.path.join(source_dir, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'r') as file:\n                content = file.read()\n                md5_hash = hashlib.md5(content.encode()).hexdigest()\n                modified_content = f\"{prefix}{md5_hash}\\n{content}\"\n\n                target_file_path = os.path.join(target_dir, filename)\n                with open(target_file_path, 'w') as target_file:\n                    target_file.write(modified_content)\n\n                file_paths.append(target_file_path)\n\n    return file_paths"}
{"task_id": "BigCodeBench/1135", "solution": "import requests\nimport collections\ndef task_func(user, API_URL = 'https://api.github.com/users/'):\n    \"\"\"\n    Retrieves the names of the repositories of a specified GitHub user, sorted in ascending order by their creation date.\n    \"\"\"\n    response = requests.get(API_URL + user + '/repos')\n    data = response.json()\n\n    if 'message' in data and data['message'] == 'Not Found':\n        return 'User not found'\n\n    repo_data = collections.defaultdict(list)\n    for repo in data:\n        repo_data[repo['name']] = repo['created_at']\n\n    sorted_repos = sorted(repo_data.items(), key=lambda x: x[1])\n\n    return [repo[0] for repo in sorted_repos]", "raw_solution": "import requests\nimport collections\n\ndef task_func(user, API_URL = 'https://api.github.com/users/'):\n    \"\"\"\n    Retrieves the names of the repositories of a specified GitHub user, sorted in ascending order by their creation date.\n    \"\"\"\n    response = requests.get(API_URL + user + '/repos')\n    data = response.json()\n\n    if 'message' in data and data['message'] == 'Not Found':\n        return 'User not found'\n\n    repo_data = collections.defaultdict(list)\n    for repo in data:\n        repo_data[repo['name']] = repo['created_at']\n\n    sorted_repos = sorted(repo_data.items(), key=lambda x: x[1])\n\n    return [repo[0] for repo in sorted_repos]\n\n# Test the function"}
{"task_id": "BigCodeBench/1136", "solution": "import bs4\nimport requests\nimport re\nimport csv\ndef task_func(url=\"http://example.com\", csv_path=\"emails.csv\", \n          regex=r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\", \n          headers={'User-Agent': 'Mozilla/5.0'}):\n    \"\"\"\n    Scrapes a web page to extract all email addresses using a specified regular expression pattern and writes them to a CSV file.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n\n    # Parse the HTML content of the page with BeautifulSoup\n    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n\n    # Find all text in the HTML content that matches the regular expression\n    matches = re.findall(regex, str(soup))\n\n    # Write the matches to a CSV file\n    with open(csv_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow([\"Emails\"])  # Write the header\n        for match in matches:\n            writer.writerow([match])  # Write each match as a new row\n\n    return csv_path", "raw_solution": "import bs4\nimport requests\nimport re\nimport csv\n\ndef task_func(url=\"http://example.com\", csv_path=\"emails.csv\", \n          regex=r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\", \n          headers={'User-Agent': 'Mozilla/5.0'}):\n    \"\"\"\n    Scrapes a web page to extract all email addresses using a specified regular expression pattern and writes them to a CSV file.\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n\n    # Parse the HTML content of the page with BeautifulSoup\n    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n\n    # Find all text in the HTML content that matches the regular expression\n    matches = re.findall(regex, str(soup))\n\n    # Write the matches to a CSV file\n    with open(csv_path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow([\"Emails\"])  # Write the header\n        for match in matches:\n            writer.writerow([match])  # Write each match as a new row\n\n    return csv_path"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n    \"\"\"\n    # Define the phone number regex pattern\n    phone_pattern = re.compile(r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')\n\n    # If the URL starts with 'http', fetch the content, otherwise assume it's a local file\n    if url.startswith('http'):\n        response = requests.get(url)\n        content = response.text\n    else:\n        with open(url, 'r') as file:\n            content = file.read()\n\n    # Use BeautifulSoup to parse the content and find all matches of the phone number pattern\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    phone_numbers = [match.group() for match in phone_pattern.finditer(soup.text)]\n\n    # Save the phone numbers to the output file in JSON format\n    with open(output_path, 'w') as file:\n        json.dump(phone_numbers, file)\n\n    return phone_numbers", "raw_solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n    \"\"\"\n    # Define the phone number regex pattern\n    phone_pattern = re.compile(r'\\(?\\b\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')\n\n    # If the URL starts with 'http', fetch the content, otherwise assume it's a local file\n    if url.startswith('http'):\n        response = requests.get(url)\n        content = response.text\n    else:\n        with open(url, 'r') as file:\n            content = file.read()\n\n    # Use BeautifulSoup to parse the content and find all matches of the phone number pattern\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    phone_numbers = [match.group() for match in phone_pattern.finditer(soup.text)]\n\n    # Save the phone numbers to the output file in JSON format\n    with open(output_path, 'w') as file:\n        json.dump(phone_numbers, file)\n\n    return phone_numbers"}
{"task_id": "BigCodeBench/1138", "solution": "import numpy as np\nimport itertools\ndef task_func(matrix):\n    # Flatten the 2D array and sort it\n    flat_array = np.sort(matrix.flatten())\n\n    # Generate all unique combinations of two elements\n    combinations = list(itertools.combinations(flat_array, 2))\n\n    return flat_array, combinations", "raw_solution": "import numpy as np\nimport itertools\n\ndef task_func(matrix):\n    # Flatten the 2D array and sort it\n    flat_array = np.sort(matrix.flatten())\n\n    # Generate all unique combinations of two elements\n    combinations = list(itertools.combinations(flat_array, 2))\n\n    return flat_array, combinations"}
{"task_id": "BigCodeBench/1139", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\ndef task_func(data):\n    # Split the data into features (X) and target (y)\n    X = data[['Hours']]\n    y = data['Scores']\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the mean squared error of the predictions\n    mse = mean_squared_error(y_test, y_pred)\n\n    return mse", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\ndef task_func(data):\n    # Split the data into features (X) and target (y)\n    X = data[['Hours']]\n    y = data['Scores']\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = model.predict(X_test)\n\n    # Calculate the mean squared error of the predictions\n    mse = mean_squared_error(y_test, y_pred)\n\n    return mse"}
